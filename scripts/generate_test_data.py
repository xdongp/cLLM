#!/usr/bin/env python3
"""
cLLM æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨

åŠŸèƒ½ï¼š
- ç”Ÿæˆ Tokenizer æµ‹è¯•æ•°æ®
- ç”Ÿæˆæ¨ç†æµ‹è¯•æ•°æ®
- ç”Ÿæˆæ€§èƒ½æµ‹è¯•æ•°æ®
- ç”Ÿæˆå‹åŠ›æµ‹è¯•æ•°æ®

ä½¿ç”¨æ–¹æ³•ï¼š
    python3 scripts/generate_test_data.py
"""

import json
import random
import string
import os
from pathlib import Path


class TestDataGenerator:
    """æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨"""
    
    def __init__(self, output_dir="tests/data"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.data = {
            "tokenizer": {},
            "inference": {},
            "performance": {},
            "stress": {},
            "scenarios": {}
        }
    
    def generate_tokenizer_data(self):
        """ç”Ÿæˆ Tokenizer æµ‹è¯•æ•°æ®"""
        print("ğŸ“ ç”Ÿæˆ Tokenizer æµ‹è¯•æ•°æ®...")
        
        # çŸ­æ–‡æœ¬ï¼ˆå¤šè¯­è¨€ï¼‰
        short_texts = [
            "Hello, world!",
            "ä½ å¥½ï¼Œä¸–ç•Œï¼",
            "ã“ã‚“ã«ã¡ã¯ã€ä¸–ç•Œï¼",
            "Bonjour le monde!",
            "Â¡Hola mundo!",
            "Test input",
            "Simple test",
            "Quick brown fox",
        ]
        
        # é•¿æ–‡æœ¬
        long_texts = [
            " ".join(["This is a long text for testing tokenizer performance."] * 50),
            " ".join(["è¿™æ˜¯ä¸€ä¸ªç”¨äºæµ‹è¯•åˆ†è¯å™¨æ€§èƒ½çš„é•¿æ–‡æœ¬ã€‚"] * 50),
            " ".join(["Mixed English and ä¸­æ–‡ text for testing."] * 30),
        ]
        
        # ç‰¹æ®Šæƒ…å†µ
        special_cases = [
            "ğŸ˜€ğŸ‰ğŸš€ğŸ’»",  # Emoji
            "Text with\nnewlines\nand\ttabs",  # æ¢è¡Œå’Œåˆ¶è¡¨ç¬¦
            "Mixedä¸­è‹±æ–‡æ—¥æœ¬èªtext",  # å¤šè¯­è¨€æ··åˆ
            "Special chars: !@#$%^&*()",  # ç‰¹æ®Šå­—ç¬¦
            "Numbers: 0123456789",  # æ•°å­—
            "URL: https://example.com/path?param=value",  # URL
            "Email: test@example.com",  # Email
            "Code: def hello(): print('Hello')",  # ä»£ç 
        ]
        
        # è¾¹ç•Œæƒ…å†µ
        boundary_cases = [
            "",  # ç©ºå­—ç¬¦ä¸²
            " ",  # å•ä¸ªç©ºæ ¼
            "   ",  # å¤šä¸ªç©ºæ ¼
            "\n",  # å•ä¸ªæ¢è¡Œ
            "\t",  # å•ä¸ªåˆ¶è¡¨ç¬¦
            "A",  # å•ä¸ªå­—ç¬¦
            "å¾ˆ",  # å•ä¸ªä¸­æ–‡å­—ç¬¦
            "a" * 1000,  # å¾ˆé•¿çš„å•ä¸ªå•è¯
        ]
        
        self.data["tokenizer"] = {
            "short_texts": short_texts,
            "long_texts": long_texts,
            "special_cases": special_cases,
            "boundary_cases": boundary_cases
        }
        
        print(f"  âœ… ç”Ÿæˆ {len(short_texts)} ä¸ªçŸ­æ–‡æœ¬")
        print(f"  âœ… ç”Ÿæˆ {len(long_texts)} ä¸ªé•¿æ–‡æœ¬")
        print(f"  âœ… ç”Ÿæˆ {len(special_cases)} ä¸ªç‰¹æ®Šæƒ…å†µ")
        print(f"  âœ… ç”Ÿæˆ {len(boundary_cases)} ä¸ªè¾¹ç•Œæƒ…å†µ")
    
    def generate_inference_data(self):
        """ç”Ÿæˆæ¨ç†æµ‹è¯•æ•°æ®"""
        print("ğŸ§  ç”Ÿæˆæ¨ç†æµ‹è¯•æ•°æ®...")
        
        prompts = [
            {
                "id": "qa_factual_1",
                "text": "What is the capital of France?",
                "max_length": 50,
                "temperature": 0.3,
                "expected_keywords": ["Paris"],
                "category": "qa_factual"
            },
            {
                "id": "qa_reasoning_1",
                "text": "If it takes 5 machines 5 minutes to make 5 widgets, how long would it take 100 machines to make 100 widgets?",
                "max_length": 100,
                "temperature": 0.3,
                "expected_keywords": ["5 minutes"],
                "category": "qa_reasoning"
            },
            {
                "id": "chat_casual_1",
                "text": "Hello! How are you today?",
                "max_length": 100,
                "temperature": 0.7,
                "expected_keywords": ["Hello", "good", "fine"],
                "category": "chat_casual"
            },
            {
                "id": "code_generation_1",
                "text": "Write a Python function to calculate the factorial of a number",
                "max_length": 200,
                "temperature": 0.3,
                "expected_keywords": ["def", "factorial", "return"],
                "category": "code_generation"
            },
            {
                "id": "summarization_1",
                "text": "Summarize the following text in one sentence: Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to natural intelligence displayed by animals including humans.",
                "max_length": 50,
                "temperature": 0.5,
                "expected_keywords": ["AI", "machines", "intelligence"],
                "category": "summarization"
            },
            {
                "id": "translation_1",
                "text": "Translate to Chinese: Hello, how are you?",
                "max_length": 50,
                "temperature": 0.3,
                "expected_keywords": ["ä½ å¥½"],
                "category": "translation"
            },
            {
                "id": "chinese_1",
                "text": "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²",
                "max_length": 200,
                "temperature": 0.7,
                "expected_keywords": ["äººå·¥æ™ºèƒ½", "å‘å±•", "å†å²"],
                "category": "chinese_qa"
            },
        ]
        
        self.data["inference"]["prompts"] = prompts
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        categories = {}
        for p in prompts:
            cat = p["category"]
            categories[cat] = categories.get(cat, 0) + 1
        
        print(f"  âœ… ç”Ÿæˆ {len(prompts)} ä¸ªæ¨ç†æµ‹è¯•ç”¨ä¾‹")
        for cat, count in categories.items():
            print(f"    - {cat}: {count} ä¸ª")
    
    def generate_performance_data(self):
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æ•°æ®"""
        print("âš¡ ç”Ÿæˆæ€§èƒ½æµ‹è¯•æ•°æ®...")
        
        self.data["performance"] = {
            "batch_sizes": [1, 2, 4, 8, 16, 32],
            "sequence_lengths": [10, 50, 100, 200, 500, 1000, 2000],
            "concurrency_levels": [1, 5, 10, 20, 50, 100],
            "test_durations_seconds": [10, 30, 60, 300],
        }
        
        print("  âœ… ç”Ÿæˆæ€§èƒ½æµ‹è¯•é…ç½®")
        print(f"    - Batch sizes: {self.data['performance']['batch_sizes']}")
        print(f"    - Sequence lengths: {self.data['performance']['sequence_lengths']}")
        print(f"    - Concurrency levels: {self.data['performance']['concurrency_levels']}")
    
    def generate_stress_data(self):
        """ç”Ÿæˆå‹åŠ›æµ‹è¯•æ•°æ®"""
        print("ğŸ’ª ç”Ÿæˆå‹åŠ›æµ‹è¯•æ•°æ®...")
        
        self.data["stress"] = {
            "duration_minutes": [5, 15, 30, 60, 120],
            "request_rates": [10, 50, 100, 200, 500, 1000],
            "payload_sizes": [100, 500, 1000, 5000, 10000],
            "patterns": [
                {
                    "name": "constant_load",
                    "description": "æ’å®šè´Ÿè½½",
                    "rate": 100,
                    "duration": 300
                },
                {
                    "name": "spike_load",
                    "description": "å°–å³°è´Ÿè½½",
                    "base_rate": 50,
                    "spike_rate": 500,
                    "spike_duration": 60
                },
                {
                    "name": "ramp_up",
                    "description": "é€æ­¥å¢åŠ è´Ÿè½½",
                    "start_rate": 10,
                    "end_rate": 200,
                    "duration": 300
                }
            ]
        }
        
        print("  âœ… ç”Ÿæˆå‹åŠ›æµ‹è¯•é…ç½®")
        print(f"    - æµ‹è¯•æ¨¡å¼: {len(self.data['stress']['patterns'])} ç§")
    
    def generate_scenario_data(self):
        """ç”Ÿæˆåœºæ™¯æµ‹è¯•æ•°æ®"""
        print("ğŸ¬ ç”Ÿæˆåœºæ™¯æµ‹è¯•æ•°æ®...")
        
        scenarios = {
            "single_turn_qa": {
                "name": "å•è½®é—®ç­”",
                "conversations": [
                    {
                        "user": "What is machine learning?",
                        "expected_keywords": ["machine learning", "algorithm", "data"]
                    },
                    {
                        "user": "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
                        "expected_keywords": ["æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ"]
                    }
                ]
            },
            "multi_turn_chat": {
                "name": "å¤šè½®å¯¹è¯",
                "conversations": [
                    {
                        "turns": [
                            {"role": "user", "content": "Hello!"},
                            {"role": "assistant", "content": "Hello! How can I help you?"},
                            {"role": "user", "content": "Can you write code?"},
                            {"role": "assistant", "content": "Yes, I can help with programming!"},
                            {"role": "user", "content": "Write a hello world in Python"}
                        ]
                    }
                ]
            },
            "code_assistance": {
                "name": "ä»£ç è¾…åŠ©",
                "tasks": [
                    {
                        "description": "Write a sorting algorithm",
                        "language": "Python",
                        "expected_keywords": ["def", "sort", "return"]
                    },
                    {
                        "description": "Debug this code: for i in range(10 print(i)",
                        "language": "Python",
                        "expected_keywords": ["syntax", "error", "parentheses"]
                    }
                ]
            }
        }
        
        self.data["scenarios"] = scenarios
        
        print(f"  âœ… ç”Ÿæˆ {len(scenarios)} ä¸ªæµ‹è¯•åœºæ™¯")
        for name, scenario in scenarios.items():
            print(f"    - {scenario['name']}")
    
    def save(self):
        """ä¿å­˜æµ‹è¯•æ•°æ®"""
        print("ğŸ’¾ ä¿å­˜æµ‹è¯•æ•°æ®...")
        
        # ä¿å­˜ä¸»æ–‡ä»¶
        main_file = self.output_dir / "test_cases.json"
        with open(main_file, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)
        print(f"  âœ… ä¸»æ–‡ä»¶: {main_file}")
        
        # ä¿å­˜åˆ†ç±»æ–‡ä»¶
        for category, data in self.data.items():
            category_file = self.output_dir / f"{category}_test_data.json"
            with open(category_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"  âœ… {category} æ•°æ®: {category_file}")
    
    def generate_all(self):
        """ç”Ÿæˆæ‰€æœ‰æµ‹è¯•æ•°æ®"""
        print("=" * 60)
        print("ğŸš€ å¼€å§‹ç”Ÿæˆæµ‹è¯•æ•°æ®")
        print("=" * 60)
        
        self.generate_tokenizer_data()
        self.generate_inference_data()
        self.generate_performance_data()
        self.generate_stress_data()
        self.generate_scenario_data()
        
        self.save()
        
        print("=" * 60)
        print("âœ… æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆï¼")
        print("=" * 60)
    
    def print_summary(self):
        """æ‰“å°æ‘˜è¦"""
        print("\nğŸ“Š æµ‹è¯•æ•°æ®æ‘˜è¦:")
        print(f"  - Tokenizer æµ‹è¯•ç”¨ä¾‹: {sum(len(v) for v in self.data['tokenizer'].values())} ä¸ª")
        print(f"  - æ¨ç†æµ‹è¯•ç”¨ä¾‹: {len(self.data['inference'].get('prompts', []))} ä¸ª")
        print(f"  - æ€§èƒ½æµ‹è¯•é…ç½®: å·²ç”Ÿæˆ")
        print(f"  - å‹åŠ›æµ‹è¯•é…ç½®: {len(self.data['stress'].get('patterns', []))} ç§æ¨¡å¼")
        print(f"  - åœºæ™¯æµ‹è¯•: {len(self.data['scenarios'])} ä¸ªåœºæ™¯")
        print(f"\nğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")


def main():
    """ä¸»å‡½æ•°"""
    generator = TestDataGenerator()
    generator.generate_all()
    generator.print_summary()


if __name__ == "__main__":
    main()
