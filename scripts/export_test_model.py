#!/usr/bin/env python3
"""
åˆ›å»ºä¸€ä¸ªç®€å•çš„PyTorchæ¨¡å‹ç”¨äºLibTorchåç«¯æµ‹è¯•
"""

import torch
import torch.nn as nn
import os

class SimpleTransformer(nn.Module):
    def __init__(self, vocab_size=32000, hidden_size=128, num_layers=2, num_heads=4):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        self.pos_encoding = nn.Embedding(512, hidden_size)  # ä½ç½®ç¼–ç 
        
        # ç®€å•çš„Transformerå±‚
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_size,
            nhead=num_heads,
            dim_feedforward=hidden_size * 4,
            dropout=0.0,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        self.lm_head = nn.Linear(hidden_size, vocab_size)
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size

    def forward(self, input_ids):
        # input_ids shape: [batch_size, seq_len]
        seq_len = input_ids.shape[1]
        
        # è¯åµŒå…¥
        embeddings = self.embedding(input_ids)
        
        # ä½ç½®ç¼–ç 
        positions = torch.arange(0, seq_len, dtype=torch.long, device=input_ids.device)
        pos_encodings = self.pos_encoding(positions)
        pos_encodings = pos_encodings.unsqueeze(0).expand(input_ids.shape[0], -1, -1)
        
        # ç»„åˆåµŒå…¥
        x = embeddings + pos_encodings
        
        # Transformerå¤„ç†
        x = self.transformer(x)
        
        # è¾“å‡ºæŠ•å½±åˆ°è¯æ±‡è¡¨
        logits = self.lm_head(x)
        
        return logits

def main():
    print("Creating simple transformer model for LibTorch testing...")
    
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = SimpleTransformer(
        vocab_size=32000,  # ä¸Qwenæ¨¡å‹ç±»ä¼¼çš„è¯æ±‡è¡¨å¤§å°
        hidden_size=128,   # å°å°ºå¯¸ä¾¿äºæµ‹è¯•
        num_layers=2,      # å°‘å±‚æ•°åŠ å¿«æµ‹è¯•
        num_heads=4        # å°‘æ³¨æ„åŠ›å¤´
    )
    
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    
    print(f"Model created with:")
    print(f"  - Vocab size: {model.vocab_size}")
    print(f"  - Hidden size: {model.hidden_size}")
    print(f"  - Num layers: {model.num_layers}")
    print(f"  - Num heads: {model.num_heads}")
    
    # åˆ›å»ºç¤ºä¾‹è¾“å…¥
    example_input = torch.randint(0, model.vocab_size, (1, 10))  # [batch_size=1, seq_len=10]
    print(f"Example input shape: {example_input.shape}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    with torch.no_grad():
        example_output = model(example_input)
        print(f"Example output shape: {example_output.shape}")
    
    # å¯¼å‡ºä¸ºTorchScriptæ¨¡å‹
    print("\nExporting model to TorchScript...")
    traced_model = torch.jit.trace(model, example_input)
    
    # ä¿å­˜æ¨¡å‹
    output_path = "tests/test_model_libtorch.pt"
    traced_model.save(output_path)
    print(f"âœ“ Model exported to {output_path}")
    
    # éªŒè¯æ¨¡å‹å¤§å°
    size_mb = os.path.getsize(output_path) / (1024 * 1024)
    print(f"Model size: {size_mb:.2f} MB")
    
    # éªŒè¯åŠ è½½
    print("\nVerifying model loading...")
    try:
        loaded_model = torch.jit.load(output_path)
        loaded_model.eval()
        
        with torch.no_grad():
            test_output = loaded_model(example_input)
        
        print("âœ“ Model loaded and tested successfully")
        print(f"Verification output shape: {test_output.shape}")
        
        # æ£€æŸ¥è¾“å‡ºæ˜¯å¦åˆç†
        if test_output.shape == example_output.shape:
            print("âœ“ Output shape matches expected")
        else:
            print("âœ— Output shape mismatch")
            
        print("\nâœ“ All tests passed! Ready for C++ integration.")
        
    except Exception as e:
        print(f"âœ— Verification failed: {e}")
        return False
    
    return True

if __name__ == "__main__":
    success = main()
    if success:
        print("\nğŸ‰ Test model creation completed successfully!")
    else:
        print("\nâŒ Test model creation failed!")
        exit(1)