/**
 * @file test_tokenizer_executor_integration.cpp
 * @brief Tokenizer â†” ModelExecutor è”è°ƒé›†æˆæµ‹è¯•
 * @author cLLM Team
 * @date 2026-01-10
 * 
 * æµ‹è¯•ç›®æ ‡:
 * 1. éªŒè¯ Tokenizer encode() è¾“å‡ºèƒ½æ­£ç¡®è¾“å…¥åˆ° ModelExecutor
 * 2. éªŒè¯ ModelExecutor generate() è¾“å‡ºèƒ½æ­£ç¡®è¾“å…¥åˆ° Tokenizer decode()
 * 3. éªŒè¯ç«¯åˆ°ç«¯æ–‡æœ¬ç”Ÿæˆæµç¨‹
 * 4. éªŒè¯æ‰¹å¤„ç†åœºæ™¯
 * 5. éªŒè¯é”™è¯¯å¤„ç†å’Œè¾¹ç•Œæƒ…å†µ
 */

#include <gtest/gtest.h>
#include <cllm/CTokenizer/tokenizer.h>
#include <cllm/CTokenizer/sentencepiece_tokenizer.h>
#include <cllm/model/executor.h>
#include <cllm/batch/input.h>
#include <cllm/batch/output.h>
#include <cllm/common/logger.h>
#include <cllm/common/config.h>
#include <memory>
#include <string>
#include <vector>
#include <chrono>
#include <fstream>

using namespace cllm;

/**
 * @brief Tokenizer-ModelExecutor é›†æˆæµ‹è¯•å¥—ä»¶
 */
class TokenizerExecutorIntegrationTest : public ::testing::Test {
protected:
    void SetUp() override {
        CLLM_INFO("=== Setting up TokenizerExecutorIntegrationTest ===");
        
        // 1. åŠ è½½æµ‹è¯•é…ç½®
        try {
            Config::instance().load("/Users/dannypan/PycharmProjects/xllm/cpp/cLLM/config/test_config.yaml");
            CLLM_INFO("Test configuration loaded");
        } catch (const std::exception& e) {
            CLLM_WARN("Failed to load test config: {}", e.what());
            CLLM_WARN("Will use default configuration");
        }
        
        // é…ç½®æµ‹è¯•æ¨¡å‹è·¯å¾„ï¼ˆä½¿ç”¨ç°æœ‰çš„ Qwen TorchScript æ¨¡å‹ï¼‰
        tokenizerModelPath_ = "/Users/dannypan/PycharmProjects/xllm/cpp/cLLM/tests/test_tokenizer.model";
        executorModelPath_ = "/Users/dannypan/PycharmProjects/xllm/cpp/cLLM/model/Qwen/qwen3_0.6b_torchscript_fp32.pt";
        
        // åˆå§‹åŒ– Tokenizerï¼ˆä½¿ç”¨ SentencePieceTokenizerï¼‰
        tokenizer_ = std::make_unique<SentencePieceTokenizer>(ModelType::LLAMA);
        
        // åŠ è½½ tokenizer æ¨¡å‹
        if (!tokenizer_->load(tokenizerModelPath_)) {
            CLLM_WARN("Failed to load tokenizer model, some tests may be skipped");
            tokenizerLoaded_ = false;
        } else {
            tokenizerLoaded_ = true;
            CLLM_INFO("Tokenizer loaded successfully");
            CLLM_INFO("  Vocab size: {}", tokenizer_->getVocabSize());
            CLLM_INFO("  BOS ID: {}", tokenizer_->getBosId());
            CLLM_INFO("  EOS ID: {}", tokenizer_->getEosId());
        }
        
        // åˆå§‹åŒ– ModelExecutorï¼ˆä½¿ç”¨ LibTorch åç«¯ï¼‰
        try {
            CLLM_INFO("Creating ModelExecutor with LibTorch backend...");
            CLLM_INFO("  Model path: {}", executorModelPath_);
            
            // ä½¿ç”¨ LibTorch åç«¯åŠ è½½ TorchScript æ¨¡å‹
            executor_ = std::make_unique<ModelExecutor>(
                executorModelPath_,  // TorchScript æ¨¡å‹è·¯å¾„
                "",                  // ä¸ä½¿ç”¨é‡åŒ–
                true,                // å¯ç”¨ SIMD
                true                 // ä½¿ç”¨ LibTorch åç«¯
            );
            
            CLLM_INFO("ModelExecutor created, loading model...");
            executor_->loadModel();
            executorLoaded_ = true;
            CLLM_INFO("âœ“ ModelExecutor loaded successfully (LibTorch backend)");
            
        } catch (const std::exception& e) {
            CLLM_ERROR("âœ— Failed to load ModelExecutor (LibTorch): {}", e.what());
            CLLM_WARN("LibTorch backend initialization failed, tests will be skipped");
            CLLM_WARN("Please check if the TorchScript model exists: {}", executorModelPath_);
            executorLoaded_ = false;
        }
        
        CLLM_INFO("=== Setup complete ===");
    }
    
    void TearDown() override {
        CLLM_INFO("=== Tearing down TokenizerExecutorIntegrationTest ===");
        tokenizer_.reset();
        executor_.reset();
    }
    
    /**
     * @brief æ£€æŸ¥æµ‹è¯•æ˜¯å¦å¯ä»¥è¿è¡Œ
     */
    bool canRunTests() const {
        return tokenizerLoaded_ && executorLoaded_;
    }
    
    std::string tokenizerModelPath_;
    std::string executorModelPath_;
    std::unique_ptr<CTokenizer> tokenizer_;
    std::unique_ptr<ModelExecutor> executor_;
    bool tokenizerLoaded_ = false;
    bool executorLoaded_ = false;
};

/**
 * @test æµ‹è¯• 1: åŸºæœ¬æ¥å£å…¼å®¹æ€§
 * éªŒè¯ Tokenizer è¾“å‡ºçš„ token IDs èƒ½å¤Ÿè¢« ModelExecutor æ¥å—
 */
TEST_F(TokenizerExecutorIntegrationTest, BasicInterfaceCompatibility) {
    if (!canRunTests()) {
        GTEST_SKIP() << "Required models not loaded, skipping test";
    }
    
    CLLM_INFO("\n=== Test 1: Basic Interface Compatibility ===");
    
    // Step 1: Tokenizer encode
    std::string inputText = "Hello, world!";
    CLLM_INFO("Input text: \"%s\"", inputText.c_str());
    
    std::vector<llama_token> tokenIds = tokenizer_->encode(inputText, true);
    CLLM_INFO("Tokenized IDs count: %zu", tokenIds.size());
    EXPECT_GT(tokenIds.size(), 0) << "Tokenizer should produce at least one token";
    
    // æ‰“å° token IDs
    std::string idsStr;
    for (size_t i = 0; i < std::min(tokenIds.size(), size_t(10)); ++i) {
        idsStr += std::to_string(tokenIds[i]) + " ";
    }
    CLLM_INFO("Token IDs (first 10): [%s]", idsStr.c_str());
    
    // Step 2: è½¬æ¢ä¸º std::vector<int> ä¾› ModelExecutor ä½¿ç”¨
    std::vector<int> executorInput(tokenIds.begin(), tokenIds.end());
    EXPECT_EQ(executorInput.size(), tokenIds.size()) << "Type conversion should preserve size";
    
    // Step 3: ModelExecutor forward
    try {
        BatchInput batchInput;
        batchInput.inputIds = executorInput;
        batchInput.batchSize = 1;
        batchInput.requestPositions = {{0, executorInput.size()}};
        batchInput.sequenceIds = {0};
        
        CLLM_INFO("Calling ModelExecutor::forward()...");
        BatchOutput output = executor_->forward(batchInput);
        
        EXPECT_GT(output.logits.size(), 0) << "ModelExecutor should produce logits";
        CLLM_INFO("Output logits size: %zu", output.logits.size());
        
        // éªŒè¯è¾“å‡ºç»´åº¦
        size_t expectedSize = executorInput.size() * executor_->getConfig().vocabSize;
        EXPECT_EQ(output.logits.size(), expectedSize) 
            << "Output size should be [num_tokens * vocab_size]";
        
        CLLM_INFO("âœ“ Interface compatibility verified");
    } catch (const std::exception& e) {
        FAIL() << "ModelExecutor::forward() failed: " << e.what();
    }
}

/**
 * @test æµ‹è¯• 2: ç«¯åˆ°ç«¯æ–‡æœ¬ç”Ÿæˆæµç¨‹
 * éªŒè¯å®Œæ•´çš„ encode â†’ generate â†’ decode æµç¨‹
 */
TEST_F(TokenizerExecutorIntegrationTest, EndToEndTextGeneration) {
    if (!canRunTests()) {
        GTEST_SKIP() << "Required models not loaded, skipping test";
    }
    
    CLLM_INFO("\n=== Test 2: End-to-End Text Generation ===");
    
    // Step 1: Encode input text
    std::string inputText = "Once upon a time";
    CLLM_INFO("Input: \"%s\"", inputText.c_str());
    
    std::vector<llama_token> inputIds = tokenizer_->encode(inputText, true);
    CLLM_INFO("Encoded to %zu tokens", inputIds.size());
    
    // Step 2: Generate new tokens
    std::vector<int> executorInput(inputIds.begin(), inputIds.end());
    
    try {
        CLLM_INFO("Generating 5 new tokens...");
        auto startTime = std::chrono::high_resolution_clock::now();
        
        std::vector<int> generatedIds = executor_->generate(
            executorInput,
            5,      // maxNewTokens
            0.7f    // temperature
        );
        
        auto endTime = std::chrono::high_resolution_clock::now();
        float elapsedMs = std::chrono::duration<float, std::milli>(endTime - startTime).count();
        
        CLLM_INFO("Generated %zu tokens in %.2f ms", generatedIds.size(), elapsedMs);
        EXPECT_GT(generatedIds.size(), 0) << "Should generate at least one token";
        EXPECT_LE(generatedIds.size(), 5) << "Should not exceed maxNewTokens";
        
        // Step 3: Decode generated tokens
        std::vector<llama_token> outputTokens(generatedIds.begin(), generatedIds.end());
        std::string outputText = tokenizer_->decode(outputTokens, true);
        
        CLLM_INFO("Generated text: \"%s\"", outputText.c_str());
        EXPECT_FALSE(outputText.empty()) << "Decoded text should not be empty";
        
        // Step 4: éªŒè¯å®Œæ•´æµç¨‹
        std::vector<llama_token> fullSequence = inputIds;
        fullSequence.insert(fullSequence.end(), outputTokens.begin(), outputTokens.end());
        std::string fullText = tokenizer_->decode(fullSequence, true);
        
        CLLM_INFO("Full generated text: \"%s\"", fullText.c_str());
        EXPECT_TRUE(fullText.find(inputText) != std::string::npos || inputText.find(fullText) != std::string::npos)
            << "Full text should contain or be contained in input text (tokenizer roundtrip)";
        
        CLLM_INFO("âœ“ End-to-end generation successful");
    } catch (const std::exception& e) {
        FAIL() << "Generation failed: " << e.what();
    }
}

/**
 * @test æµ‹è¯• 3: æ‰¹å¤„ç†åœºæ™¯
 * éªŒè¯å¤šä¸ªè¯·æ±‚çš„æ‰¹å¤„ç†
 */
TEST_F(TokenizerExecutorIntegrationTest, BatchProcessing) {
    if (!canRunTests()) {
        GTEST_SKIP() << "Required models not loaded, skipping test";
    }
    
    CLLM_INFO("\n=== Test 3: Batch Processing ===");
    
    // å‡†å¤‡å¤šä¸ªè¾“å…¥æ–‡æœ¬
    std::vector<std::string> inputTexts = {
        "Hello world",
        "How are you",
        "Nice to meet you"
    };
    
    std::vector<std::vector<llama_token>> batchTokenIds;
    std::vector<int> flattenedIds;
    std::vector<std::pair<size_t, size_t>> requestPositions;
    
    size_t currentPos = 0;
    
    // Step 1: Batch encode
    CLLM_INFO("Encoding %zu texts...", inputTexts.size());
    for (size_t i = 0; i < inputTexts.size(); ++i) {
        std::vector<llama_token> ids = tokenizer_->encode(inputTexts[i], true);
        batchTokenIds.push_back(ids);
        
        size_t startPos = currentPos;
        for (auto id : ids) {
            flattenedIds.push_back(static_cast<int>(id));
        }
        currentPos += ids.size();
        requestPositions.push_back({startPos, ids.size()});
        
        CLLM_INFO("  Text %zu: \"%s\" â†’ %zu tokens", i, inputTexts[i].c_str(), ids.size());
    }
    
    // Step 2: Batch forward
    try {
        BatchInput batchInput;
        batchInput.inputIds = flattenedIds;
        batchInput.batchSize = inputTexts.size();
        batchInput.requestPositions = requestPositions;
        batchInput.sequenceIds = {0, 1, 2};
        
        CLLM_INFO("Calling batch forward with %zu requests...", batchInput.batchSize);
        BatchOutput output = executor_->forward(batchInput);
        
        EXPECT_GT(output.logits.size(), 0) << "Batch output should have logits";
        CLLM_INFO("Batch output logits size: %zu", output.logits.size());
        
        // Step 3: Extract logits for each request
        for (size_t i = 0; i < inputTexts.size(); ++i) {
            FloatArray requestLogits = output.getLogitsForRequest(i);
            EXPECT_GT(requestLogits.size(), 0) << "Request " << i << " should have logits";
            CLLM_INFO("  Request %zu logits size: %zu", i, requestLogits.size());
        }
        
        CLLM_INFO("âœ“ Batch processing successful");
    } catch (const std::exception& e) {
        FAIL() << "Batch processing failed: " << e.what();
    }
}

/**
 * @test æµ‹è¯• 4: ç‰¹æ®Š Token å¤„ç†
 * éªŒè¯ BOS/EOS/PAD ç­‰ç‰¹æ®Š token çš„å¤„ç†
 */
TEST_F(TokenizerExecutorIntegrationTest, SpecialTokenHandling) {
    if (!canRunTests()) {
        GTEST_SKIP() << "Required models not loaded, skipping test";
    }
    
    CLLM_INFO("\n=== Test 4: Special Token Handling ===");
    
    // è·å–ç‰¹æ®Š token IDs
    llama_token bosId = tokenizer_->getBosId();
    llama_token eosId = tokenizer_->getEosId();
    llama_token padId = tokenizer_->getPadId();
    
    CLLM_INFO("Special tokens: BOS=%d, EOS=%d, PAD=%d", bosId, eosId, padId);
    
    // Step 1: æµ‹è¯•å¸¦ç‰¹æ®Š token çš„åºåˆ—
    std::vector<int> sequenceWithSpecialTokens = {
        static_cast<int>(bosId),
        100, 200, 300,  // æ™®é€š tokens
        static_cast<int>(eosId)
    };
    
    try {
        BatchInput batchInput;
        batchInput.inputIds = sequenceWithSpecialTokens;
        batchInput.batchSize = 1;
        batchInput.requestPositions = {{0, sequenceWithSpecialTokens.size()}};
        batchInput.sequenceIds = {0};
        
        CLLM_INFO("Testing sequence with special tokens...");
        BatchOutput output = executor_->forward(batchInput);
        
        EXPECT_GT(output.logits.size(), 0) << "Should handle special tokens correctly";
        CLLM_INFO("âœ“ Special tokens handled correctly");
        
        // Step 2: Decode åº”è¯¥è·³è¿‡ç‰¹æ®Š tokens (skipSpecialTokens=true)
        std::vector<llama_token> tokensToDeode(
            sequenceWithSpecialTokens.begin(), 
            sequenceWithSpecialTokens.end()
        );
        std::string decodedWithoutSpecial = tokenizer_->decode(tokensToDeode, true);
        std::string decodedWithSpecial = tokenizer_->decode(tokensToDeode, false);
        
        CLLM_INFO("Decoded without special tokens: \"%s\"", decodedWithoutSpecial.c_str());
        CLLM_INFO("Decoded with special tokens: \"%s\"", decodedWithSpecial.c_str());
        
        // éªŒè¯ skipSpecialTokens å‚æ•°çš„æ•ˆæœ
        EXPECT_NE(decodedWithoutSpecial, decodedWithSpecial) 
            << "skipSpecialTokens parameter should affect output";
        
    } catch (const std::exception& e) {
        FAIL() << "Special token handling failed: " << e.what();
    }
}

/**
 * @test æµ‹è¯• 5: è¾¹ç•Œæƒ…å†µå¤„ç†
 * éªŒè¯ç©ºè¾“å…¥ã€è¶…é•¿è¾“å…¥ç­‰è¾¹ç•Œæƒ…å†µ
 */
TEST_F(TokenizerExecutorIntegrationTest, EdgeCases) {
    if (!canRunTests()) {
        GTEST_SKIP() << "Required models not loaded, skipping test";
    }
    
    CLLM_INFO("\n=== Test 5: Edge Cases ===");
    
    // Case 1: ç©ºå­—ç¬¦ä¸²
    CLLM_INFO("Testing empty string...");
    std::vector<llama_token> emptyTokens = tokenizer_->encode("", true);
    CLLM_INFO("  Empty string encoded to %zu tokens", emptyTokens.size());
    // æŸäº› tokenizer å¯èƒ½ä¼šæ·»åŠ  BOS/EOSï¼Œæ‰€ä»¥ä¸ä¸€å®šä¸ºç©º
    
    // Case 2: å•å­—ç¬¦
    CLLM_INFO("Testing single character...");
    std::vector<llama_token> singleChar = tokenizer_->encode("a", true);
    EXPECT_GT(singleChar.size(), 0) << "Single character should produce tokens";
    CLLM_INFO("  Single char 'a' encoded to %zu tokens", singleChar.size());
    
    // Case 3: è¶…é•¿è¾“å…¥ (æµ‹è¯•æ€§èƒ½å’Œå†…å­˜)
    CLLM_INFO("Testing long input...");
    std::string longInput(500, 'a');  // 500 ä¸ªå­—ç¬¦
    std::vector<llama_token> longTokens = tokenizer_->encode(longInput, true);
    CLLM_INFO("  Long input (%zu chars) encoded to %zu tokens", longInput.size(), longTokens.size());
    EXPECT_GT(longTokens.size(), 0) << "Long input should produce tokens";
    
    // Case 4: ç‰¹æ®Šå­—ç¬¦
    CLLM_INFO("Testing special characters...");
    std::string specialChars = "!@#$%^&*()_+-=[]{}|;:',.<>?/~`";
    std::vector<llama_token> specialTokens = tokenizer_->encode(specialChars, true);
    CLLM_INFO("  Special chars encoded to %zu tokens", specialTokens.size());
    
    // Case 5: Unicode å­—ç¬¦
    CLLM_INFO("Testing Unicode characters...");
    std::string unicodeText = "ä½ å¥½ä¸–ç•Œ ğŸŒ";
    std::vector<llama_token> unicodeTokens = tokenizer_->encode(unicodeText, true);
    CLLM_INFO("  Unicode text encoded to %zu tokens", unicodeTokens.size());
    
    // å°è¯• decode å›æ¥éªŒè¯
    std::string decoded = tokenizer_->decode(unicodeTokens, true);
    CLLM_INFO("  Decoded: \"%s\"", decoded.c_str());
    
    CLLM_INFO("âœ“ Edge cases handled");
}

/**
 * @test æµ‹è¯• 6: æ€§èƒ½åŸºå‡†æµ‹è¯•
 * æµ‹é‡ Tokenizer å’Œ ModelExecutor çš„æ€§èƒ½
 */
TEST_F(TokenizerExecutorIntegrationTest, PerformanceBenchmark) {
    if (!canRunTests()) {
        GTEST_SKIP() << "Required models not loaded, skipping test";
    }
    
    CLLM_INFO("\n=== Test 6: Performance Benchmark ===");
    
    std::string testText = "The quick brown fox jumps over the lazy dog";
    const int iterations = 100;
    
    // Benchmark 1: Tokenizer encode
    auto encodeStart = std::chrono::high_resolution_clock::now();
    std::vector<llama_token> encodedIds;
    for (int i = 0; i < iterations; ++i) {
        encodedIds = tokenizer_->encode(testText, true);
    }
    auto encodeEnd = std::chrono::high_resolution_clock::now();
    float encodeTime = std::chrono::duration<float, std::milli>(encodeEnd - encodeStart).count();
    float encodeAvg = encodeTime / iterations;
    
    CLLM_INFO("Tokenizer encode:");
    CLLM_INFO("  Total time: %.2f ms (%d iterations)", encodeTime, iterations);
    CLLM_INFO("  Average: %.4f ms/op", encodeAvg);
    CLLM_INFO("  Throughput: %.2f ops/sec", 1000.0f / encodeAvg);
    
    // Benchmark 2: ModelExecutor forward
    std::vector<int> executorInput(encodedIds.begin(), encodedIds.end());
    BatchInput batchInput;
    batchInput.inputIds = executorInput;
    batchInput.batchSize = 1;
    batchInput.requestPositions = {{0, executorInput.size()}};
    batchInput.sequenceIds = {0};
    
    auto forwardStart = std::chrono::high_resolution_clock::now();
    for (int i = 0; i < iterations; ++i) {
        BatchOutput output = executor_->forward(batchInput);
    }
    auto forwardEnd = std::chrono::high_resolution_clock::now();
    float forwardTime = std::chrono::duration<float, std::milli>(forwardEnd - forwardStart).count();
    float forwardAvg = forwardTime / iterations;
    
    CLLM_INFO("ModelExecutor forward:");
    CLLM_INFO("  Total time: %.2f ms (%d iterations)", forwardTime, iterations);
    CLLM_INFO("  Average: %.4f ms/op", forwardAvg);
    CLLM_INFO("  Throughput: %.2f ops/sec", 1000.0f / forwardAvg);
    
    // Benchmark 3: Tokenizer decode
    auto decodeStart = std::chrono::high_resolution_clock::now();
    for (int i = 0; i < iterations; ++i) {
        std::string decoded = tokenizer_->decode(encodedIds, true);
    }
    auto decodeEnd = std::chrono::high_resolution_clock::now();
    float decodeTime = std::chrono::duration<float, std::milli>(decodeEnd - decodeStart).count();
    float decodeAvg = decodeTime / iterations;
    
    CLLM_INFO("Tokenizer decode:");
    CLLM_INFO("  Total time: %.2f ms (%d iterations)", decodeTime, iterations);
    CLLM_INFO("  Average: %.4f ms/op", decodeAvg);
    CLLM_INFO("  Throughput: %.2f ops/sec", 1000.0f / decodeAvg);
    
    // æ€§èƒ½æ–­è¨€ (è®¾ç½®åˆç†çš„æ€§èƒ½è¦æ±‚)
    EXPECT_LT(encodeAvg, 10.0f) << "Encode should be faster than 10ms";
    EXPECT_LT(decodeAvg, 10.0f) << "Decode should be faster than 10ms";
    
    CLLM_INFO("âœ“ Performance benchmark completed");
}

/**
 * @test æµ‹è¯• 7: é”™è¯¯å¤„ç†
 * éªŒè¯å¼‚å¸¸æƒ…å†µçš„å¤„ç†
 */
TEST_F(TokenizerExecutorIntegrationTest, ErrorHandling) {
    if (!canRunTests()) {
        GTEST_SKIP() << "Required models not loaded, skipping test";
    }
    
    CLLM_INFO("\n=== Test 7: Error Handling ===");
    
    // Error 1: æ— æ•ˆçš„ token ID
    CLLM_INFO("Testing invalid token ID...");
    std::vector<llama_token> invalidIds = {-1, 999999, -999};
    std::string decoded = tokenizer_->decode(invalidIds, false);
    CLLM_INFO("  Decoded invalid IDs: \"%s\"", decoded.c_str());
    // åº”è¯¥ä¸ä¼šå´©æºƒï¼Œå¯èƒ½è¿”å› UNK æˆ–ç©ºå­—ç¬¦ä¸²
    
    // Error 2: ç©ºçš„ inputIds
    CLLM_INFO("Testing empty input IDs...");
    try {
        BatchInput emptyBatch;
        emptyBatch.inputIds = {};
        emptyBatch.batchSize = 0;
        emptyBatch.requestPositions = {};
        emptyBatch.sequenceIds = {};
        
        // è¿™åº”è¯¥æŠ›å‡ºå¼‚å¸¸æˆ–è¿”å›ç©ºç»“æœ
        // BatchOutput output = executor_->forward(emptyBatch);
        CLLM_INFO("  Empty batch handling: implementation-dependent");
    } catch (const std::exception& e) {
        CLLM_INFO("  Empty batch correctly throws exception: %s", e.what());
    }
    
    CLLM_INFO("âœ“ Error handling verified");
}

/**
 * @brief ä¸»å‡½æ•°
 */
int main(int argc, char** argv) {
    ::testing::InitGoogleTest(&argc, argv);
    
    CLLM_INFO("========================================");
    CLLM_INFO("Tokenizer â†” ModelExecutor Integration Test");
    CLLM_INFO("========================================");
    
    int result = RUN_ALL_TESTS();
    
    CLLM_INFO("========================================");
    CLLM_INFO("Test execution completed with result: %d", result);
    CLLM_INFO("========================================");
    
    return result;
}
