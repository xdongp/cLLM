#include <gtest/gtest.h>
#include "cllm/CTokenizer/deepseek_tokenizer.h"
#include <string>
#include <vector>
#include <thread>
#include <chrono>

using namespace cllm;

/**
 * DeepSeekåˆ†è¯å™¨é¢„å¤„ç†åŠŸèƒ½å•å…ƒæµ‹è¯•
 * 
 * æ³¨æ„: è¿™äº›æµ‹è¯•ä¸éœ€è¦å®é™…çš„æ¨¡å‹æ–‡ä»¶
 * ä¸»è¦æµ‹è¯•é¢„å¤„ç†é€»è¾‘çš„æ­£ç¡®æ€§å’Œé²æ£’æ€§
 */

class DeepSeekPreprocessingUnitTest : public ::testing::Test {
protected:
    void SetUp() override {
        // å‡†å¤‡æµ‹è¯•ç¯å¢ƒ
    }
    
    void TearDown() override {
        // æ¸…ç†æµ‹è¯•ç¯å¢ƒ
    }
};

// ========== åŸºç¡€æ¥å£æµ‹è¯• ==========

TEST_F(DeepSeekPreprocessingUnitTest, TokenizerConstruction) {
    // æµ‹è¯•ä¸åŒæ¨¡å‹ç±»å‹çš„æ„é€ 
    EXPECT_NO_THROW({
        DeepSeekTokenizer llmTokenizer(ModelType::DEEPSEEK_LLM);
        EXPECT_EQ(llmTokenizer.getModelType(), ModelType::DEEPSEEK_LLM);
    });
    
    EXPECT_NO_THROW({
        DeepSeekTokenizer coderTokenizer(ModelType::DEEPSEEK_CODER);
        EXPECT_EQ(coderTokenizer.getModelType(), ModelType::DEEPSEEK_CODER);
    });
    
    EXPECT_NO_THROW({
        DeepSeekTokenizer llm3Tokenizer(ModelType::DEEPSEEK3_LLM);
        EXPECT_EQ(llm3Tokenizer.getModelType(), ModelType::DEEPSEEK3_LLM);
    });
}

TEST_F(DeepSeekPreprocessingUnitTest, EncodeWithoutModelNoCrash) {
    // æµ‹è¯•åœ¨æ²¡æœ‰åŠ è½½æ¨¡å‹çš„æƒ…å†µä¸‹è°ƒç”¨encodeä¸ä¼šå´©æºƒ
    DeepSeekTokenizer tokenizer(ModelType::DEEPSEEK_LLM);
    
    std::vector<std::string> testCases = {
        "",
        "Hello World",
        "ä½ å¥½ä¸–ç•Œ",
        "123456",
        "!@#$%"
    };
    
    for (const auto& testCase : testCases) {
        EXPECT_NO_THROW({
            auto tokens = tokenizer.encode(testCase);
            // æ²¡æœ‰æ¨¡å‹æ—¶åº”è¯¥è¿”å›ç©ºå‘é‡
            EXPECT_TRUE(tokens.empty());
        }) << "Crashed for input: " << testCase;
    }
}

TEST_F(DeepSeekPreprocessingUnitTest, DecodeWithoutModelNoCrash) {
    // æµ‹è¯•åœ¨æ²¡æœ‰åŠ è½½æ¨¡å‹çš„æƒ…å†µä¸‹è°ƒç”¨decodeä¸ä¼šå´©æºƒ
    DeepSeekTokenizer tokenizer(ModelType::DEEPSEEK_CODER);
    
    std::vector<std::vector<llama_token>> testCases = {
        {},
        {1, 2, 3},
        {100, 200, 300}
    };
    
    for (const auto& testCase : testCases) {
        EXPECT_NO_THROW({
            std::string decoded = tokenizer.decode(testCase);
            // æ²¡æœ‰æ¨¡å‹æ—¶åº”è¯¥è¿”å›ç©ºå­—ç¬¦ä¸²
            EXPECT_TRUE(decoded.empty());
        });
    }
}

// ========== ç‰¹æ®ŠTokenæµ‹è¯• ==========

TEST_F(DeepSeekPreprocessingUnitTest, SpecialTokensWithoutModel) {
    DeepSeekTokenizer tokenizer(ModelType::DEEPSEEK_LLM);
    
    // æ²¡æœ‰æ¨¡å‹æ—¶,ç‰¹æ®Štokenåº”è¯¥è¿”å›é»˜è®¤å€¼
    EXPECT_LE(tokenizer.getBosId(), -1);
    EXPECT_LE(tokenizer.getEosId(), -1);
    EXPECT_LE(tokenizer.getPadId(), -1);
    EXPECT_LE(tokenizer.getUnkId(), -1);
}

// ========== è¯æ±‡è¡¨æ“ä½œæµ‹è¯• ==========

TEST_F(DeepSeekPreprocessingUnitTest, VocabOperationsWithoutModel) {
    DeepSeekTokenizer tokenizer(ModelType::DEEPSEEK_CODER);
    
    // æ²¡æœ‰æ¨¡å‹æ—¶,è¯æ±‡è¡¨å¤§å°åº”è¯¥ä¸º0
    EXPECT_EQ(tokenizer.getVocabSize(), 0);
    
    // æ²¡æœ‰æ¨¡å‹æ—¶,IDåˆ°tokençš„è½¬æ¢åº”è¯¥è¿”å›ç©ºå­—ç¬¦ä¸²
    std::string token = tokenizer.idToToken(100);
    EXPECT_TRUE(token.empty());
    
    // æ²¡æœ‰æ¨¡å‹æ—¶,tokenåˆ°IDçš„è½¬æ¢åº”è¯¥è¿”å›0æˆ–-1ï¼ˆå–å†³äºSentencePieceå®ç°ï¼‰
    llama_token id = tokenizer.tokenToId("test");
    EXPECT_TRUE(id == 0 || id == -1) << "Expected 0 or -1, got " << id;
}

// ========== æ¨¡å‹ç±»å‹éªŒè¯æµ‹è¯• ==========

TEST_F(DeepSeekPreprocessingUnitTest, ModelTypeConsistency) {
    std::vector<std::pair<ModelType, std::string>> testCases = {
        {ModelType::DEEPSEEK_LLM, "DeepSeek LLM"},
        {ModelType::DEEPSEEK_CODER, "DeepSeek Coder"},
        {ModelType::DEEPSEEK3_LLM, "DeepSeek3 LLM"}
    };
    
    for (const auto& [modelType, name] : testCases) {
        DeepSeekTokenizer tokenizer(modelType);
        EXPECT_EQ(tokenizer.getModelType(), modelType) 
            << "Model type mismatch for: " << name;
    }
}

// ========== è¾¹ç•Œæ¡ä»¶æµ‹è¯• ==========

TEST_F(DeepSeekPreprocessingUnitTest, BoundaryEmptyInput) {
    std::vector<ModelType> models = {
        ModelType::DEEPSEEK_LLM,
        ModelType::DEEPSEEK_CODER,
        ModelType::DEEPSEEK3_LLM
    };
    
    for (auto modelType : models) {
        DeepSeekTokenizer tokenizer(modelType);
        
        // ç©ºå­—ç¬¦ä¸²è¾“å…¥
        EXPECT_NO_THROW({
            auto tokens = tokenizer.encode("");
            EXPECT_TRUE(tokens.empty());
        });
        
        // ç©ºå‘é‡è§£ç 
        EXPECT_NO_THROW({
            std::string decoded = tokenizer.decode({});
            EXPECT_TRUE(decoded.empty());
        });
    }
}

TEST_F(DeepSeekPreprocessingUnitTest, BoundarySingleCharacter) {
    DeepSeekTokenizer tokenizer(ModelType::DEEPSEEK_LLM);
    
    std::vector<std::string> testCases = {"A", "1", " ", "ä¸­", "\n", "\t"};
    
    for (const auto& testCase : testCases) {
        EXPECT_NO_THROW({
            auto tokens = tokenizer.encode(testCase);
            // æ²¡æœ‰æ¨¡å‹æ—¶è¿”å›ç©ºå‘é‡
            EXPECT_TRUE(tokens.empty());
        }) << "Failed for: " << testCase;
    }
}

TEST_F(DeepSeekPreprocessingUnitTest, BoundaryVeryLongText) {
    DeepSeekTokenizer tokenizer(ModelType::DEEPSEEK_CODER);
    
    // éå¸¸é•¿çš„æ–‡æœ¬
    std::string longText(100000, 'A');
    
    EXPECT_NO_THROW({
        auto tokens = tokenizer.encode(longText);
        EXPECT_TRUE(tokens.empty()); // æ²¡æœ‰æ¨¡å‹æ—¶è¿”å›ç©ºå‘é‡
    });
}

// ========== ç‰¹æ®Šå­—ç¬¦æµ‹è¯• ==========

TEST_F(DeepSeekPreprocessingUnitTest, SpecialCharactersHandling) {
    DeepSeekTokenizer tokenizer(ModelType::DEEPSEEK_LLM);
    
    std::vector<std::string> testCases = {
        "!@#$%^&*()",
        "<>[]{}()",
        "\\n\\t\\r",
        "Î±Î²Î³Î´ÎµÎ¶",
        "ã‚ã„ã†ãˆãŠ",
        "í•œêµ­ì–´",
        "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©",
        "ğŸŒâœ¨ğŸš€"
    };
    
    for (const auto& testCase : testCases) {
        EXPECT_NO_THROW({
            auto tokens = tokenizer.encode(testCase);
            EXPECT_TRUE(tokens.empty());
        }) << "Failed for: " << testCase;
    }
}

// ========== ç¼–ç /è§£ç é€‰é¡¹æµ‹è¯• ==========

TEST_F(DeepSeekPreprocessingUnitTest, EncodeWithSpecialTokensOption) {
    DeepSeekTokenizer tokenizer(ModelType::DEEPSEEK_CODER);
    
    std::string text = "Hello World";
    
    // æµ‹è¯•å¸¦ç‰¹æ®Štokençš„ç¼–ç 
    EXPECT_NO_THROW({
        auto tokensWithSpecial = tokenizer.encode(text, true);
        EXPECT_TRUE(tokensWithSpecial.empty());
    });
    
    // æµ‹è¯•ä¸å¸¦ç‰¹æ®Štokençš„ç¼–ç 
    EXPECT_NO_THROW({
        auto tokensWithoutSpecial = tokenizer.encode(text, false);
        EXPECT_TRUE(tokensWithoutSpecial.empty());
    });
}

TEST_F(DeepSeekPreprocessingUnitTest, DecodeWithSpecialTokensOption) {
    DeepSeekTokenizer tokenizer(ModelType::DEEPSEEK3_LLM);
    
    std::vector<llama_token> tokens = {1, 2, 3};
    
    // æµ‹è¯•è·³è¿‡ç‰¹æ®Štokençš„è§£ç 
    EXPECT_NO_THROW({
        std::string decoded1 = tokenizer.decode(tokens, true);
        EXPECT_TRUE(decoded1.empty());
    });
    
    // æµ‹è¯•ä¸è·³è¿‡ç‰¹æ®Štokençš„è§£ç 
    EXPECT_NO_THROW({
        std::string decoded2 = tokenizer.decode(tokens, false);
        EXPECT_TRUE(decoded2.empty());
    });
}

// ========== å¤šçº¿ç¨‹å®‰å…¨æ€§åŸºç¡€æµ‹è¯• ==========

TEST_F(DeepSeekPreprocessingUnitTest, ThreadSafetyBasic) {
    DeepSeekTokenizer tokenizer(ModelType::DEEPSEEK_LLM);
    
    std::vector<std::thread> threads;
    
    for (int i = 0; i < 10; ++i) {
        threads.emplace_back([&tokenizer, i]() {
            std::string text = "Test " + std::to_string(i);
            auto tokens = tokenizer.encode(text);
            EXPECT_TRUE(tokens.empty());
            
            int vocabSize = tokenizer.getVocabSize();
            EXPECT_EQ(vocabSize, 0);
        });
    }
    
    for (auto& t : threads) {
        t.join();
    }
}

// ========== å¼‚å¸¸å¤„ç†æµ‹è¯• ==========

TEST_F(DeepSeekPreprocessingUnitTest, InvalidTokenIds) {
    DeepSeekTokenizer tokenizer(ModelType::DEEPSEEK_CODER);
    
    std::vector<llama_token> invalidTokens = {-1, -100, 999999};
    
    EXPECT_NO_THROW({
        std::string decoded = tokenizer.decode(invalidTokens);
        EXPECT_TRUE(decoded.empty());
    });
}

TEST_F(DeepSeekPreprocessingUnitTest, LoadNonExistentModel) {
    DeepSeekTokenizer tokenizer(ModelType::DEEPSEEK_LLM);
    
    // å°è¯•åŠ è½½ä¸å­˜åœ¨çš„æ¨¡å‹æ–‡ä»¶
    EXPECT_NO_THROW({
        bool loaded = tokenizer.load("nonexistent/model/path");
        EXPECT_FALSE(loaded);
    });
    
    // åŠ è½½å¤±è´¥å,åˆ†è¯å™¨åº”è¯¥ä»ç„¶å¯ç”¨
    EXPECT_NO_THROW({
        auto tokens = tokenizer.encode("test");
        EXPECT_TRUE(tokens.empty());
    });
}

// ========== æ€§èƒ½æµ‹è¯•ï¼ˆæ¥å£å“åº”æ—¶é—´ï¼‰ ==========

TEST_F(DeepSeekPreprocessingUnitTest, InterfaceResponsiveness) {
    DeepSeekTokenizer tokenizer(ModelType::DEEPSEEK_LLM);
    
    auto start = std::chrono::high_resolution_clock::now();
    
    // å¿«é€Ÿè°ƒç”¨å¤šä¸ªæ¥å£
    int vocabSize = tokenizer.getVocabSize();
    llama_token bosId = tokenizer.getBosId();
    llama_token eosId = tokenizer.getEosId();
    std::string token = tokenizer.idToToken(100);
    llama_token id = tokenizer.tokenToId("test");
    
    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);
    
    // æ¥å£è°ƒç”¨åº”åœ¨åˆç†æ—¶é—´å†…å®Œæˆï¼ˆå³ä½¿æ²¡æœ‰åŠ è½½æ¨¡å‹ï¼‰
    EXPECT_LT(duration.count(), 1000); // åº”è¯¥åœ¨1æ¯«ç§’å†…å®Œæˆ
}

// ========== æ¨¡å‹ç±»å‹æšä¸¾æµ‹è¯• ==========

TEST_F(DeepSeekPreprocessingUnitTest, ModelTypeEnumValues) {
    // éªŒè¯æ¨¡å‹ç±»å‹æšä¸¾å€¼çš„æœ‰æ•ˆæ€§
    EXPECT_NE(ModelType::DEEPSEEK_LLM, ModelType::DEEPSEEK_CODER);
    EXPECT_NE(ModelType::DEEPSEEK_LLM, ModelType::DEEPSEEK3_LLM);
    EXPECT_NE(ModelType::DEEPSEEK_CODER, ModelType::DEEPSEEK3_LLM);
}

// æµ‹è¯•å…¥å£
int main(int argc, char **argv) {
    ::testing::InitGoogleTest(&argc, argv);
    return RUN_ALL_TESTS();
}
