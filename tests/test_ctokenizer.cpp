#include <gtest/gtest.h>
#include "cllm/CTokenizer/manager.h"
#include "cllm/CTokenizer/tokenizer.h"
#include "cllm/CTokenizer/sentencepiece_tokenizer.h"
#include "cllm/CTokenizer/qwen_tokenizer.h"
#include "cllm/CTokenizer/deepseek_tokenizer.h"
#include "cllm/CTokenizer/model_detector.h"
#include <chrono>
#include <thread>

using namespace cllm;

class CTokenizerTest : public ::testing::Test {
protected:
    void SetUp() override {
        // å‡†å¤‡æµ‹è¯•ç¯å¢ƒ
    }
    
    void TearDown() override {
        // æ¸…ç†æµ‹è¯•ç¯å¢ƒ
    }
    
    size_t getCurrentMemoryUsage() {
        return 0;
    }
    
    TokenizerManager manager;
};

// åŸºç¡€åŠŸèƒ½æµ‹è¯•
TEST_F(CTokenizerTest, EncodeDecodeBasic) {
    // ç”±äºæˆ‘ä»¬æ²¡æœ‰å®é™…çš„æµ‹è¯•æ¨¡å‹æ–‡ä»¶ï¼Œæˆ‘ä»¬æµ‹è¯•æ¥å£çš„å¯ç”¨æ€§
    SentencePieceTokenizer tokenizer(ModelType::QWEN);
    
    // æˆ‘ä»¬ä¸èƒ½åŠ è½½å®é™…æ¨¡å‹ï¼Œå› ä¸ºæ²¡æœ‰æµ‹è¯•æ–‡ä»¶ï¼Œæ‰€ä»¥è¿™é‡Œä¸»è¦æ˜¯éªŒè¯æ¥å£å¯ç”¨æ€§
    EXPECT_EQ(tokenizer.getModelType(), ModelType::QWEN);
    
    // æµ‹è¯•ç©ºè¾“å…¥æƒ…å†µ
    auto emptyTokens = tokenizer.encode("");
    // ç©ºå­—ç¬¦ä¸²å¯èƒ½äº§ç”Ÿç‰¹æ®Štokenæˆ–ç©ºç»“æœï¼Œè¿™å–å†³äºå…·ä½“å®ç°
}

TEST_F(CTokenizerTest, VocabOperations) {
    SentencePieceTokenizer tokenizer(ModelType::QWEN);
    
    // æµ‹è¯•è¯æ±‡è¡¨æ“ä½œæ¥å£
    int vocabSize = tokenizer.getVocabSize();
    // å¦‚æœæ¨¡å‹æœªåŠ è½½ï¼Œè¯æ±‡è¡¨å¤§å°å¯èƒ½ä¸º0
    EXPECT_GE(vocabSize, 0);
    
    // æµ‹è¯•IDåˆ°Tokençš„è½¬æ¢ï¼ˆå¯¹äºæœªåŠ è½½æ¨¡å‹çš„æƒ…å†µï¼‰
    std::string token = tokenizer.idToToken(100);
    // æœªåŠ è½½æ¨¡å‹æ—¶ï¼Œåº”è¿”å›ç©ºå­—ç¬¦ä¸²æˆ–é»˜è®¤å€¼
    EXPECT_TRUE(true); // æ¥å£è°ƒç”¨ä¸ä¼šå´©æºƒ
    
    // æµ‹è¯•Tokenåˆ°IDçš„è½¬æ¢
    llama_token id = tokenizer.tokenToId("test");
    EXPECT_TRUE(true); // æ¥å£è°ƒç”¨ä¸ä¼šå´©æºƒ
}

TEST_F(CTokenizerTest, SpecialTokens) {
    SentencePieceTokenizer tokenizer(ModelType::QWEN);
    
    // æµ‹è¯•ç‰¹æ®ŠTokenæ¥å£
    llama_token bosId = tokenizer.getBosId();
    llama_token eosId = tokenizer.getEosId();
    llama_token padId = tokenizer.getPadId();
    llama_token unkId = tokenizer.getUnkId();
    
    // æœªåŠ è½½æ¨¡å‹æ—¶ï¼Œç‰¹æ®Štoken IDé€šå¸¸ä¸ºè´Ÿæ•°
    EXPECT_LE(bosId, -1);
    EXPECT_LE(eosId, -1);
    EXPECT_LE(padId, -1);
    EXPECT_LE(unkId, -1);
}

// QwenTokenizeræµ‹è¯•
TEST_F(CTokenizerTest, QwenFimDetection) {
    QwenTokenizer tokenizer;
    
    // æµ‹è¯•FIMæ ‡è®°æ£€æµ‹
    EXPECT_TRUE(tokenizer.needsFimProcessing("<|fim_pre|>test<|fim_suf|>content<|fim_end|>"));
    EXPECT_TRUE(tokenizer.needsFimProcessing("test `` code ``"));
    EXPECT_FALSE(tokenizer.needsFimProcessing("regular text"));
}

// DeepSeekTokenizeræµ‹è¯•
TEST_F(CTokenizerTest, DeepSeekModelTypes) {
    // æµ‹è¯•ä¸åŒDeepSeekæ¨¡å‹ç±»å‹çš„æ„é€ 
    DeepSeekTokenizer llmTokenizer(ModelType::DEEPSEEK_LLM);
    DeepSeekTokenizer coderTokenizer(ModelType::DEEPSEEK_CODER);
    DeepSeekTokenizer llm3Tokenizer(ModelType::DEEPSEEK3_LLM);
    
    EXPECT_EQ(llmTokenizer.getModelType(), ModelType::DEEPSEEK_LLM);
    EXPECT_EQ(coderTokenizer.getModelType(), ModelType::DEEPSEEK_CODER);
    EXPECT_EQ(llm3Tokenizer.getModelType(), ModelType::DEEPSEEK3_LLM);
}

// ModelDetectoræµ‹è¯•
TEST_F(CTokenizerTest, ModelDetectorDefault) {
    // æµ‹è¯•æ— æ•ˆé…ç½®æ–‡ä»¶çš„å¤„ç†
    ModelType type = ModelDetector::detectModelType("nonexistent/config.json");
    EXPECT_EQ(type, ModelType::SPM); // åº”è¯¥è¿”å›é»˜è®¤ç±»å‹
}

// TokenizerManageræµ‹è¯•
TEST_F(CTokenizerTest, TokenizerManagerGet) {
    TokenizerManager manager;
    
    // æµ‹è¯•è·å–åˆ†è¯å™¨ï¼ˆè™½ç„¶æ— æ³•åŠ è½½å®é™…æ¨¡å‹ï¼Œä½†å¯ä»¥æµ‹è¯•ç®¡ç†å™¨é€»è¾‘ï¼‰
    CTokenizer* qwenTokenizer = manager.getTokenizer("qwen");
    EXPECT_NE(qwenTokenizer, nullptr);
    EXPECT_EQ(qwenTokenizer->getModelType(), ModelType::QWEN);
    
    CTokenizer* deepseekTokenizer = manager.getTokenizer("deepseek-coder");
    EXPECT_NE(deepseekTokenizer, nullptr);
    EXPECT_EQ(deepseekTokenizer->getModelType(), ModelType::DEEPSEEK_CODER);
    
    CTokenizer* llamaTokenizer = manager.getTokenizer("llama");
    EXPECT_NE(llamaTokenizer, nullptr);
    EXPECT_EQ(llamaTokenizer->getModelType(), ModelType::LLAMA);
}

TEST_F(CTokenizerTest, TokenizerManagerCache) {
    // æµ‹è¯•åˆ†è¯å™¨ç¼“å­˜è¡Œä¸º
    TokenizerManager manager;
    
    CTokenizer* tokenizer1 = manager.getTokenizer("qwen");
    CTokenizer* tokenizer2 = manager.getTokenizer("qwen");
    
    // å¯¹äºç›¸åŒçš„æ¨¡å‹ç±»å‹ï¼Œåº”è¯¥è¿”å›ç›¸åŒçš„å®ä¾‹ï¼ˆç”±ç®¡ç†å™¨ç¼“å­˜ï¼‰
    // æ³¨æ„ï¼šè¿™å–å†³äºå…·ä½“å®ç°ï¼Œå¯èƒ½ä¸æ˜¯ä¸¥æ ¼æ„ä¹‰ä¸Šçš„å•ä¾‹
    EXPECT_NE(tokenizer1, nullptr);
    EXPECT_NE(tokenizer2, nullptr);
}

// æ¨¡å‹ç±»å‹æšä¸¾æµ‹è¯•
TEST_F(CTokenizerTest, ModelTypeValues) {
    // æµ‹è¯•æ¨¡å‹ç±»å‹æšä¸¾å€¼
    EXPECT_EQ(ModelType::AUTO, ModelType::AUTO);
    EXPECT_EQ(ModelType::QWEN, ModelType::QWEN);
    EXPECT_EQ(ModelType::QWEN2, ModelType::QWEN2);
    EXPECT_EQ(ModelType::DEEPSEEK_LLM, ModelType::DEEPSEEK_LLM);
    EXPECT_EQ(ModelType::DEEPSEEK_CODER, ModelType::DEEPSEEK_CODER);
    EXPECT_EQ(ModelType::DEEPSEEK3_LLM, ModelType::DEEPSEEK3_LLM);
    EXPECT_EQ(ModelType::LLAMA, ModelType::LLAMA);
    EXPECT_EQ(ModelType::BERT, ModelType::BERT);
    EXPECT_EQ(ModelType::GPT2, ModelType::GPT2);
    EXPECT_EQ(ModelType::SPM, ModelType::SPM);
    EXPECT_EQ(ModelType::BPE, ModelType::BPE);
    EXPECT_EQ(ModelType::WPM, ModelType::WPM);
}

// è¾¹ç•Œæ¡ä»¶æµ‹è¯•
TEST_F(CTokenizerTest, BoundaryConditions) {
    SentencePieceTokenizer tokenizer(ModelType::QWEN);
    
    // æµ‹è¯•å„ç§è¾¹ç•Œæ¡ä»¶
    std::vector<std::string> testInputs = {
        "",           // ç©ºå­—ç¬¦ä¸²
        " ",          // å•ç©ºæ ¼
        "\n",         // å•æ¢è¡Œ
        "\t",         // å•åˆ¶è¡¨ç¬¦
        "A",          // å•å­—ç¬¦
        std::string(10, 'A'), // çŸ­å­—ç¬¦ä¸²
    };
    
    for (const auto& input : testInputs) {
        // æµ‹è¯•ç¼–ç æ¥å£ä¸ä¼šå´©æºƒ
        auto tokens = tokenizer.encode(input);
        EXPECT_TRUE(true); // åªè¦ä¸å´©æºƒå°±ç®—é€šè¿‡
        
        // æµ‹è¯•è§£ç æ¥å£ä¸ä¼šå´©æºƒ
        std::string decoded = tokenizer.decode(tokens);
        EXPECT_TRUE(true); // åªè¦ä¸å´©æºƒå°±ç®—é€šè¿‡
    }
}

// æ€§èƒ½æµ‹è¯• - ä¸»è¦æ˜¯æµ‹è¯•æ¥å£å“åº”æ—¶é—´
TEST_F(CTokenizerTest, InterfaceResponsiveness) {
    SentencePieceTokenizer tokenizer(ModelType::QWEN);
    
    auto start = std::chrono::high_resolution_clock::now();
    
    // å¿«é€Ÿè°ƒç”¨å¤šä¸ªæ¥å£
    int vocabSize = tokenizer.getVocabSize();
    llama_token bosId = tokenizer.getBosId();
    llama_token eosId = tokenizer.getEosId();
    std::string token = tokenizer.idToToken(100);
    
    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);
    
    // æ¥å£è°ƒç”¨åº”åœ¨åˆç†æ—¶é—´å†…å®Œæˆï¼ˆå³ä½¿æ²¡æœ‰åŠ è½½æ¨¡å‹ï¼‰
    EXPECT_LT(duration.count(), 1000); // åº”è¯¥åœ¨1æ¯«ç§’å†…å®Œæˆ
}

// å¤šçº¿ç¨‹å®‰å…¨æ€§æµ‹è¯•ï¼ˆåŸºæœ¬éªŒè¯ï¼‰
TEST_F(CTokenizerTest, ThreadSafetyBasic) {
    TokenizerManager manager;
    
    // å¯åŠ¨å¤šä¸ªçº¿ç¨‹åŒæ—¶è·å–åˆ†è¯å™¨
    std::vector<std::thread> threads;
    
    for (int i = 0; i < 5; ++i) {
        threads.emplace_back([&manager, i]() {
            CTokenizer* tok = manager.getTokenizer("qwen");
            EXPECT_NE(tok, nullptr);
            // æ¨¡æ‹Ÿä¸€äº›æ“ä½œ
            EXPECT_EQ(tok->getModelType(), ModelType::QWEN);
        });
    }
    
    for (auto& t : threads) {
        t.join();
    }
}

// æµ‹è¯•å®Œæˆ
int main(int argc, char **argv) {
    ::testing::InitGoogleTest(&argc, argv);
    return RUN_ALL_TESTS();
}

// ============ é›†æˆæµ‹è¯• ============

TEST_F(CTokenizerTest, IntegrationEndToEnd) {
    auto tokenizer = this->manager.getTokenizer("qwen");
    ASSERT_NE(tokenizer, nullptr);
    
    std::string input = "This is a test sentence for end-to-end validation.";
    auto tokens = tokenizer->encode(input);
    
    int vocabSize = tokenizer->getVocabSize();
    EXPECT_GE(vocabSize, 0);
    
    if (!tokens.empty()) {
        std::string output = tokenizer->decode(tokens);
        EXPECT_EQ(input, output);
    }
}

TEST_F(CTokenizerTest, IntegrationMultiModelSupport) {
    std::vector<std::string> modelTypes = {"qwen", "deepseek-llm", "deepseek-coder", "llama"};
    
    for (const auto& modelType : modelTypes) {
        auto tokenizer = this->manager.getTokenizer(modelType);
        ASSERT_NE(tokenizer, nullptr) << "Failed to get tokenizer for " << modelType;
        
        std::string testText = "Test text for " + modelType;
        auto tokens = tokenizer->encode(testText);
        
        if (!tokens.empty()) {
            std::string decoded = tokenizer->decode(tokens);
            EXPECT_EQ(decoded, testText) << "Decoding mismatch for " << modelType;
        }
    }
}

TEST_F(CTokenizerTest, IntegrationBatchEncodeDecode) {
    auto tokenizer = this->manager.getTokenizer("qwen");
    ASSERT_NE(tokenizer, nullptr);
    
    std::vector<std::string> texts = {
        "Hello, world!",
        "This is a test sentence.",
        "Another test with numbers: 12345",
        "Mixed content: Hello ä¸–ç•Œ ğŸŒ"
    };
    
    for (const auto& text : texts) {
        auto tokens = tokenizer->encode(text);
        
        if (!tokens.empty()) {
            std::string decoded = tokenizer->decode(tokens);
            EXPECT_EQ(decoded, text);
        }
    }
}

// ============ æ€§èƒ½æµ‹è¯• ============

TEST_F(CTokenizerTest, PerformanceEncodeSpeed) {
    auto tokenizer = this->manager.getTokenizer("qwen");
    ASSERT_NE(tokenizer, nullptr);
    
    std::string longText;
    for (int i = 0; i < 1000; ++i) {
        longText += "This is a test sentence for performance evaluation. ";
    }
    
    auto start = std::chrono::high_resolution_clock::now();
    auto tokens = tokenizer->encode(longText);
    auto end = std::chrono::high_resolution_clock::now();
    
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
    
    if (!tokens.empty()) {
        EXPECT_LT(duration.count(), 1000);
        
        double speed = (double)longText.length() / (duration.count() / 1000.0);
        EXPECT_GT(speed, 50000);
    }
}

TEST_F(CTokenizerTest, PerformanceDecodeSpeed) {
    auto tokenizer = this->manager.getTokenizer("qwen");
    ASSERT_NE(tokenizer, nullptr);
    
    std::string text = "Performance test text. ";
    auto tokens = tokenizer->encode(text);
    
    if (tokens.empty()) {
        return;
    }
    
    std::vector<std::vector<llama_token>> batchTokens;
    for (int i = 0; i < 1000; ++i) {
        batchTokens.push_back(tokens);
    }
    
    auto start = std::chrono::high_resolution_clock::now();
    for (const auto& tokenList : batchTokens) {
        std::string decoded = tokenizer->decode(tokenList);
    }
    auto end = std::chrono::high_resolution_clock::now();
    
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
    EXPECT_LT(duration.count(), 1000);
}

TEST_F(CTokenizerTest, PerformanceMemoryUsage) {
    auto tokenizer = this->manager.getTokenizer("qwen");
    ASSERT_NE(tokenizer, nullptr);
    
    size_t initialMemory = getCurrentMemoryUsage();
    
    for (int i = 0; i < 10000; ++i) {
        std::string text = "Test " + std::to_string(i);
        auto tokens = tokenizer->encode(text);
        std::string decoded = tokenizer->decode(tokens);
    }
    
    size_t finalMemory = getCurrentMemoryUsage();
    
    if (initialMemory > 0 && finalMemory > 0) {
        EXPECT_LT(finalMemory - initialMemory, 10 * 1024 * 1024);
    }
}

TEST_F(CTokenizerTest, PerformanceInterfaceResponsiveness) {
    auto tokenizer = this->manager.getTokenizer("qwen");
    ASSERT_NE(tokenizer, nullptr);
    
    auto start = std::chrono::high_resolution_clock::now();
    
    int vocabSize = tokenizer->getVocabSize();
    llama_token bosId = tokenizer->getBosId();
    llama_token eosId = tokenizer->getEosId();
    std::string token = tokenizer->idToToken(100);
    
    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);
    
    EXPECT_LT(duration.count(), 1000);
}

// ============ éªŒè¯æµ‹è¯• ============

TEST_F(CTokenizerTest, ValidationCrossPlatformConsistency) {
    auto tokenizer = this->manager.getTokenizer("qwen");
    ASSERT_NE(tokenizer, nullptr);
    
    std::vector<std::string> testCases = {
        "Hello, world!",
        "æµ‹è¯•ä¸­æ–‡åˆ†è¯",
        "Test with numbers: 123456789",
        "Mixed: Hello ä¸–ç•Œ ğŸŒ emoji",
        "Special chars: !@#$%^&*()",
        "Long text with multiple sentences. This is sentence two. And this is three."
    };
    
    for (const auto& testCase : testCases) {
        auto tokens = tokenizer->encode(testCase);
        
        if (!tokens.empty()) {
            std::string decoded = tokenizer->decode(tokens);
            EXPECT_EQ(testCase, decoded) << "Mismatch for test case: " << testCase;
        }
        
        int vocabSize = tokenizer->getVocabSize();
        EXPECT_GE(vocabSize, 0);
    }
}

TEST_F(CTokenizerTest, ValidationModelSpecificFeatures) {
    QwenTokenizer qwenTokenizer;
    
    std::string code = "def function():\n    pass";
    auto tokens = qwenTokenizer.encode(code);
    
    if (!tokens.empty()) {
        std::string decoded = qwenTokenizer.decode(tokens);
        EXPECT_EQ(decoded, code);
    }
    
    DeepSeekTokenizer deepseekTokenizer(ModelType::DEEPSEEK_CODER);
    
    std::string code2 = "class MyClass:\n    def method(self):\n        return True";
    auto tokens2 = deepseekTokenizer.encode(code2);
    
    if (!tokens2.empty()) {
        std::string decoded2 = deepseekTokenizer.decode(tokens2);
        EXPECT_EQ(decoded2, code2);
    }
}

// ============ å›å½’æµ‹è¯• ============

TEST_F(CTokenizerTest, RegressionKnownIssues) {
    auto tokenizer = this->manager.getTokenizer("qwen");
    ASSERT_NE(tokenizer, nullptr);
    
    std::vector<std::string> problematicInputs = {
        "",
        " ",
        "\n",
        "\t",
        "\r\n",
        std::string(1000, 'A'),
        "A" + std::string(1000, 'B') + "C",
        "!@#$%^&*()_+-=[]{}|;:,.<>?",
        "Î±Î²Î³Î´ÎµÎ¶Î·Î¸Î¹ÎºÎ»Î¼Î½Î¾Î¿Ï€ÏÏƒÏ„Ï…Ï†Ï‡ÏˆÏ‰",
        "ã‚ã„ã†ãˆãŠã‹ããã‘ã“",
        "í•œêµ­ì–´ í…ŒìŠ¤íŠ¸",
        "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©",
        "Ğ ÑƒÑÑĞºĞ¸Ğ¹",
        " ğŸŒ âœ¨ ğŸš€ "
    };
    
    for (const auto& input : problematicInputs) {
        try {
            auto tokens = tokenizer->encode(input);
            
            if (!tokens.empty()) {
                std::string decoded = tokenizer->decode(tokens);
                EXPECT_EQ(decoded, input) << "Regression detected for input: " << input;
            }
        } catch (const std::exception& e) {
            ADD_FAILURE() << "Exception thrown for input '" << input << "': " << e.what();
        }
    }
}

// ============ è¾¹ç•Œå’Œå¼‚å¸¸æµ‹è¯• ============

TEST_F(CTokenizerTest, BoundaryConditionsExtended) {
    auto tokenizer = this->manager.getTokenizer("qwen");
    ASSERT_NE(tokenizer, nullptr);
    
    std::vector<std::string> testInputs = {
        "",
        " ",
        "\n",
        "\t",
        "A",
        std::string(10, 'A'),
        std::string(100, 'A'),
        std::string(1000, 'A'),
        std::string(10000, 'A'),
        "Hello, ä¸–ç•Œ! ğŸŒ",
        "\0\1\2\3",
        "Line1\nLine2\nLine3",
        "Tab\tTab\tTab"
    };
    
    for (const auto& input : testInputs) {
        auto tokens = tokenizer->encode(input);
        EXPECT_TRUE(true);
        
        std::string decoded = tokenizer->decode(tokens);
        EXPECT_TRUE(true);
    }
}

TEST_F(CTokenizerTest, SpecialTokensWithEncoding) {
    auto tokenizer = this->manager.getTokenizer("qwen");
    ASSERT_NE(tokenizer, nullptr);
    
    std::string text = "Hello";
    auto idsWithoutSpecial = tokenizer->encode(text, false);
    auto idsWithSpecial = tokenizer->encode(text, true);
    
    EXPECT_GE(idsWithSpecial.size(), idsWithoutSpecial.size());
}

TEST_F(CTokenizerTest, VocabOperationsExtended) {
    auto tokenizer = this->manager.getTokenizer("qwen");
    ASSERT_NE(tokenizer, nullptr);
    
    int vocabSize = tokenizer->getVocabSize();
    EXPECT_GE(vocabSize, 0);
    
    if (vocabSize > 0) {
        std::string token = tokenizer->idToToken(100);
        llama_token id = tokenizer->tokenToId(token);
        
        if (!token.empty()) {
            EXPECT_EQ(token, tokenizer->idToToken(id));
        }
    }
}

// ============ QwenTokenizerä¸“é¡¹æµ‹è¯• ============

TEST_F(CTokenizerTest, QwenFimProcessing) {
    QwenTokenizer tokenizer;
    
    std::string text = "<|fim_pre|>def hello():<|fim_suf|>    return 'world'<|fim_end|>";
    auto ids = tokenizer.encode(text);
    
    if (!ids.empty()) {
        std::string decoded = tokenizer.decode(ids);
        if (!decoded.empty()) {
            EXPECT_EQ(decoded, text);
        }
    }
}

TEST_F(CTokenizerTest, QwenFimDetectionExtended) {
    QwenTokenizer tokenizer;
    
    EXPECT_TRUE(tokenizer.needsFimProcessing("<|fim_pre|>test<|fim_suf|>content<|fim_end|>"));
    EXPECT_TRUE(tokenizer.needsFimProcessing("test `` code ``"));
    EXPECT_FALSE(tokenizer.needsFimProcessing("regular text"));
    EXPECT_FALSE(tokenizer.needsFimProcessing(""));
    EXPECT_FALSE(tokenizer.needsFimProcessing("   "));
}

// ============ DeepSeekTokenizerä¸“é¡¹æµ‹è¯• ============

TEST_F(CTokenizerTest, DeepSeekPreprocessing) {
    DeepSeekTokenizer tokenizer(ModelType::DEEPSEEK_CODER);
    
    std::string code = "class MyClass:\n    def method(self):\n        return True";
    auto tokens = tokenizer.encode(code);
    
    if (!tokens.empty()) {
        std::string decoded = tokenizer.decode(tokens);
        EXPECT_EQ(decoded, code);
    }
}

TEST_F(CTokenizerTest, DeepSeekModelTypesExtended) {
    DeepSeekTokenizer llmTokenizer(ModelType::DEEPSEEK_LLM);
    DeepSeekTokenizer coderTokenizer(ModelType::DEEPSEEK_CODER);
    DeepSeekTokenizer llm3Tokenizer(ModelType::DEEPSEEK3_LLM);
    
    EXPECT_EQ(llmTokenizer.getModelType(), ModelType::DEEPSEEK_LLM);
    EXPECT_EQ(coderTokenizer.getModelType(), ModelType::DEEPSEEK_CODER);
    EXPECT_EQ(llm3Tokenizer.getModelType(), ModelType::DEEPSEEK3_LLM);
    
    std::string text = "Hello world";
    
    auto llmTokens = llmTokenizer.encode(text);
    if (!llmTokens.empty()) {
        EXPECT_FALSE(llmTokens.empty());
    }
    
    auto coderTokens = coderTokenizer.encode(text);
    if (!coderTokens.empty()) {
        EXPECT_FALSE(coderTokens.empty());
    }
    
    auto llm3Tokens = llm3Tokenizer.encode(text);
    if (!llm3Tokens.empty()) {
        EXPECT_FALSE(llm3Tokens.empty());
    }
}