# âš¡ æ€§èƒ½ä¼˜åŒ–ä¸“é¡¹è§„åˆ™

> **è§¦å‘æ¡ä»¶**: ç”¨æˆ·æåˆ°"ä¼˜åŒ–"ã€"åŠ é€Ÿ"ã€"æ€§èƒ½"ã€"æ…¢"æ—¶ä½¿ç”¨æœ¬è§„åˆ™

---

## ğŸ¯ ä¼˜åŒ–ç›®æ ‡

- æå‡æ¨ç†ååé‡ (tokens/s)
- é™ä½å»¶è¿Ÿ (é¦–tokenæ—¶é—´)
- å‡å°‘å†…å­˜å ç”¨
- æé«˜å¹¶å‘å¤„ç†èƒ½åŠ›

---

## ğŸ” æ€§èƒ½åˆ†ææµç¨‹

### 1. Profiling (å¿…é¡»å…ˆæ‰§è¡Œ)

```bash
# CPU Profiling
perf record -g ./bin/cllm_server
perf report

# æˆ–ä½¿ç”¨gprof
g++ -pg -o cllm_server ...
./cllm_server
gprof cllm_server gmon.out > analysis.txt

# å†…å­˜Profiling
valgrind --tool=massif ./bin/cllm_server
ms_print massif.out.*
```

### 2. çƒ­ç‚¹è¯†åˆ«

```markdown
å…³æ³¨ä»¥ä¸‹çƒ­ç‚¹:
1. âœ… Tokenizer encode/decode
2. âœ… Model forward pass
3. âœ… KVCacheæ“ä½œ
4. âœ… æ•°æ®æ‹·è´ (CPUâ†”GPU)
5. âœ… çº¿ç¨‹åŒæ­¥å¼€é”€
```

---

## ğŸ“‹ ä¼˜åŒ–æ£€æŸ¥æ¸…å•

### CPUä¼˜åŒ–

- [ ] **é¿å…ä¸å¿…è¦çš„æ‹·è´**
  ```cpp
  // âŒ å€¼ä¼ é€’
  void process(std::vector<int> data);
  
  // âœ… å¼•ç”¨ä¼ é€’
  void process(const std::vector<int>& data);
  
  // âœ… ç§»åŠ¨è¯­ä¹‰
  void setData(std::vector<int>&& data) {
      data_ = std::move(data);
  }
  ```

- [ ] **é¢„åˆ†é…å†…å­˜**
  ```cpp
  std::vector<int> tokens;
  tokens.reserve(estimatedSize);  // âœ… é¿å…å¤šæ¬¡realloc
  
  for (int i = 0; i < n; ++i) {
      tokens.push_back(i);  // ä¸ä¼šè§¦å‘realloc
  }
  ```

- [ ] **ä½¿ç”¨å¹¶è¡Œç®—æ³•**
  ```cpp
  #include <BS_thread_pool.hpp>
  
  BS::thread_pool pool(std::thread::hardware_concurrency());
  
  // å¹¶è¡Œå¤„ç†batch
  pool.parallelize_loop(0, batchSize, 
      [&](int start, int end) {
          for (int i = start; i < end; ++i) {
              processBatch(batches[i]);
          }
      }
  );
  pool.wait();
  ```

- [ ] **å‡å°‘è™šå‡½æ•°è°ƒç”¨**
  ```cpp
  // âŒ é¢‘ç¹è™šå‡½æ•°è°ƒç”¨
  for (auto& item : items) {
      item->virtualMethod();  // æ¯æ¬¡æŸ¥è™šè¡¨
  }
  
  // âœ… æ‰¹é‡å¤„ç†
  batchProcess(items);  // ä¸€æ¬¡è™šå‡½æ•°è°ƒç”¨
  ```

- [ ] **å†…è”å°å‡½æ•°**
  ```cpp
  // âœ… ä½¿ç”¨inlineé¿å…å‡½æ•°è°ƒç”¨å¼€é”€
  inline int add(int a, int b) {
      return a + b;
  }
  ```

### å†…å­˜ä¼˜åŒ–

- [ ] **å¯¹è±¡æ± å¤ç”¨**
  ```cpp
  class ObjectPool {
  public:
      torch::Tensor acquire() {
          if (!pool_.empty()) {
              auto tensor = pool_.back();
              pool_.pop_back();
              return tensor;
          }
          return torch::empty({1024});
      }
      
      void release(torch::Tensor tensor) {
          pool_.push_back(tensor);
      }
      
  private:
      std::vector<torch::Tensor> pool_;
  };
  ```

- [ ] **å‡å°‘ä¸´æ—¶å¯¹è±¡**
  ```cpp
  // âŒ åˆ›å»ºä¸´æ—¶å¯¹è±¡
  std::string result = getPrefix() + getSuffix();
  
  // âœ… ç›´æ¥æ„é€ 
  std::string result;
  result.reserve(estimatedSize);
  result.append(getPrefix());
  result.append(getSuffix());
  ```

- [ ] **æ™ºèƒ½æŒ‡é’ˆæ€§èƒ½**
  ```cpp
  // âœ… ä¼˜å…ˆä½¿ç”¨unique_ptr (æ— å¼•ç”¨è®¡æ•°å¼€é”€)
  std::unique_ptr<Tokenizer> tokenizer;
  
  // âš ï¸  shared_ptræœ‰åŸå­æ“ä½œå¼€é”€
  std::shared_ptr<Tokenizer> tokenizer;
  
  // âœ… åœ¨éœ€è¦å…±äº«æ—¶å†ç”¨shared_ptr
  ```

### I/Oä¼˜åŒ–

- [ ] **å‡å°‘ç£ç›˜I/O**
  ```cpp
  // âœ… ç¼“å­˜tokenizer
  static std::unordered_map<std::string, Tokenizer> tokenizerCache;
  
  // âœ… mmapå¤§æ–‡ä»¶
  int fd = open(path.c_str(), O_RDONLY);
  void* data = mmap(nullptr, fileSize, PROT_READ, MAP_PRIVATE, fd, 0);
  ```

- [ ] **å¼‚æ­¥I/O**
  ```cpp
  #include <asio.hpp>
  
  asio::io_context io;
  asio::post(io, []() {
      // å¼‚æ­¥åŠ è½½æ¨¡å‹
      loadModel();
  });
  ```

### å¹¶å‘ä¼˜åŒ–

- [ ] **å‡å°‘é”ç«äº‰**
  ```cpp
  // âŒ ç²—ç²’åº¦é”
  std::lock_guard<std::mutex> lock(globalMutex_);
  // ... é•¿æ—¶é—´æ“ä½œ ...
  
  // âœ… ç»†ç²’åº¦é”
  {
      std::lock_guard<std::mutex> lock(cacheMutex_);
      auto item = cache_.get(key);
  }
  // é‡Šæ”¾é”åå†å¤„ç†
  process(item);
  ```

- [ ] **ä½¿ç”¨æ— é”æ•°æ®ç»“æ„**
  ```cpp
  // âœ… åŸå­æ“ä½œ
  std::atomic<size_t> counter{0};
  counter.fetch_add(1, std::memory_order_relaxed);
  
  // âœ… çº¿ç¨‹å±€éƒ¨å­˜å‚¨
  thread_local std::vector<int> localCache;
  ```

- [ ] **æ‰¹å¤„ç†å‡å°‘åŒæ­¥**
  ```cpp
  // âŒ æ¯ä¸ªè¯·æ±‚éƒ½åŒæ­¥
  for (auto& req : requests) {
      mutex_.lock();
      process(req);
      mutex_.unlock();
  }
  
  // âœ… æ‰¹é‡å¤„ç†
  mutex_.lock();
  for (auto& req : requests) {
      process(req);
  }
  mutex_.unlock();
  ```

---

## ğŸš€ cLLMç‰¹å®šä¼˜åŒ–

### Tokenizerä¼˜åŒ–

```cpp
// 1. ç¼“å­˜encodeç»“æœ
class TokenCache {
    std::unordered_map<std::string, std::vector<int>> cache_;
    size_t maxSize_ = 10000;
    
public:
    std::optional<std::vector<int>> get(const std::string& text) {
        auto it = cache_.find(text);
        return it != cache_.end() ? std::make_optional(it->second) : std::nullopt;
    }
    
    void put(const std::string& text, std::vector<int> ids) {
        if (cache_.size() < maxSize_) {
            cache_[text] = std::move(ids);
        }
    }
};

// 2. æ‰¹é‡encode
std::vector<std::vector<int>> batchEncode(
    const std::vector<std::string>& texts
) {
    BS::thread_pool pool;
    std::vector<std::future<std::vector<int>>> futures;
    
    for (const auto& text : texts) {
        futures.push_back(pool.submit_task([&, text]() {
            return tokenizer_->encode(text);
        }));
    }
    
    std::vector<std::vector<int>> results;
    for (auto& f : futures) {
        results.push_back(f.get());
    }
    return results;
}
```

### KVCacheä¼˜åŒ–

```cpp
// 1. é¢„åˆ†é…cache
class KVCache {
    std::vector<torch::Tensor> preallocated_;
    
public:
    KVCache(size_t capacity) {
        preallocated_.reserve(capacity);
        for (size_t i = 0; i < capacity; ++i) {
            preallocated_.push_back(torch::empty({...}));
        }
    }
    
    torch::Tensor acquire() {
        if (!preallocated_.empty()) {
            auto tensor = preallocated_.back();
            preallocated_.pop_back();
            return tensor;
        }
        return torch::empty({...});
    }
};

// 2. åˆ†å—ç®¡ç†
class ChunkedKVCache {
    static constexpr size_t CHUNK_SIZE = 64;
    std::vector<std::unique_ptr<CacheChunk>> chunks_;
    
    torch::Tensor get(size_t index) {
        size_t chunkIdx = index / CHUNK_SIZE;
        size_t offset = index % CHUNK_SIZE;
        return chunks_[chunkIdx]->get(offset);
    }
};
```

### Model Executorä¼˜åŒ–

```cpp
// 1. Batchæ¨ç†
class BatchedExecutor {
    std::vector<Request> buffer_;
    size_t batchSize_ = 32;
    
public:
    void submit(Request req) {
        buffer_.push_back(std::move(req));
        if (buffer_.size() >= batchSize_) {
            processBatch(buffer_);
            buffer_.clear();
        }
    }
    
private:
    void processBatch(const std::vector<Request>& batch) {
        // æ‰“åŒ…è¾“å…¥
        auto inputIds = packInputs(batch);
        
        // æ‰¹é‡æ¨ç†
        auto outputs = model_->forward(inputIds);
        
        // åˆ†å‘ç»“æœ
        distributeOutputs(batch, outputs);
    }
};

// 2. æµæ°´çº¿å¹¶è¡Œ
class PipelineExecutor {
    BS::thread_pool prefetchPool_;
    BS::thread_pool inferencePool_;
    
public:
    void execute(Request req) {
        // Stage 1: é¢„å¤„ç† (CPU)
        prefetchPool_.submit_task([&, req]() {
            auto tokens = tokenizer_->encode(req.text);
            
            // Stage 2: æ¨ç† (GPU)
            inferencePool_.submit_task([&, tokens]() {
                auto output = model_->forward(tokens);
                
                // Stage 3: åå¤„ç† (CPU)
                prefetchPool_.submit_task([&, output]() {
                    auto text = tokenizer_->decode(output);
                    req.callback(text);
                });
            });
        });
    }
};
```

---

## ğŸ“Š æ€§èƒ½ç›‘æ§

### 1. æ·»åŠ è®¡æ—¶å™¨

```cpp
#include "cllm/common/timer.h"

void TokenizerManager::encode(const std::string& text) {
    Timer timer("encode");
    
    auto result = tokenizer_->encode(text);
    
    float elapsed = timer.elapsed();
    CLLM_DEBUG("Encode took %.2f ms", elapsed);
    
    // æ›´æ–°ç»Ÿè®¡
    stats_.addEncodeTime(elapsed);
    
    return result;
}
```

### 2. ç»Ÿè®¡ä¿¡æ¯æ”¶é›†

```cpp
class PerformanceStats {
public:
    void recordLatency(float ms) {
        latencies_.push_back(ms);
        totalLatency_ += ms;
        ++count_;
    }
    
    float getAvgLatency() const {
        return count_ > 0 ? totalLatency_ / count_ : 0.0f;
    }
    
    float getP50Latency() const {
        if (latencies_.empty()) return 0.0f;
        auto sorted = latencies_;
        std::sort(sorted.begin(), sorted.end());
        return sorted[sorted.size() / 2];
    }
    
    float getP99Latency() const {
        if (latencies_.empty()) return 0.0f;
        auto sorted = latencies_;
        std::sort(sorted.begin(), sorted.end());
        return sorted[sorted.size() * 99 / 100];
    }
    
private:
    std::vector<float> latencies_;
    float totalLatency_ = 0.0f;
    size_t count_ = 0;
};
```

### 3. å®æ—¶ç›‘æ§

```cpp
class PerformanceMonitor {
    std::atomic<size_t> requestsProcessed_{0};
    std::atomic<size_t> tokensGenerated_{0};
    std::chrono::steady_clock::time_point startTime_;
    
public:
    PerformanceMonitor() : startTime_(std::chrono::steady_clock::now()) {}
    
    void recordRequest() {
        requestsProcessed_.fetch_add(1);
    }
    
    void recordTokens(size_t count) {
        tokensGenerated_.fetch_add(count);
    }
    
    void printStats() {
        auto now = std::chrono::steady_clock::now();
        auto elapsed = std::chrono::duration<float>(now - startTime_).count();
        
        float reqPerSec = requestsProcessed_.load() / elapsed;
        float tokensPerSec = tokensGenerated_.load() / elapsed;
        
        CLLM_INFO("Performance:");
        CLLM_INFO("  Requests/s: %.2f", reqPerSec);
        CLLM_INFO("  Tokens/s: %.2f", tokensPerSec);
    }
};
```

---

## ğŸ¯ ä¼˜åŒ–ç›®æ ‡åŸºå‡†

### å½“å‰åŸºçº¿ (éœ€æµ‹é‡)

```markdown
- Tokenizer encode: ? ms/request
- Tokenizer decode: ? ms/request
- Model forward: ? ms/token
- End-to-end latency: ? ms
- Throughput: ? tokens/s
- Memory usage: ? MB
```

### ä¼˜åŒ–ç›®æ ‡

```markdown
- Tokenizer: æå‡ 3-5x
- Model inference: æå‡ 2x (é€šè¿‡batch)
- å¹¶å‘èƒ½åŠ›: æ”¯æŒ 100+ å¹¶å‘è¯·æ±‚
- å†…å­˜: å‡å°‘ 20-30%
```

---

## ğŸ” Profilingå·¥å…·ä½¿ç”¨

### perf (Linux)

```bash
# è®°å½•æ€§èƒ½æ•°æ®
perf record -g -F 99 ./bin/cllm_server

# ç”ŸæˆæŠ¥å‘Š
perf report

# ç«ç„°å›¾
git clone https://github.com/brendangregg/FlameGraph
perf script | FlameGraph/stackcollapse-perf.pl | FlameGraph/flamegraph.pl > flame.svg
```

### Instruments (macOS)

```bash
# Time Profiler
instruments -t "Time Profiler" -D trace.trace ./bin/cllm_server

# Allocations
instruments -t "Allocations" -D trace.trace ./bin/cllm_server
```

### Valgrind

```bash
# å†…å­˜æ³„æ¼æ£€æµ‹
valgrind --leak-check=full ./bin/cllm_server

# å†…å­˜profiling
valgrind --tool=massif ./bin/cllm_server
ms_print massif.out.12345
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

- **C++æ€§èƒ½ä¼˜åŒ–**: [Optimized C++](https://www.oreilly.com/library/view/optimized-c/9781491922057/)
- **å¹¶è¡Œç¼–ç¨‹**: [C++ Concurrency in Action](https://www.manning.com/books/c-plus-plus-concurrency-in-action-second-edition)
- **æ€§èƒ½åˆ†æ**: [Systems Performance](http://www.brendangregg.com/systems-performance-2nd-edition-book.html)

---

**æœ€åæ›´æ–°**: 2026-01-11
