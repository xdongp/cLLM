#!/usr/bin/env python3
"""
å°† HuggingFace Qwen3 æ¨¡å‹å¯¼å‡ºä¸º TorchScript (.pt) æ ¼å¼

ä½¿ç”¨æ–¹æ³•ï¼š
    python export_qwen_torchscript.py --model-path ./Qwen/Qwen3-0.6B --output-path ./Qwen/qwen3_0.6b.pt
"""

import argparse
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from pathlib import Path


class Qwen3Wrapper(torch.nn.Module):
    """
    Qwen3 æ¨¡å‹åŒ…è£…å™¨ï¼Œç”¨äº TorchScript å¯¼å‡º
    
    å°† HuggingFace æ¨¡å‹åŒ…è£…ä¸ºç®€å•çš„ forward æ¥å£
    """
    def __init__(self, model):
        super().__init__()
        self.model = model
    
    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            input_ids: [batch_size, seq_len] è¾“å…¥ token IDs
            
        Returns:
            logits: [batch_size, seq_len, vocab_size] è¾“å‡º logits
        """
        outputs = self.model(input_ids)
        return outputs.logits


def export_qwen_torchscript(
    model_path: str,
    output_path: str,
    use_fp16: bool = False,
    use_int8: bool = False
):
    """
    å¯¼å‡º Qwen3 æ¨¡å‹ä¸º TorchScript æ ¼å¼
    
    Args:
        model_path: HuggingFace æ¨¡å‹è·¯å¾„
        output_path: è¾“å‡º .pt æ–‡ä»¶è·¯å¾„
        use_fp16: æ˜¯å¦ä½¿ç”¨ FP16 é‡åŒ–
        use_int8: æ˜¯å¦ä½¿ç”¨ INT8 åŠ¨æ€é‡åŒ–
    """
    print(f"[export_qwen_torchscript] Loading model from: {model_path}")
    
    # åŠ è½½æ¨¡å‹å’Œ tokenizer
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float32 if not use_fp16 else torch.float16,
        trust_remote_code=True
    )
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    
    print(f"[export_qwen_torchscript] Model config:")
    print(f"  vocab_size: {model.config.vocab_size}")
    print(f"  hidden_size: {model.config.hidden_size}")
    print(f"  num_layers: {model.config.num_hidden_layers}")
    print(f"  num_attention_heads: {model.config.num_attention_heads}")
    print(f"  num_key_value_heads: {model.config.num_key_value_heads}")
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    
    # åº”ç”¨é‡åŒ–
    if use_int8:
        print("[export_qwen_torchscript] Applying INT8 dynamic quantization...")
        
        # æ£€æŸ¥é‡åŒ–å¼•æ“æ”¯æŒ
        supported_engines = torch.backends.quantized.supported_engines
        print(f"[export_qwen_torchscript] Available quantization engines: {supported_engines}")
        
        # å°è¯•è®¾ç½®é‡åŒ–å¼•æ“ï¼ˆä¼˜å…ˆä½¿ç”¨ qnnpackï¼Œåœ¨ macOS ARM ä¸Šé€šå¸¸å¯ç”¨ï¼‰
        try:
            if 'qnnpack' in supported_engines:
                torch.backends.quantized.engine = 'qnnpack'
                print("[export_qwen_torchscript] Using qnnpack quantization engine")
            elif 'fbgemm' in supported_engines:
                torch.backends.quantized.engine = 'fbgemm'
                print("[export_qwen_torchscript] Using fbgemm quantization engine")
            else:
                print("[export_qwen_torchscript] Warning: No quantization engine available!")
                print("[export_qwen_torchscript] Supported engines: " + str(supported_engines))
                raise RuntimeError(
                    "No quantization engine available. This is common on macOS/Apple Silicon. "
                    "Please use --fp16 instead, or install a PyTorch build with quantization support."
                )
        except Exception as e:
            print(f"[export_qwen_torchscript] Error setting quantization engine: {e}")
            raise
        
        # ç¡®ä¿æ¨¡å‹åœ¨ CPU ä¸Šï¼ˆé‡åŒ–éœ€è¦ CPUï¼‰
        model = model.cpu()
        
        try:
            # å°è¯•ä½¿ç”¨æ–°çš„ API (torch.ao.quantization) å¦‚æœå¯ç”¨
            try:
                model = torch.ao.quantization.quantize_dynamic(
                    model,
                    {torch.nn.Linear},
                    dtype=torch.qint8
                )
            except (AttributeError, ImportError):
                # å›é€€åˆ°æ—§ API
                model = torch.quantization.quantize_dynamic(
                    model,
                    {torch.nn.Linear},
                    dtype=torch.qint8
                )
        except RuntimeError as e:
            if "NoQEngine" in str(e) or "quantized::linear_prepack" in str(e):
                print("\n[export_qwen_torchscript] âŒ INT8 quantization failed!")
                print("[export_qwen_torchscript] Reason: Quantization engine not available")
                print("[export_qwen_torchscript] This is common on macOS, especially Apple Silicon.")
                print("\n[export_qwen_torchscript] ğŸ’¡ Solution:")
                print("  1. Use FP16 instead: --fp16")
                print("  2. Or skip quantization: remove --int8 flag")
                print("\n[export_qwen_torchscript] To use INT8, you need:")
                print("  - PyTorch built with quantization support")
                print("  - Or quantize on Linux/x86 machine, then deploy to macOS")
                raise RuntimeError(
                    f"INT8 quantization failed: {e}\n"
                    "Tip: Use --fp16 instead, or remove --int8 flag to export FP32 model."
                ) from e
            else:
                raise
    elif use_fp16:
        print("[export_qwen_torchscript] Converting to FP16...")
        model = model.half()
    
    # åŒ…è£…æ¨¡å‹
    wrapped_model = Qwen3Wrapper(model)
    wrapped_model.eval()
    
    # åˆ›å»ºç¤ºä¾‹è¾“å…¥ï¼ˆç”¨äº tracingï¼‰
    # NOTE: seq_len å†³å®šäº†å¯¼å‡ºæ¨¡å‹èƒ½å¤„ç†çš„æœ€å¤§åºåˆ—é•¿åº¦ï¼
    # ä¹‹å‰ç”¨ (1, 8) å¯¼è‡´æ¨¡å‹åªèƒ½å¤„ç† 8 tokenï¼Œç°åœ¨æ”¹ä¸º 128
    example_input = torch.randint(0, model.config.vocab_size, (1, 128), dtype=torch.long)
    
    print(f"[export_qwen_torchscript] Tracing model with example input shape: {example_input.shape}")
    
    # ä½¿ç”¨ torch.jit.trace å¯¼å‡º
    with torch.no_grad():
        traced_model = torch.jit.trace(wrapped_model, example_input)
    
    # ä¿å­˜ TorchScript æ¨¡å‹
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    print(f"[export_qwen_torchscript] Saving TorchScript model to: {output_path}")
    torch.jit.save(traced_model, str(output_path))
    
    # éªŒè¯å¯¼å‡º
    print("[export_qwen_torchscript] Verifying exported model...")
    loaded_model = torch.jit.load(str(output_path))
    test_output = loaded_model(example_input)
    print(f"  Test output shape: {test_output.shape}")
    print(f"  Expected shape: [1, 8, {model.config.vocab_size}]")
    
    # ä¿å­˜æ¨¡å‹é…ç½®
    config_path = output_path.with_suffix(".json")
    config_data = {
        "vocab_size": model.config.vocab_size,
        "hidden_size": model.config.hidden_size,
        "num_layers": model.config.num_hidden_layers,
        "num_attention_heads": model.config.num_attention_heads,
        "num_key_value_heads": model.config.num_key_value_heads,
        "intermediate_size": model.config.intermediate_size,
        "max_position_embeddings": model.config.max_position_embeddings,
        "dtype": "int8" if use_int8 else ("fp16" if use_fp16 else "fp32")
    }
    
    import json
    with config_path.open("w", encoding="utf-8") as f:
        json.dump(config_data, f, indent=2)
    
    print(f"[export_qwen_torchscript] Model config saved to: {config_path}")
    print("[export_qwen_torchscript] Export completed successfully!")
    
    file_size_mb = output_path.stat().st_size / (1024 * 1024)
    print(f"[export_qwen_torchscript] Model file size: {file_size_mb:.2f} MB")


def main():
    parser = argparse.ArgumentParser(description="Export Qwen3 model to TorchScript format")
    parser.add_argument(
        "--model-path",
        type=str,
        default="./Qwen/Qwen3-0.6B",
        help="Path to HuggingFace Qwen3 model"
    )
    parser.add_argument(
        "--output-path",
        type=str,
        default="./Qwen/qwen3_0.6b_torchscript.pt",
        help="Output TorchScript model path (.pt file)"
    )
    parser.add_argument(
        "--fp16",
        action="store_true",
        help="Export in FP16 format"
    )
    parser.add_argument(
        "--int8",
        action="store_true",
        help="Apply INT8 dynamic quantization"
    )
    
    args = parser.parse_args()
    
    export_qwen_torchscript(
        model_path=args.model_path,
        output_path=args.output_path,
        use_fp16=args.fp16,
        use_int8=args.int8
    )


if __name__ == "__main__":
    main()
