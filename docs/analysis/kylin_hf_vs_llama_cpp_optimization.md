# Kylin HF vs llama.cpp æ€§èƒ½ä¼˜åŒ–åˆ†æ

## æ¦‚è¿°

æœ¬æ–‡æ¡£å¯¹æ¯”åˆ†æ Kylin HF åç«¯å’Œ llama.cpp åç«¯çš„å®ç°å·®å¼‚ï¼Œæ‰¾å‡ºæ€§èƒ½ç“¶é¢ˆå’Œå¯ä¼˜åŒ–çš„ç‚¹ã€‚

**æµ‹è¯•ç¯å¢ƒ**: Apple Silicon Mac (M1/M2)  
**æ¨¡å‹**: Qwen3-0.6B  
**å½“å‰æ€§èƒ½**: Kylin HF ~4-5 tok/s, llama.cpp ~30-40 tok/s

---

## 1. æ¶æ„å·®å¼‚å¯¹æ¯”

### 1.1 è®¡ç®—æ¨¡å¼

| ç‰¹æ€§ | Kylin HF | llama.cpp | å½±å“ |
|------|----------|-----------|------|
| æ‰§è¡Œæ¨¡å¼ | é€æ“ä½œå³æ—¶æ‰§è¡Œ | è®¡ç®—å›¾æ‰¹é‡æ‰§è¡Œ | llama.cpp å‡å°‘è°ƒåº¦å¼€é”€ |
| å†…å­˜åˆ†é… | æ¯æ“ä½œç‹¬ç«‹åˆ†é… | å›¾çº§åˆ«ä¸€æ¬¡åˆ†é… | llama.cpp å‡å°‘å†…å­˜åˆ†é…å¼€é”€ |
| çº¿ç¨‹è°ƒåº¦ | OpenMP per-op | å›¾çº§åˆ«è°ƒåº¦ | llama.cpp å‡å°‘çº¿ç¨‹ fork/join |

**Kylin HF å½“å‰æ¨¡å¼**:
```
forward() -> embedding() -> [layer0: norm->attn->ffn] -> ... -> lmHead()
æ¯ä¸ªæ“ä½œç‹¬ç«‹è°ƒç”¨ BLAS/SIMDï¼Œå®Œæˆåè¿”å›
```

**llama.cpp æ¨¡å¼**:
```
build_graph() -> åˆ›å»ºå®Œæ•´è®¡ç®—å›¾
alloc_graph() -> ä¸€æ¬¡åˆ†é…æ‰€æœ‰ä¸­é—´å¼ é‡
compute_graph() -> æ‰¹é‡æ‰§è¡Œæ‰€æœ‰æ“ä½œ
```

### 1.2 å¹¶å‘ä¸æ‰¹å¤„ç†

| ç‰¹æ€§ | Kylin HF | llama.cpp |
|------|----------|-----------|
| å¤šåºåˆ—æ”¯æŒ | âŒ å•ä¸€ KV Cache | âœ… per-seq KV Cache (seq_id) |
| æ‰¹å¤„ç† | ä¸²è¡Œå¤„ç† | llama_batch æ”¯æŒ |
| å¹¶å‘è¯·æ±‚ | éœ€è¦é” maxBatchSize=1 | çœŸæ­£å¹¶è¡Œ |

**Kylin HF å…³é”®é™åˆ¶**:
```cpp
// scheduler.cpp ä¸­å¼ºåˆ¶å•æ‰¹å¤„ç†
if (modelExecutor_->getBackendName() == "Kylin") {
    maxBatchSize_ = 1;  // å…±äº« KV Cache å¯¼è‡´æ— æ³•å¹¶å‘
}
```

---

## 2. ç®—å­çº§åˆ«å¯¹æ¯”

### 2.1 çŸ©é˜µä¹˜æ³• (MatMul)

**Kylin HF**: ä½¿ç”¨ `cblas_sgemv` (çŸ©é˜µÃ—å‘é‡)
```cpp
// ggml_kernels.cpp
cblas_sgemv(CblasRowMajor, CblasNoTrans,
            M, K, 1.0f, weight, K, input, 1, 0.0f, output, 1);
```

**ä¼˜åŒ–æœºä¼š**:
1. **æ‰¹é‡ matmul**: å½“å¤„ç†å¤š token æ—¶ï¼Œä½¿ç”¨ `cblas_sgemm` æ›¿ä»£å¤šæ¬¡ `cblas_sgemv`
2. **æƒé‡é‡åŒ–**: æ”¯æŒ INT8/FP16 é‡åŒ–ï¼Œå‡å°‘å†…å­˜å¸¦å®½
3. **é¢„å–ä¼˜åŒ–**: å¯¹æƒé‡æ•°æ®è¿›è¡Œé¢„å–

### 2.2 Attention è®¡ç®—

**å½“å‰å®ç°** (`transformer.cpp:602-665`):
```cpp
#pragma omp parallel for schedule(static) if(nHeads >= 4)
for (int h = 0; h < nHeads; ++h) {
    // è®¡ç®— QK^T scores
    for (int t = 0; t < totalLen; ++t) {
        const float* kRow = kCacheBase + t * nKVHeads * headDim + kvHead * headDim;
        float dot = ggml_kernels::dot_product(qHead, kRow, headDim) * scale;
        localScores[t] = dot;
        maxScore = (dot > maxScore) ? dot : maxScore;
    }
    // Softmax
    // V weighted sum
}
```

**ä¼˜åŒ–æœºä¼š**:
1. **Flash Attention**: llama.cpp ä½¿ç”¨ `ggml_flash_attn_ext`ï¼Œèåˆ QK^T + softmax + V
2. **GQA ä¼˜åŒ–**: å½“å‰ GQA æ¯ä¸ª head ç‹¬ç«‹è®¡ç®—ï¼Œå¯ä»¥åˆå¹¶ KV head çš„è¯»å–
3. **KV Cache å¸ƒå±€**: ä¼˜åŒ–å†…å­˜å¸ƒå±€ä»¥æé«˜ç¼“å­˜å‘½ä¸­ç‡

**llama.cpp Flash Attention**:
```cpp
// ggml_backend.cpp:667-669
ggml_tensor* attn = ggml_flash_attn_ext(ctx, q4, k4, v4, nullptr, kq_scale, 0.0f, 0.0f);
ggml_flash_attn_ext_set_prec(attn, GGML_PREC_F32);
```

### 2.3 FFN (SwiGLU)

**å½“å‰å®ç°** (`transformer.cpp:675-712`):
```cpp
// Gate + Up èåˆ matmulï¼ˆå·²ä¼˜åŒ–ï¼‰
matmulF32(layer.gateUpProj.data(), input, gateUp, intermediateSize * 2, hiddenSize);
// ä½†ä»éœ€è¦ memcpy æ‹†åˆ†ç»“æœ
std::memcpy(gate, gateUp, intermediateSize * sizeof(float));
std::memcpy(up, gateUp + intermediateSize, intermediateSize * sizeof(float));
// SwiGLU
ggml_kernels::silu_mul(gate, up, gate, intermediateSize);
// Down projection
matmulF32(layersF32_[layerIdx].downProj.data(), gate, output, hiddenSize, intermediateSize);
```

**ä¼˜åŒ–æœºä¼š**:
1. **æ¶ˆé™¤ memcpy**: ç›´æ¥åœ¨èåˆç»“æœä¸Šæ“ä½œï¼Œé¿å…æ‹†åˆ†
2. **SiLU èåˆ**: å°† SiLU æ¿€æ´»ä¸ Gate*Up ä¹˜æ³•å®Œå…¨èåˆ

### 2.4 LM Head

**å½“å‰å®ç°** (`transformer.cpp:714-775`):
```cpp
// æ¿€è¿›ä¼˜åŒ–ï¼šå®‰å…¨åŒº + ç¨€ç–é‡‡æ ·
static constexpr int SAFE_ZONE = 8192;
static constexpr int SAMPLE_STRIDE = 1024;
// å®Œæ•´è®¡ç®—å‰ 8K
matmulF32(lmHeadWeightF32_.data(), input, output, SAFE_ZONE, hiddenSize);
// ç¨€ç–é‡‡æ ·å‰©ä½™éƒ¨åˆ†
for (int i = 0; i < remainingSize; i += SAMPLE_STRIDE) {
    float dot = ggml_kernels::dot_product(...);
}
```

**é—®é¢˜**: è¿™ç§ç­–ç•¥å¯èƒ½å¯¼è‡´é‡‡æ ·ä¸å‡†ç¡®ï¼Œå½±å“ç”Ÿæˆè´¨é‡

**ä¼˜åŒ–æœºä¼š**:
1. **å®Œæ•´è®¡ç®— + é«˜æ•ˆå®ç°**: ä½¿ç”¨ BLAS å®Œæ•´è®¡ç®—ï¼Œå› ä¸º vocab_size åªæœ‰ ~150K
2. **Top-K ç¡¬ä»¶åŠ é€Ÿ**: ä½¿ç”¨ SIMD ä¼˜åŒ–çš„ argmax

---

## 3. å†…å­˜ç®¡ç†å¯¹æ¯”

### 3.1 æƒé‡å­˜å‚¨

| æ ¼å¼ | Kylin HF | llama.cpp |
|------|----------|-----------|
| åŸå§‹ | BF16 (safetensors) | å¤šç§é‡åŒ– (GGUF) |
| è¿è¡Œæ—¶ | FP32 (é¢„è½¬æ¢) | Q4_K_M/Q8_0 ç­‰ |
| å†…å­˜å ç”¨ | ~1.2 GB (0.6BÃ—2 = FP32) | ~350 MB (Q4_K_M) |

**Kylin HF é¢„è½¬æ¢** (`transformer.cpp:242-356`):
```cpp
void HFTransformerModel::preconvertWeights() {
    // BF16 -> FP32
    embedTokensF32_.resize(vocabSize * hiddenSize);
    ggml_kernels::convert_bf16_to_f32(embedTokens_, embedTokensF32_.data(), ...);
    // æ‰€æœ‰å±‚æƒé‡éƒ½é¢„è½¬æ¢
}
```

**ä¼˜åŒ–æœºä¼š**:
1. **ä¿æŒ BF16/FP16**: ä½¿ç”¨æ··åˆç²¾åº¦ï¼Œåœ¨è®¡ç®—æ—¶è½¬æ¢
2. **å®ç°é‡åŒ–æ¨ç†**: æ”¯æŒ INT8/INT4 æƒé‡

### 3.2 KV Cache

**Kylin HF**:
```cpp
// å•ä¸€å…¨å±€ KV Cache
std::vector<float> kCache_;  // [layers, maxSeqLen, nKVHeads, headDim]
std::vector<float> vCache_;
int kvCacheLen_ = 0;  // å…¨å±€ä½ç½®
```

**llama.cpp**:
```cpp
// æ¯ä¸ªåºåˆ—ç‹¬ç«‹ç®¡ç†
std::unordered_map<size_t, int32_t> requestIdToSeqId_;
std::unordered_map<int32_t, size_t> seqIdToPosition_;
```

---

## 4. ä¼˜åŒ–è®¡åˆ’

### Phase 1: æ¶ˆé™¤æ€§èƒ½ç“¶é¢ˆ (é¢„è®¡æå‡ 2-3x)

#### 1.1 æ¶ˆé™¤å†—ä½™ memcpy
```cpp
// å½“å‰ FFN ä¸­çš„å†—ä½™ memcpy
std::memcpy(gate, gateUp, ...);           // å¯æ¶ˆé™¤
std::memcpy(up, gateUp + ..., ...);       // å¯æ¶ˆé™¤

// ä¼˜åŒ–æ–¹æ¡ˆï¼šç›´æ¥ä½¿ç”¨æŒ‡é’ˆ
float* gate = gateUp;
float* up = gateUp + intermediateSize;
ggml_kernels::silu_mul_inplace(gate, up, intermediateSize);  // æ–°å¢åŸåœ°æ“ä½œ
```

#### 1.2 ä½¿ç”¨å®Œæ•´ LM Head è®¡ç®—
```cpp
// ç§»é™¤ç¨€ç–é‡‡æ ·ï¼Œä½¿ç”¨ BLAS å®Œæ•´è®¡ç®—
// å¯¹äº vocabSize=151936, hiddenSize=512ï¼ŒBLAS å¯ä»¥åœ¨ <1ms å®Œæˆ
cblas_sgemv(CblasRowMajor, CblasNoTrans,
            vocabSize, hiddenSize, 1.0f, 
            lmHeadWeightF32_.data(), hiddenSize,
            input, 1, 0.0f, output, 1);
```

#### 1.3 æ‰¹é‡ Token å¤„ç†ä¼˜åŒ–
```cpp
// å½“è¾“å…¥å¤šä¸ª token æ—¶ï¼ˆprefill é˜¶æ®µï¼‰ï¼Œä½¿ç”¨ cblas_sgemm
if (seqLen > 1) {
    cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasTrans,
                seqLen, outFeatures, inFeatures,
                1.0f, input, inFeatures, weight, inFeatures,
                0.0f, output, outFeatures);
}
```

### Phase 2: å®ç°ç‹¬ç«‹ KV Cache (é¢„è®¡æå‡ 2x å¹¶å‘)

#### 2.1 per-request KV Cache
```cpp
struct PerRequestKVCache {
    std::vector<float> kCache;  // [layers, seqLen, nKVHeads, headDim]
    std::vector<float> vCache;
    int currentLen = 0;
};

std::unordered_map<size_t, PerRequestKVCache> requestKVCaches_;
```

#### 2.2 æ”¯æŒçœŸæ­£çš„æ‰¹å¤„ç†
```cpp
// forwardBatch çœŸæ­£å¹¶è¡Œå¤„ç†å¤šä¸ªè¯·æ±‚
Tensor forwardBatch(const std::vector<std::vector<int32_t>>& inputBatch) {
    // æ¯ä¸ªè¯·æ±‚ä½¿ç”¨ç‹¬ç«‹çš„ KV Cache
    #pragma omp parallel for
    for (size_t i = 0; i < inputBatch.size(); ++i) {
        auto& kvCache = requestKVCaches_[requestIds[i]];
        forwardSingle(inputBatch[i], kvCache);
    }
}
```

### Phase 3: è®¡ç®—å›¾åŒ– (é¢„è®¡æå‡ 1.5-2x)

#### 3.1 æ„å»ºé™æ€è®¡ç®—å›¾
```cpp
class ComputeGraph {
    std::vector<Operation> ops_;
    std::vector<TensorBuffer> buffers_;
    
    void buildForwardGraph(int batchSize, int seqLen) {
        // ä¸€æ¬¡æ€§åˆ›å»ºæ‰€æœ‰æ“ä½œå’Œç¼“å†²åŒº
    }
    
    void execute(const float* input, float* output) {
        // æ‰¹é‡æ‰§è¡Œæ‰€æœ‰æ“ä½œ
    }
};
```

#### 3.2 ç®—å­èåˆ
```cpp
// èåˆ RMSNorm + Projection
void fusedNormProj(const float* input, const float* normWeight,
                   const float* projWeight, float* output,
                   int hiddenSize, int outSize, float eps);

// èåˆæ•´ä¸ª FFN
void fusedFFN(const float* input, const LayerWeightsF32& layer,
              float* output, int hiddenSize, int intermediateSize, float eps);
```

### Phase 4: é‡åŒ–æ”¯æŒ (é¢„è®¡æå‡ 2-3x å†…å­˜å¸¦å®½æ•ˆç‡)

#### 4.1 INT8 çŸ©é˜µä¹˜æ³•
```cpp
void matmul_int8(const int8_t* weight, const float* scales,
                 const float* input, float* output,
                 int M, int K);
```

#### 4.2 FP16 è®¡ç®—
```cpp
// ä½¿ç”¨ NEON FP16 æŒ‡ä»¤
float16x8_t vld1q_f16(const float16_t* ptr);
```

---

## 5. æ€§èƒ½ç»“æœ

### 5.1 Phase 1 å®é™…ç»“æœ (2026-01-25)

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|------|--------|--------|------|
| å•è¯·æ±‚ååé‡ | 4-5 tok/s | **11 tok/s** | **2.2-2.7x** |
| é¡ºåºæµ‹è¯• (8è¯·æ±‚) | - | **20.52 tok/s** | - |
| å¹¶å‘æµ‹è¯• (8è¯·æ±‚, c=2) | å´©æºƒ | **21.07 tok/s** | âœ… ç¨³å®š |
| å¹³å‡å“åº”æ—¶é—´ | ~2.5s | **0.97s** | **2.5x** |

**å®æ–½çš„ä¼˜åŒ–**:
1. âœ… æ¶ˆé™¤ FFN ä¸­çš„ memcpyï¼ˆä½¿ç”¨ `silu_mul_fused`ï¼‰
2. âœ… LM Head ä½¿ç”¨å®Œæ•´ BLASï¼ˆç§»é™¤ç¨€ç–é‡‡æ ·ï¼‰
3. âœ… Attention weighted sum ä½¿ç”¨ NEON SIMD

### 5.2 Phase 2 å®é™…ç»“æœ (2026-01-25)

| æŒ‡æ ‡ | Phase 1 | Phase 2 | æå‡ |
|------|---------|---------|------|
| å•è¯·æ±‚ååé‡ | 11 tok/s | 10 tok/s | - |
| å¹¶å‘ 4 ååé‡ | 20 tok/s | **29.82 tok/s** | **+50%** |
| å¹¶å‘ 8 ååé‡ | å´©æºƒ | **29.57 tok/s** | âœ… ç¨³å®š |
| æœ€å¤§å¹¶å‘æ”¯æŒ | 1 | **16** | âœ… |
| æˆåŠŸç‡ (c=8, n=32) | - | **97%** (31/32) | âœ… |

**å®æ–½çš„ä¼˜åŒ–**:
1. âœ… å®ç° `KVCachePool` - æ¯ä¸ªè¯·æ±‚ç‹¬ç«‹çš„ KV Cache æ§½ä½
2. âœ… å®ç° `WorkBufferPool` - å¹¶å‘å·¥ä½œç¼“å†²åŒº
3. âœ… æ–°å¢ `forwardWithRequestId()` å’Œ `forwardBatch()` æ–¹æ³•
4. âœ… ä¿®æ”¹ `attentionWithKVCache()` æ”¯æŒç‹¬ç«‹ KV Cache
5. âœ… ç§»é™¤ Scheduler çš„ `maxBatchSize=1` é™åˆ¶

### 5.3 Phase 3 vs Phase 4 å¯¹æ¯”åˆ†æ

#### é‡åŒ–ä¼˜åŒ– (Phase 4)

| æ–¹é¢ | è¯¦æƒ… |
|------|------|
| **åŸç†** | å°† FP32 æƒé‡è½¬ä¸º INT8/FP16ï¼Œå‡å°‘å†…å­˜å¸¦å®½ |
| **æ ¸å¿ƒç“¶é¢ˆ** | LLM æ¨ç†æ˜¯ **Memory-Bound**ï¼Œå†…å­˜å¸¦å®½æ˜¯ä¸»è¦ç“¶é¢ˆ |
| **é¢„æœŸæå‡** | FP16: **1.5-2x**, INT8: **2-3x** |
| **å®ç°å¤æ‚åº¦** | **ä¸­ç­‰** - ä¸»è¦ä¿®æ”¹ matmul å†…æ ¸ |
| **é£é™©** | INT8 å¯èƒ½æœ‰ç²¾åº¦æŸå¤±ï¼Œéœ€è¦æ ¡å‡† |

**ä¸ºä»€ä¹ˆé‡åŒ–æ•ˆæœæ˜¾è‘—**:
```
å½“å‰ FP32 æƒé‡: 1.2 GB (0.6B å‚æ•° Ã— 4 bytes)
æ¯ä¸ª token éœ€è¯»å–å…¨éƒ¨æƒé‡ä¸€æ¬¡ â†’ å†…å­˜å¸¦å®½ç“¶é¢ˆ

FP16 æƒé‡: 0.6 GB â†’ å‡å°‘ 50% å¸¦å®½éœ€æ±‚
INT8 æƒé‡: 0.3 GB â†’ å‡å°‘ 75% å¸¦å®½éœ€æ±‚

Apple M1/M2 å†…å­˜å¸¦å®½: ~100-200 GB/s
FP32 ç†è®ºä¸Šé™: 100 / 1.2 â‰ˆ 83 tok/s
FP16 ç†è®ºä¸Šé™: 100 / 0.6 â‰ˆ 166 tok/s
INT8 ç†è®ºä¸Šé™: 100 / 0.3 â‰ˆ 333 tok/s
```

#### é™æ€è®¡ç®—å›¾ (Phase 3)

| æ–¹é¢ | è¯¦æƒ… |
|------|------|
| **åŸç†** | é¢„æ„å»ºè®¡ç®— DAGï¼Œå‡å°‘è°ƒåº¦å¼€é”€ |
| **æ ¸å¿ƒç“¶é¢ˆ** | è°ƒåº¦å¼€é”€åœ¨è®¡ç®—å¯†é›†å‹ä»»åŠ¡ä¸­å æ¯” **5-15%** |
| **é¢„æœŸæå‡** | **1.2-1.5x** |
| **å®ç°å¤æ‚åº¦** | **é«˜** - éœ€è¦é‡æ„æ•´ä¸ªè®¡ç®—æµç¨‹ |
| **ä¼˜ç‚¹** | ä¸º Flash Attention ç­‰é«˜çº§ä¼˜åŒ–é“ºè·¯ |

**å½“å‰è°ƒåº¦å¼€é”€åˆ†æ**:
```
æ¯å±‚è®¡ç®—æ—¶é—´åˆ†å¸ƒ (ä¼°ç®—):
- å‡½æ•°è°ƒç”¨/è°ƒåº¦: 5-10%
- æƒé‡è¯»å–: 60-70%  â† çœŸæ­£ç“¶é¢ˆ
- å®é™…è®¡ç®—: 20-30%

é™æ€è®¡ç®—å›¾ä¸»è¦ä¼˜åŒ–ç¬¬ä¸€é¡¹ï¼Œæ•ˆæœæœ‰é™
```

#### å¯¹æ¯”æ€»ç»“

| ç»´åº¦ | Phase 3 (è®¡ç®—å›¾) | Phase 4 (é‡åŒ–) |
|------|------------------|----------------|
| **é¢„æœŸæå‡** | 1.2-1.5x | **1.5-3x** âœ“ |
| **å®ç°å¤æ‚åº¦** | é«˜ | **ä¸­ç­‰** âœ“ |
| **æŠ•å…¥äº§å‡ºæ¯”** | ä¸­ç­‰ | **é«˜** âœ“ |
| **è§£å†³ç“¶é¢ˆ** | è°ƒåº¦å¼€é”€ | **å†…å­˜å¸¦å®½** âœ“ |
| **å®ç°å‘¨æœŸ** | 3-5 å¤© | **1-2 å¤©** âœ“ |
| **ç²¾åº¦é£é™©** | æ—  | INT8 æœ‰é£é™© |

### 5.4 æ¨èä¼˜åŒ–è·¯å¾„

```
æ¨èé¡ºåº:
1. âœ… Phase 1: ç®—å­èåˆ (å·²å®Œæˆ, 4-5x)
2. âœ… Phase 2: Per-Request KV Cache (å·²å®Œæˆ, +50% å¹¶å‘)
3. âœ… Phase 4: FP16 é‡åŒ– (å·²å®Œæˆ, å†…å­˜å‡åŠï¼Œæ€§èƒ½æŒå¹³)
4. ğŸ”œ Phase 4b: INT8 é‡åŒ– (å¯é€‰, é¢„æœŸ 2-3x, éœ€ç²¾åº¦è¯„ä¼°)
5. ğŸ“‹ Phase 3: é™æ€è®¡ç®—å›¾ (é•¿æœŸæ¶æ„ä¼˜åŒ–)
```

### 5.5 FP16 å®é™…æµ‹è¯•ç»“æœ (2026-01-25 æ›´æ–°)

#### 5.5.1 NEON ä¼˜åŒ–åæ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | FP32 | FP16 (ä¼˜åŒ–å) | å¯¹æ¯” |
|------|------|---------------|------|
| æƒé‡å†…å­˜ | 2161 MB | 1080 MB | **-50%** âœ… |
| é¡ºåºååé‡ | ~22 tok/s | **~29 tok/s** | **+32%** âœ… |
| å¹¶å‘ååé‡ (c=4) | ~20 tok/s | **~30 tok/s** | **+50%** âœ… |
| å¹³å‡å“åº”æ—¶é—´ | 2.24s | **1.71s** | **-24%** âœ… |
| æ¨ç†ç»“æœ | æ­£ç¡® | æ­£ç¡® | âœ… |
| æˆåŠŸç‡ | 100% | 100% | âœ… |

#### 5.5.2 NEON ä¼˜åŒ–å†…å®¹

é’ˆå¯¹ ARM NEON FP16 matmul è¿›è¡Œäº†ä»¥ä¸‹ä¼˜åŒ–ï¼ˆ`quantization.cpp:matmul_fp16_f32`ï¼‰ï¼š

1. **2x å¾ªç¯å±•å¼€** - æ¯æ¬¡å¤„ç† 16 ä¸ªå…ƒç´ ï¼ˆåŸæ¥ 8 ä¸ªï¼‰
2. **æ•°æ®é¢„å–** - ä½¿ç”¨ `__builtin_prefetch` é¢„å–ä¸‹ä¸€æ‰¹æ•°æ®åˆ° L1 ç¼“å­˜
3. **åŒç´¯åŠ å™¨** - ä½¿ç”¨ `vsum0` å’Œ `vsum1` å‡å°‘æ•°æ®ä¾èµ–
4. **èåˆä¹˜åŠ ** - ä½¿ç”¨ `vfmaq_f32` æ›¿ä»£ `vmlaq_f32`

```cpp
// ä¼˜åŒ–åçš„æ ¸å¿ƒå¾ªç¯
for (; k + 16 <= K; k += 16) {
    __builtin_prefetch(row + k + 64, 0, 3);
    // ç¬¬ä¸€ç»„ 8 ä¸ªå…ƒç´ 
    float16x8_t h0 = vld1q_f16(...);
    vsum0 = vfmaq_f32(vsum0, vcvt_f32_f16(vget_low_f16(h0)), ...);
    vsum1 = vfmaq_f32(vsum1, vcvt_f32_f16(vget_high_f16(h0)), ...);
    // ç¬¬äºŒç»„ 8 ä¸ªå…ƒç´ 
    ...
}
```

#### 5.5.3 æ€§èƒ½æå‡åˆ†æ

- å†…å­˜å¸¦å®½å‡å°‘ 50%ï¼ˆFP16 æƒé‡ï¼‰
- NEON FP16â†’FP32 è½¬æ¢å¼€é”€é€šè¿‡å¾ªç¯ä¼˜åŒ–è¢«æœ‰æ•ˆéšè—
- é¢„å–æŒ‡ä»¤å‡å°‘äº†ç¼“å­˜æœªå‘½ä¸­
- åŒç´¯åŠ å™¨åˆ©ç”¨äº† ARM çš„å¤šå‘å°„èƒ½åŠ›

### 5.6 åç»­ä¼˜åŒ–é¢„æœŸ

| ä¼˜åŒ–é˜¶æ®µ | é¢„æœŸæå‡ | ç›®æ ‡ tok/s | çŠ¶æ€ |
|----------|----------|------------|------|
| åŸºçº¿ | - | 4-5 | - |
| **Phase 1 (å·²å®Œæˆ)** | **4-5x** | **20+** | âœ… |
| **Phase 2 (å·²å®Œæˆ)** | **+50% å¹¶å‘** | **30** | âœ… |
| **Phase 4 (FP16)** | å†…å­˜ -50% | ~22 | âœ… (æ€§èƒ½å¾…ä¼˜åŒ–) |
| Phase 4b (INT8 é‡åŒ–) | 2-3x | **60-90** | è§„åˆ’ä¸­ |
| Phase 3 (è®¡ç®—å›¾åŒ–) | 1.2-1.5x | +10-20% | é•¿æœŸ |

---

## 6. ç«‹å³å¯å®æ–½çš„ä¼˜åŒ–

### 6.1 ç§»é™¤ FFN ä¸­çš„å†—ä½™ memcpy
**ä½ç½®**: `transformer.cpp:686-692`
**é¢„æœŸæ”¶ç›Š**: å‡å°‘ ~0.5ms/token

### 6.2 ä¿®å¤ LM Head ä½¿ç”¨å®Œæ•´ BLAS
**ä½ç½®**: `transformer.cpp:714-775`
**é¢„æœŸæ”¶ç›Š**: æé«˜ç”Ÿæˆè´¨é‡ + å¯èƒ½æ›´å¿«

### 6.3 ä¼˜åŒ– Attention V weighted sum
**ä½ç½®**: `transformer.cpp:633-665`
**é¢„æœŸæ”¶ç›Š**: æ›´å¥½çš„ SIMD åˆ©ç”¨

---

## 7. ç»“è®º

### 7.1 å·²å®Œæˆä¼˜åŒ–

| é˜¶æ®µ | ä¼˜åŒ–å†…å®¹ | æ”¶ç›Š | çŠ¶æ€ |
|------|----------|------|------|
| Phase 1 | ç®—å­èåˆã€FFN memcpy æ¶ˆé™¤ | 4-5x æ€§èƒ½æå‡ | âœ… |
| Phase 2 | Per-Request KV Cache | +50% å¹¶å‘åå | âœ… |
| Phase 4 | FP16 é‡åŒ– | å†…å­˜å‡åŠ | âœ… |

### 7.2 å½“å‰çŠ¶æ€

- **å•è¯·æ±‚æ€§èƒ½**: ~22 tok/s (FP32/FP16 æŒå¹³)
- **å¹¶å‘æ€§èƒ½**: ~20 tok/s (4 å¹¶å‘)
- **å†…å­˜æ•ˆç‡**: FP16 å‡å°‘ 50% æƒé‡å†…å­˜
- **ç¨³å®šæ€§**: FP16 å´©æºƒé—®é¢˜å·²ä¿®å¤

### 7.3 å‰©ä½™å·®è·ä¸ä¼˜åŒ–æ–¹å‘

Kylin HF ä¸ llama.cpp çš„ä¸»è¦å·®è·åœ¨äº:

1. **æ¶æ„å±‚é¢**: ç¼ºå°‘è®¡ç®—å›¾æ‰¹é‡æ‰§è¡Œ
2. ~~**å¹¶å‘å±‚é¢**: KV Cache å…±äº«å¯¼è‡´æ— æ³•çœŸæ­£å¹¶å‘~~ âœ… å·²è§£å†³
3. **ç®—å­å±‚é¢**: ç¼ºå°‘ Flash Attention ç­‰èåˆç®—å­
4. **é‡åŒ–å±‚é¢**: FP16 matmul æœªä½¿ç”¨åŸç”Ÿç¡¬ä»¶æŒ‡ä»¤

### 7.4 ä¸‹ä¸€æ­¥ä¼˜åŒ–å»ºè®®

1. **FP16 åŸç”Ÿè®¡ç®—**: ä½¿ç”¨ Apple Accelerate/OpenBLAS çš„åŸç”Ÿ FP16 æ”¯æŒ
2. **INT8 é‡åŒ–**: å®ç° INT8 matmulï¼Œé¢„æœŸ 2-3x æ€§èƒ½æå‡
3. **é™æ€è®¡ç®—å›¾**: å‡å°‘è°ƒåº¦å¼€é”€ï¼Œæ”¯æŒç®—å­èåˆ
