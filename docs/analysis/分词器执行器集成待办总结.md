# Tokenizer ↔ ModelExecutor 联调 TODO 执行摘要

**日期**: 2026-01-10  
**状态**: 策略调整完成，准备执行  
**策略**: 从 Kylin 后端切换到 LibTorch 后端

---

## 📋 策略变更说明

### 原计划
- 使用 Kylin 自研后端 + 占位权重
- 预计时间: 4.5-7小时
- 风险: 配置复杂，占位权重不确定性高

### 新计划 ✅
- 使用 LibTorch 后端 + 真实模型
- 预计时间: 3.5-5.5小时（节省1-1.5小时）
- 优势:
  - ✅ LibTorch 后端更成熟稳定
  - ✅ 真实模型测试更可靠
  - ✅ 减少配置问题，加速验证
  - ✅ 避免占位权重的不确定性

---

## 🎯 更新后的任务清单

| ID | 任务 | 状态 | 预计时间 | 关键步骤 |
|----|------|------|---------|---------|
| 1 | 配置 LibTorch 后端测试环境 | 🔄 进行中 | 0.5-1h | 导出测试模型、验证依赖 |
| 2 | 修改测试代码使用 LibTorch 后端 | ⏳ 待开始 | 0.5h | 更新 SetUp()、修改路径 |
| 3 | 运行完整的7个联调测试用例 | ⏳ 待开始 | 1h | 执行测试、记录结果 |
| 4 | 分析测试结果并修复发现的问题 | ⏳ 待开始 | 1-2h | 定位问题、实施修复 |
| 5 | 生成最终联调测试报告 | ⏳ 待开始 | 0.5-1h | 汇总数据、撰写报告 |

**总预计时间**: 3.5-5.5小时

---

## ✅ 已完成的工作

### 1. 测试框架搭建 ✅
- ✅ 创建集成测试文件: `tests/test_tokenizer_executor_integration.cpp` (~510行)
- ✅ 实现7个全面的测试用例
- ✅ 更新 CMakeLists.txt 配置
- ✅ 编译通过（仅2个非阻塞警告）

### 2. Logger 问题修复 ✅
- ✅ 识别格式化语法问题（`%s` → `{}`）
- ✅ 修复测试代码中的格式化调用
- ✅ 验证 spdlog 语法正确性

### 3. 策略调整 ✅
- ✅ 分析 Kylin 后端配置复杂性
- ✅ 决定切换到 LibTorch 后端
- ✅ 更新 TODO 文档和任务清单
- ✅ 创建模型导出脚本: `scripts/export_test_model.py`

---

## 🚀 下一步行动 (任务1)

### Step 1: 导出 LibTorch 测试模型 ⏳

**执行命令**:
```bash
cd /Users/dannypan/PycharmProjects/xllm/cpp/cLLM
python3 scripts/export_test_model.py
```

**预期输出**:
- 模型文件: `tests/test_model_libtorch.pt`
- 文件大小: ~4-5 MB
- 参数量: ~4M parameters

**成功标准**:
- [ ] 模型文件生成成功
- [ ] 加载验证通过
- [ ] 输出差异 < 1e-5

---

### Step 2: 验证 LibTorch 依赖 ⏳

**检查命令**:
```bash
cd build
cmake .. 2>&1 | grep -i "torch\|libtorch"
```

**预期输出**:
```
-- Found Torch: /path/to/libtorch
-- Torch version: 2.x.x
```

**如果失败**:
- 选项A: 使用已安装的 PyTorch
  ```bash
  python3 -c "import torch; print(torch.__path__[0])"
  ```
- 选项B: 下载预编译 LibTorch
  - 访问: https://pytorch.org/get-started/locally/
  - 选择 macOS, C++/Java, CPU
  - 解压并配置 CMAKE_PREFIX_PATH

---

### Step 3: 修改测试代码 ⏳

**文件**: `tests/test_tokenizer_executor_integration.cpp`

**关键修改**:
```cpp
// 在 SetUp() 函数中
executor_ = std::make_unique<ModelExecutor>(
    "tests/test_model_libtorch.pt",  // ← 使用 LibTorch 模型
    "",     // 不使用量化
    true,   // 启用 SIMD
    true    // ← 改为 true (使用 LibTorch 后端)
);
```

**需要添加**:
```cpp
#include <filesystem>  // 用于检查文件是否存在

// 在 SetUp() 中检查
if (!std::filesystem::exists("tests/test_model_libtorch.pt")) {
    CLLM_WARN("LibTorch model not found");
    executorLoaded_ = false;
    return;
}
```

---

### Step 4: 重新编译并运行 ⏳

```bash
cd build
make test_tokenizer_executor_integration -j8
./bin/test_tokenizer_executor_integration --gtest_color=yes
```

**预期结果**:
- ✅ 编译通过
- ✅ ModelExecutor 初始化成功（LibTorch 后端）
- ✅ 至少部分测试用例通过

---

## 📊 7个测试用例概览

| # | 测试用例 | 验证目标 | 优先级 |
|---|---------|---------|--------|
| 1 | BasicInterfaceCompatibility | 基本接口兼容性 | P0 |
| 2 | EndToEndTextGeneration | 端到端文本生成流程 | P0 |
| 3 | BatchProcessing | 批处理场景 | P1 |
| 4 | SpecialTokenHandling | 特殊Token处理 | P1 |
| 5 | EdgeCases | 边界情况处理 | P2 |
| 6 | PerformanceBenchmark | 性能基准测试 | P2 |
| 7 | ErrorHandling | 错误处理验证 | P2 |

**目标通过率**: ≥ 85% (6/7)

---

## 🎯 关键里程碑

### Milestone 1: 环境就绪 (任务1) ⏳
- [ ] LibTorch 模型导出成功
- [ ] LibTorch 依赖验证通过
- [ ] 测试代码修改完成
- [ ] 编译无错误

**预计完成**: 1小时内

---

### Milestone 2: 测试执行 (任务2-3) ⏳
- [ ] 所有测试用例执行完毕
- [ ] 至少6个测试通过
- [ ] 性能数据收集完成

**预计完成**: 1.5小时内

---

### Milestone 3: 问题修复 (任务4) ⏳
- [ ] 失败测试根因分析
- [ ] 关键问题修复
- [ ] 回归测试通过

**预计完成**: 2小时内

---

### Milestone 4: 报告生成 (任务5) ⏳
- [ ] 最终测试报告生成
- [ ] 联调就绪度评估 (目标 ≥ 90%)
- [ ] 后续行动计划明确

**预计完成**: 0.5小时内

---

## 📁 相关文档

| 文档 | 路径 | 说明 |
|------|------|------|
| 详细 TODO | `docs/analysis/tokenizer_executor_integration_TODO.md` | 完整任务清单 |
| 初步报告 | `docs/analysis/tokenizer_executor_integration_report.md` | 接口兼容性分析 |
| 测试代码 | `tests/test_tokenizer_executor_integration.cpp` | 集成测试实现 |
| 模型导出脚本 | `scripts/export_test_model.py` | LibTorch 模型生成 |

---

## 💡 关键决策

### 为什么切换到 LibTorch 后端？

**技术原因**:
1. ✅ **成熟度更高**: LibTorch 是官方后端，经过充分测试
2. ✅ **真实场景**: 使用真实模型，测试结果更可信
3. ✅ **调试友好**: 错误信息清晰，问题定位容易
4. ✅ **生态支持**: 丰富的文档和社区支持

**项目原因**:
1. ✅ **加速验证**: 减少配置时间，快速验证接口
2. ✅ **降低风险**: 避免 Kylin 后端占位权重的不确定性
3. ✅ **节省时间**: 预计节省1-1.5小时
4. ✅ **提高质量**: 更可靠的测试结果

**长期计划**:
- 当前: 使用 LibTorch 完成联调验证 ✅
- 未来: Kylin 后端独立测试和优化 ⏳
- 最终: 两个后端都支持并能自由切换 🎯

---

## ⚠️ 风险和缓解

### 风险1: LibTorch 依赖缺失
**概率**: 中  
**影响**: 高  
**缓解**:
- 提供多种安装方案
- 准备离线 LibTorch 包
- 如需要可退回 Kylin 方案

### 风险2: 模型导出失败
**概率**: 低  
**影响**: 中  
**缓解**:
- 简化模型结构
- 提供预导出的模型文件
- 详细的错误日志

### 风险3: 接口不兼容
**概率**: 低  
**影响**: 高  
**缓解**:
- 已完成接口兼容性静态分析（100%通过）
- 逐个测试用例验证
- 准备适配代码

---

## 🎉 成功标准

### 联调就绪度 ≥ 90%

| 维度 | 目标 | 当前 |
|------|------|------|
| 接口设计 | 100% | 100% ✅ |
| 测试框架 | 100% | 100% ✅ |
| 编译构建 | 100% | 100% ✅ |
| 测试执行 | ≥ 85% | 0% ⏳ |
| 文档完整性 | ≥ 90% | 95% ✅ |

**当前总体就绪度**: 79%  
**目标总体就绪度**: ≥ 90%

---

## 📞 联系和支持

**问题反馈**:
- 技术问题: 查看详细 TODO 文档
- 编译问题: 检查 CMakeLists.txt 配置
- 运行时问题: 查看日志文件 `logs/*.log`

**参考资料**:
- LibTorch 文档: https://pytorch.org/cppdocs/
- cLLM 开发指南: `docs/development_guide.md`
- 模块分析报告: `docs/analysis/`

---

**文档版本**: 1.0  
**最后更新**: 2026-01-10  
**维护人**: AI Assistant

---

## 🚀 立即开始

**第一步**: 导出测试模型
```bash
cd /Users/dannypan/PycharmProjects/xllm/cpp/cLLM
python3 scripts/export_test_model.py
```

**第二步**: 检查输出
```bash
ls -lh tests/test_model_libtorch.pt
```

**第三步**: 继续后续任务
按照 TODO 清单逐项推进！🎯
