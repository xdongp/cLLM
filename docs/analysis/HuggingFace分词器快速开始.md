# HuggingFace Tokenizerå¿«é€Ÿä¸Šæ‰‹æŒ‡å—

> **ç›®æ ‡è¯»è€…**: cLLMå¼€å‘è€…  
> **é¢„è®¡æ—¶é—´**: 30åˆ†é’Ÿå®Œæˆç¯å¢ƒé…ç½®å’Œç¬¬ä¸€ä¸ªç¤ºä¾‹  
> **å‰ç½®æ¡ä»¶**: macOS/Linux + CMake + C++17

---

## ğŸš€ å¿«é€Ÿå¼€å§‹ (5åˆ†é’Ÿ)

### Step 1: å®‰è£…tokenizers-cpp

#### macOS
```bash
# å®‰è£…Rust (å¦‚æœæ²¡æœ‰)
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# ç¼–è¯‘å®‰è£…tokenizers-cpp
git clone https://github.com/mlc-ai/tokenizers-cpp /tmp/tokenizers-cpp
cd /tmp/tokenizers-cpp
mkdir build && cd build
cmake .. -DCMAKE_INSTALL_PREFIX=/opt/homebrew
make -j8
sudo make install

# éªŒè¯å®‰è£…
ls /opt/homebrew/include/tokenizers_cpp.h
ls /opt/homebrew/lib/libtokenizers_cpp.dylib
```

#### Linux (Ubuntu/Debian)
```bash
# å®‰è£…ä¾èµ–
sudo apt-get update
sudo apt-get install -y cargo rustc cmake g++ git

# ç¼–è¯‘å®‰è£…
git clone https://github.com/mlc-ai/tokenizers-cpp /tmp/tokenizers-cpp
cd /tmp/tokenizers-cpp
mkdir build && cd build
cmake .. -DCMAKE_INSTALL_PREFIX=/usr/local
make -j$(nproc)
sudo make install

# éªŒè¯
pkg-config --cflags --libs tokenizers_cpp
```

### Step 2: ç¼–è¯‘cLLM (å¯ç”¨HFæ”¯æŒ)

```bash
cd /Users/dannypan/PycharmProjects/xllm/cpp/cLLM
mkdir -p build && cd build

# âœ… å¯ç”¨tokenizers-cppæ”¯æŒ
cmake .. -DUSE_TOKENIZERS_CPP=ON

# ç¼–è¯‘
make -j8

# éªŒè¯HF Tokenizerå·²å¯ç”¨
grep "USE_TOKENIZERS_CPP" CMakeCache.txt
# åº”è¾“å‡º: USE_TOKENIZERS_CPP:BOOL=ON
```

### Step 3: æµ‹è¯•è¿è¡Œ

```bash
# è¿è¡ŒHTTP Serveræµ‹è¯• (ä½¿ç”¨HF Tokenizer)
cd build
./bin/test_http_server_direct

# é¢„æœŸè¾“å‡º:
# [INFO] Detected HuggingFace format (tokenizer.json)
# [INFO] HFTokenizer loaded successfully
# [INFO] Vocab size: 151936, BOS: 151643, EOS: 151645
# [PASS] All tests passed!
```

---

## ğŸ“ ä»£ç ç¤ºä¾‹

### ç¤ºä¾‹1: åŸºç¡€ä½¿ç”¨ (è‡ªåŠ¨æ£€æµ‹æ ¼å¼)

```cpp
#include "cllm/tokenizer/base_tokenizer.h"

int main() {
    // âœ… è‡ªåŠ¨æ£€æµ‹tokenizer.json â†’ ä½¿ç”¨HFTokenizer
    auto tokenizer = cllm::TokenizerFactory::create(
        "/path/to/Qwen3-0.6B",
        cllm::TokenizerFactory::Backend::AUTO
    );
    
    // ç¼–ç 
    std::string text = "Hello, world!";
    auto ids = tokenizer->encode(text, true);
    
    // è¾“å‡ºToken IDs
    std::cout << "Token IDs: [";
    for (size_t i = 0; i < ids.size(); ++i) {
        if (i > 0) std::cout << ", ";
        std::cout << ids[i];
    }
    std::cout << "]" << std::endl;
    
    // è§£ç 
    std::string decoded = tokenizer->decode(ids, true);
    std::cout << "Decoded: \"" << decoded << "\"" << std::endl;
    
    return 0;
}
```

**ç¼–è¯‘è¿è¡Œ**:
```bash
g++ -std=c++17 example.cpp -o example \
    -I/opt/homebrew/include \
    -L/opt/homebrew/lib \
    -lcllm_core -ltokenizers_cpp

./example
# è¾“å‡º:
# Token IDs: [151643, 9707, 11, 1879, 0, 151645]
# Decoded: "Hello, world!"
```

### ç¤ºä¾‹2: å¼ºåˆ¶ä½¿ç”¨HF Tokenizer

```cpp
#include "cllm/tokenizer/hf_tokenizer.h"

int main() {
    // âœ… å¼ºåˆ¶ä½¿ç”¨HuggingFaceæ ¼å¼
    auto tokenizer = cllm::TokenizerFactory::create(
        "/path/to/model",
        cllm::TokenizerFactory::Backend::HUGGINGFACE  // æ˜¾å¼æŒ‡å®š
    );
    
    // è·å–è¯è¡¨ä¿¡æ¯
    std::cout << "Vocab size: " << tokenizer->getVocabSize() << std::endl;
    std::cout << "BOS ID: " << tokenizer->getBosId() << std::endl;
    std::cout << "EOS ID: " << tokenizer->getEosId() << std::endl;
    
    // Token ID â†’ å­—ç¬¦ä¸²
    std::cout << "BOS Token: " << tokenizer->idToToken(tokenizer->getBosId()) << std::endl;
    
    return 0;
}
```

### ç¤ºä¾‹3: Chat Templateæ”¯æŒ

```cpp
#include "cllm/tokenizer/hf_tokenizer.h"

int main() {
    auto hfTokenizer = dynamic_cast<cllm::HFTokenizer*>(
        cllm::TokenizerFactory::create(
            "/path/to/Qwen3-0.6B",
            cllm::TokenizerFactory::Backend::HUGGINGFACE
        ).get()
    );
    
    // æ„é€ èŠå¤©æ¶ˆæ¯
    std::vector<cllm::ChatMessage> messages = {
        {"system", "You are a helpful assistant"},
        {"user", "What is the capital of France?"}
    };
    
    // åº”ç”¨Chat Templateå¹¶ç¼–ç 
    auto ids = hfTokenizer->applyChatTemplate(messages, true);
    
    std::cout << "Tokenized chat: " << ids.size() << " tokens" << std::endl;
    
    return 0;
}
```

### ç¤ºä¾‹4: æ‰¹é‡å¤„ç†

```cpp
#include "cllm/tokenizer/base_tokenizer.h"
#include <vector>
#include <chrono>

int main() {
    auto tokenizer = cllm::TokenizerFactory::create("/path/to/model");
    
    // å‡†å¤‡æ‰¹é‡æ–‡æœ¬
    std::vector<std::string> texts = {
        "Hello, world!",
        "How are you today?",
        "This is a test sentence.",
        "Machine learning is amazing!"
    };
    
    // æ‰¹é‡ç¼–ç  (å¹¶è¡Œå¤„ç†)
    auto start = std::chrono::high_resolution_clock::now();
    auto results = tokenizer->batchEncode(texts, true);
    auto end = std::chrono::high_resolution_clock::now();
    
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
    
    std::cout << "Encoded " << texts.size() << " texts in " 
              << duration.count() << " ms" << std::endl;
    
    // æ‰¹é‡è§£ç 
    auto decodedTexts = tokenizer->batchDecode(results, true);
    
    for (size_t i = 0; i < texts.size(); ++i) {
        std::cout << "Original: \"" << texts[i] << "\"" << std::endl;
        std::cout << "Decoded:  \"" << decodedTexts[i] << "\"" << std::endl;
        std::cout << "Tokens:   " << results[i].size() << std::endl << std::endl;
    }
    
    return 0;
}
```

### ç¤ºä¾‹5: å¢é‡è§£ç  (æµå¼ç”Ÿæˆ)

```cpp
#include "cllm/tokenizer/hf_tokenizer.h"
#include <iostream>

int main() {
    auto hfTokenizer = dynamic_cast<cllm::HFTokenizer*>(
        cllm::TokenizerFactory::create("/path/to/model").get()
    );
    
    // åˆ›å»ºå¢é‡è§£ç å™¨
    auto decoder = hfTokenizer->createIncrementalDecoder();
    
    // æ¨¡æ‹Ÿæµå¼ç”Ÿæˆ
    std::vector<cllm::token_id_t> generatedTokens = {
        151643,  // BOS
        9707,    // "Hello"
        11,      // ","
        1879,    // " world"
        0,       // "!"
        151645   // EOS
    };
    
    std::cout << "Streaming output: ";
    for (auto tokenId : generatedTokens) {
        std::string chunk = decoder->add(tokenId);
        if (!chunk.empty()) {
            std::cout << chunk << std::flush;  // å®æ—¶è¾“å‡º
        }
    }
    
    // å®Œæˆè§£ç 
    std::cout << decoder->finish() << std::endl;
    
    return 0;
}
```

---

## ğŸ”§ é…ç½®é€‰é¡¹

### CMakeé€‰é¡¹

```cmake
# å¯ç”¨HuggingFace tokenizersæ”¯æŒ
option(USE_TOKENIZERS_CPP "Use tokenizers-cpp" ON)

# å¼ºåˆ¶ä½¿ç”¨SentencePiece (åº”æ€¥å›æ»š)
option(FORCE_SENTENCEPIECE "Force SentencePiece backend" OFF)
```

### è¿è¡Œæ—¶é…ç½®

#### ç¯å¢ƒå˜é‡
```bash
# å¼ºåˆ¶ä½¿ç”¨ç‰¹å®šbackend
export CLLM_TOKENIZER_BACKEND=huggingface  # æˆ– sentencepiece

# è°ƒè¯•æ¨¡å¼
export CLLM_LOG_LEVEL=DEBUG
```

#### é…ç½®æ–‡ä»¶ (config/tokenizer.yaml)
```yaml
tokenizer:
  # backendé€‰æ‹©: auto | huggingface | sentencepiece | native
  backend: auto
  
  # æ¨¡å‹è·¯å¾„
  model_path: /path/to/model
  
  # ç¼“å­˜é…ç½®
  cache:
    enabled: true
    max_size: 10000  # LRUç¼“å­˜å¤§å°
    
  # æ€§èƒ½é…ç½®
  performance:
    enable_metrics: true
    batch_size: 32
```

---

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜1: tokenizers-cppæ‰¾ä¸åˆ°

**é”™è¯¯**:
```
CMake Error: Could not find tokenizers_cpp
```

**è§£å†³**:
```bash
# æ£€æŸ¥å®‰è£…ä½ç½®
ls /opt/homebrew/include/tokenizers_cpp.h
ls /usr/local/include/tokenizers_cpp.h

# å¦‚æœæ²¡æœ‰,é‡æ–°å®‰è£…
cd /tmp/tokenizers-cpp
rm -rf build && mkdir build && cd build
cmake .. -DCMAKE_INSTALL_PREFIX=/opt/homebrew
make -j8 && sudo make install

# æ¸…ç†CMakeç¼“å­˜
cd /path/to/cLLM/build
rm CMakeCache.txt
cmake .. -DUSE_TOKENIZERS_CPP=ON
```

### é—®é¢˜2: tokenizer.jsonæ‰¾ä¸åˆ°

**é”™è¯¯**:
```
[ERROR] tokenizer.json not found: /path/to/model/tokenizer.json
```

**è§£å†³**:
```bash
# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
ls -la /path/to/model/tokenizer.json

# å¦‚æœä¸å­˜åœ¨,ä¸‹è½½æ­£ç¡®çš„æ¨¡å‹
huggingface-cli download Qwen/Qwen3-0.6B --local-dir /path/to/model

# æˆ–ä»æœ¬åœ°æ¨¡å‹å¤åˆ¶
cp /path/to/correct/tokenizer.json /path/to/model/
```

### é—®é¢˜3: ç¼–ç ç»“æœä¸ºç©º

**ç—‡çŠ¶**:
```cpp
auto ids = tokenizer->encode("Hello");
// ids.size() == 0  (é”™è¯¯)
```

**è¯Šæ–­**:
```cpp
// æ£€æŸ¥tokenizeræ˜¯å¦åŠ è½½æˆåŠŸ
if (tokenizer->getVocabSize() == 0) {
    std::cerr << "Tokenizer not loaded properly!" << std::endl;
}

// æ£€æŸ¥ç‰¹æ®ŠToken
std::cout << "BOS: " << tokenizer->getBosId() << std::endl;
std::cout << "EOS: " << tokenizer->getEosId() << std::endl;
```

**è§£å†³**:
```cpp
// ç¡®ä¿æ­£ç¡®åŠ è½½
auto tokenizer = cllm::TokenizerFactory::create(modelPath);
if (!tokenizer) {
    throw std::runtime_error("Failed to create tokenizer");
}

// éªŒè¯åŠŸèƒ½
std::string testText = "test";
auto ids = tokenizer->encode(testText, false);  // ä¸æ·»åŠ ç‰¹æ®ŠToken
if (ids.empty()) {
    std::cerr << "Encode failed!" << std::endl;
}
```

### é—®é¢˜4: æ€§èƒ½ä¸è¾¾é¢„æœŸ

**ç—‡çŠ¶**: ç¼–ç é€Ÿåº¦ < 50 MB/s

**ä¼˜åŒ–æªæ–½**:
```cpp
// 1. å¯ç”¨ç¼“å­˜
tokenizer->enableCache(true);

// 2. ä½¿ç”¨æ‰¹å¤„ç†
std::vector<std::string> texts = {...};
auto results = tokenizer->batchEncode(texts, true);  // å¹¶è¡Œå¤„ç†

// 3. é¢„çƒ­tokenizer
tokenizer->encode("warmup", true);  // ç¬¬ä¸€æ¬¡è°ƒç”¨è¾ƒæ…¢

// 4. æ£€æŸ¥ç¼–è¯‘ä¼˜åŒ–
// CMakeLists.txtä¸­ç¡®ä¿:
# set(CMAKE_CXX_FLAGS_RELEASE "-O3 -march=native")
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†

### é¢„æœŸæ€§èƒ½æŒ‡æ ‡

| æ“ä½œ | æ–‡æœ¬é•¿åº¦ | é¢„æœŸé€Ÿåº¦ | å®æµ‹é€Ÿåº¦ |
|------|---------|---------|---------|
| ç¼–ç  (è‹±æ–‡) | 100å­—ç¬¦ | >100 MB/s | _å¾…æµ‹_ |
| ç¼–ç  (ä¸­æ–‡) | 100å­—ç¬¦ | >80 MB/s | _å¾…æµ‹_ |
| è§£ç  | 100 tokens | >50 MB/s | _å¾…æµ‹_ |
| æ‰¹å¤„ç† (x64) | 100å­—ç¬¦/ä¸ª | >200 MB/s | _å¾…æµ‹_ |

### åŸºå‡†æµ‹è¯•å‘½ä»¤

```bash
cd build

# è¿è¡ŒåŸºå‡†æµ‹è¯•
./bin/benchmark_tokenizers --benchmark_filter=HFTokenizer

# è¾“å‡ºç¤ºä¾‹:
# BM_HFTokenizer_Encode            1000000 ns/op
# BM_HFTokenizer_Decode            2000000 ns/op
# BM_HFTokenizer_BatchEncode/64     500000 ns/op
```

---

## ğŸ”„ ä»SentencePieceè¿ç§»

### è¿ç§»æ¸…å•

#### ä»£ç å±‚é¢

**æ—§ä»£ç ** (SentencePiece):
```cpp
#include "cllm/tokenizer/tokenizer.h"

auto tokenizer = std::make_unique<cllm::Tokenizer>(modelPath);
tokenizer->loadModel(modelPath + "/tokenizer.model");
auto ids = tokenizer->encode(text, true);
```

**æ–°ä»£ç ** (ç»Ÿä¸€æ¥å£):
```cpp
#include "cllm/tokenizer/base_tokenizer.h"

// âœ… è‡ªåŠ¨é€‰æ‹©æœ€ä½³backend (HFæˆ–SentencePiece)
auto tokenizer = cllm::TokenizerFactory::create(modelPath);
auto ids = tokenizer->encode(text, true);
```

#### æ¨¡å‹æ–‡ä»¶

**æ£€æŸ¥æ¨¡å‹ç›®å½•**:
```bash
ls /path/to/model/
# å¦‚æœæœ‰tokenizer.json â†’ è‡ªåŠ¨ä½¿ç”¨HF
# å¦‚æœæœ‰tokenizer.model â†’ è‡ªåŠ¨ä½¿ç”¨SentencePiece
```

**è½¬æ¢å·¥å…·** (å¦‚æœéœ€è¦):
```python
# ä»SentencePieceè½¬æ¢ä¸ºHFæ ¼å¼
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("/path/to/old/model")
tokenizer.save_pretrained("/path/to/new/model")
# ä¼šç”Ÿæˆtokenizer.json
```

#### é…ç½®æ–‡ä»¶

**æ—§é…ç½®**:
```yaml
tokenizer:
  type: sentencepiece
  model_file: tokenizer.model
```

**æ–°é…ç½®**:
```yaml
tokenizer:
  backend: auto  # è‡ªåŠ¨æ£€æµ‹
  model_path: /path/to/model
```

---

## ğŸ“š APIå‚è€ƒ

### BaseTokenizeræ ¸å¿ƒæ¥å£

```cpp
class BaseTokenizer {
public:
    // åŠ è½½æ¨¡å‹
    virtual bool load(const std::string& modelPath) = 0;
    
    // ç¼–ç /è§£ç 
    virtual TokenSequence encode(const std::string& text, bool addSpecialTokens = true) = 0;
    virtual std::string decode(const TokenSequence& ids, bool skipSpecialTokens = true) = 0;
    
    // æ‰¹å¤„ç†
    virtual std::vector<TokenSequence> batchEncode(
        const std::vector<std::string>& texts, 
        bool addSpecialTokens = true
    );
    
    virtual std::vector<std::string> batchDecode(
        const std::vector<TokenSequence>& sequences,
        bool skipSpecialTokens = true
    );
    
    // ä¿¡æ¯æŸ¥è¯¢
    virtual int getVocabSize() const = 0;
    virtual token_id_t getBosId() const;
    virtual token_id_t getEosId() const;
    virtual token_id_t getPadId() const;
    virtual token_id_t getUnkId() const;
    
    // Tokenè½¬æ¢
    virtual std::string idToToken(token_id_t id) const = 0;
    virtual token_id_t tokenToId(const std::string& token) const = 0;
    
    // å·¥å…·æ–¹æ³•
    virtual bool isSpecialToken(token_id_t id) const;
    virtual std::vector<std::string> tokenize(const std::string& text);
};
```

### TokenizerFactoryå·¥å‚æ–¹æ³•

```cpp
class TokenizerFactory {
public:
    enum class Backend {
        AUTO,           // è‡ªåŠ¨æ£€æµ‹ (æ¨è)
        HUGGINGFACE,    // å¼ºåˆ¶HF
        SENTENCEPIECE,  // å¼ºåˆ¶SentencePiece
        NATIVE          // è‡ªç ”å®ç°
    };
    
    // åˆ›å»ºtokenizer
    static std::unique_ptr<BaseTokenizer> create(
        const std::string& modelPath,
        Backend backend = Backend::AUTO,
        ModelType modelType = ModelType::AUTO
    );
};
```

### HFTokenizeræ‰©å±•åŠŸèƒ½

```cpp
class HFTokenizer : public BaseTokenizer {
public:
    // Chat Template
    TokenSequence applyChatTemplate(
        const std::vector<ChatMessage>& messages,
        bool addGenerationPrompt = false
    );
    
    // å¢é‡è§£ç 
    class IncrementalDecoder {
    public:
        std::string add(token_id_t tokenId);
        std::string finish();
        void reset();
    };
    
    std::unique_ptr<IncrementalDecoder> createIncrementalDecoder();
    
    // åˆ†è¯ (è¿”å›å­—ç¬¦ä¸²åˆ—è¡¨)
    std::vector<std::string> tokenize(const std::string& text);
    
    // ç‰¹æ®ŠTokenåˆ¤æ–­
    bool isSpecialToken(token_id_t tokenId) const;
};
```

---

## ğŸ“ è¿›é˜¶ä¸»é¢˜

### è‡ªå®šä¹‰é¢„å¤„ç†

```cpp
class MyTokenizer : public cllm::HFTokenizer {
public:
    TokenSequence encode(const std::string& text, bool addSpecialTokens) override {
        // è‡ªå®šä¹‰é¢„å¤„ç†
        std::string processed = preprocessText(text);
        
        // è°ƒç”¨çˆ¶ç±»æ–¹æ³•
        return HFTokenizer::encode(processed, addSpecialTokens);
    }
    
private:
    std::string preprocessText(const std::string& text) {
        // ä¾‹: ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        std::string result = text;
        // ... è‡ªå®šä¹‰é€»è¾‘
        return result;
    }
};
```

### æ€§èƒ½ç›‘æ§

```cpp
auto tokenizer = cllm::TokenizerFactory::create(modelPath);

// ç¼–ç å¤šæ¬¡
for (int i = 0; i < 1000; ++i) {
    tokenizer->encode("test text", true);
}

// è·å–æ€§èƒ½æŒ‡æ ‡
auto metrics = tokenizer->getMetrics();
std::cout << "Avg encode time: " << metrics.avgEncodeTime() << " ms" << std::endl;
std::cout << "Total encodes: " << metrics.encodeCount << std::endl;
```

### å¤šTokenizerå…±å­˜

```cpp
// åŒæ—¶ä½¿ç”¨HFå’ŒSentencePiece
auto hfTokenizer = cllm::TokenizerFactory::create(
    "/path/to/qwen",
    cllm::TokenizerFactory::Backend::HUGGINGFACE
);

auto spTokenizer = cllm::TokenizerFactory::create(
    "/path/to/llama",
    cllm::TokenizerFactory::Backend::SENTENCEPIECE
);

// æ ¹æ®ä»»åŠ¡é€‰æ‹©
auto ids1 = hfTokenizer->encode("Modern model text", true);
auto ids2 = spTokenizer->encode("Legacy model text", true);
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### âœ… æ¨èåšæ³•

1. **ä½¿ç”¨AUTO backend**
   ```cpp
   // âœ… æ¨è: è‡ªåŠ¨æ£€æµ‹
   auto tokenizer = TokenizerFactory::create(modelPath);
   ```

2. **å¯ç”¨ç¼“å­˜**
   ```cpp
   // å¯¹äºé‡å¤æ–‡æœ¬,å¯ç”¨ç¼“å­˜å¯æå‡10å€æ€§èƒ½
   tokenizer->enableCache(true);
   ```

3. **æ‰¹é‡å¤„ç†**
   ```cpp
   // âœ… æ¨è: æ‰¹é‡å¤„ç† (å¹¶è¡Œ)
   auto results = tokenizer->batchEncode(texts, true);
   
   // âŒ é¿å…: é€ä¸ªå¤„ç†
   for (const auto& text : texts) {
       auto ids = tokenizer->encode(text, true);  // ä¸²è¡Œ,æ…¢
   }
   ```

4. **å¼‚å¸¸å¤„ç†**
   ```cpp
   try {
       auto tokenizer = TokenizerFactory::create(modelPath);
       auto ids = tokenizer->encode(text, true);
   } catch (const std::exception& e) {
       std::cerr << "Error: " << e.what() << std::endl;
       // å›é€€æˆ–é”™è¯¯å¤„ç†
   }
   ```

### âŒ é¿å…çš„åšæ³•

1. **ç¡¬ç¼–ç backend**
   ```cpp
   // âŒ é¿å…: é™¤éæœ‰ç‰¹æ®Šéœ€æ±‚
   auto tokenizer = TokenizerFactory::create(
       modelPath, 
       TokenizerFactory::Backend::SENTENCEPIECE  // é™åˆ¶å…¼å®¹æ€§
   );
   ```

2. **é¢‘ç¹åˆ›å»ºtokenizer**
   ```cpp
   // âŒ é¿å…: æ¯æ¬¡éƒ½åˆ›å»ºæ–°å®ä¾‹
   for (int i = 0; i < 1000; ++i) {
       auto tokenizer = TokenizerFactory::create(modelPath);  // å¾ˆæ…¢!
       tokenizer->encode(texts[i], true);
   }
   
   // âœ… æ¨è: å¤ç”¨å®ä¾‹
   auto tokenizer = TokenizerFactory::create(modelPath);
   for (int i = 0; i < 1000; ++i) {
       tokenizer->encode(texts[i], true);
   }
   ```

3. **å¿½ç•¥é”™è¯¯å¤„ç†**
   ```cpp
   // âŒ é¿å…: ä¸æ£€æŸ¥è¿”å›å€¼
   auto ids = tokenizer->encode(text, true);
   // å¦‚æœidsä¸ºç©ºæ€ä¹ˆåŠ?
   
   // âœ… æ¨è: æ£€æŸ¥ç»“æœ
   auto ids = tokenizer->encode(text, true);
   if (ids.empty()) {
       std::cerr << "Encode failed!" << std::endl;
       return;
   }
   ```

---

## ğŸ†˜ è·å–å¸®åŠ©

### æ–‡æ¡£
- å®Œæ•´æŠ€æœ¯æ–¹æ¡ˆ: [hf_tokenizer_migration_strategy.md](./hf_tokenizer_migration_strategy.md)
- æ‰§è¡Œæ‘˜è¦: [tokenizer_migration_executive_summary.md](./tokenizer_migration_executive_summary.md)
- APIæ–‡æ¡£: [å¾…ç”Ÿæˆ]

### ç¤¾åŒºæ”¯æŒ
- GitHub Issues: [æäº¤é—®é¢˜]
- é‚®ä»¶åˆ—è¡¨: team@cllm-project.org
- Slacké¢‘é“: #tokenizer-support

### å¸¸è§é—®é¢˜
- FAQ: [docs/FAQ.md](../FAQ.md)
- æ•…éšœæ’æŸ¥: è§ä¸Šæ–‡"æ•…éšœæ’æŸ¥"ç« èŠ‚

---

## âœ… æ€»ç»“

æ­å–œ!ä½ å·²ç»æŒæ¡äº†HuggingFace Tokenizerçš„åŸºç¡€ä½¿ç”¨:

1. âœ… å®‰è£…tokenizers-cppä¾èµ–
2. âœ… ç¼–è¯‘cLLM (å¯ç”¨HFæ”¯æŒ)
3. âœ… è¿è¡Œç¬¬ä¸€ä¸ªç¤ºä¾‹
4. âœ… ç†è§£æ ¸å¿ƒAPI
5. âœ… äº†è§£æœ€ä½³å®è·µ

**ä¸‹ä¸€æ­¥**:
- å°è¯•åŠ è½½è‡ªå·±çš„æ¨¡å‹
- é›†æˆåˆ°HTTP Server
- æ€§èƒ½ä¼˜åŒ–ä¸è°ƒä¼˜
- è´¡çŒ®ä»£ç ä¸åé¦ˆ

**Happy Tokenizing! ğŸ‰**

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2026-01-11  
**ç»´æŠ¤è€…**: cLLM Core Team
