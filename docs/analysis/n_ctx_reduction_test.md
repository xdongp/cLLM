# n_ctx 减少测试报告

## 测试目的

测试减少 `n_ctx`（上下文长度）是否能缓解性能退化问题。

## 测试配置

### 测试1：原始配置（n_ctx=2048）
- **n_ctx**: 2048
- **测试方法**: 每次请求只生成1个token，连续10次请求
- **序列长度**: 始终为4（prompt）

### 测试2：减少配置（n_ctx=512）
- **n_ctx**: 512（减少75%）
- **测试方法**: 每次请求只生成1个token，连续10次请求
- **序列长度**: 始终为4（prompt）

## 测试结果

### n_ctx=2048 的性能

| 请求 | 响应时间 | Tokens/s | 性能下降 |
|------|---------|----------|----------|
| 1 | 0.06s | 16.66 | - |
| 2 | 0.08s | 12.07 | 33% |
| 3 | 0.12s | 8.44 | 100% |
| 4 | 0.13s | 7.55 | 117% |
| 5 | 0.15s | 6.83 | 150% |
| 6 | 0.25s | 4.01 | 317% |

### n_ctx=512 的性能

| 请求 | 响应时间 | Tokens/s | 性能下降 |
|------|---------|----------|----------|
| 1 | 0.05s | 19.70 | - |
| 2 | 0.08s | 12.30 | 60% |
| 3 | 0.13s | 7.64 | 160% |
| 4 | 0.16s | 6.33 | 220% |
| 5 | 0.15s | 6.61 | 200% |
| 6 | 0.27s | 3.71 | 440% |

## 分析

### 发现1：第一次请求稍快

- **n_ctx=2048**: 0.06秒
- **n_ctx=512**: 0.05秒
- **改善**: 约17%更快

**原因**：更小的内存分配，初始化更快。

### 发现2：性能退化更严重

- **n_ctx=2048**: 请求2下降33%，请求6下降317%
- **n_ctx=512**: 请求2下降60%，请求6下降440%

**原因**：
1. 更小的 `n_ctx` 意味着更少的内存插槽
2. 内存碎片化更快（可用空间更少）
3. llama.cpp 需要更频繁地整理内存

### 发现3：问题不在内存大小

**证据**：
- 即使减少75%的内存分配，性能退化仍然存在
- 性能退化甚至更严重

**结论**：
- 问题不在于内存大小
- 问题在于 llama.cpp 的内存管理机制（碎片化）

## 结论

### 减少 n_ctx 的效果

1. ✅ **第一次请求稍快**：更小的内存分配，初始化更快
2. ❌ **性能退化更严重**：更少的内存插槽导致碎片化更快
3. ❌ **不能缓解性能退化**：问题在于内存管理机制，而不是内存大小

### 根本原因

性能退化的根本原因：
- llama.cpp 的 `llama_memory_seq_rm` 不释放内存，只是标记为未使用
- 内存碎片累积，影响后续请求的性能
- 这是 llama.cpp 的固有特性，与内存大小无关

### 建议

1. **不建议减少 n_ctx**：
   - 虽然第一次请求稍快，但性能退化更严重
   - 限制了最大上下文长度

2. **其他方案**：
   - 定期重启服务器（每N个请求后）
   - 使用 GPU 加速
   - 等待 llama.cpp 改进内存管理

3. **如果必须减少 n_ctx**：
   - 确保 `n_ctx` 足够大，以容纳预期的序列长度
   - 监控性能退化情况
   - 考虑定期重启服务器

## 代码修改

测试代码修改位置：
- `src/inference/llama_cpp_backend.cpp:196`
- 临时设置 `testNctx = 512` 进行测试

**注意**：这是临时测试代码，生产环境应使用配置值。
