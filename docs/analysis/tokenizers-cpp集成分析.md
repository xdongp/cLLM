# tokenizers-cpp é›†æˆåˆ†æä¸è¡¥å…¨è®¡åˆ’

**åˆ†ææ—¥æœŸ**: 2026-01-11  
**å½“å‰çŠ¶æ€**: åŸºç¡€é›†æˆå®Œæˆï¼Œéœ€è¦è¡¥å……æµ‹è¯•å’ŒéªŒè¯

---

## ğŸ“Š å½“å‰é›†æˆçŠ¶æ€

### âœ… å·²å®Œæˆéƒ¨åˆ†

#### 1. CMake é…ç½® âœ…
- [x] `USE_TOKENIZERS_CPP` é€‰é¡¹ï¼ˆé»˜è®¤ONï¼‰
- [x] è‡ªåŠ¨æŸ¥æ‰¾ tokenizers-cpp å¤´æ–‡ä»¶å’Œåº“
- [x] ç¼–è¯‘å®šä¹‰ `USE_TOKENIZERS_CPP`
- [x] åº“é“¾æ¥é…ç½®
- [x] å›é€€æœºåˆ¶ï¼ˆæ‰¾ä¸åˆ°æ—¶é™çº§åˆ° Nativeï¼‰

**ä½ç½®**: `CMakeLists.txt` ç¬¬ 58-104 è¡Œ

#### 2. HFTokenizer å®ç° âœ…
- [x] å¤´æ–‡ä»¶å®šä¹‰ (`include/cllm/tokenizer/hf_tokenizer.h`)
- [x] å®Œæ•´å®ç° (`src/tokenizer/hf_tokenizer.cpp`)
- [x] æ ¸å¿ƒåŠŸèƒ½:
  - `load()` - åŠ è½½ tokenizer.json
  - `encode()` - æ–‡æœ¬ç¼–ç 
  - `decode()` - Token è§£ç 
  - `loadConfig()` - åŠ è½½ç‰¹æ®Š Token é…ç½®
  - ç‰¹æ®Š Token å¤„ç†

**ç‰¹æ€§**:
- âœ… æ¡ä»¶ç¼–è¯‘ (`#ifdef USE_TOKENIZERS_CPP`)
- âœ… å¼‚å¸¸å¤„ç†
- âœ… æ—¥å¿—è¾“å‡º
- âœ… ç‰¹æ®Š Token æ”¯æŒ

#### 3. TokenizerManager é›†æˆ âœ…
- [x] è‡ªåŠ¨æ£€æµ‹ tokenizer æ ¼å¼
- [x] HuggingFace ä¼˜å…ˆç­–ç•¥
- [x] å®ç°ç±»å‹é€‰æ‹© (AUTO/HF/NATIVE)
- [x] ç»Ÿä¸€æ¥å£å°è£…

**ä½ç½®**: `src/tokenizer/manager.cpp` ç¬¬ 80-134 è¡Œ

#### 4. å®‰è£…è„šæœ¬ âœ…
- [x] è·¨å¹³å°æ”¯æŒ (macOS/Linux)
- [x] Rust è‡ªåŠ¨å®‰è£…
- [x] tokenizers-cpp è‡ªåŠ¨ç¼–è¯‘å®‰è£…
- [x] å®‰è£…å‰æ£€æµ‹

**ä½ç½®**: `scripts/install_tokenizers_cpp.sh`

#### 5. æ–‡æ¡£ âœ…
- [x] å®‰è£…æŒ‡å—
- [x] æ•…éšœæ’æŸ¥
- [x] æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨

**ä½ç½®**: `docs/guides/Tokenizersåº“å®‰è£…æŒ‡å—.md`

---

## âš ï¸ å¾…è¡¥å……éƒ¨åˆ†

### 1. æµ‹è¯•ç”¨ä¾‹ âŒ (ä¼˜å…ˆçº§: ğŸ”´ é«˜)

**å½“å‰çŠ¶æ€**: 
- åªæœ‰åŸºç¡€çš„æ¥å£æµ‹è¯•
- ç¼ºå°‘ HFTokenizer çš„å®é™…æµ‹è¯•
- ç¼ºå°‘é›†æˆæµ‹è¯•

**éœ€è¦è¡¥å……**:

#### 1.1 HFTokenizer å•å…ƒæµ‹è¯•
```cpp
// tests/test_hf_tokenizer.cpp (æ–°å»º)
TEST(HFTokenizerTest, LoadTokenizerJson) {
    // æµ‹è¯•åŠ è½½ tokenizer.json
}

TEST(HFTokenizerTest, EncodeBasicText) {
    // æµ‹è¯•åŸºæœ¬ç¼–ç 
}

TEST(HFTokenizerTest, DecodeTokens) {
    // æµ‹è¯•è§£ç 
}

TEST(HFTokenizerTest, SpecialTokens) {
    // æµ‹è¯•ç‰¹æ®Š Token
}

TEST(HFTokenizerTest, ChineseText) {
    // æµ‹è¯•ä¸­æ–‡ç¼–ç 
}

TEST(HFTokenizerTest, MixedLanguage) {
    // æµ‹è¯•æ··åˆè¯­è¨€
}
```

#### 1.2 é›†æˆæµ‹è¯•
```cpp
// tests/test_tokenizer_integration.cpp (æ–°å»º)
TEST(TokenizerIntegrationTest, AutoDetection) {
    // æµ‹è¯•è‡ªåŠ¨æ£€æµ‹ HF vs Native
}

TEST(TokenizerIntegrationTest, FallbackMechanism) {
    // æµ‹è¯•å›é€€æœºåˆ¶
}

TEST(TokenizerIntegrationTest, PerformanceComparison) {
    // æ€§èƒ½å¯¹æ¯”æµ‹è¯•
}
```

---

### 2. ç¼–è¯‘éªŒè¯ âš ï¸ (ä¼˜å…ˆçº§: ğŸŸ¡ ä¸­)

**éœ€è¦éªŒè¯**:
- [ ] åœ¨æ²¡æœ‰ tokenizers-cpp çš„ç¯å¢ƒä¸‹ç¼–è¯‘ (å›é€€æœºåˆ¶)
- [ ] åœ¨æœ‰ tokenizers-cpp çš„ç¯å¢ƒä¸‹ç¼–è¯‘
- [ ] é“¾æ¥æ˜¯å¦æ­£ç¡®
- [ ] è¿è¡Œæ—¶åŠ è½½æ˜¯å¦æ­£å¸¸

---

### 3. é”™è¯¯å¤„ç†å¢å¼º âš ï¸ (ä¼˜å…ˆçº§: ğŸŸ¡ ä¸­)

**å½“å‰é—®é¢˜**:
- tokenizers-cpp API é”™è¯¯å¤„ç†ä¸å¤Ÿå®Œå–„
- ç¼ºå°‘è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯

**æ”¹è¿›ç‚¹**:

#### 3.1 å¢å¼º load() é”™è¯¯å¤„ç†
```cpp
bool HFTokenizer::load(const std::string& modelPath) {
    // æ·»åŠ æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
    if (!fs::exists(tokenizerJsonPath)) {
        CLLM_ERROR("tokenizer.json not found at: %s", tokenizerJsonPath.c_str());
        CLLM_ERROR("Please ensure the model directory contains:");
        CLLM_ERROR("  - tokenizer.json (required)");
        CLLM_ERROR("  - tokenizer_config.json (optional)");
        return false;
    }
    
    // æ·»åŠ æ–‡ä»¶æ ¼å¼éªŒè¯
    // ...
}
```

#### 3.2 å¢å¼º encode/decode é”™è¯¯å¤„ç†
```cpp
std::vector<int> HFTokenizer::encode(const std::string& text, bool addSpecialTokens) {
    if (text.empty()) {
        CLLM_WARN("Empty text provided to encode()");
        return {};
    }
    
    if (!tokenizer_) {
        CLLM_ERROR("Tokenizer not initialized. Call load() first.");
        return {};
    }
    
    try {
        // ç°æœ‰ä»£ç ...
    } catch (const std::exception& e) {
        CLLM_ERROR("Encode failed for text length %zu: %s", text.size(), e.what());
        CLLM_ERROR("Text preview: %s", text.substr(0, 100).c_str());
        return {};
    }
}
```

---

### 4. æ€§èƒ½ä¼˜åŒ– ğŸŸ¢ (ä¼˜å…ˆçº§: ä½)

**ä¼˜åŒ–ç‚¹**:
1. æ‰¹é‡ç¼–ç æ”¯æŒ
2. ç¼“å­˜ä¼˜åŒ–
3. å†…å­˜æ± 

```cpp
// æ‰¹é‡ç¼–ç æ¥å£
std::vector<std::vector<int>> HFTokenizer::encodeBatch(
    const std::vector<std::string>& texts,
    bool addSpecialTokens = true
);
```

---

### 5. æ–‡æ¡£è¡¥å…… âš ï¸ (ä¼˜å…ˆçº§: ğŸŸ¡ ä¸­)

**éœ€è¦è¡¥å……**:

#### 5.1 API æ–‡æ¡£
- HFTokenizer ç±»çš„å®Œæ•´ Doxygen æ³¨é‡Š
- ä½¿ç”¨ç¤ºä¾‹ä»£ç 
- å¸¸è§é—®é¢˜

#### 5.2 é›†æˆæŒ‡å—
- å¦‚ä½•åœ¨é¡¹ç›®ä¸­ä½¿ç”¨ HFTokenizer
- ä¸ NativeTokenizer çš„å¯¹æ¯”
- æ€§èƒ½å¯¹æ¯”æ•°æ®

---

## ğŸ”§ è¡¥å…¨è®¡åˆ’

### é˜¶æ®µ1: æµ‹è¯•ç”¨ä¾‹ (1-2å°æ—¶)

**ä»»åŠ¡**:
1. âœ… åˆ›å»º `tests/test_hf_tokenizer.cpp`
2. âœ… å®ç°æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•
3. âœ… å®ç°é›†æˆæµ‹è¯•
4. âœ… æ·»åŠ æµ‹è¯•åˆ° CMake

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡
- [ ] æµ‹è¯•è¦†ç›–ç‡ > 80%

---

### é˜¶æ®µ2: é”™è¯¯å¤„ç†å¢å¼º (30åˆ†é’Ÿ)

**ä»»åŠ¡**:
1. âœ… å¢å¼º load() é”™è¯¯ä¿¡æ¯
2. âœ… å¢å¼º encode/decode é”™è¯¯å¤„ç†
3. âœ… æ·»åŠ è¾“å…¥éªŒè¯

**éªŒæ”¶æ ‡å‡†**:
- [ ] é”™è¯¯ä¿¡æ¯æ¸…æ™°æ˜“æ‡‚
- [ ] åŒ…å«è§£å†³æ–¹æ¡ˆæç¤º

---

### é˜¶æ®µ3: ç¼–è¯‘éªŒè¯ (30åˆ†é’Ÿ)

**ä»»åŠ¡**:
1. âœ… æµ‹è¯•æœ‰/æ—  tokenizers-cpp ç¼–è¯‘
2. âœ… éªŒè¯å›é€€æœºåˆ¶
3. âœ… éªŒè¯é“¾æ¥å’Œè¿è¡Œ

**éªŒæ”¶æ ‡å‡†**:
- [ ] ä¸¤ç§æƒ…å†µéƒ½èƒ½æ­£å¸¸ç¼–è¯‘
- [ ] å›é€€æœºåˆ¶æ­£å¸¸å·¥ä½œ

---

### é˜¶æ®µ4: æ–‡æ¡£è¡¥å…… (30åˆ†é’Ÿ)

**ä»»åŠ¡**:
1. âœ… è¡¥å…… API æ–‡æ¡£
2. âœ… æ·»åŠ ä½¿ç”¨ç¤ºä¾‹
3. âœ… æ›´æ–°é›†æˆæŒ‡å—

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ–‡æ¡£å®Œæ•´æ¸…æ™°
- [ ] åŒ…å«å®é™…ç¤ºä¾‹

---

## ğŸ“‹ è¯¦ç»†ä»»åŠ¡æ¸…å•

### ä»»åŠ¡1: åˆ›å»º HFTokenizer å•å…ƒæµ‹è¯•

**æ–‡ä»¶**: `tests/test_hf_tokenizer.cpp`

```cpp
#include <gtest/gtest.h>
#include "cllm/tokenizer/hf_tokenizer.h"
#include <filesystem>
#include <fstream>

namespace cllm {
namespace test {

class HFTokenizerTest : public ::testing::Test {
protected:
    void SetUp() override {
        // åˆ›å»ºæµ‹è¯•ç›®å½•å’Œæ¨¡æ‹Ÿ tokenizer.json
        testDir_ = "./temp_hf_test";
        std::filesystem::create_directory(testDir_);
        
        // åˆ›å»ºç®€å•çš„ tokenizer.json (éœ€è¦çœŸå®çš„ tokenizer.json)
        // æˆ–è€…è·³è¿‡æµ‹è¯•å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨
    }
    
    void TearDown() override {
        if (std::filesystem::exists(testDir_)) {
            std::filesystem::remove_all(testDir_);
        }
    }
    
    std::string testDir_;
};

#ifdef USE_TOKENIZERS_CPP

TEST_F(HFTokenizerTest, LoadValidTokenizer) {
    // æµ‹è¯•åŠ è½½æœ‰æ•ˆçš„ tokenizer.json
}

TEST_F(HFTokenizerTest, LoadInvalidPath) {
    // æµ‹è¯•åŠ è½½æ— æ•ˆè·¯å¾„
    HFTokenizer tokenizer;
    EXPECT_FALSE(tokenizer.load("/nonexistent/path"));
}

TEST_F(HFTokenizerTest, EncodeEnglishText) {
    // æµ‹è¯•è‹±æ–‡ç¼–ç 
}

TEST_F(HFTokenizerTest, EncodeChineseText) {
    // æµ‹è¯•ä¸­æ–‡ç¼–ç 
}

TEST_F(HFTokenizerTest, DecodeTokens) {
    // æµ‹è¯•è§£ç 
}

TEST_F(HFTokenizerTest, SpecialTokens) {
    // æµ‹è¯•ç‰¹æ®Š Token
}

#else

TEST(HFTokenizerDisabledTest, RequiresCompileFlag) {
    // æµ‹è¯•æœªå¯ç”¨æ—¶çš„è¡Œä¸º
    GTEST_SKIP() << "USE_TOKENIZERS_CPP not enabled";
}

#endif

}  // namespace test
}  // namespace cllm
```

---

### ä»»åŠ¡2: å¢å¼ºé”™è¯¯å¤„ç†

**æ–‡ä»¶**: `src/tokenizer/hf_tokenizer.cpp`

åœ¨ç°æœ‰ä»£ç åŸºç¡€ä¸Šå¢å¼º:

1. **è¯¦ç»†çš„æ–‡ä»¶æ£€æŸ¥**
2. **è¾“å…¥éªŒè¯**
3. **æ›´å¥½çš„å¼‚å¸¸ä¿¡æ¯**
4. **æ¢å¤å»ºè®®**

---

### ä»»åŠ¡3: æ›´æ–° CMakeLists.txt

æ·»åŠ æ–°çš„æµ‹è¯•æ–‡ä»¶:

```cmake
if(BUILD_TESTS)
    # æ·»åŠ  HFTokenizer æµ‹è¯•
    add_executable(test_hf_tokenizer
        tests/test_hf_tokenizer.cpp
    )
    target_link_libraries(test_hf_tokenizer
        cllm_core
        gtest
        gtest_main
    )
    add_test(NAME HFTokenizerTest COMMAND test_hf_tokenizer)
endif()
```

---

### ä»»åŠ¡4: åˆ›å»ºç¤ºä¾‹ä»£ç 

**æ–‡ä»¶**: `examples/hf_tokenizer_example.cpp`

```cpp
#include "cllm/tokenizer/hf_tokenizer.h"
#include "cllm/common/logger.h"
#include <iostream>

int main(int argc, char** argv) {
    if (argc < 2) {
        std::cerr << "Usage: " << argv[0] << " <model_path>" << std::endl;
        return 1;
    }
    
    std::string modelPath = argv[1];
    
    // åˆ›å»º HFTokenizer
    cllm::HFTokenizer tokenizer;
    
    // åŠ è½½æ¨¡å‹
    if (!tokenizer.load(modelPath)) {
        std::cerr << "Failed to load tokenizer" << std::endl;
        return 1;
    }
    
    // ç¼–ç æµ‹è¯•
    std::string text = "Hello, ä¸–ç•Œï¼è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ã€‚";
    auto tokens = tokenizer.encode(text);
    
    std::cout << "Text: " << text << std::endl;
    std::cout << "Tokens (" << tokens.size() << "): ";
    for (auto id : tokens) {
        std::cout << id << " ";
    }
    std::cout << std::endl;
    
    // è§£ç æµ‹è¯•
    std::string decoded = tokenizer.decode(tokens);
    std::cout << "Decoded: " << decoded << std::endl;
    
    // Token ä¿¡æ¯
    std::cout << "Vocab size: " << tokenizer.getVocabSize() << std::endl;
    std::cout << "BOS ID: " << tokenizer.getBosId() << std::endl;
    std::cout << "EOS ID: " << tokenizer.getEosId() << std::endl;
    
    return 0;
}
```

---

## ğŸ¯ ä¼˜å…ˆçº§æ€»ç»“

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | é¢„è®¡æ—¶é—´ | çŠ¶æ€ |
|------|--------|----------|------|
| HFTokenizer å•å…ƒæµ‹è¯• | ğŸ”´ é«˜ | 1å°æ—¶ | â³ å¾…å®Œæˆ |
| é›†æˆæµ‹è¯• | ğŸ”´ é«˜ | 30åˆ†é’Ÿ | â³ å¾…å®Œæˆ |
| é”™è¯¯å¤„ç†å¢å¼º | ğŸŸ¡ ä¸­ | 30åˆ†é’Ÿ | â³ å¾…å®Œæˆ |
| ç¼–è¯‘éªŒè¯ | ğŸŸ¡ ä¸­ | 30åˆ†é’Ÿ | â³ å¾…å®Œæˆ |
| æ–‡æ¡£è¡¥å…… | ğŸŸ¡ ä¸­ | 30åˆ†é’Ÿ | â³ å¾…å®Œæˆ |
| ç¤ºä¾‹ä»£ç  | ğŸŸ¡ ä¸­ | 20åˆ†é’Ÿ | â³ å¾…å®Œæˆ |
| æ€§èƒ½ä¼˜åŒ– | ğŸŸ¢ ä½ | 1å°æ—¶ | ğŸ“… æœªæ¥ |

**æ€»è®¡**: çº¦ 3.5 å°æ—¶

---

## âœ… éªŒæ”¶æ ‡å‡†

### 1. ç¼–è¯‘æµ‹è¯•
- [ ] `cmake .. -DUSE_TOKENIZERS_CPP=ON` æˆåŠŸ
- [ ] `cmake .. -DUSE_TOKENIZERS_CPP=OFF` æˆåŠŸ
- [ ] æ— ç¼–è¯‘è­¦å‘Š

### 2. åŠŸèƒ½æµ‹è¯•
- [ ] æ‰€æœ‰å•å…ƒæµ‹è¯•é€šè¿‡
- [ ] é›†æˆæµ‹è¯•é€šè¿‡
- [ ] å®é™…æ¨¡å‹åŠ è½½æˆåŠŸ

### 3. æ–‡æ¡£æµ‹è¯•
- [ ] å®‰è£…è„šæœ¬å¯æ‰§è¡Œ
- [ ] æ–‡æ¡£ç¤ºä¾‹å¯è¿è¡Œ
- [ ] API æ–‡æ¡£å®Œæ•´

### 4. æ€§èƒ½æµ‹è¯•
- [ ] ç¼–ç é€Ÿåº¦ > 1000 tokens/s
- [ ] å†…å­˜å ç”¨åˆç†
- [ ] æ— å†…å­˜æ³„æ¼

---

## ğŸ“š å‚è€ƒèµ„æº

- **tokenizers-cpp GitHub**: https://github.com/mlc-ai/tokenizers-cpp
- **HuggingFace tokenizers**: https://github.com/huggingface/tokenizers
- **cLLM è®¾è®¡æ–‡æ¡£**: `docs/architecture/cLLMè¯¦ç»†è®¾è®¡.md`
- **Tokenizer æ¨¡å—è®¾è®¡**: `docs/modules/Tokenizeræ¨¡å—è®¾è®¡.md`

---

**åˆ†æå®Œæˆ**  
**ä¸‹ä¸€æ­¥**: å¼€å§‹å®æ–½é˜¶æ®µ1 - åˆ›å»ºæµ‹è¯•ç”¨ä¾‹
