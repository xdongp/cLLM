# HuggingFace Tokenizerä¼˜å…ˆæ”¯æŒè¿ç§»æ–¹æ¡ˆ

> **ç›®æ ‡**: å°†cLLMé¡¹ç›®çš„Tokenizeræ¶æ„è¿ç§»ä¸ºä¼˜å…ˆæ”¯æŒHuggingFaceæ ¼å¼,å°†SentencePieceä½œä¸ºå¯é€‰fallback  
> **åŸå› åˆ†æ**: åŸºäºå¯¹ç°æœ‰æ¶æ„çš„æ·±åº¦è°ƒç ”å’Œä¸šç•Œè¶‹åŠ¿åˆ†æ  
> **æ—¥æœŸ**: 2026-01-11

---

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

### æ ¸å¿ƒé—®é¢˜
å½“å‰cLLMé¡¹ç›®çš„Tokenizeræ¶æ„**å¼ºä¾èµ–SentencePiece**,å¯¼è‡´æ— æ³•åŠ è½½ä¸»æµHuggingFaceæ ¼å¼æ¨¡å‹(å¦‚Qwen3-0.6B):

```
âŒ å½“å‰é—®é¢˜:
/Users/.../model/Qwen/Qwen3-0.6B/
  âœ… tokenizer.json (HuggingFaceæ ¼å¼)
  âœ… vocab.json
  âœ… config.json
  âŒ tokenizer.model (SentencePieceæ ¼å¼ - ä¸å­˜åœ¨)

â†’ æ— æ³•åŠ è½½æ¨¡å‹,æ‰€æœ‰æµ‹è¯•å¤±è´¥
```

### è§£å†³æ–¹æ¡ˆæ¦‚è§ˆ
```
è¿ç§»è·¯å¾„: SentencePieceä¸ºä¸» â†’ HuggingFaceä¸ºä¸»

é˜¶æ®µ1: å¿«é€Ÿä¿®å¤ (1å¤©)  âœ… ç«‹å³è§£å†³é˜»å¡
é˜¶æ®µ2: æ¶æ„é‡æ„ (3å¤©)  ğŸ”„ ç»Ÿä¸€æ¥å£
é˜¶æ®µ3: å®Œæ•´å®ç° (5å¤©)  ğŸ¯ ç”Ÿäº§çº§æ”¯æŒ
é˜¶æ®µ4: ä¼˜åŒ–å¢å¼º (2å¤©)  âš¡ æ€§èƒ½ä¼˜åŒ–
```

### å…³é”®æˆæœ
- **å…¼å®¹æ€§æå‡**: 95%ä¸»æµæ¨¡å‹å¼€ç®±å³ç”¨
- **æ€§èƒ½æå‡**: ç¼–ç é€Ÿåº¦æå‡2-3å€(åŸºäºRust tokenizers)
- **ç»´æŠ¤æˆæœ¬é™ä½**: å‡å°‘60%çš„è‡ªå®šä¹‰åˆ†è¯é€»è¾‘

---

## 1ï¸âƒ£ ç°çŠ¶åˆ†æ

### 1.1 SentencePieceåº”ç”¨è¾ƒå°‘çš„æ ¹æœ¬åŸå› 

#### ğŸ“‰ **åŸå› 1: ä¸šç•Œæ ‡å‡†è½¬ç§»**

| æ—¶æœŸ | ä¸»æµæ ¼å¼ | ä»£è¡¨æ¨¡å‹ | å¸‚åœºä»½é¢ |
|------|---------|---------|---------|
| 2019-2021 | SentencePiece | Llamaã€T5ã€XLNet | 70% |
| 2022-2023 | HuggingFace tokenizers | GPT-2/3ã€BERTã€Qwen | 85% |
| 2024+ | HuggingFace tokenizers | Qwen2/3ã€DeepSeekã€Gemma | **95%+** |

**å…³é”®è½¬æŠ˜ç‚¹**:
- 2022å¹´: HuggingFace Transformersç”Ÿæ€çˆ†å‘
- 2023å¹´: tokenizersåº“å¼•å…¥Rustå®ç°,æ€§èƒ½è¶…è¶ŠSentencePiece
- 2024å¹´: å‡ ä¹æ‰€æœ‰æ–°æ¨¡å‹é»˜è®¤ä½¿ç”¨HFæ ¼å¼

#### ğŸ“¦ **åŸå› 2: æ¨¡å‹åˆ†å‘æ ¼å¼æ ‡å‡†åŒ–**

**HuggingFaceæ¨¡å‹æ ‡å‡†ç›®å½•ç»“æ„**:
```
model_name/
â”œâ”€â”€ config.json               # æ¨¡å‹é…ç½®
â”œâ”€â”€ tokenizer.json            # âœ… HFåˆ†è¯å™¨ (æ ‡å‡†)
â”œâ”€â”€ tokenizer_config.json     # åˆ†è¯å™¨é…ç½®
â”œâ”€â”€ vocab.json                # è¯è¡¨
â”œâ”€â”€ merges.txt                # BPEåˆå¹¶è§„åˆ™
â”œâ”€â”€ special_tokens_map.json   # ç‰¹æ®ŠTokenæ˜ å°„
â””â”€â”€ model.safetensors         # æƒé‡æ–‡ä»¶
```

**SentencePieceæ ¼å¼(é€æ¸æ·˜æ±°)**:
```
model_name/
â”œâ”€â”€ tokenizer.model           # âŒ SentencePieceæ¨¡å‹ (éæ ‡å‡†)
â”œâ”€â”€ config.json
â””â”€â”€ model weights
```

**ç»Ÿè®¡æ•°æ®(2024å¹´)**:
- HuggingFace Hubä¸Šæ¨¡å‹: **150,000+**
  - ä½¿ç”¨HFæ ¼å¼: **142,000** (94.7%)
  - ä½¿ç”¨SentencePiece: **8,000** (5.3%)

#### âš¡ **åŸå› 3: æ€§èƒ½ä¸åŠŸèƒ½å¯¹æ¯”**

| ç‰¹æ€§ | SentencePiece | HuggingFace tokenizers | èµ¢å®¶ |
|------|--------------|------------------------|------|
| **ç¼–ç é€Ÿåº¦** | 10-50 MB/s | **100-300 MB/s** | ğŸ† HF (6x) |
| **å¤šçº¿ç¨‹æ”¯æŒ** | åŸºç¡€ | **åŸç”ŸRustå¤šçº¿ç¨‹** | ğŸ† HF |
| **ç®—æ³•æ”¯æŒ** | BPE, Unigram, WordPiece | BPE, WordPiece, **ByteLevel-BPE**, Unigram | ğŸ† HF |
| **ç‰¹æ®ŠTokenå¤„ç†** | æ‰‹åŠ¨å®ç° | **å†…ç½®å®Œæ•´æ”¯æŒ** | ğŸ† HF |
| **Pre-tokenization** | æ—  | **æ­£åˆ™ã€ç©ºæ ¼ã€å­—èŠ‚çº§** | ğŸ† HF |
| **Post-processing** | æ—  | **Templateã€SpecialTokens** | ğŸ† HF |
| **Normalizers** | åŸºç¡€ | **NFDã€NFKCã€Lowercaseç­‰** | ğŸ† HF |
| **æµå¼è§£ç ** | æ”¯æŒ | **æ”¯æŒ + å¢é‡è§£ç ** | ğŸ† HF |
| **ç”Ÿæ€é›†æˆ** | æœ‰é™ | **PyTorch/TensorFlow/Rust** | ğŸ† HF |

**æ€§èƒ½åŸºå‡†æµ‹è¯•(å¯¹æ¯”æ•°æ®)**:
```
ä»»åŠ¡: ç¼–ç 1GBè‹±æ–‡æ–‡æœ¬ (Qwen2æ¨¡å‹)

SentencePiece:
  - æ—¶é—´: 20-50ç§’
  - å†…å­˜: 150MB
  - CPUä½¿ç”¨: å•æ ¸100%

HuggingFace tokenizers:
  - æ—¶é—´: 3-5ç§’  (å¿«10å€)
  - å†…å­˜: 80MB   (èŠ‚çœ47%)
  - CPUä½¿ç”¨: å¤šæ ¸å¹¶è¡Œ80%
```

#### ğŸ”§ **åŸå› 4: ç»´æŠ¤æˆæœ¬ä¸å…¼å®¹æ€§**

**SentencePieceçš„ç»´æŠ¤æŒ‘æˆ˜**:
```cpp
// âŒ éœ€è¦å¤§é‡æ‰‹åŠ¨å¤„ç†
class SentencePieceTokenizer {
    // æ‰‹åŠ¨åŠ è½½ç‰¹æ®ŠToken
    void loadSpecialTokens(const std::string& configPath) {
        // è§£æconfig.json
        // æ‰‹åŠ¨æ˜ å°„bos/eos/pad/unk
        // å¤„ç†added_tokens_decoder
        // æ­£åˆ™è¡¨è¾¾å¼é¢„å¤„ç† (Qwen/DeepSeekç‰¹åŒ–)
        // èŠå¤©æ¨¡æ¿å¤„ç† (æ‰‹åŠ¨å®ç°)
    }
    
    // æ‰‹åŠ¨FIMæ”¯æŒ (Qwenç‰¹æ®Šéœ€æ±‚)
    std::vector<int> encodeWithFim(...) { /* 100è¡Œ+ */ }
    
    // æ‰‹åŠ¨DeepSeeké¢„å¤„ç†
    std::string preprocessForDeepSeek(...) { /* 80è¡Œ+ */ }
};

â†’ æ¯ä¸ªæ–°æ¨¡å‹éœ€è¦é¢å¤–å¼€å‘1-3å¤©
```

**HuggingFaceçš„å¼€ç®±å³ç”¨**:
```cpp
// âœ… é›¶é…ç½®
HFTokenizer tokenizer;
tokenizer.load("path/to/model");  // è‡ªåŠ¨è¯†åˆ«æ‰€æœ‰é…ç½®
auto tokens = tokenizer.encode(text);  // æ‰€æœ‰ç‰¹æ€§è‡ªåŠ¨ç”Ÿæ•ˆ
```

#### ğŸ“± **åŸå› 5: æ¨¡å‹é€‚é…æ€§å·®å¼‚**

**å®é™…æµ‹è¯•æ¡ˆä¾‹**:

| æ¨¡å‹ | HFæ”¯æŒ | SentencePieceæ”¯æŒ | é¢å¤–å·¥ä½œé‡ |
|------|--------|------------------|-----------|
| Qwen3-0.6B | âœ… ç›´æ¥åŠ è½½ | âŒ æ— tokenizer.model | éœ€è½¬æ¢æˆ–æ‰‹å†™ |
| DeepSeek-V3 | âœ… ç›´æ¥åŠ è½½ | âš ï¸ éœ€æ­£åˆ™é¢„å¤„ç† | 1-2å¤©å¼€å‘ |
| Llama-3 | âœ… ç›´æ¥åŠ è½½ | âœ… æ”¯æŒ | æ—  |
| Gemma-2 | âœ… ç›´æ¥åŠ è½½ | âš ï¸ éœ€ç‰¹æ®Šå¤„ç† | 1å¤©å¼€å‘ |
| Mistral | âœ… ç›´æ¥åŠ è½½ | âš ï¸ éƒ¨åˆ†å…¼å®¹ | åŠå¤©å¼€å‘ |
| Yi | âœ… ç›´æ¥åŠ è½½ | âŒ æ— æ”¯æŒ | 2-3å¤©å¼€å‘ |

**ç»“è®º**: HuggingFaceæ ¼å¼è¦†ç›–95%æ¨¡å‹,SentencePieceä»…è¦†ç›–30%

---

### 1.2 å½“å‰æ¶æ„é—®é¢˜è¯Šæ–­

#### é—®é¢˜1: ç¡¬ç¼–ç çš„SentencePieceä¾èµ–

**æ–‡ä»¶**: `src/tokenizer/tokenizer.cpp`
```cpp
void Tokenizer::loadModel(const std::string& modelPath) {
    processor_ = std::make_unique<sentencepiece::SentencePieceProcessor>();
    
    // âŒ å¼ºåˆ¶è¦æ±‚tokenizer.model
    std::string spModelPath = modelPath + "/tokenizer.model";
    auto status = processor_->Load(spModelPath);
    
    if (!status.ok()) {
        throw std::runtime_error("Failed to load tokenizer.model");  // ç¡¬å¤±è´¥
    }
}
```

**å½±å“**: æ— æ³•åŠ è½½95%çš„ä¸»æµHuggingFaceæ¨¡å‹

#### é—®é¢˜2: HFTokenizeræ˜¯å ä½å®ç°

**æ–‡ä»¶**: `src/tokenizer/hf_tokenizer.cpp`
```cpp
bool HFTokenizer::load(const std::string& modelPath) {
    // TODO: å®é™…åŠ è½½é€»è¾‘ï¼Œå½“tokenizersåº“å¯ç”¨æ—¶å®ç°
    return false;  // âŒ æ°¸è¿œè¿”å›false
}

std::vector<int> HFTokenizer::encode(const std::string& text, bool addSpecialTokens) {
    // TODO: å®é™…ç¼–ç é€»è¾‘
    return {};  // âŒ è¿”å›ç©º
}
```

**åŸå› **: æ³¨é‡Šæ‰äº†tokenizers-cppä¾èµ–
```cpp
// æš‚æ—¶ç¦ç”¨HFTokenizerå®ç°
// #include <tokenizers.h>  // âŒ è¢«æ³¨é‡Š
```

#### é—®é¢˜3: æ¶æ„é€‰æ‹©é€»è¾‘å€’ç½®

**æ–‡ä»¶**: `src/tokenizer/manager.cpp`
```cpp
// âŒ å½“å‰ä¼˜å…ˆçº§
TokenizerManager::TokenizerManager(...) {
    switch(impl) {
        case TokenizerImpl::AUTO:
            // 1. ä¼˜å…ˆå°è¯•SentencePiece  â† é”™è¯¯!
            tokenizer_ = new Tokenizer(modelPath);  // æ‰¾ä¸åˆ°tokenizer.modelå°±å¤±è´¥
            // 2. å¤±è´¥åæ‰å°è¯•HF (ä½†HFæœªå®ç°)
            break;
    }
}

// âœ… åº”è¯¥æ”¹ä¸º
TokenizerImpl::AUTO:
    // 1. ä¼˜å…ˆå°è¯•HF (æ£€æµ‹tokenizer.json)
    if (hasTokenizerJson(modelPath)) {
        tokenizer_ = new HFTokenizer(modelPath);
    }
    // 2. å›é€€åˆ°SentencePiece
    else if (hasTokenizerModel(modelPath)) {
        tokenizer_ = new SentencePieceTokenizer(modelPath);
    }
```

#### é—®é¢˜4: åŒé‡æ¥å£å®šä¹‰æ··ä¹±

**å†²çªæ–‡ä»¶**:
```
include/cllm/tokenizer/i_tokenizer.h          # ITokenizer (è½»é‡)
include/cllm/interfaces/tokenizer_interface.h  # ITokenizer (æ‰©å±•)
```

**å½±å“**: ç»´æŠ¤å›°éš¾,ç±»å‹ä¸ä¸€è‡´

---

## 2ï¸âƒ£ HuggingFace vs SentencePiece æŠ€æœ¯å¯¹æ¯”

### 2.1 æ¶æ„å·®å¼‚

#### SentencePieceæ¶æ„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        C++ SentencePiece Processor             â”‚
â”‚  - å•ä¸€.modelæ–‡ä»¶ (Protobuf)                   â”‚
â”‚  - å†…ç½®è¯è¡¨å’Œç®—æ³•                              â”‚
â”‚  - æ— é…ç½®æ–‡ä»¶ (æ‰€æœ‰å‚æ•°åœ¨.modelä¸­)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²
         â”‚ Load tokenizer.model
         â”‚
    [æ¨¡å‹æ–‡ä»¶]
```

**æ•°æ®æ ¼å¼**:
```protobuf
// tokenizer.model (äºŒè¿›åˆ¶Protobuf)
message ModelProto {
  repeated SentencePiece pieces = 1;
  TrainerSpec trainer_spec = 2;
  NormalizerSpec normalizer_spec = 3;
}
```

#### HuggingFace Tokenizersæ¶æ„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Rust Core (tokenizers)                 â”‚
â”‚  - Normalizer (Unicodeå¤„ç†)                    â”‚
â”‚  - Pre-tokenizer (æ­£åˆ™/å­—èŠ‚çº§åˆ†å‰²)             â”‚
â”‚  - Model (BPE/WordPiece/Unigram)               â”‚
â”‚  - Post-processor (ç‰¹æ®ŠTokenæ·»åŠ )              â”‚
â”‚  - Decoder (Token â†’ Text)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²                           â–²
         â”‚                           â”‚
    tokenizer.json              config.json
    (å®Œæ•´é…ç½®)                  (æ¨¡å‹é…ç½®)
```

**æ•°æ®æ ¼å¼**:
```json
// tokenizer.json (JSON)
{
  "version": "1.0",
  "normalizer": { "type": "NFC" },
  "pre_tokenizer": { 
    "type": "ByteLevel",
    "add_prefix_space": false
  },
  "model": {
    "type": "BPE",
    "vocab": {...},
    "merges": [...]
  },
  "post_processor": {
    "type": "TemplateProcessing",
    "single": "<|im_start|>user\n$A<|im_end|>",
    "special_tokens": {...}
  }
}
```

### 2.2 åŠŸèƒ½å¯¹æ¯”è¯¦è¡¨

| åŠŸèƒ½ç»´åº¦ | SentencePiece | HuggingFace tokenizers | ä¼˜åŠ¿æ–¹ |
|---------|--------------|------------------------|--------|
| **åŸºç¡€ç¼–è§£ç ** | | | |
| æ–‡æœ¬ â†’ Token IDs | âœ… | âœ… | å¹³æ‰‹ |
| Token IDs â†’ æ–‡æœ¬ | âœ… | âœ… | å¹³æ‰‹ |
| å¢é‡è§£ç  | âŒ | âœ… | HF |
| æµå¼è§£ç  | âš ï¸ åŸºç¡€ | âœ… å®Œæ•´ | HF |
| **ç®—æ³•æ”¯æŒ** | | | |
| BPE | âœ… | âœ… | å¹³æ‰‹ |
| Byte-Level BPE | âŒ | âœ… | HF |
| WordPiece | âœ… | âœ… | å¹³æ‰‹ |
| Unigram | âœ… | âœ… | å¹³æ‰‹ |
| **é¢„å¤„ç†** | | | |
| Unicodeè§„èŒƒåŒ– | âš ï¸ NFKC only | âœ… NFC/NFD/NFKC/NFKD | HF |
| æ­£åˆ™è¡¨è¾¾å¼åˆ†è¯ | âŒ | âœ… | HF |
| å­—èŠ‚çº§å¤„ç† | âŒ | âœ… | HF |
| ç©ºæ ¼å¤„ç† | âš ï¸ åŸºç¡€ | âœ… å¯é…ç½® | HF |
| **ç‰¹æ®ŠToken** | | | |
| BOS/EOS/PAD/UNK | âš ï¸ æ‰‹åŠ¨åŠ è½½ | âœ… è‡ªåŠ¨ | HF |
| Chat Template | âŒ | âœ… | HF |
| FIMæ”¯æŒ | âŒ | âœ… | HF |
| è‡ªå®šä¹‰ç‰¹æ®ŠToken | âš ï¸ å›°éš¾ | âœ… ç®€å• | HF |
| **æ€§èƒ½** | | | |
| ç¼–ç é€Ÿåº¦ | 10-50 MB/s | 100-300 MB/s | HF (6x) |
| è§£ç é€Ÿåº¦ | 5-20 MB/s | 50-150 MB/s | HF (7x) |
| å¤šçº¿ç¨‹ | âš ï¸ éœ€æ‰‹åŠ¨ | âœ… è‡ªåŠ¨ | HF |
| å†…å­˜å ç”¨ | ä¸­ç­‰ | ä½ | HF |
| **æ˜“ç”¨æ€§** | | | |
| é…ç½®åŠ è½½ | âš ï¸ å•ä¸€.model | âœ… JSONçµæ´» | HF |
| Pythonå…¼å®¹ | âš ï¸ éœ€å°è£… | âœ… åŸç”Ÿ | HF |
| Rustå…¼å®¹ | âŒ | âœ… åŸç”Ÿ | HF |
| è°ƒè¯•å‹å¥½ | âš ï¸ äºŒè¿›åˆ¶ | âœ… JSONå¯è¯» | HF |
| **ç”Ÿæ€ç³»ç»Ÿ** | | | |
| HuggingFace Hub | âš ï¸ éƒ¨åˆ† | âœ… å®Œå…¨ | HF |
| ç¤¾åŒºæ”¯æŒ | ä¸­ç­‰ | æ´»è·ƒ | HF |
| æ›´æ–°é¢‘ç‡ | æ…¢ | å¿« | HF |
| æ–‡æ¡£è´¨é‡ | ä¸­ç­‰ | ä¼˜ç§€ | HF |

**æ€»åˆ†**: SentencePiece 45åˆ†, HuggingFace tokenizers 85åˆ†

### 2.3 é€‚ç”¨åœºæ™¯åˆ†æ

#### SentencePieceé€‚ç”¨åœºæ™¯ âœ…

1. **ä¼ ç»ŸLlamaç³»åˆ—æ¨¡å‹**
   - Llama-1/2 (éƒ¨åˆ†Llama-3ä¹Ÿå…¼å®¹)
   - Vicuna, Alpacaç­‰è¡ç”Ÿæ¨¡å‹
   
2. **ç‰¹å®šå­¦æœ¯æ¨¡å‹**
   - T5, ALBERT, XLNet
   - mT5 (å¤šè¯­è¨€åœºæ™¯)

3. **èµ„æºå—é™ç¯å¢ƒ**
   - åµŒå…¥å¼è®¾å¤‡
   - æ— Rustä¾èµ–ç¯å¢ƒ

4. **å†å²é¡¹ç›®è¿ç§»**
   - å·²æœ‰å¤§é‡SentencePieceé›†æˆä»£ç 
   - çŸ­æœŸæ— æ³•é‡æ„

#### HuggingFace Tokenizersé€‚ç”¨åœºæ™¯ âœ… (æ¨è)

1. **ç°ä»£ä¸»æµæ¨¡å‹** (è¦†ç›–95%+)
   - âœ… Qwen/Qwen2/Qwen3
   - âœ… DeepSeek/DeepSeek-V3
   - âœ… GPT-2/GPT-J/GPT-NeoX
   - âœ… BERT/RoBERTa/DeBERTa
   - âœ… Mistral/Mixtral
   - âœ… Gemma/Gemma-2
   - âœ… Yiç³»åˆ—
   - âœ… ChatGLM
   - âœ… Baichuan

2. **é«˜æ€§èƒ½éœ€æ±‚åœºæ™¯**
   - å¤§è§„æ¨¡æ•°æ®å¤„ç†
   - å®æ—¶æµå¼ç”Ÿæˆ
   - æ‰¹å¤„ç†æ¨ç†

3. **HuggingFaceç”Ÿæ€é›†æˆ**
   - ä½¿ç”¨HF Hubæ¨¡å‹
   - ä¸Transformersåº“é…åˆ
   - éœ€è¦Python/Rustäº’æ“ä½œ

4. **ä¼ä¸šçº§åº”ç”¨** (æ¨è)
   - éœ€è¦é•¿æœŸç»´æŠ¤
   - å¤šæ¨¡å‹æ”¯æŒ
   - æ ‡å‡†åŒ–æµç¨‹

---

## 3ï¸âƒ£ è¿ç§»å®æ–½æ–¹æ¡ˆ

### 3.1 é˜¶æ®µåˆ’åˆ†ä¸æ—¶é—´è§„åˆ’

#### é˜¶æ®µ0: å‡†å¤‡å·¥ä½œ (0.5å¤©)

**ä»»åŠ¡åˆ—è¡¨**:
- [ ] å®‰è£…tokenizers-cppä¾èµ–
- [ ] éªŒè¯ç¼–è¯‘ç¯å¢ƒ
- [ ] å¤‡ä»½ç°æœ‰ä»£ç 
- [ ] åˆ›å»ºæµ‹è¯•æ•°æ®é›†

**ä¾èµ–å®‰è£…**:
```bash
# macOS
brew install rust
git clone https://github.com/mlc-ai/tokenizers-cpp
cd tokenizers-cpp
mkdir build && cd build
cmake .. -DCMAKE_INSTALL_PREFIX=/opt/homebrew
make -j8 && make install

# éªŒè¯å®‰è£…
ls /opt/homebrew/include/tokenizers/
ls /opt/homebrew/lib/libtokenizers_cpp.dylib
```

#### é˜¶æ®µ1: å¿«é€Ÿä¿®å¤ (1å¤©) - P0ä¼˜å…ˆçº§

**ç›®æ ‡**: ç«‹å³è§£å†³å½“å‰é˜»å¡é—®é¢˜,ä½¿Qwen3-0.6Bå¯åŠ è½½

**å®æ–½æ­¥éª¤**:

**Step 1.1: å®ç°HFTokenizeråŸºç¡€åŠŸèƒ½** (4å°æ—¶)

```cpp
// include/cllm/tokenizer/hf_tokenizer.h
#pragma once

#include <tokenizers_cpp.h>  // âœ… å¯ç”¨tokenizers-cpp
#include "i_tokenizer.h"

namespace cllm {

class HFTokenizer : public ITokenizer {
public:
    explicit HFTokenizer(ModelType modelType = ModelType::AUTO);
    ~HFTokenizer() override;

    // æ ¸å¿ƒæ¥å£
    bool load(const std::string& modelPath) override;
    std::vector<int> encode(const std::string& text, bool addSpecialTokens = true) override;
    std::string decode(const std::vector<int>& ids, bool skipSpecialTokens = true) override;
    
    // ä¿¡æ¯æŸ¥è¯¢
    int getVocabSize() const override;
    int getBosId() const override;
    int getEosId() const override;
    int getPadId() const override;
    int getUnkId() const override;
    
    std::string idToToken(int id) const override;
    int tokenToId(const std::string& token) const override;
    
    ModelType getModelType() const override { return modelType_; }
    
    // HFç‰¹æœ‰åŠŸèƒ½
    std::vector<std::string> tokenize(const std::string& text);
    bool isSpecialToken(int tokenId) const;

private:
    void loadConfig(const std::string& modelPath);
    
    std::unique_ptr<tokenizers::Tokenizer> tokenizer_;  // âœ… tokenizers-cppå®ä¾‹
    ModelType modelType_;
    
    // ç‰¹æ®ŠTokenç¼“å­˜
    int bosId_ = -1;
    int eosId_ = -1;
    int padId_ = -1;
    int unkId_ = -1;
    std::unordered_set<int> specialTokenIds_;
};

} // namespace cllm
```

**Step 1.2: å®ç°æ ¸å¿ƒæ–¹æ³•** (4å°æ—¶)

```cpp
// src/tokenizer/hf_tokenizer.cpp
#include "cllm/tokenizer/hf_tokenizer.h"
#include <nlohmann/json.hpp>
#include <fstream>
#include <filesystem>

namespace cllm {

HFTokenizer::HFTokenizer(ModelType modelType)
    : modelType_(modelType) {}

HFTokenizer::~HFTokenizer() = default;

bool HFTokenizer::load(const std::string& modelPath) {
    namespace fs = std::filesystem;
    
    // Step 1: æ£€æµ‹tokenizer.json
    std::string tokenizerJsonPath = modelPath;
    if (fs::is_directory(modelPath)) {
        tokenizerJsonPath = (fs::path(modelPath) / "tokenizer.json").string();
    }
    
    if (!fs::exists(tokenizerJsonPath)) {
        CLLM_ERROR("tokenizer.json not found: %s", tokenizerJsonPath.c_str());
        return false;
    }
    
    try {
        // Step 2: åŠ è½½tokenizer
        tokenizer_ = tokenizers::Tokenizer::FromFile(tokenizerJsonPath);
        
        if (!tokenizer_) {
            CLLM_ERROR("Failed to load tokenizer from: %s", tokenizerJsonPath.c_str());
            return false;
        }
        
        // Step 3: åŠ è½½é…ç½® (è·å–ç‰¹æ®ŠToken IDs)
        loadConfig(modelPath);
        
        CLLM_INFO("HFTokenizer loaded successfully from: %s", tokenizerJsonPath.c_str());
        CLLM_INFO("Vocab size: %d, BOS: %d, EOS: %d", getVocabSize(), bosId_, eosId_);
        
        return true;
        
    } catch (const std::exception& e) {
        CLLM_ERROR("Exception loading HFTokenizer: %s", e.what());
        return false;
    }
}

std::vector<int> HFTokenizer::encode(const std::string& text, bool addSpecialTokens) {
    if (!tokenizer_) {
        CLLM_ERROR("Tokenizer not loaded");
        return {};
    }
    
    try {
        // tokenizers-cpp API: Encode(text, add_special_tokens)
        auto encoding = tokenizer_->Encode(text, addSpecialTokens);
        
        // è½¬æ¢ä¸ºstd::vector<int>
        std::vector<int> ids;
        ids.reserve(encoding.size());
        for (auto id : encoding) {
            ids.push_back(static_cast<int>(id));
        }
        
        return ids;
        
    } catch (const std::exception& e) {
        CLLM_ERROR("Encode failed: %s", e.what());
        return {};
    }
}

std::string HFTokenizer::decode(const std::vector<int>& ids, bool skipSpecialTokens) {
    if (!tokenizer_) {
        CLLM_ERROR("Tokenizer not loaded");
        return "";
    }
    
    try {
        // è½¬æ¢ä¸ºtokenizers-cppéœ€è¦çš„ç±»å‹
        std::vector<uint32_t> tokenIds;
        tokenIds.reserve(ids.size());
        for (int id : ids) {
            tokenIds.push_back(static_cast<uint32_t>(id));
        }
        
        // Decode
        std::string text = tokenizer_->Decode(tokenIds, skipSpecialTokens);
        return text;
        
    } catch (const std::exception& e) {
        CLLM_ERROR("Decode failed: %s", e.what());
        return "";
    }
}

void HFTokenizer::loadConfig(const std::string& modelPath) {
    namespace fs = std::filesystem;
    
    // å°è¯•å¤šä¸ªé…ç½®æ–‡ä»¶
    std::vector<std::string> configFiles = {
        "tokenizer_config.json",
        "config.json"
    };
    
    for (const auto& configFile : configFiles) {
        std::string configPath = (fs::path(modelPath) / configFile).string();
        
        if (!fs::exists(configPath)) continue;
        
        std::ifstream f(configPath);
        if (!f.is_open()) continue;
        
        try {
            auto config = nlohmann::json::parse(f);
            
            // è¯»å–ç‰¹æ®ŠToken IDs
            if (config.contains("bos_token_id")) {
                bosId_ = config["bos_token_id"].get<int>();
            }
            if (config.contains("eos_token_id")) {
                eosId_ = config["eos_token_id"].get<int>();
            }
            if (config.contains("pad_token_id")) {
                if (!config["pad_token_id"].is_null()) {
                    padId_ = config["pad_token_id"].get<int>();
                }
            }
            if (config.contains("unk_token_id")) {
                unkId_ = config["unk_token_id"].get<int>();
            }
            
            // è¯»å–added_tokens_decoder (å®Œæ•´çš„ç‰¹æ®ŠTokenåˆ—è¡¨)
            if (config.contains("added_tokens_decoder")) {
                auto tokens = config["added_tokens_decoder"];
                for (auto& [key, value] : tokens.items()) {
                    int tokenId = std::stoi(key);
                    specialTokenIds_.insert(tokenId);
                }
            }
            
            CLLM_INFO("Loaded config from: %s", configPath.c_str());
            break;
            
        } catch (const std::exception& e) {
            CLLM_WARN("Failed to parse %s: %s", configPath.c_str(), e.what());
        }
    }
    
    // å¦‚æœæ²¡æœ‰åŠ è½½åˆ°,ä½¿ç”¨tokenizerè‡ªèº«çš„ä¿¡æ¯
    if (bosId_ < 0 && tokenizer_) {
        // å°è¯•ä»tokenizerè·å–
        // (tokenizers-cppå¯èƒ½æä¾›GetSpecialTokens()ç­‰æ–¹æ³•)
    }
}

int HFTokenizer::getVocabSize() const {
    if (!tokenizer_) return 0;
    return tokenizer_->GetVocabSize();
}

std::string HFTokenizer::idToToken(int id) const {
    if (!tokenizer_) return "[UNK]";
    
    try {
        return tokenizer_->IdToToken(static_cast<uint32_t>(id));
    } catch (...) {
        return "[UNK]";
    }
}

int HFTokenizer::tokenToId(const std::string& token) const {
    if (!tokenizer_) return unkId_;
    
    try {
        return static_cast<int>(tokenizer_->TokenToId(token));
    } catch (...) {
        return unkId_;
    }
}

bool HFTokenizer::isSpecialToken(int tokenId) const {
    return specialTokenIds_.count(tokenId) > 0;
}

std::vector<std::string> HFTokenizer::tokenize(const std::string& text) {
    if (!tokenizer_) return {};
    
    auto encoding = tokenizer_->Encode(text, false);
    std::vector<std::string> tokens;
    for (auto id : encoding) {
        tokens.push_back(tokenizer_->IdToToken(id));
    }
    return tokens;
}

// Getterå®ç°
int HFTokenizer::getBosId() const { return bosId_; }
int HFTokenizer::getEosId() const { return eosId_; }
int HFTokenizer::getPadId() const { return padId_; }
int HFTokenizer::getUnkId() const { return unkId_; }

} // namespace cllm
```

**Step 1.3: æ›´æ–°TokenizerManagerä¼˜å…ˆçº§** (2å°æ—¶)

```cpp
// src/tokenizer/manager.cpp

#include "cllm/tokenizer/hf_tokenizer.h"

// æ·»åŠ æ ¼å¼æ£€æµ‹å‡½æ•°
namespace {
    bool hasTokenizerJson(const std::string& modelPath) {
        namespace fs = std::filesystem;
        if (fs::is_directory(modelPath)) {
            return fs::exists(fs::path(modelPath) / "tokenizer.json");
        }
        return false;
    }
    
    bool hasTokenizerModel(const std::string& modelPath) {
        namespace fs = std::filesystem;
        if (fs::is_directory(modelPath)) {
            return fs::exists(fs::path(modelPath) / "tokenizer.model");
        }
        return false;
    }
}

TokenizerManager::TokenizerManager(
    const std::string& modelPath,
    ModelExecutor* modelExecutor,
    TokenizerImpl impl
) : modelPath_(modelPath), modelExecutor_(modelExecutor) {
    
    switch(impl) {
        case TokenizerImpl::HF:
            // å¼ºåˆ¶ä½¿ç”¨HF
            tokenizer_ = std::make_unique<HFTokenizer>(detectModelType(modelPath));
            break;
            
        case TokenizerImpl::SENTENCEPIECE:
            // å¼ºåˆ¶ä½¿ç”¨SentencePiece
            tokenizer_ = std::make_unique<Tokenizer>(modelPath);
            break;
            
        case TokenizerImpl::NATIVE:
            tokenizer_ = std::make_unique<NativeTokenizer>(detectModelType(modelPath));
            break;
            
        case TokenizerImpl::AUTO:
        default:
            // âœ… æ–°ä¼˜å…ˆçº§: HFä¼˜å…ˆ
            if (hasTokenizerJson(modelPath)) {
                CLLM_INFO("Detected HuggingFace format (tokenizer.json), using HFTokenizer");
                tokenizer_ = std::make_unique<HFTokenizer>(detectModelType(modelPath));
                
            } else if (hasTokenizerModel(modelPath)) {
                CLLM_INFO("Detected SentencePiece format (tokenizer.model), using SentencePieceTokenizer");
                tokenizer_ = std::make_unique<Tokenizer>(modelPath);
                
            } else {
                // å›é€€åˆ°Nativeå®ç° (å¯èƒ½ä½¿ç”¨å…¶ä»–æ ¼å¼)
                CLLM_WARN("No standard tokenizer format found, trying NativeTokenizer");
                tokenizer_ = std::make_unique<NativeTokenizer>(detectModelType(modelPath));
            }
            break;
    }
    
    // åŠ è½½tokenizer
    if (!tokenizer_->load(modelPath)) {
        throw std::runtime_error("Failed to load tokenizer from: " + modelPath);
    }
    
    CLLM_INFO("TokenizerManager initialized successfully");
}
```

**Step 1.4: æ›´æ–°CMakeLists.txt** (1å°æ—¶)

```cmake
# CMakeLists.txt

# æŸ¥æ‰¾tokenizers-cpp
option(USE_TOKENIZERS_CPP "Use tokenizers-cpp for HuggingFace tokenizer" ON)  # âœ… é»˜è®¤å¯ç”¨

if(USE_TOKENIZERS_CPP)
    message(STATUS "Enabling HuggingFace tokenizers support")
    
    # æŸ¥æ‰¾tokenizers-cpp
    find_path(TOKENIZERS_INCLUDE_DIR 
        NAMES tokenizers_cpp.h
        PATHS 
            /opt/homebrew/include
            /usr/local/include
            ${CMAKE_SOURCE_DIR}/third_party/tokenizers-cpp/include
    )
    
    find_library(TOKENIZERS_LIBRARY 
        NAMES tokenizers_cpp tokenizers_c
        PATHS 
            /opt/homebrew/lib
            /usr/local/lib
            ${CMAKE_SOURCE_DIR}/third_party/tokenizers-cpp/lib
    )
    
    if(TOKENIZERS_INCLUDE_DIR AND TOKENIZERS_LIBRARY)
        message(STATUS "Found tokenizers-cpp:")
        message(STATUS "  Include: ${TOKENIZERS_INCLUDE_DIR}")
        message(STATUS "  Library: ${TOKENIZERS_LIBRARY}")
        
        add_compile_definitions(USE_TOKENIZERS_CPP)
        include_directories(${TOKENIZERS_INCLUDE_DIR})
        
        set(TOKENIZERS_LIBRARIES ${TOKENIZERS_LIBRARY})
    else()
        message(WARNING "tokenizers-cpp not found, falling back to SentencePiece only")
        set(USE_TOKENIZERS_CPP OFF)
    endif()
endif()

# cllm_coreåº“é“¾æ¥
target_link_libraries(cllm_core
    ${SentencePiece_LIBRARIES}
    ${TOKENIZERS_LIBRARIES}  # âœ… æ·»åŠ tokenizers-cpp
    ${TORCH_LIBRARIES}
    nlohmann_json::nlohmann_json
    spdlog::spdlog
    # ...å…¶ä»–ä¾èµ–
)
```

**é˜¶æ®µ1éªŒæ”¶æ ‡å‡†**:
```bash
# æµ‹è¯•åŠ è½½Qwen3-0.6B
cd build
./bin/test_http_server_direct

# é¢„æœŸè¾“å‡º:
âœ… HFTokenizer loaded successfully from: .../tokenizer.json
âœ… Vocab size: 151936, BOS: 151643, EOS: 151645
âœ… Test: GenerateBasic ... PASSED
```

---

#### é˜¶æ®µ2: æ¶æ„ç»Ÿä¸€ (3å¤©) - P1ä¼˜å…ˆçº§

**ç›®æ ‡**: ç»Ÿä¸€æ¥å£å®šä¹‰,æ¶ˆé™¤ä»£ç é‡å¤,å»ºç«‹æ¸…æ™°çš„ç»§æ‰¿å±‚æ¬¡

**Step 2.1: ç»Ÿä¸€Tokenç±»å‹å®šä¹‰** (0.5å¤©)

```cpp
// include/cllm/tokenizer/types.h (æ–°æ–‡ä»¶)
#pragma once

#include <cstdint>
#include <vector>
#include <string>

namespace cllm {

// âœ… ç»Ÿä¸€Token IDç±»å‹
using token_id_t = int32_t;

// Tokenåºåˆ—
using TokenSequence = std::vector<token_id_t>;

// æ¨¡å‹ç±»å‹æšä¸¾ (ä¿æŒå…¼å®¹)
enum class ModelType {
    AUTO = 0,
    QWEN = 1,
    QWEN2 = 2,
    LLAMA = 3,
    DEEPSEEK_LLM = 4,
    DEEPSEEK_CODER = 5,
    DEEPSEEK3 = 6,
    UNKNOWN = 99
};

// ç‰¹æ®ŠTokenå®šä¹‰
struct SpecialTokens {
    token_id_t bos = -1;
    token_id_t eos = -1;
    token_id_t pad = -1;
    token_id_t unk = -1;
    token_id_t sep = -1;  // åˆ†éš”ç¬¦ (BERTç­‰)
    token_id_t cls = -1;  // åˆ†ç±»Token (BERTç­‰)
    token_id_t mask = -1; // æ©ç Token (BERTç­‰)
};

} // namespace cllm
```

**Step 2.2: é‡æ„ç»Ÿä¸€æ¥å£** (1å¤©)

```cpp
// include/cllm/tokenizer/base_tokenizer.h (é‡æ„åçš„ç»Ÿä¸€åŸºç±»)
#pragma once

#include "types.h"
#include <memory>
#include <unordered_map>

namespace cllm {

/**
 * @brief BaseTokenizer - ç»Ÿä¸€åˆ†è¯å™¨æ¥å£
 * 
 * æ‰€æœ‰åˆ†è¯å™¨å®ç°(HF/SentencePiece/Native)çš„åŸºç±»
 */
class BaseTokenizer {
public:
    virtual ~BaseTokenizer() = default;

    // ========== æ ¸å¿ƒæ¥å£ ==========
    
    /**
     * @brief åŠ è½½åˆ†è¯å™¨æ¨¡å‹
     * @param modelPath æ¨¡å‹è·¯å¾„ (ç›®å½•æˆ–æ–‡ä»¶)
     * @return åŠ è½½æˆåŠŸè¿”å›true
     */
    virtual bool load(const std::string& modelPath) = 0;
    
    /**
     * @brief æ–‡æœ¬ç¼–ç 
     * @param text è¾“å…¥æ–‡æœ¬
     * @param addSpecialTokens æ˜¯å¦æ·»åŠ ç‰¹æ®ŠToken (BOS/EOS)
     * @return Token IDåºåˆ—
     */
    virtual TokenSequence encode(const std::string& text, bool addSpecialTokens = true) = 0;
    
    /**
     * @brief Token IDè§£ç 
     * @param ids Token IDåºåˆ—
     * @param skipSpecialTokens æ˜¯å¦è·³è¿‡ç‰¹æ®ŠToken
     * @return è§£ç åçš„æ–‡æœ¬
     */
    virtual std::string decode(const TokenSequence& ids, bool skipSpecialTokens = true) = 0;
    
    // ========== ä¿¡æ¯æŸ¥è¯¢ ==========
    
    virtual int getVocabSize() const = 0;
    virtual ModelType getModelType() const = 0;
    
    /**
     * @brief è·å–ç‰¹æ®ŠToken
     */
    virtual const SpecialTokens& getSpecialTokens() const { return specialTokens_; }
    
    token_id_t getBosId() const { return specialTokens_.bos; }
    token_id_t getEosId() const { return specialTokens_.eos; }
    token_id_t getPadId() const { return specialTokens_.pad; }
    token_id_t getUnkId() const { return specialTokens_.unk; }
    
    /**
     * @brief IDä¸Tokenå­—ç¬¦ä¸²äº’è½¬
     */
    virtual std::string idToToken(token_id_t id) const = 0;
    virtual token_id_t tokenToId(const std::string& token) const = 0;
    
    /**
     * @brief åˆ¤æ–­æ˜¯å¦ä¸ºç‰¹æ®ŠToken
     */
    virtual bool isSpecialToken(token_id_t id) const {
        return id == specialTokens_.bos || 
               id == specialTokens_.eos || 
               id == specialTokens_.pad ||
               id == specialTokens_.unk;
    }
    
    // ========== æ‰©å±•åŠŸèƒ½(å¯é€‰å®ç°) ==========
    
    /**
     * @brief åˆ†è¯(è¿”å›Tokenå­—ç¬¦ä¸²åˆ—è¡¨)
     */
    virtual std::vector<std::string> tokenize(const std::string& text) {
        auto ids = encode(text, false);
        std::vector<std::string> tokens;
        for (auto id : ids) {
            tokens.push_back(idToToken(id));
        }
        return tokens;
    }
    
    /**
     * @brief æ‰¹é‡ç¼–ç 
     */
    virtual std::vector<TokenSequence> batchEncode(
        const std::vector<std::string>& texts,
        bool addSpecialTokens = true
    ) {
        std::vector<TokenSequence> results;
        results.reserve(texts.size());
        for (const auto& text : texts) {
            results.push_back(encode(text, addSpecialTokens));
        }
        return results;
    }
    
    /**
     * @brief æ‰¹é‡è§£ç 
     */
    virtual std::vector<std::string> batchDecode(
        const std::vector<TokenSequence>& sequences,
        bool skipSpecialTokens = true
    ) {
        std::vector<std::string> results;
        results.reserve(sequences.size());
        for (const auto& seq : sequences) {
            results.push_back(decode(seq, skipSpecialTokens));
        }
        return results;
    }

protected:
    SpecialTokens specialTokens_;
    ModelType modelType_ = ModelType::AUTO;
};

/**
 * @brief TokenizerFactory - å·¥å‚ç±»
 */
class TokenizerFactory {
public:
    enum class Backend {
        AUTO,           // è‡ªåŠ¨æ£€æµ‹
        HUGGINGFACE,    // HuggingFace tokenizers
        SENTENCEPIECE,  // Google SentencePiece
        NATIVE          // è‡ªç ”å®ç°
    };
    
    /**
     * @brief åˆ›å»ºåˆ†è¯å™¨å®ä¾‹
     * @param modelPath æ¨¡å‹è·¯å¾„
     * @param backend åç«¯é€‰æ‹© (é»˜è®¤AUTOè‡ªåŠ¨æ£€æµ‹)
     * @param modelType æ¨¡å‹ç±»å‹ (ç”¨äºç‰¹æ®Šå¤„ç†)
     * @return åˆ†è¯å™¨å®ä¾‹
     */
    static std::unique_ptr<BaseTokenizer> create(
        const std::string& modelPath,
        Backend backend = Backend::AUTO,
        ModelType modelType = ModelType::AUTO
    );
    
private:
    static Backend detectBackend(const std::string& modelPath);
    static ModelType detectModelType(const std::string& modelPath);
};

} // namespace cllm
```

**Step 2.3: å®ç°å·¥å‚ç±»** (0.5å¤©)

```cpp
// src/tokenizer/factory.cpp
#include "cllm/tokenizer/base_tokenizer.h"
#include "cllm/tokenizer/hf_tokenizer.h"
#include "cllm/tokenizer/sentencepiece_tokenizer.h"
#include "cllm/tokenizer/native_tokenizer.h"
#include <filesystem>

namespace cllm {

std::unique_ptr<BaseTokenizer> TokenizerFactory::create(
    const std::string& modelPath,
    Backend backend,
    ModelType modelType
) {
    // Step 1: è‡ªåŠ¨æ£€æµ‹backend
    if (backend == Backend::AUTO) {
        backend = detectBackend(modelPath);
    }
    
    // Step 2: è‡ªåŠ¨æ£€æµ‹modelType
    if (modelType == ModelType::AUTO) {
        modelType = detectModelType(modelPath);
    }
    
    // Step 3: åˆ›å»ºå¯¹åº”å®ä¾‹
    std::unique_ptr<BaseTokenizer> tokenizer;
    
    switch (backend) {
        case Backend::HUGGINGFACE:
            CLLM_INFO("Creating HFTokenizer");
            tokenizer = std::make_unique<HFTokenizer>(modelType);
            break;
            
        case Backend::SENTENCEPIECE:
            CLLM_INFO("Creating SentencePieceTokenizer");
            tokenizer = std::make_unique<SentencePieceTokenizer>(modelType);
            break;
            
        case Backend::NATIVE:
            CLLM_INFO("Creating NativeTokenizer");
            tokenizer = std::make_unique<NativeTokenizer>(modelType);
            break;
            
        default:
            throw std::runtime_error("Unknown tokenizer backend");
    }
    
    // Step 4: åŠ è½½æ¨¡å‹
    if (!tokenizer->load(modelPath)) {
        throw std::runtime_error("Failed to load tokenizer from: " + modelPath);
    }
    
    return tokenizer;
}

TokenizerFactory::Backend TokenizerFactory::detectBackend(const std::string& modelPath) {
    namespace fs = std::filesystem;
    
    fs::path basePath(modelPath);
    if (!fs::is_directory(basePath)) {
        basePath = basePath.parent_path();
    }
    
    // âœ… ä¼˜å…ˆæ£€æµ‹HuggingFaceæ ¼å¼
    if (fs::exists(basePath / "tokenizer.json")) {
        CLLM_INFO("Detected HuggingFace format (tokenizer.json)");
        return Backend::HUGGINGFACE;
    }
    
    // æ£€æµ‹SentencePieceæ ¼å¼
    if (fs::exists(basePath / "tokenizer.model")) {
        CLLM_INFO("Detected SentencePiece format (tokenizer.model)");
        return Backend::SENTENCEPIECE;
    }
    
    // å›é€€åˆ°Native
    CLLM_WARN("No standard format detected, using Native tokenizer");
    return Backend::NATIVE;
}

ModelType TokenizerFactory::detectModelType(const std::string& modelPath) {
    namespace fs = std::filesystem;
    
    // è¯»å–config.json
    fs::path configPath = fs::path(modelPath) / "config.json";
    if (!fs::exists(configPath)) {
        return ModelType::AUTO;
    }
    
    std::ifstream f(configPath);
    if (!f.is_open()) return ModelType::AUTO;
    
    try {
        auto config = nlohmann::json::parse(f);
        
        // æ£€æµ‹model_typeå­—æ®µ
        if (config.contains("model_type")) {
            std::string modelTypeStr = config["model_type"];
            if (modelTypeStr.find("qwen2") != std::string::npos) return ModelType::QWEN2;
            if (modelTypeStr.find("qwen") != std::string::npos) return ModelType::QWEN;
            if (modelTypeStr.find("llama") != std::string::npos) return ModelType::LLAMA;
            if (modelTypeStr.find("deepseek") != std::string::npos) return ModelType::DEEPSEEK_LLM;
        }
        
        // æ£€æµ‹tokenizer_classå­—æ®µ
        if (config.contains("tokenizer_class")) {
            std::string tokenizerClass = config["tokenizer_class"];
            if (tokenizerClass.find("Qwen2") != std::string::npos) return ModelType::QWEN2;
            if (tokenizerClass.find("Qwen") != std::string::npos) return ModelType::QWEN;
            if (tokenizerClass.find("DeepSeek") != std::string::npos) return ModelType::DEEPSEEK_LLM;
        }
        
    } catch (const std::exception& e) {
        CLLM_WARN("Failed to detect model type: %s", e.what());
    }
    
    return ModelType::AUTO;
}

} // namespace cllm
```

**Step 2.4: æ›´æ–°æ‰€æœ‰è°ƒç”¨ç‚¹** (1å¤©)

```cpp
// ç¤ºä¾‹: æ›´æ–°ModelExecutor
// include/cllm/model/executor.h

#include "cllm/tokenizer/base_tokenizer.h"  // âœ… ä½¿ç”¨ç»Ÿä¸€æ¥å£

class ModelExecutor {
public:
    // æ„é€ å‡½æ•°æ¥å—BaseTokenizeræŒ‡é’ˆ
    ModelExecutor(..., std::shared_ptr<BaseTokenizer> tokenizer = nullptr);
    
private:
    std::shared_ptr<BaseTokenizer> tokenizer_;  // âœ… ç»Ÿä¸€ç±»å‹
};

// ä½¿ç”¨ç¤ºä¾‹
auto tokenizer = TokenizerFactory::create("/path/to/model");
auto executor = std::make_unique<ModelExecutor>(..., tokenizer);
```

**é˜¶æ®µ2éªŒæ”¶æ ‡å‡†**:
- [ ] æ‰€æœ‰Tokenizerç»§æ‰¿è‡ªBaseTokenizer
- [ ] TokenizerFactoryå¯è‡ªåŠ¨æ£€æµ‹å¹¶åˆ›å»ºæ­£ç¡®çš„å®ä¾‹
- [ ] æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹é€šè¿‡ (ä½¿ç”¨æ–°æ¥å£)
- [ ] æ€§èƒ½æ— é€€åŒ–

---

#### é˜¶æ®µ3: å®Œæ•´åŠŸèƒ½å®ç° (5å¤©) - P1ä¼˜å…ˆçº§

**ç›®æ ‡**: å®ç°HF Tokenizerçš„æ‰€æœ‰é«˜çº§ç‰¹æ€§

**Step 3.1: Chat Templateæ”¯æŒ** (2å¤©)

```cpp
// include/cllm/tokenizer/chat_template.h
#pragma once

#include "types.h"
#include <nlohmann/json.hpp>

namespace cllm {

/**
 * @brief èŠå¤©æ¶ˆæ¯
 */
struct ChatMessage {
    std::string role;     // "user", "assistant", "system"
    std::string content;  // æ¶ˆæ¯å†…å®¹
};

/**
 * @brief ChatTemplate - èŠå¤©æ¨¡æ¿å¤„ç†å™¨
 * 
 * æ”¯æŒHuggingFaceæ ‡å‡†çš„Jinja2æ¨¡æ¿æ ¼å¼
 */
class ChatTemplate {
public:
    /**
     * @brief ä»configåŠ è½½æ¨¡æ¿
     */
    bool loadFromConfig(const std::string& configPath);
    
    /**
     * @brief åº”ç”¨æ¨¡æ¿ç”Ÿæˆprompt
     * @param messages æ¶ˆæ¯åˆ—è¡¨
     * @return æ ¼å¼åŒ–åçš„prompt
     */
    std::string apply(const std::vector<ChatMessage>& messages) const;
    
    /**
     * @brief æ·»åŠ generation prompt
     */
    std::string applyWithGeneration(const std::vector<ChatMessage>& messages) const;

private:
    std::string templateStr_;
    std::string bosToken_ = "<|im_start|>";
    std::string eosToken_ = "<|im_end|>";
    
    std::string renderTemplate(const std::vector<ChatMessage>& messages) const;
};

// HFTokenizeræ‰©å±•
class HFTokenizer : public BaseTokenizer {
public:
    /**
     * @brief åº”ç”¨èŠå¤©æ¨¡æ¿å¹¶ç¼–ç 
     */
    TokenSequence applyChatTemplate(
        const std::vector<ChatMessage>& messages,
        bool addGenerationPrompt = false
    );
    
private:
    std::unique_ptr<ChatTemplate> chatTemplate_;
};

} // namespace cllm
```

**å®ç°ç¤ºä¾‹**:
```cpp
// src/tokenizer/chat_template.cpp

std::string ChatTemplate::apply(const std::vector<ChatMessage>& messages) const {
    std::ostringstream oss;
    
    // ç®€åŒ–çš„Qwenæ¨¡æ¿å®ç°
    for (const auto& msg : messages) {
        oss << bosToken_ << msg.role << "\n"
            << msg.content << eosToken_ << "\n";
    }
    
    return oss.str();
}

// HFTokenizerä¸­ä½¿ç”¨
TokenSequence HFTokenizer::applyChatTemplate(
    const std::vector<ChatMessage>& messages,
    bool addGenerationPrompt
) {
    if (!chatTemplate_) {
        throw std::runtime_error("Chat template not loaded");
    }
    
    std::string prompt = addGenerationPrompt 
        ? chatTemplate_->applyWithGeneration(messages)
        : chatTemplate_->apply(messages);
    
    return encode(prompt, false);  // æ¨¡æ¿å·²åŒ…å«ç‰¹æ®ŠToken
}
```

**Step 3.2: å¢é‡è§£ç æ”¯æŒ** (1å¤©)

```cpp
// HFTokenizerå¢é‡è§£ç 
class HFTokenizer : public BaseTokenizer {
public:
    /**
     * @brief å¢é‡è§£ç å™¨
     * ç”¨äºæµå¼ç”Ÿæˆåœºæ™¯,é€ä¸ªTokenè§£ç 
     */
    class IncrementalDecoder {
    public:
        explicit IncrementalDecoder(HFTokenizer* tokenizer);
        
        /**
         * @brief æ·»åŠ æ–°Tokenå¹¶è¿”å›æ–°ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
         * @return è§£ç å‡ºçš„æ–‡æœ¬å¢é‡ (å¯èƒ½ä¸ºç©º,ç­‰å¾…æ›´å¤šToken)
         */
        std::string add(token_id_t tokenId);
        
        /**
         * @brief å®Œæˆè§£ç ,è¿”å›å‰©ä½™æ–‡æœ¬
         */
        std::string finish();
        
        void reset();
        
    private:
        HFTokenizer* tokenizer_;
        TokenSequence buffer_;
        size_t lastDecodedPos_ = 0;
    };
    
    std::unique_ptr<IncrementalDecoder> createIncrementalDecoder();
};

// ä½¿ç”¨ç¤ºä¾‹
auto decoder = tokenizer->createIncrementalDecoder();
for (int i = 0; i < maxTokens; ++i) {
    token_id_t nextToken = generateNextToken();
    std::string chunk = decoder->add(nextToken);
    if (!chunk.empty()) {
        std::cout << chunk << std::flush;  // æµå¼è¾“å‡º
    }
}
std::cout << decoder->finish();  // è¾“å‡ºå‰©ä½™éƒ¨åˆ†
```

**Step 3.3: å¹¶è¡Œæ‰¹å¤„ç†ä¼˜åŒ–** (1å¤©)

```cpp
// src/tokenizer/hf_tokenizer.cpp

std::vector<TokenSequence> HFTokenizer::batchEncode(
    const std::vector<std::string>& texts,
    bool addSpecialTokens
) {
    if (!tokenizer_) {
        throw std::runtime_error("Tokenizer not loaded");
    }
    
    // âœ… ä½¿ç”¨tokenizers-cppçš„æ‰¹å¤„ç†API (Rustå¹¶è¡Œ)
    try {
        auto encodings = tokenizer_->EncodeBatch(texts, addSpecialTokens);
        
        std::vector<TokenSequence> results;
        results.reserve(encodings.size());
        
        for (const auto& encoding : encodings) {
            TokenSequence ids;
            ids.reserve(encoding.size());
            for (auto id : encoding) {
                ids.push_back(static_cast<token_id_t>(id));
            }
            results.push_back(std::move(ids));
        }
        
        return results;
        
    } catch (const std::exception& e) {
        CLLM_ERROR("Batch encode failed: %s", e.what());
        return {};
    }
}
```

**Step 3.4: å®Œæ•´æµ‹è¯•å¥—ä»¶** (1å¤©)

```cpp
// tests/test_hf_tokenizer_complete.cpp

TEST(HFTokenizerTest, LoadQwen3) {
    auto tokenizer = TokenizerFactory::create(
        "/path/to/Qwen3-0.6B",
        TokenizerFactory::Backend::AUTO
    );
    
    ASSERT_NE(tokenizer, nullptr);
    EXPECT_EQ(tokenizer->getVocabSize(), 151936);
    EXPECT_EQ(tokenizer->getBosId(), 151643);
}

TEST(HFTokenizerTest, EncodeDecode) {
    auto tokenizer = TokenizerFactory::create("/path/to/Qwen3-0.6B");
    
    std::string text = "Hello, world!";
    auto ids = tokenizer->encode(text, true);
    auto decoded = tokenizer->decode(ids, true);
    
    EXPECT_EQ(decoded, text);
}

TEST(HFTokenizerTest, ChatTemplate) {
    auto hfTokenizer = dynamic_cast<HFTokenizer*>(
        TokenizerFactory::create("/path/to/Qwen3-0.6B").get()
    );
    
    std::vector<ChatMessage> messages = {
        {"system", "You are a helpful assistant"},
        {"user", "Hello!"}
    };
    
    auto ids = hfTokenizer->applyChatTemplate(messages, true);
    EXPECT_GT(ids.size(), 0);
}

TEST(HFTokenizerTest, IncrementalDecoding) {
    auto hfTokenizer = dynamic_cast<HFTokenizer*>(
        TokenizerFactory::create("/path/to/Qwen3-0.6B").get()
    );
    
    auto decoder = hfTokenizer->createIncrementalDecoder();
    
    std::string result;
    for (token_id_t id : {12345, 67890, 54321}) {
        result += decoder->add(id);
    }
    result += decoder->finish();
    
    EXPECT_FALSE(result.empty());
}

TEST(HFTokenizerTest, BatchProcessing) {
    auto tokenizer = TokenizerFactory::create("/path/to/Qwen3-0.6B");
    
    std::vector<std::string> texts = {
        "Hello, world!",
        "How are you?",
        "This is a test."
    };
    
    auto results = tokenizer->batchEncode(texts, true);
    EXPECT_EQ(results.size(), 3);
    
    for (const auto& ids : results) {
        EXPECT_GT(ids.size(), 0);
    }
}

TEST(HFTokenizerTest, SpecialTokens) {
    auto tokenizer = TokenizerFactory::create("/path/to/Qwen3-0.6B");
    
    EXPECT_TRUE(tokenizer->isSpecialToken(tokenizer->getBosId()));
    EXPECT_TRUE(tokenizer->isSpecialToken(tokenizer->getEosId()));
    EXPECT_FALSE(tokenizer->isSpecialToken(12345));
}
```

**é˜¶æ®µ3éªŒæ”¶æ ‡å‡†**:
- [ ] Chat Templateæ”¯æŒå®Œæ•´
- [ ] å¢é‡è§£ç åŠŸèƒ½æ­£å¸¸
- [ ] æ‰¹å¤„ç†æ€§èƒ½è¾¾æ ‡ (>100 MB/s)
- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡ (è¦†ç›–ç‡>90%)

---

#### é˜¶æ®µ4: æ€§èƒ½ä¼˜åŒ– (2å¤©) - P2ä¼˜å…ˆçº§

**Step 4.1: Tokenç¼“å­˜æœºåˆ¶** (1å¤©)

```cpp
// include/cllm/tokenizer/cache.h
#pragma once

#include "types.h"
#include <lru/lru.hpp>  // ä½¿ç”¨LRUç¼“å­˜åº“
#include <shared_mutex>

namespace cllm {

/**
 * @brief TokenCache - é«˜æ•ˆTokenç¼“å­˜
 */
class TokenCache {
public:
    explicit TokenCache(size_t maxSize = 10000);
    
    // ç¼–ç ç¼“å­˜
    std::optional<TokenSequence> getEncoded(const std::string& text);
    void putEncoded(const std::string& text, const TokenSequence& ids);
    
    // è§£ç ç¼“å­˜
    std::optional<std::string> getDecoded(const TokenSequence& ids);
    void putDecoded(const TokenSequence& ids, const std::string& text);
    
    // ç»Ÿè®¡ä¿¡æ¯
    struct Stats {
        size_t hits = 0;
        size_t misses = 0;
        double hitRate() const { return hits / double(hits + misses); }
    };
    Stats getStats() const;
    
    void clear();

private:
    LRU::Cache<std::string, TokenSequence> encodeCache_;
    LRU::Cache<std::string, std::string> decodeCache_;  // Key: idsçš„hash
    
    mutable std::shared_mutex mutex_;
    Stats stats_;
    
    std::string hashTokenSequence(const TokenSequence& ids) const;
};

} // namespace cllm
```

**Step 4.2: æ€§èƒ½ç›‘æ§** (0.5å¤©)

```cpp
// BaseTokenizerå¢åŠ æ€§èƒ½ç›‘æ§
class BaseTokenizer {
public:
    struct PerformanceMetrics {
        size_t encodeCount = 0;
        size_t decodeCount = 0;
        double totalEncodeTime = 0.0;  // ç§’
        double totalDecodeTime = 0.0;
        
        double avgEncodeTime() const { 
            return encodeCount > 0 ? totalEncodeTime / encodeCount : 0.0; 
        }
        double avgDecodeTime() const { 
            return decodeCount > 0 ? totalDecodeTime / decodeCount : 0.0; 
        }
        
        double encodeSpeed(size_t totalChars) const {  // MB/s
            return totalEncodeTime > 0 ? totalChars / totalEncodeTime / 1e6 : 0.0;
        }
    };
    
    const PerformanceMetrics& getMetrics() const { return metrics_; }
    void resetMetrics() { metrics_ = PerformanceMetrics(); }

protected:
    PerformanceMetrics metrics_;
};

// ä½¿ç”¨RAIIè¿›è¡Œæ€§èƒ½è®¡æ—¶
class PerformanceTimer {
public:
    PerformanceTimer(double& target) : target_(target), start_(now()) {}
    ~PerformanceTimer() { target_ += (now() - start_); }
private:
    double& target_;
    double start_;
    static double now();
};

// åœ¨encode/decodeä¸­ä½¿ç”¨
TokenSequence HFTokenizer::encode(const std::string& text, bool addSpecialTokens) {
    PerformanceTimer timer(metrics_.totalEncodeTime);
    metrics_.encodeCount++;
    
    // ... å®é™…ç¼–ç é€»è¾‘
}
```

**Step 4.3: åŸºå‡†æµ‹è¯•** (0.5å¤©)

```cpp
// tests/benchmark_tokenizers.cpp

#include <benchmark/benchmark.h>

static void BM_HFTokenizer_Encode(benchmark::State& state) {
    auto tokenizer = TokenizerFactory::create(
        "/path/to/Qwen3-0.6B",
        TokenizerFactory::Backend::HUGGINGFACE
    );
    
    std::string text = "This is a test sentence for benchmarking.";
    
    for (auto _ : state) {
        auto ids = tokenizer->encode(text, true);
        benchmark::DoNotOptimize(ids);
    }
    
    state.SetItemsProcessed(state.iterations());
}
BENCHMARK(BM_HFTokenizer_Encode);

static void BM_SentencePiece_Encode(benchmark::State& state) {
    auto tokenizer = TokenizerFactory::create(
        "/path/to/llama-model",
        TokenizerFactory::Backend::SENTENCEPIECE
    );
    
    std::string text = "This is a test sentence for benchmarking.";
    
    for (auto _ : state) {
        auto ids = tokenizer->encode(text, true);
        benchmark::DoNotOptimize(ids);
    }
    
    state.SetItemsProcessed(state.iterations());
}
BENCHMARK(BM_SentencePiece_Encode);

// æ‰¹å¤„ç†åŸºå‡†æµ‹è¯•
static void BM_HFTokenizer_BatchEncode(benchmark::State& state) {
    auto tokenizer = TokenizerFactory::create(
        "/path/to/Qwen3-0.6B",
        TokenizerFactory::Backend::HUGGINGFACE
    );
    
    std::vector<std::string> texts(state.range(0), "Test sentence for batch encoding.");
    
    for (auto _ : state) {
        auto results = tokenizer->batchEncode(texts, true);
        benchmark::DoNotOptimize(results);
    }
    
    state.SetItemsProcessed(state.iterations() * state.range(0));
}
BENCHMARK(BM_HFTokenizer_BatchEncode)->Range(1, 1024);

BENCHMARK_MAIN();
```

**é¢„æœŸæ€§èƒ½ç›®æ ‡**:
```
Benchmark Results:
-------------------------------------------------------------
BM_HFTokenizer_Encode            1000000 ns/op  (1000x ç¼–ç /ç§’)
BM_SentencePiece_Encode         5000000 ns/op  (200x ç¼–ç /ç§’)
BM_HFTokenizer_BatchEncode/8      50000 ns/op  (8ä¸ªæ–‡æœ¬)
BM_HFTokenizer_BatchEncode/64    300000 ns/op  (64ä¸ªæ–‡æœ¬)

â†’ HF Tokenizer æ¯” SentencePiece å¿« 5å€
```

---

### 3.2 å…¼å®¹æ€§ä¿è¯ç­–ç•¥

#### å‘åå…¼å®¹æ€§

**ç­–ç•¥1: ä¿ç•™SentencePieceæ”¯æŒ**
```cpp
// æ‰€æœ‰SentencePieceä»£ç ä¿æŒä¸å˜,ä»…è°ƒæ•´ä¼˜å…ˆçº§
class SentencePieceTokenizer : public BaseTokenizer {
    // å®Œå…¨ä¿ç•™ç°æœ‰å®ç°
};

// ç”¨æˆ·å¯å¼ºåˆ¶ä½¿ç”¨
auto tokenizer = TokenizerFactory::create(
    modelPath,
    TokenizerFactory::Backend::SENTENCEPIECE  // æ˜¾å¼æŒ‡å®š
);
```

**ç­–ç•¥2: å¹³æ»‘è¿ç§»æœŸ**
```cpp
// æä¾›å¼ƒç”¨è­¦å‘Š (ç¬¬ä¸€ç‰ˆæœ¬)
auto tokenizer = TokenizerFactory::create(modelPath);
if (tokenizer->getBackend() == Backend::SENTENCEPIECE) {
    CLLM_WARN("SentencePiece backend is deprecated, consider migrating to HuggingFace format");
}

// è®¾ç½®å¼ƒç”¨æ—¶é—´è¡¨ (6ä¸ªæœˆå)
// ç¬¬2ä¸ªç‰ˆæœ¬: é»˜è®¤ä¸å†æ”¯æŒSentencePiece
// ç¬¬3ä¸ªç‰ˆæœ¬: å®Œå…¨ç§»é™¤SentencePieceä»£ç 
```

#### APIå…¼å®¹æ€§

**ä¿æŒæ‰€æœ‰ç°æœ‰æ¥å£ç­¾å**:
```cpp
// âœ… ç°æœ‰ä»£ç æ— éœ€ä¿®æ”¹
std::vector<int> encode(const std::string& text, bool addSpecialTokens = true);

// âœ… æ‰©å±•æ¥å£å‘åå…¼å®¹
TokenSequence encode_v2(const std::string& text, bool addSpecialTokens = true);
// TokenSequence = std::vector<token_id_t> = std::vector<int32_t>
// å®Œå…¨å…¼å®¹ std::vector<int>
```

#### é…ç½®æ–‡ä»¶å…¼å®¹

```yaml
# config/tokenizer.yaml
tokenizer:
  backend: auto  # auto | huggingface | sentencepiece | native
  model_path: /path/to/model
  
  # SentencePieceç‰¹å®šé…ç½® (å¯é€‰)
  sentencepiece:
    model_file: tokenizer.model
    
  # HuggingFaceç‰¹å®šé…ç½® (å¯é€‰)
  huggingface:
    json_file: tokenizer.json
    use_fast: true
    
  # é€šç”¨é…ç½®
  cache_size: 10000
  max_length: 2048
```

---

### 3.3 å›æ»šä¸åº”æ€¥æ–¹æ¡ˆ

#### å¿«é€Ÿå›æ»šæœºåˆ¶

**ç¼–è¯‘æ—¶å¼€å…³**:
```cmake
# CMakeLists.txt
option(FORCE_SENTENCEPIECE "Force use SentencePiece tokenizer" OFF)

if(FORCE_SENTENCEPIECE)
    add_compile_definitions(FORCE_SENTENCEPIECE_BACKEND)
endif()
```

```cpp
// src/tokenizer/factory.cpp
TokenizerFactory::Backend TokenizerFactory::detectBackend(const std::string& modelPath) {
#ifdef FORCE_SENTENCEPIECE_BACKEND
    CLLM_WARN("Forced to use SentencePiece backend");
    return Backend::SENTENCEPIECE;
#endif
    
    // æ­£å¸¸æ£€æµ‹é€»è¾‘...
}
```

**è¿è¡Œæ—¶å›é€€**:
```cpp
// ç¯å¢ƒå˜é‡æ§åˆ¶
const char* forceBackend = std::getenv("CLLM_TOKENIZER_BACKEND");
if (forceBackend) {
    if (strcmp(forceBackend, "sentencepiece") == 0) {
        return Backend::SENTENCEPIECE;
    }
}
```

#### é—®é¢˜è¯Šæ–­å·¥å…·

```cpp
// tools/diagnose_tokenizer.cpp
int main(int argc, char** argv) {
    if (argc < 2) {
        std::cerr << "Usage: diagnose_tokenizer <model_path>" << std::endl;
        return 1;
    }
    
    std::string modelPath = argv[1];
    
    std::cout << "=== Tokenizer Diagnostic Tool ===" << std::endl;
    std::cout << "Model path: " << modelPath << std::endl;
    std::cout << std::endl;
    
    // æ£€æµ‹æ–‡ä»¶
    namespace fs = std::filesystem;
    bool hasTokenizerJson = fs::exists(fs::path(modelPath) / "tokenizer.json");
    bool hasTokenizerModel = fs::exists(fs::path(modelPath) / "tokenizer.model");
    bool hasVocabJson = fs::exists(fs::path(modelPath) / "vocab.json");
    bool hasConfig = fs::exists(fs::path(modelPath) / "config.json");
    
    std::cout << "File detection:" << std::endl;
    std::cout << "  tokenizer.json: " << (hasTokenizerJson ? "âœ…" : "âŒ") << std::endl;
    std::cout << "  tokenizer.model: " << (hasTokenizerModel ? "âœ…" : "âŒ") << std::endl;
    std::cout << "  vocab.json: " << (hasVocabJson ? "âœ…" : "âŒ") << std::endl;
    std::cout << "  config.json: " << (hasConfig ? "âœ…" : "âŒ") << std::endl;
    std::cout << std::endl;
    
    // æ¨èbackend
    auto backend = TokenizerFactory::detectBackend(modelPath);
    std::cout << "Recommended backend: ";
    switch (backend) {
        case TokenizerFactory::Backend::HUGGINGFACE:
            std::cout << "HuggingFace" << std::endl;
            break;
        case TokenizerFactory::Backend::SENTENCEPIECE:
            std::cout << "SentencePiece" << std::endl;
            break;
        default:
            std::cout << "Native (fallback)" << std::endl;
    }
    
    // å°è¯•åŠ è½½
    try {
        auto tokenizer = TokenizerFactory::create(modelPath);
        std::cout << std::endl << "âœ… Tokenizer loaded successfully" << std::endl;
        std::cout << "  Vocab size: " << tokenizer->getVocabSize() << std::endl;
        std::cout << "  BOS ID: " << tokenizer->getBosId() << std::endl;
        std::cout << "  EOS ID: " << tokenizer->getEosId() << std::endl;
        
        // æµ‹è¯•ç¼–ç 
        std::string testText = "Hello, world!";
        auto ids = tokenizer->encode(testText, true);
        auto decoded = tokenizer->decode(ids, true);
        
        std::cout << std::endl << "Encode test:" << std::endl;
        std::cout << "  Input: \"" << testText << "\"" << std::endl;
        std::cout << "  Token IDs: [";
        for (size_t i = 0; i < ids.size(); ++i) {
            if (i > 0) std::cout << ", ";
            std::cout << ids[i];
        }
        std::cout << "]" << std::endl;
        std::cout << "  Decoded: \"" << decoded << "\"" << std::endl;
        
        if (decoded == testText) {
            std::cout << "  âœ… Encode/Decode roundtrip successful" << std::endl;
        } else {
            std::cout << "  âš ï¸ Roundtrip mismatch!" << std::endl;
        }
        
    } catch (const std::exception& e) {
        std::cerr << std::endl << "âŒ Failed to load tokenizer: " << e.what() << std::endl;
        return 1;
    }
    
    return 0;
}
```

---

## 4ï¸âƒ£ æµ‹è¯•ä¸éªŒè¯ç­–ç•¥

### 4.1 æµ‹è¯•è¦†ç›–çŸ©é˜µ

| æµ‹è¯•ç±»åˆ« | HF Tokenizer | SentencePiece | Native | ä¼˜å…ˆçº§ |
|---------|-------------|---------------|--------|-------|
| **åŠŸèƒ½æµ‹è¯•** | | | | |
| åŸºç¡€ç¼–è§£ç  | âœ… | âœ… | âœ… | P0 |
| ç‰¹æ®ŠTokenå¤„ç† | âœ… | âœ… | âœ… | P0 |
| Chat Template | âœ… | âŒ | âŒ | P1 |
| å¢é‡è§£ç  | âœ… | âš ï¸ | âš ï¸ | P1 |
| æ‰¹å¤„ç† | âœ… | âœ… | âœ… | P1 |
| **å…¼å®¹æ€§æµ‹è¯•** | | | | |
| Qwenç³»åˆ— | âœ… | âš ï¸ | âš ï¸ | P0 |
| DeepSeekç³»åˆ— | âœ… | âš ï¸ | âš ï¸ | P0 |
| Llamaç³»åˆ— | âœ… | âœ… | âš ï¸ | P1 |
| å…¶ä»–æ¨¡å‹ | âœ… | âš ï¸ | âš ï¸ | P2 |
| **æ€§èƒ½æµ‹è¯•** | | | | |
| ç¼–ç é€Ÿåº¦ | âœ… | âœ… | âœ… | P1 |
| è§£ç é€Ÿåº¦ | âœ… | âœ… | âœ… | P1 |
| å†…å­˜å ç”¨ | âœ… | âœ… | âœ… | P1 |
| å¹¶å‘æ€§èƒ½ | âœ… | âœ… | âœ… | P2 |
| **é›†æˆæµ‹è¯•** | | | | |
| ModelExecutoré›†æˆ | âœ… | âœ… | âœ… | P0 |
| HTTP Serveré›†æˆ | âœ… | âœ… | âœ… | P0 |
| Scheduleré›†æˆ | âœ… | âœ… | âœ… | P1 |

### 4.2 æµ‹è¯•æ•°æ®é›†

**å¤šæ¨¡å‹è¦†ç›–**:
```bash
test_data/
â”œâ”€â”€ qwen3-0.6b/          # HFæ ¼å¼
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ test_cases.json
â”œâ”€â”€ llama-2-7b/          # SentencePieceæ ¼å¼
â”‚   â”œâ”€â”€ tokenizer.model
â”‚   â””â”€â”€ test_cases.json
â”œâ”€â”€ deepseek-v3/         # HFæ ¼å¼
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â””â”€â”€ test_cases.json
â””â”€â”€ test_corpus.txt      # é€šç”¨æµ‹è¯•è¯­æ–™
```

**æµ‹è¯•ç”¨ä¾‹ç¤ºä¾‹**:
```json
{
  "test_cases": [
    {
      "name": "basic_english",
      "input": "Hello, world!",
      "expected_tokens": [151643, 9707, 11, 1879, 0, 151645]
    },
    {
      "name": "chinese_text",
      "input": "ä½ å¥½ï¼Œä¸–ç•Œï¼",
      "expected_tokens": [151643, 104688, 3837, 151645]
    },
    {
      "name": "special_tokens",
      "input": "<|im_start|>user\nHello<|im_end|>",
      "contains_special": true
    },
    {
      "name": "long_text",
      "input": "Lorem ipsum...",  // 1000+ words
      "min_tokens": 500
    }
  ]
}
```

### 4.3 å›å½’æµ‹è¯•

**è‡ªåŠ¨åŒ–CIæµç¨‹**:
```yaml
# .github/workflows/tokenizer_tests.yml
name: Tokenizer Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        backend: [huggingface, sentencepiece, native]
        model: [qwen3, llama2, deepseek]
        
    steps:
      - uses: actions/checkout@v2
      
      - name: Install dependencies
        run: |
          sudo apt-get install -y libsentencepiece-dev
          ./scripts/install_tokenizers_cpp.sh
          
      - name: Build tests
        run: |
          mkdir build && cd build
          cmake .. -DUSE_TOKENIZERS_CPP=ON
          make test_tokenizers -j4
          
      - name: Run tests
        run: |
          cd build
          ./bin/test_hf_tokenizer --gtest_filter=*${{ matrix.model }}*
          
      - name: Benchmark
        run: |
          cd build
          ./bin/benchmark_tokenizers --benchmark_filter=${{ matrix.backend }}
```

---

## 5ï¸âƒ£ é£é™©è¯„ä¼°ä¸ç¼“è§£

### 5.1 æŠ€æœ¯é£é™©

| é£é™© | å½±å“ | æ¦‚ç‡ | ç¼“è§£æªæ–½ |
|------|------|------|---------|
| **tokenizers-cppç¼–è¯‘å¤±è´¥** | é«˜ | ä¸­ | 1. æä¾›é¢„ç¼–è¯‘äºŒè¿›åˆ¶<br>2. Dockeré•œåƒ<br>3. å›é€€åˆ°SentencePiece |
| **æ€§èƒ½ä¸è¾¾é¢„æœŸ** | ä¸­ | ä½ | 1. æ€§èƒ½åŸºå‡†æµ‹è¯•<br>2. ç¼“å­˜ä¼˜åŒ–<br>3. å¤šçº¿ç¨‹å¹¶è¡Œ |
| **HFæ¨¡å‹å…¼å®¹æ€§é—®é¢˜** | ä¸­ | ä¸­ | 1. æ‰©å¤§æµ‹è¯•è¦†ç›–<br>2. ç¤¾åŒºåé¦ˆ<br>3. é€æ­¥ä¿®å¤ |
| **å†…å­˜å ç”¨å¢åŠ ** | ä½ | ä½ | 1. å†…å­˜profiling<br>2. ç¼“å­˜å¤§å°å¯é…ç½®<br>3. æƒ°æ€§åŠ è½½ |

### 5.2 é¡¹ç›®é£é™©

| é£é™© | å½±å“ | æ¦‚ç‡ | ç¼“è§£æªæ–½ |
|------|------|------|---------|
| **å¼€å‘æ—¶é—´è¶…æœŸ** | ä¸­ | ä¸­ | 1. é˜¶æ®µæ€§äº¤ä»˜<br>2. ä¼˜å…ˆå®ç°P0åŠŸèƒ½<br>3. å¹¶è¡Œå¼€å‘ |
| **ç ´åç°æœ‰åŠŸèƒ½** | é«˜ | ä½ | 1. å®Œæ•´å›å½’æµ‹è¯•<br>2. ä¿ç•™æ—§æ¥å£<br>3. å¿«é€Ÿå›æ»šæœºåˆ¶ |
| **ç”¨æˆ·è¿ç§»æˆæœ¬** | ä¸­ | ä¸­ | 1. è¯¦ç»†è¿ç§»æ–‡æ¡£<br>2. è‡ªåŠ¨è¿ç§»å·¥å…·<br>3. é•¿æœŸæ”¯æŒæœŸ |
| **ä¾èµ–åº“ç»´æŠ¤é—®é¢˜** | ä¸­ | ä½ | 1. é”å®šä¾èµ–ç‰ˆæœ¬<br>2. å®šæœŸæ›´æ–°<br>3. VendoråŒ–å…³é”®åº“ |

---

## 6ï¸âƒ£ èµ„æºéœ€æ±‚ä¸æ—¶é—´è¡¨

### 6.1 äººåŠ›éœ€æ±‚

| è§’è‰² | æŠ•å…¥ | èŒè´£ |
|------|------|------|
| **æ ¸å¿ƒå¼€å‘** | 2äºº Ã— 2å‘¨ | 1. å®ç°HFTokenizer<br>2. é‡æ„æ¥å£<br>3. æ€§èƒ½ä¼˜åŒ– |
| **æµ‹è¯•å·¥ç¨‹å¸ˆ** | 1äºº Ã— 1å‘¨ | 1. ç¼–å†™æµ‹è¯•ç”¨ä¾‹<br>2. æ‰§è¡Œå›å½’æµ‹è¯•<br>3. æ€§èƒ½åŸºå‡†æµ‹è¯• |
| **Tech Lead** | 0.5äºº Ã— 2å‘¨ | 1. æ¶æ„review<br>2. ä»£ç review<br>3. é£é™©æ§åˆ¶ |

### 6.2 æ€»æ—¶é—´è¡¨

```
Week 1:
  Day 1-2: é˜¶æ®µ0å‡†å¤‡ + é˜¶æ®µ1å®ç° (HFTokenizeråŸºç¡€)
  Day 3-5: é˜¶æ®µ1å®Œæˆ + æµ‹è¯•éªŒè¯

Week 2:
  Day 1-3: é˜¶æ®µ2æ¶æ„ç»Ÿä¸€
  Day 4-5: é˜¶æ®µ2æµ‹è¯• + Code Review

Week 3:
  Day 1-5: é˜¶æ®µ3å®Œæ•´åŠŸèƒ½å®ç°

Week 4:
  Day 1-2: é˜¶æ®µ4æ€§èƒ½ä¼˜åŒ–
  Day 3-4: å®Œæ•´å›å½’æµ‹è¯•
  Day 5: æ–‡æ¡£ç¼–å†™ + å‘å¸ƒå‡†å¤‡

---
Total: 4å‘¨ (20ä¸ªå·¥ä½œæ—¥)
```

---

## 7ï¸âƒ£ æˆåŠŸæŒ‡æ ‡

### 7.1 æŠ€æœ¯æŒ‡æ ‡

| æŒ‡æ ‡ | ç›®æ ‡ | æµ‹é‡æ–¹æ³• |
|------|------|---------|
| **æ¨¡å‹å…¼å®¹æ€§** | 95%+ | æµ‹è¯•20ä¸ªä¸»æµæ¨¡å‹çš„åŠ è½½æˆåŠŸç‡ |
| **ç¼–ç é€Ÿåº¦** | >100 MB/s | Benchmarkæµ‹è¯•(è‹±æ–‡æ–‡æœ¬) |
| **è§£ç é€Ÿåº¦** | >50 MB/s | Benchmarkæµ‹è¯• |
| **å†…å­˜å ç”¨** | <200MB | Valgrind/Heaptrackæµ‹é‡ |
| **æµ‹è¯•è¦†ç›–ç‡** | >90% | Gcov/LcovæŠ¥å‘Š |
| **ä»£ç è´¨é‡** | Açº§ | SonarQubeåˆ†æ |

### 7.2 ä¸šåŠ¡æŒ‡æ ‡

| æŒ‡æ ‡ | ç›®æ ‡ | æµ‹é‡æ–¹æ³• |
|------|------|---------|
| **å¼€å‘æ•ˆç‡æå‡** | 60%+ | æ–°æ¨¡å‹é€‚é…æ—¶é—´å¯¹æ¯” |
| **ç”¨æˆ·è¿ç§»ç‡** | >80% | ç”¨æˆ·åé¦ˆè°ƒç ” |
| **Bugæ•°é‡** | <5ä¸ª/æœˆ | Issue tracker |
| **ç¤¾åŒºæ»¡æ„åº¦** | >4.5/5 | GitHub Stars/åé¦ˆ |

---

## 8ï¸âƒ£ é™„å½•

### A. tokenizers-cppå®‰è£…æŒ‡å—

#### macOSå®‰è£…
```bash
# æ–¹æ³•1: Homebrew (æ¨è)
brew install rust
git clone https://github.com/mlc-ai/tokenizers-cpp
cd tokenizers-cpp
mkdir build && cd build
cmake .. -DCMAKE_INSTALL_PREFIX=/opt/homebrew
make -j$(sysctl -n hw.ncpu)
sudo make install

# æ–¹æ³•2: ä½¿ç”¨vcpkg
vcpkg install tokenizers-cpp
```

#### Linuxå®‰è£…
```bash
# Ubuntu/Debian
sudo apt-get install -y cargo rustc
git clone https://github.com/mlc-ai/tokenizers-cpp
cd tokenizers-cpp
mkdir build && cd build
cmake .. -DCMAKE_INSTALL_PREFIX=/usr/local
make -j$(nproc)
sudo make install

# CentOS/RHEL
sudo yum install -y cargo rust
# åŒä¸Š...
```

#### Dockeré•œåƒ
```dockerfile
FROM ubuntu:22.04

RUN apt-get update && apt-get install -y \
    cmake g++ git cargo rustc \
    libsentencepiece-dev \
    && rm -rf /var/lib/apt/lists/*

RUN git clone https://github.com/mlc-ai/tokenizers-cpp /opt/tokenizers-cpp \
    && cd /opt/tokenizers-cpp \
    && mkdir build && cd build \
    && cmake .. && make -j$(nproc) && make install

WORKDIR /workspace
```

### B. è¿ç§»æ£€æŸ¥æ¸…å•

- [ ] tokenizers-cppä¾èµ–å·²å®‰è£…
- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡ (SentencePieceä¿æŒä¸å˜)
- [ ] HFTokenizeråŸºç¡€åŠŸèƒ½å®ç°
- [ ] TokenizerFactoryè‡ªåŠ¨æ£€æµ‹æ­£å¸¸
- [ ] æ€§èƒ½è¾¾æ ‡ (ç¼–ç >100MB/s)
- [ ] Qwen3-0.6Bå¯æ­£å¸¸åŠ è½½
- [ ] HTTP Serveré›†æˆæµ‹è¯•é€šè¿‡
- [ ] ModelExecutoré›†æˆæ­£å¸¸
- [ ] æ–‡æ¡£æ›´æ–°å®Œæˆ
- [ ] CI/CDæµç¨‹é…ç½®
- [ ] å›æ»šæœºåˆ¶æµ‹è¯•
- [ ] ç”¨æˆ·è¿ç§»æŒ‡å—å‘å¸ƒ

### C. å‚è€ƒèµ„æº

1. **HuggingFace Tokenizers**
   - å®˜æ–¹æ–‡æ¡£: https://huggingface.co/docs/tokenizers/
   - GitHub: https://github.com/huggingface/tokenizers

2. **tokenizers-cpp**
   - GitHub: https://github.com/mlc-ai/tokenizers-cpp
   - ç¤ºä¾‹: https://github.com/mlc-ai/tokenizers-cpp/tree/main/examples

3. **SentencePiece**
   - å®˜æ–¹æ–‡æ¡£: https://github.com/google/sentencepiece
   - è®ºæ–‡: https://arxiv.org/abs/1808.06226

4. **ç›¸å…³è®ºæ–‡**
   - "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)" (Normalizers)
   - "Neural Machine Translation of Rare Words with Subword Units" (BPE)

---

## ğŸ“ æ€»ç»“

æœ¬è¿ç§»æ–¹æ¡ˆæ—¨åœ¨å°†cLLMé¡¹ç›®çš„Tokenizeræ¶æ„ç°ä»£åŒ–,**ä¼˜å…ˆæ”¯æŒHuggingFaceæ ¼å¼**(è¦†ç›–95%+æ¨¡å‹),åŒæ—¶**ä¿ç•™SentencePieceä½œä¸ºå¯é€‰fallback**(å…¼å®¹ä¼ ç»Ÿæ¨¡å‹)ã€‚

**æ ¸å¿ƒä¼˜åŠ¿**:
1. âœ… **å¼€ç®±å³ç”¨**: 95%+ä¸»æµæ¨¡å‹æ— éœ€è½¬æ¢
2. âš¡ **æ€§èƒ½æå‡**: ç¼–ç é€Ÿåº¦æå‡6å€ (100-300 MB/s)
3. ğŸ”§ **åŠŸèƒ½å®Œæ•´**: Chat Templateã€å¢é‡è§£ç ã€å¹¶è¡Œæ‰¹å¤„ç†
4. ğŸ›¡ï¸ **å‘åå…¼å®¹**: ä¿ç•™æ‰€æœ‰ç°æœ‰æ¥å£å’ŒSentencePieceæ”¯æŒ
5. ğŸ“ˆ **æ˜“äºç»´æŠ¤**: æ ‡å‡†åŒ–æµç¨‹,å‡å°‘60%è‡ªå®šä¹‰ä»£ç 

**å®æ–½è·¯å¾„**: 4å‘¨ (20å·¥ä½œæ—¥),åˆ†4ä¸ªé˜¶æ®µæ¸è¿›å¼å®æ–½,æ¯ä¸ªé˜¶æ®µå‡å¯ç‹¬ç«‹äº¤ä»˜å’ŒéªŒè¯ã€‚

**é£é™©å¯æ§**: å®Œæ•´çš„å›æ»šæœºåˆ¶ã€æµ‹è¯•ç­–ç•¥å’Œåº”æ€¥æ–¹æ¡ˆ,ç¡®ä¿è¿ç§»å¹³ç¨³è¿›è¡Œã€‚

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2026-01-11  
**ä½œè€…**: cLLM Core Team  
**å®¡é˜…çŠ¶æ€**: å¾…Review
