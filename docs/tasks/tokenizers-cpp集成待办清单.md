# tokenizers-cpp é›†æˆå¾…åŠæ¸…å•

## ğŸ“‹ æ–‡æ¡£æ¦‚è¿°

**åˆ›å»ºæ—¥æœŸ**: 2026-01-11  
**çŠ¶æ€**: éƒ¨åˆ†å®Œæˆï¼Œéœ€ç»§ç»­é›†æˆ  
**ä¼˜å…ˆçº§**: é«˜  
**é¢„è®¡å·¥ä½œé‡**: 3-5 å¤©

---

## âœ… å·²å®Œæˆçš„å·¥ä½œ

### 1. ä»£ç å‡†å¤‡ï¼ˆ100%ï¼‰
- [x] HFTokenizer å¤´æ–‡ä»¶å®šä¹‰ (`include/cllm/tokenizer/hf_tokenizer.h`)
- [x] HFTokenizer åŸºç¡€å®ç° (`src/tokenizer/hf_tokenizer.cpp`)
- [x] å•å…ƒæµ‹è¯•ä»£ç  (`tests/test_hf_tokenizer.cpp` - 17ä¸ªæµ‹è¯•)
- [x] ç¤ºä¾‹ä»£ç  (`examples/hf_tokenizer_example.cpp` - 5ä¸ªç¤ºä¾‹)
- [x] CMakeLists.txt é…ç½®æ”¯æŒ
- [x] å®‰è£…è„šæœ¬ (`scripts/install_tokenizers_cpp.sh`)

### 2. åº“å‡†å¤‡ï¼ˆ100%ï¼‰
- [x] å…‹éš† tokenizers-cpp åˆ° `third_party/`
- [x] åˆå§‹åŒ–å­æ¨¡å—ï¼ˆmsgpack, sentencepieceï¼‰
- [x] ç¼–è¯‘ tokenizers-cppï¼ˆç”Ÿæˆ `libtokenizers_c.a` å’Œ `libtokenizers_cpp.a`ï¼‰

### 3. æ–‡æ¡£ï¼ˆ100%ï¼‰
- [x] Tokenizersåº“å®‰è£…æŒ‡å—
- [x] tokenizers-cppé›†æˆåˆ†æ
- [x] tokenizers-cppé›†æˆå®ŒæˆæŠ¥å‘Š
- [x] tokenizers-cppé›†æˆéªŒè¯æŒ‡å—
- [x] tokenizers-cppé›†æˆæ‰§è¡Œæ€»ç»“

---

## ğŸš§ æœªå®Œæˆçš„ä»»åŠ¡

### ä»»åŠ¡åˆ†ç±»

```
â”œâ”€â”€ æ ¸å¿ƒä»»åŠ¡ï¼ˆå¿…é¡»å®Œæˆï¼‰
â”‚   â”œâ”€â”€ Task-1: API é€‚é…ä¿®å¤
â”‚   â”œâ”€â”€ Task-2: ç¼–è¯‘å’Œé“¾æ¥é…ç½®
â”‚   â””â”€â”€ Task-3: æµ‹è¯•éªŒè¯
â”‚
â”œâ”€â”€ åŠŸèƒ½å¢å¼ºï¼ˆé‡è¦ï¼‰
â”‚   â”œâ”€â”€ Task-4: ç‰¹æ®ŠTokenå¤„ç†
â”‚   â”œâ”€â”€ Task-5: æ‰¹é‡å¤„ç†æ”¯æŒ
â”‚   â””â”€â”€ Task-6: æ€§èƒ½ä¼˜åŒ–
â”‚
â””â”€â”€ æ–‡æ¡£å’Œç»´æŠ¤ï¼ˆå¯é€‰ï¼‰
    â”œâ”€â”€ Task-7: APIæ–‡æ¡£æ›´æ–°
    â”œâ”€â”€ Task-8: æ•…éšœæ’æŸ¥æŒ‡å—
    â””â”€â”€ Task-9: CI/CD é›†æˆ
```

---

## ğŸ“Œ æ ¸å¿ƒä»»åŠ¡ï¼ˆå¿…é¡»å®Œæˆï¼‰

### Task-1: API é€‚é…ä¿®å¤ ğŸ”´ é«˜ä¼˜å…ˆçº§

**é—®é¢˜æè¿°**:  
å½“å‰ `HFTokenizer` å®ç°ä½¿ç”¨çš„ API ä¸ `tokenizers-cpp` å®é™… API ä¸åŒ¹é…ï¼š
- ä½¿ç”¨äº†ä¸å­˜åœ¨çš„ `FromFile()` æ–¹æ³•
- `Encode()` å’Œ `Decode()` å‚æ•°ä¸åŒ¹é…
- ç¼ºå°‘æ–‡ä»¶è¯»å–é€»è¾‘

**éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶**:
- `src/tokenizer/hf_tokenizer.cpp`

**å…·ä½“ä»»åŠ¡**:

#### Task-1.1: ä¿®å¤ load() æ–¹æ³•
```cpp
// ç°æœ‰ä»£ç ï¼ˆé”™è¯¯ï¼‰:
tokenizer_ = tokenizers::Tokenizer::FromFile(tokenizerJsonPath);

// éœ€è¦æ”¹ä¸º:
// 1. è¯»å– tokenizer.json æ–‡ä»¶å†…å®¹
std::ifstream f(tokenizerJsonPath);
std::string json_blob((std::istreambuf_iterator<char>(f)), 
                      std::istreambuf_iterator<char>());

// 2. ä½¿ç”¨ FromBlobJSON åˆ›å»º
tokenizer_ = tokenizers::Tokenizer::FromBlobJSON(json_blob);
```

**éš¾åº¦**: â­ï¸â­ï¸  
**é¢„è®¡æ—¶é—´**: 30 åˆ†é’Ÿ  
**éªŒè¯æ–¹å¼**: ç¼–è¯‘é€šè¿‡

---

#### Task-1.2: ä¿®å¤ encode() æ–¹æ³•
```cpp
// ç°æœ‰ä»£ç ï¼ˆé”™è¯¯ï¼‰:
auto encoding = tokenizer_->Encode(text, addSpecialTokens);

// éœ€è¦æ”¹ä¸º:
auto encoding = tokenizer_->Encode(text);

// æ³¨æ„: tokenizers-cpp ä¸æ”¯æŒ addSpecialTokens å‚æ•°
// éœ€è¦æ‰‹åŠ¨å¤„ç†ç‰¹æ®ŠToken
```

**éš¾åº¦**: â­ï¸â­ï¸  
**é¢„è®¡æ—¶é—´**: 20 åˆ†é’Ÿ  
**éªŒè¯æ–¹å¼**: ç¼–è¯‘é€šè¿‡

---

#### Task-1.3: ä¿®å¤ decode() æ–¹æ³•
```cpp
// ç°æœ‰ä»£ç ï¼ˆé”™è¯¯ï¼‰:
std::string text = tokenizer_->Decode(tokenIds, skipSpecialTokens);

// éœ€è¦æ”¹ä¸º:
std::string text = tokenizer_->Decode(tokenIds);

// æ³¨æ„: tokenizers-cpp ä¸æ”¯æŒ skipSpecialTokens å‚æ•°
// éœ€è¦æ‰‹åŠ¨è¿‡æ»¤ç‰¹æ®ŠToken
```

**éš¾åº¦**: â­ï¸â­ï¸  
**é¢„è®¡æ—¶é—´**: 20 åˆ†é’Ÿ  
**éªŒè¯æ–¹å¼**: ç¼–è¯‘é€šè¿‡

---

#### Task-1.4: ä¿®å¤ tokenize() æ–¹æ³•
```cpp
// ç°æœ‰ä»£ç ï¼ˆé”™è¯¯ï¼‰:
auto encoding = tokenizer_->Encode(text, false);

// éœ€è¦æ”¹ä¸º:
auto encoding = tokenizer_->Encode(text);
```

**éš¾åº¦**: â­ï¸  
**é¢„è®¡æ—¶é—´**: 10 åˆ†é’Ÿ  
**éªŒè¯æ–¹å¼**: ç¼–è¯‘é€šè¿‡

---

#### Task-1.5: æ›´æ–°ç±»å‹å®šä¹‰
```cpp
// æ£€æŸ¥æ‰€æœ‰ä½¿ç”¨ uint32_t çš„åœ°æ–¹ï¼Œæ”¹ä¸º int32_t
// tokenizers-cpp ä½¿ç”¨ int32_t è€Œä¸æ˜¯ uint32_t

// ç¤ºä¾‹:
std::vector<uint32_t> tokenIds;  // é”™è¯¯
std::vector<int32_t> tokenIds;   // æ­£ç¡®
```

**éš¾åº¦**: â­ï¸  
**é¢„è®¡æ—¶é—´**: 10 åˆ†é’Ÿ  
**éªŒè¯æ–¹å¼**: ç¼–è¯‘é€šè¿‡

---

### Task-2: ç¼–è¯‘å’Œé“¾æ¥é…ç½® ğŸ”´ é«˜ä¼˜å…ˆçº§

**é—®é¢˜æè¿°**:  
è™½ç„¶ CMake èƒ½æ‰¾åˆ°åº“ï¼Œä½†å¯èƒ½å­˜åœ¨é“¾æ¥é—®é¢˜

**å…·ä½“ä»»åŠ¡**:

#### Task-2.1: éªŒè¯åº“é“¾æ¥
```bash
# ç¡®ä¿ä»¥ä¸‹åº“éƒ½è¢«æ­£ç¡®é“¾æ¥:
- libtokenizers_cpp.a  (C++ åŒ…è£…å±‚)
- libtokenizers_c.a    (Rust æ ¸å¿ƒåº“)
- libsentencepiece.a   (SentencePiece ä¾èµ–)

# æ£€æŸ¥å‘½ä»¤:
cd build
make test_hf_tokenizer VERBOSE=1 | grep "tokenizers"
```

**éš¾åº¦**: â­ï¸â­ï¸â­ï¸  
**é¢„è®¡æ—¶é—´**: 1 å°æ—¶  
**éªŒè¯æ–¹å¼**: ç¼–è¯‘é“¾æ¥æˆåŠŸ

---

#### Task-2.2: è§£å†³æ½œåœ¨çš„ç¬¦å·å†²çª
```bash
# Rust åº“å¯èƒ½éœ€è¦é¢å¤–çš„ç³»ç»Ÿåº“
# macOS å¯èƒ½éœ€è¦:
- Security.framework
- Foundation.framework

# æ›´æ–° CMakeLists.txt:
if(APPLE)
    target_link_libraries(cllm_core
        ${TOKENIZERS_LIBRARIES}
        "-framework Security"
        "-framework Foundation"
    )
endif()
```

**éš¾åº¦**: â­ï¸â­ï¸â­ï¸  
**é¢„è®¡æ—¶é—´**: 1 å°æ—¶  
**éªŒè¯æ–¹å¼**: é“¾æ¥æˆåŠŸï¼Œæ—  undefined symbols é”™è¯¯

---

#### Task-2.3: æ·»åŠ  Rust æ ‡å‡†åº“ä¾èµ–
```bash
# tokenizers-cpp ä¾èµ– Rustï¼Œå¯èƒ½éœ€è¦é“¾æ¥:
- pthread
- dl (Linux)
- resolv (macOS)

# æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ åˆ° CMakeLists.txt
```

**éš¾åº¦**: â­ï¸â­ï¸  
**é¢„è®¡æ—¶é—´**: 30 åˆ†é’Ÿ  
**éªŒè¯æ–¹å¼**: é“¾æ¥æˆåŠŸ

---

### Task-3: æµ‹è¯•éªŒè¯ ğŸŸ¡ ä¸­ä¼˜å…ˆçº§

**å…·ä½“ä»»åŠ¡**:

#### Task-3.1: ç¼–è¯‘æµ‹è¯•ç¨‹åº
```bash
cd build
cmake .. -DUSE_TOKENIZERS_CPP=ON
make test_hf_tokenizer -j8
```

**éš¾åº¦**: â­ï¸  
**é¢„è®¡æ—¶é—´**: 10 åˆ†é’Ÿï¼ˆå‡è®¾ Task-1 å’Œ Task-2 å®Œæˆï¼‰  
**éªŒè¯æ–¹å¼**: ç¼–è¯‘æˆåŠŸï¼Œç”Ÿæˆ `bin/test_hf_tokenizer`

---

#### Task-3.2: è¿è¡ŒåŸºæœ¬æµ‹è¯•ï¼ˆä¸éœ€è¦æ¨¡å‹ï¼‰
```bash
cd build
./bin/test_hf_tokenizer --gtest_filter="HFTokenizerBasicTest.*"

# é¢„æœŸ: 8ä¸ªåŸºæœ¬æµ‹è¯•é€šè¿‡
```

**éš¾åº¦**: â­ï¸â­ï¸  
**é¢„è®¡æ—¶é—´**: 20 åˆ†é’Ÿ  
**éªŒè¯æ–¹å¼**: æ‰€æœ‰åŸºæœ¬æµ‹è¯•é€šè¿‡

---

#### Task-3.3: å‡†å¤‡æµ‹è¯•æ¨¡å‹
```bash
# ä¸‹è½½ä¸€ä¸ª HuggingFace æ¨¡å‹ï¼ˆåŒ…å« tokenizer.jsonï¼‰
# æ¨è: Qwen/Qwen2-7B-Instruct æˆ– meta-llama/Llama-2-7b-hf

# è®¾ç½®ç¯å¢ƒå˜é‡:
export CLLM_TEST_MODEL_PATH=/path/to/model
```

**éš¾åº¦**: â­ï¸  
**é¢„è®¡æ—¶é—´**: 30 åˆ†é’Ÿï¼ˆå–å†³äºä¸‹è½½é€Ÿåº¦ï¼‰  
**éªŒè¯æ–¹å¼**: æ¨¡å‹ç›®å½•åŒ…å« `tokenizer.json` å’Œ `config.json`

---

#### Task-3.4: è¿è¡Œé›†æˆæµ‹è¯•
```bash
cd build
export CLLM_TEST_MODEL_PATH=/path/to/model
./bin/test_hf_tokenizer --gtest_filter="HFTokenizerIntegrationTest.*"

# é¢„æœŸ: 6ä¸ªé›†æˆæµ‹è¯•é€šè¿‡
```

**éš¾åº¦**: â­ï¸â­ï¸â­ï¸  
**é¢„è®¡æ—¶é—´**: 1 å°æ—¶  
**éªŒè¯æ–¹å¼**: æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡

---

#### Task-3.5: è¿è¡Œç¤ºä¾‹ç¨‹åº
```bash
cd build
./bin/hf_tokenizer_example /path/to/model

# é¢„æœŸ: 5ä¸ªç¤ºä¾‹æ­£å¸¸è¿è¡Œï¼Œè¾“å‡ºæ­£ç¡®
```

**éš¾åº¦**: â­ï¸â­ï¸  
**é¢„è®¡æ—¶é—´**: 30 åˆ†é’Ÿ  
**éªŒè¯æ–¹å¼**: ç¤ºä¾‹è¿è¡Œæ— é”™è¯¯ï¼Œè¾“å‡ºåˆç†

---

## ğŸ¯ åŠŸèƒ½å¢å¼ºï¼ˆé‡è¦ï¼‰

### Task-4: ç‰¹æ®ŠTokenå¤„ç† ğŸŸ¡ ä¸­ä¼˜å…ˆçº§

**é—®é¢˜æè¿°**:  
`tokenizers-cpp` API ä¸æ”¯æŒ `addSpecialTokens` å’Œ `skipSpecialTokens` å‚æ•°ï¼Œéœ€è¦æ‰‹åŠ¨å®ç°

**å…·ä½“ä»»åŠ¡**:

#### Task-4.1: å®ç° addSpecialTokens åŠŸèƒ½
```cpp
std::vector<int> HFTokenizer::encode(const std::string& text, bool addSpecialTokens) {
    auto ids = tokenizer_->Encode(text);
    
    if (addSpecialTokens) {
        // åœ¨å¼€å¤´æ·»åŠ  BOS token
        if (bosId_ != -1) {
            ids.insert(ids.begin(), bosId_);
        }
        // åœ¨ç»“å°¾æ·»åŠ  EOS token
        if (eosId_ != -1) {
            ids.push_back(eosId_);
        }
    }
    
    return ids;
}
```

**éš¾åº¦**: â­ï¸â­ï¸â­ï¸  
**é¢„è®¡æ—¶é—´**: 1 å°æ—¶  
**éªŒè¯æ–¹å¼**: æµ‹è¯•ç”¨ä¾‹éªŒè¯ç‰¹æ®ŠTokenæ­£ç¡®æ·»åŠ 

---

#### Task-4.2: å®ç° skipSpecialTokens åŠŸèƒ½
```cpp
std::string HFTokenizer::decode(const std::vector<int>& ids, bool skipSpecialTokens) {
    std::vector<int32_t> tokenIds;
    
    for (int id : ids) {
        // å¦‚æœéœ€è¦è·³è¿‡ç‰¹æ®ŠToken
        if (skipSpecialTokens && isSpecialToken(id)) {
            continue;
        }
        tokenIds.push_back(static_cast<int32_t>(id));
    }
    
    return tokenizer_->Decode(tokenIds);
}
```

**éš¾åº¦**: â­ï¸â­ï¸â­ï¸  
**é¢„è®¡æ—¶é—´**: 1 å°æ—¶  
**éªŒè¯æ–¹å¼**: æµ‹è¯•ç”¨ä¾‹éªŒè¯ç‰¹æ®ŠTokenæ­£ç¡®è¿‡æ»¤

---

#### Task-4.3: æ›´æ–°æµ‹è¯•ç”¨ä¾‹
```cpp
// æ›´æ–°ç°æœ‰æµ‹è¯•ç”¨ä¾‹ï¼ŒéªŒè¯ç‰¹æ®ŠTokenå¤„ç†
TEST_F(HFTokenizerIntegrationTest, SpecialTokensHandling) {
    // æµ‹è¯• addSpecialTokens = true
    auto ids_with = tokenizer_->encode(text, true);
    EXPECT_EQ(ids_with.front(), tokenizer_->getBosId());
    EXPECT_EQ(ids_with.back(), tokenizer_->getEosId());
    
    // æµ‹è¯• addSpecialTokens = false
    auto ids_without = tokenizer_->encode(text, false);
    EXPECT_NE(ids_without.front(), tokenizer_->getBosId());
}
```

**éš¾åº¦**: â­ï¸â­ï¸  
**é¢„è®¡æ—¶é—´**: 30 åˆ†é’Ÿ  
**éªŒè¯æ–¹å¼**: æµ‹è¯•é€šè¿‡

---

### Task-5: æ‰¹é‡å¤„ç†æ”¯æŒ ğŸŸ¢ ä½ä¼˜å…ˆçº§

**å…·ä½“ä»»åŠ¡**:

#### Task-5.1: å®ç°æ‰¹é‡ç¼–ç 
```cpp
std::vector<std::vector<int>> HFTokenizer::encodeBatch(
    const std::vector<std::string>& texts,
    bool addSpecialTokens) {
    
    std::vector<std::vector<int>> results;
    results.reserve(texts.size());
    
    for (const auto& text : texts) {
        results.push_back(encode(text, addSpecialTokens));
    }
    
    return results;
}
```

**éš¾åº¦**: â­ï¸â­ï¸  
**é¢„è®¡æ—¶é—´**: 30 åˆ†é’Ÿ  
**éªŒè¯æ–¹å¼**: æµ‹è¯•æ‰¹é‡å¤„ç†æ­£ç¡®æ€§

---

#### Task-5.2: å®ç°æ‰¹é‡è§£ç 
```cpp
std::vector<std::string> HFTokenizer::decodeBatch(
    const std::vector<std::vector<int>>& batch_ids,
    bool skipSpecialTokens) {
    
    std::vector<std::string> results;
    results.reserve(batch_ids.size());
    
    for (const auto& ids : batch_ids) {
        results.push_back(decode(ids, skipSpecialTokens));
    }
    
    return results;
}
```

**éš¾åº¦**: â­ï¸â­ï¸  
**é¢„è®¡æ—¶é—´**: 30 åˆ†é’Ÿ  
**éªŒè¯æ–¹å¼**: æµ‹è¯•æ‰¹é‡å¤„ç†æ­£ç¡®æ€§

---

### Task-6: æ€§èƒ½ä¼˜åŒ– ğŸŸ¢ ä½ä¼˜å…ˆçº§

**å…·ä½“ä»»åŠ¡**:

#### Task-6.1: æ·»åŠ ç¼“å­˜æœºåˆ¶
```cpp
// ç¼“å­˜å¸¸ç”¨æ–‡æœ¬çš„ç¼–ç ç»“æœ
class HFTokenizer {
private:
    std::unordered_map<std::string, std::vector<int>> encodeCache_;
    size_t maxCacheSize_ = 10000;
};
```

**éš¾åº¦**: â­ï¸â­ï¸â­ï¸  
**é¢„è®¡æ—¶é—´**: 2 å°æ—¶  
**éªŒè¯æ–¹å¼**: æ€§èƒ½æµ‹è¯•ï¼Œç¼“å­˜å‘½ä¸­ç‡ > 50%

---

#### Task-6.2: å¹¶è¡Œæ‰¹é‡å¤„ç†
```cpp
// ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†æ‰¹é‡è¯·æ±‚
std::vector<std::vector<int>> HFTokenizer::encodeBatchParallel(
    const std::vector<std::string>& texts) {
    // ä½¿ç”¨ BS_thread_pool.hpp
}
```

**éš¾åº¦**: â­ï¸â­ï¸â­ï¸â­ï¸  
**é¢„è®¡æ—¶é—´**: 3 å°æ—¶  
**éªŒè¯æ–¹å¼**: æ€§èƒ½æµ‹è¯•ï¼ŒåŠ é€Ÿæ¯” > 2x

---

## ğŸ“š æ–‡æ¡£å’Œç»´æŠ¤ï¼ˆå¯é€‰ï¼‰

### Task-7: APIæ–‡æ¡£æ›´æ–° ğŸŸ¢ ä½ä¼˜å…ˆçº§

#### Task-7.1: æ›´æ–°å¤´æ–‡ä»¶æ³¨é‡Š
```cpp
// ä¸ºæ‰€æœ‰å…¬å…±æ–¹æ³•æ·»åŠ è¯¦ç»†çš„ Doxygen æ³¨é‡Š
/**
 * @brief ç¼–ç æ–‡æœ¬ä¸ºToken IDs
 * @param text è¾“å…¥æ–‡æœ¬
 * @param addSpecialTokens æ˜¯å¦æ·»åŠ ç‰¹æ®ŠTokenï¼ˆBOS/EOSï¼‰
 * @return Token IDs å‘é‡
 * @note tokenizers-cpp ä¸åŸç”Ÿæ”¯æŒç‰¹æ®ŠTokenå‚æ•°ï¼Œç”±æœ¬ç±»æ‰‹åŠ¨å¤„ç†
 */
std::vector<int> encode(const std::string& text, bool addSpecialTokens = true);
```

**éš¾åº¦**: â­ï¸â­ï¸  
**é¢„è®¡æ—¶é—´**: 1 å°æ—¶  
**éªŒè¯æ–¹å¼**: æ–‡æ¡£ç”ŸæˆæˆåŠŸ

---

#### Task-7.2: åˆ›å»º API å‚è€ƒæ–‡æ¡£
```markdown
# HFTokenizer API å‚è€ƒ

## ç±»æ–¹æ³•

### encode()
- åŠŸèƒ½: å°†æ–‡æœ¬ç¼–ç ä¸ºToken IDs
- å‚æ•°: ...
- è¿”å›å€¼: ...
- ç¤ºä¾‹: ...
```

**éš¾åº¦**: â­ï¸â­ï¸  
**é¢„è®¡æ—¶é—´**: 2 å°æ—¶  
**éªŒè¯æ–¹å¼**: æ–‡æ¡£å®Œæ•´æ¸…æ™°

---

### Task-8: æ•…éšœæ’æŸ¥æŒ‡å— ğŸŸ¢ ä½ä¼˜å…ˆçº§

#### Task-8.1: æ”¶é›†å¸¸è§é—®é¢˜
```markdown
# HFTokenizer æ•…éšœæ’æŸ¥

## é—®é¢˜1: ç¼–è¯‘é”™è¯¯ - undefined reference to `tokenizers::Tokenizer::FromFile`
è§£å†³æ–¹æ¡ˆ: ...

## é—®é¢˜2: è¿è¡Œæ—¶é”™è¯¯ - Failed to load tokenizer
è§£å†³æ–¹æ¡ˆ: ...
```

**éš¾åº¦**: â­ï¸â­ï¸  
**é¢„è®¡æ—¶é—´**: 1 å°æ—¶  
**ä¾èµ–**: å®Œæˆ Task-1 åˆ° Task-3

---

#### Task-8.2: åˆ›å»ºè°ƒè¯•æ£€æŸ¥æ¸…å•
```markdown
## HFTokenizer è°ƒè¯•æ£€æŸ¥æ¸…å•

ç¼–è¯‘é˜¶æ®µ:
- [ ] tokenizers-cpp å·²æ­£ç¡®å®‰è£…
- [ ] CMake æ‰¾åˆ°äº† tokenizers-cpp åº“
- [ ] é“¾æ¥äº†æ‰€æœ‰å¿…éœ€çš„åº“

è¿è¡Œé˜¶æ®µ:
- [ ] tokenizer.json æ–‡ä»¶å­˜åœ¨
- [ ] æ¨¡å‹è·¯å¾„æ­£ç¡®
- [ ] é…ç½®æ–‡ä»¶å¯è¯»
```

**éš¾åº¦**: â­ï¸  
**é¢„è®¡æ—¶é—´**: 30 åˆ†é’Ÿ

---

### Task-9: CI/CD é›†æˆ ğŸŸ¢ ä½ä¼˜å…ˆçº§

#### Task-9.1: æ·»åŠ  GitHub Actions å·¥ä½œæµ
```yaml
name: Build and Test HFTokenizer

on: [push, pull_request]

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Install Rust
        run: curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
      - name: Install tokenizers-cpp
        run: ./scripts/install_tokenizers_cpp.sh
      - name: Build cLLM
        run: |
          mkdir build && cd build
          cmake .. -DUSE_TOKENIZERS_CPP=ON
          make -j$(nproc)
      - name: Run tests
        run: cd build && ./bin/test_hf_tokenizer
```

**éš¾åº¦**: â­ï¸â­ï¸â­ï¸  
**é¢„è®¡æ—¶é—´**: 2 å°æ—¶  
**éªŒè¯æ–¹å¼**: GitHub Actions è¿è¡ŒæˆåŠŸ

---

## ğŸ“Š ä»»åŠ¡ä¼˜å…ˆçº§çŸ©é˜µ

| ä»»åŠ¡ID | ä»»åŠ¡åç§° | ä¼˜å…ˆçº§ | éš¾åº¦ | é¢„è®¡æ—¶é—´ | ä¾èµ– |
|--------|----------|--------|------|----------|------|
| **Task-1.1** | ä¿®å¤ load() æ–¹æ³• | ğŸ”´ é«˜ | â­ï¸â­ï¸ | 30åˆ†é’Ÿ | æ—  |
| **Task-1.2** | ä¿®å¤ encode() æ–¹æ³• | ğŸ”´ é«˜ | â­ï¸â­ï¸ | 20åˆ†é’Ÿ | Task-1.1 |
| **Task-1.3** | ä¿®å¤ decode() æ–¹æ³• | ğŸ”´ é«˜ | â­ï¸â­ï¸ | 20åˆ†é’Ÿ | Task-1.1 |
| **Task-1.4** | ä¿®å¤ tokenize() æ–¹æ³• | ğŸ”´ é«˜ | â­ï¸ | 10åˆ†é’Ÿ | Task-1.1 |
| **Task-1.5** | æ›´æ–°ç±»å‹å®šä¹‰ | ğŸ”´ é«˜ | â­ï¸ | 10åˆ†é’Ÿ | æ—  |
| **Task-2.1** | éªŒè¯åº“é“¾æ¥ | ğŸ”´ é«˜ | â­ï¸â­ï¸â­ï¸ | 1å°æ—¶ | Task-1.* |
| **Task-2.2** | è§£å†³ç¬¦å·å†²çª | ğŸ”´ é«˜ | â­ï¸â­ï¸â­ï¸ | 1å°æ—¶ | Task-2.1 |
| **Task-2.3** | æ·»åŠ Rustä¾èµ– | ğŸ”´ é«˜ | â­ï¸â­ï¸ | 30åˆ†é’Ÿ | Task-2.1 |
| **Task-3.1** | ç¼–è¯‘æµ‹è¯•ç¨‹åº | ğŸŸ¡ ä¸­ | â­ï¸ | 10åˆ†é’Ÿ | Task-1.*, Task-2.* |
| **Task-3.2** | è¿è¡ŒåŸºæœ¬æµ‹è¯• | ğŸŸ¡ ä¸­ | â­ï¸â­ï¸ | 20åˆ†é’Ÿ | Task-3.1 |
| **Task-3.3** | å‡†å¤‡æµ‹è¯•æ¨¡å‹ | ğŸŸ¡ ä¸­ | â­ï¸ | 30åˆ†é’Ÿ | æ—  |
| **Task-3.4** | è¿è¡Œé›†æˆæµ‹è¯• | ğŸŸ¡ ä¸­ | â­ï¸â­ï¸â­ï¸ | 1å°æ—¶ | Task-3.1, Task-3.3 |
| **Task-3.5** | è¿è¡Œç¤ºä¾‹ç¨‹åº | ğŸŸ¡ ä¸­ | â­ï¸â­ï¸ | 30åˆ†é’Ÿ | Task-3.1, Task-3.3 |
| **Task-4.1** | å®ç° addSpecialTokens | ğŸŸ¡ ä¸­ | â­ï¸â­ï¸â­ï¸ | 1å°æ—¶ | Task-3.* |
| **Task-4.2** | å®ç° skipSpecialTokens | ğŸŸ¡ ä¸­ | â­ï¸â­ï¸â­ï¸ | 1å°æ—¶ | Task-3.* |
| **Task-4.3** | æ›´æ–°æµ‹è¯•ç”¨ä¾‹ | ğŸŸ¡ ä¸­ | â­ï¸â­ï¸ | 30åˆ†é’Ÿ | Task-4.1, Task-4.2 |
| **Task-5.1** | å®ç°æ‰¹é‡ç¼–ç  | ğŸŸ¢ ä½ | â­ï¸â­ï¸ | 30åˆ†é’Ÿ | Task-3.* |
| **Task-5.2** | å®ç°æ‰¹é‡è§£ç  | ğŸŸ¢ ä½ | â­ï¸â­ï¸ | 30åˆ†é’Ÿ | Task-3.* |
| **Task-6.1** | æ·»åŠ ç¼“å­˜æœºåˆ¶ | ğŸŸ¢ ä½ | â­ï¸â­ï¸â­ï¸ | 2å°æ—¶ | Task-3.* |
| **Task-6.2** | å¹¶è¡Œæ‰¹é‡å¤„ç† | ğŸŸ¢ ä½ | â­ï¸â­ï¸â­ï¸â­ï¸ | 3å°æ—¶ | Task-5.* |
| **Task-7.1** | æ›´æ–°å¤´æ–‡ä»¶æ³¨é‡Š | ğŸŸ¢ ä½ | â­ï¸â­ï¸ | 1å°æ—¶ | Task-1.* |
| **Task-7.2** | åˆ›å»ºAPIå‚è€ƒæ–‡æ¡£ | ğŸŸ¢ ä½ | â­ï¸â­ï¸ | 2å°æ—¶ | Task-7.1 |
| **Task-8.1** | æ”¶é›†å¸¸è§é—®é¢˜ | ğŸŸ¢ ä½ | â­ï¸â­ï¸ | 1å°æ—¶ | Task-3.* |
| **Task-8.2** | åˆ›å»ºè°ƒè¯•æ£€æŸ¥æ¸…å• | ğŸŸ¢ ä½ | â­ï¸ | 30åˆ†é’Ÿ | Task-8.1 |
| **Task-9.1** | æ·»åŠ CI/CD | ğŸŸ¢ ä½ | â­ï¸â­ï¸â­ï¸ | 2å°æ—¶ | Task-3.* |

---

## ğŸ¯ æ¨èæ‰§è¡Œé¡ºåº

### é˜¶æ®µ1: æ ¸å¿ƒä¿®å¤ï¼ˆå¿…é¡»å®Œæˆï¼‰â±ï¸ é¢„è®¡ 4 å°æ—¶

```
Day 1 ä¸Šåˆ:
1. Task-1.1 â†’ Task-1.2 â†’ Task-1.3 â†’ Task-1.4 â†’ Task-1.5
   (API é€‚é…ä¿®å¤ï¼Œçº¦ 1.5 å°æ—¶)

Day 1 ä¸‹åˆ:
2. Task-2.1 â†’ Task-2.2 â†’ Task-2.3
   (ç¼–è¯‘å’Œé“¾æ¥é…ç½®ï¼Œçº¦ 2.5 å°æ—¶)
```

### é˜¶æ®µ2: æµ‹è¯•éªŒè¯ï¼ˆå¿…é¡»å®Œæˆï¼‰â±ï¸ é¢„è®¡ 3 å°æ—¶

```
Day 2 ä¸Šåˆ:
3. Task-3.1 â†’ Task-3.2
   (ç¼–è¯‘å’ŒåŸºæœ¬æµ‹è¯•ï¼Œçº¦ 30 åˆ†é’Ÿ)

4. Task-3.3 (å¹¶è¡Œè¿›è¡Œï¼Œä¸‹è½½æ¨¡å‹)

Day 2 ä¸‹åˆ:
5. Task-3.4 â†’ Task-3.5
   (é›†æˆæµ‹è¯•å’Œç¤ºä¾‹ï¼Œçº¦ 1.5 å°æ—¶)
```

### é˜¶æ®µ3: åŠŸèƒ½å¢å¼ºï¼ˆé‡è¦ï¼‰â±ï¸ é¢„è®¡ 5 å°æ—¶

```
Day 3:
6. Task-4.1 â†’ Task-4.2 â†’ Task-4.3
   (ç‰¹æ®ŠTokenå¤„ç†ï¼Œçº¦ 2.5 å°æ—¶)

7. Task-5.1 â†’ Task-5.2
   (æ‰¹é‡å¤„ç†ï¼Œçº¦ 1 å°æ—¶)

8. Task-6.1 (å¯é€‰)
   (ç¼“å­˜ä¼˜åŒ–ï¼Œçº¦ 2 å°æ—¶)
```

### é˜¶æ®µ4: æ–‡æ¡£å’Œç»´æŠ¤ï¼ˆå¯é€‰ï¼‰â±ï¸ é¢„è®¡ 7 å°æ—¶

```
Day 4-5:
9. Task-7.1 â†’ Task-7.2
   (API æ–‡æ¡£ï¼Œçº¦ 3 å°æ—¶)

10. Task-8.1 â†’ Task-8.2
    (æ•…éšœæ’æŸ¥ï¼Œçº¦ 1.5 å°æ—¶)

11. Task-9.1
    (CI/CDï¼Œçº¦ 2 å°æ—¶)
```

---

## ğŸ”§ Agent åˆ†å·¥å»ºè®®

### Agent-1: æ ¸å¿ƒå¼€å‘è€…ï¼ˆC++ ä¸“å®¶ï¼‰
**è´Ÿè´£ä»»åŠ¡**:
- Task-1.* (API é€‚é…ä¿®å¤)
- Task-2.* (ç¼–è¯‘å’Œé“¾æ¥é…ç½®)
- Task-4.* (ç‰¹æ®ŠTokenå¤„ç†)
- Task-5.* (æ‰¹é‡å¤„ç†)

**æŠ€èƒ½è¦æ±‚**:
- ç†Ÿæ‚‰ C++17
- äº†è§£ CMake
- æœ‰ Rust FFI ç»éªŒæ›´ä½³

---

### Agent-2: æµ‹è¯•å·¥ç¨‹å¸ˆ
**è´Ÿè´£ä»»åŠ¡**:
- Task-3.* (æµ‹è¯•éªŒè¯)
- Task-4.3 (æµ‹è¯•ç”¨ä¾‹æ›´æ–°)
- Task-8.* (æ•…éšœæ’æŸ¥æŒ‡å—)

**æŠ€èƒ½è¦æ±‚**:
- ç†Ÿæ‚‰ Google Test
- äº†è§£æ¨¡å‹æ–‡ä»¶æ ¼å¼
- æœ‰æµ‹è¯•ç»éªŒ

---

### Agent-3: æ€§èƒ½ä¼˜åŒ–å·¥ç¨‹å¸ˆ
**è´Ÿè´£ä»»åŠ¡**:
- Task-6.* (æ€§èƒ½ä¼˜åŒ–)

**æŠ€èƒ½è¦æ±‚**:
- ç†Ÿæ‚‰å¤šçº¿ç¨‹ç¼–ç¨‹
- äº†è§£ç¼“å­˜è®¾è®¡
- æœ‰æ€§èƒ½åˆ†æç»éªŒ

---

### Agent-4: æ–‡æ¡£å·¥ç¨‹å¸ˆ
**è´Ÿè´£ä»»åŠ¡**:
- Task-7.* (API æ–‡æ¡£)
- Task-8.* (æ•…éšœæ’æŸ¥)

**æŠ€èƒ½è¦æ±‚**:
- æŠ€æœ¯å†™ä½œèƒ½åŠ›
- äº†è§£ Markdown å’Œ Doxygen
- æœ‰ API æ–‡æ¡£ç»éªŒ

---

### Agent-5: DevOps å·¥ç¨‹å¸ˆ
**è´Ÿè´£ä»»åŠ¡**:
- Task-9.* (CI/CD é›†æˆ)

**æŠ€èƒ½è¦æ±‚**:
- ç†Ÿæ‚‰ GitHub Actions
- äº†è§£ Docker
- æœ‰ CI/CD ç»éªŒ

---

## ğŸ“ è¿›åº¦è·Ÿè¸ªæ¨¡æ¿

### ä»»åŠ¡è¿›åº¦è¡¨

| ä»»åŠ¡ID | è´Ÿè´£Agent | çŠ¶æ€ | å¼€å§‹æ—¶é—´ | å®Œæˆæ—¶é—´ | å¤‡æ³¨ |
|--------|-----------|------|----------|----------|------|
| Task-1.1 | Agent-1 | â³ å¾…å¼€å§‹ | - | - | - |
| Task-1.2 | Agent-1 | â³ å¾…å¼€å§‹ | - | - | - |
| ... | ... | ... | ... | ... | ... |

### çŠ¶æ€è¯´æ˜
- â³ å¾…å¼€å§‹
- ğŸƒ è¿›è¡Œä¸­
- âœ… å·²å®Œæˆ
- âŒ å·²é˜»å¡
- âš ï¸ éœ€è¦å¸®åŠ©

---

## ğŸš¨ å·²çŸ¥é—®é¢˜å’Œé£é™©

### é—®é¢˜1: tokenizers-cpp API é™åˆ¶
**æè¿°**: tokenizers-cpp çš„ API æ¯”è¾ƒç®€å•ï¼Œä¸æ”¯æŒç‰¹æ®ŠTokenå‚æ•°  
**å½±å“**: éœ€è¦æ‰‹åŠ¨å¤„ç†ç‰¹æ®ŠToken  
**é£é™©ç­‰çº§**: ğŸŸ¡ ä¸­  
**ç¼“è§£æªæ–½**: Task-4 ä¸“é—¨å¤„ç†è¿™ä¸ªé—®é¢˜

### é—®é¢˜2: Rust åº“é“¾æ¥å¤æ‚
**æè¿°**: tokenizers-cpp ä¾èµ– Rustï¼Œé“¾æ¥å¯èƒ½å¤æ‚  
**å½±å“**: å¯èƒ½å‡ºç° undefined symbols é”™è¯¯  
**é£é™©ç­‰çº§**: ğŸ”´ é«˜  
**ç¼“è§£æªæ–½**: Task-2 ä¸“é—¨å¤„ç†é“¾æ¥é—®é¢˜

### é—®é¢˜3: æ¨¡å‹å…¼å®¹æ€§
**æè¿°**: ä¸åŒæ¨¡å‹çš„ tokenizer.json æ ¼å¼å¯èƒ½ç•¥æœ‰å·®å¼‚  
**å½±å“**: å¯èƒ½æ— æ³•åŠ è½½æŸäº›æ¨¡å‹  
**é£é™©ç­‰çº§**: ğŸŸ¡ ä¸­  
**ç¼“è§£æªæ–½**: å¤šæ¨¡å‹æµ‹è¯•ï¼ˆTask-3.4ï¼‰

---

## ğŸ“ è”ç³»å’Œåä½œ

### é—®é¢˜åé¦ˆ
- é‡åˆ°é—®é¢˜æ—¶ï¼Œåœ¨å¯¹åº” Task ä¸­æ·»åŠ æ³¨é‡Š
- ä½¿ç”¨ GitHub Issues è·Ÿè¸ª Bug
- é‡è¦é—®é¢˜åŠæ—¶æ²Ÿé€š

### ä»£ç å®¡æŸ¥
- æ¯ä¸ª Task å®Œæˆåæäº¤ PR
- è‡³å°‘ä¸€ä¸ªå…¶ä»– Agent å®¡æŸ¥
- é€šè¿‡æ‰€æœ‰æµ‹è¯•ååˆå¹¶

### æ–‡æ¡£æ›´æ–°
- å®Œæˆä»»åŠ¡åæ›´æ–°æœ¬æ–‡æ¡£
- æ ‡è®°ä»»åŠ¡çŠ¶æ€
- è®°å½•é‡åˆ°çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

---

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£
- [tokenizers-cpp GitHub](https://github.com/mlc-ai/tokenizers-cpp)
- [HuggingFace Tokenizers](https://github.com/huggingface/tokenizers)
- [Rust FFI æŒ‡å—](https://doc.rust-lang.org/nomicon/ffi.html)

### é¡¹ç›®å†…æ–‡æ¡£
- `docs/guides/Tokenizersåº“å®‰è£…æŒ‡å—.md`
- `docs/guides/tokenizers-cppé›†æˆéªŒè¯æŒ‡å—.md`
- `docs/guides/tokenizers-cppé›†æˆæ‰§è¡Œæ€»ç»“.md`

### ç›¸å…³ä»£ç 
- `include/cllm/tokenizer/hf_tokenizer.h`
- `src/tokenizer/hf_tokenizer.cpp`
- `tests/test_hf_tokenizer.cpp`
- `examples/hf_tokenizer_example.cpp`

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2026-01-11  
**ç»´æŠ¤è€…**: AI Assistant  
**çŠ¶æ€**: âœ… å®Œæ•´ï¼Œå¯ç”¨äºåˆ†å·¥æ‰§è¡Œ
