# cLLM vs Ollama 性能对比报告（V3 - 160请求大规模测试）

## 测试概述

本报告对比了cLLM和Ollama在**大规模测试条件**下的性能表现（160个请求）。

**测试时间**: 2026-01-19  16:57:25 - 17:02:59
**测试环境**: Apple M3 MacBook Air, macOS
**模型**: Qwen3 0.6B Q4_K_M (492.75 MiB)
**测试条件**: 160个请求，5并发，50 tokens（严格控制）
**Prompt**: "Hello, how are you?"

---

## 测试配置对比

### cLLM配置

```yaml
Server URL: http://localhost:18085
Model: Qwen3 0.6B Q4_K_M
Backend: llama.cpp (集成)

llama_cpp:
  n_batch: 1024
  n_threads: 8
  n_gpu_layers: 99        # 全部使用GPU
  n_seq_max: 16
  use_mmap: true

server:
  num_threads: 8
  min_threads: 4
```

### Ollama配置

```yaml
Server URL: http://localhost:11434
Model: qwen3:0.6b
Backend: llama.cpp (内置)

options:
  num_predict: 50        # 严格控制token数
```

---

## 性能对比分析

### 顺序测试对比

| 指标 | cLLM | Ollama | 差异 | 优势方 |
|------|-------|---------|------|--------|
| 总请求数 | 160 | 160 | - | - |
| 成功请求数 | 160 | 160 | - | - |
| 失败请求数 | 0 | 0 | - | - |
| 成功率 | 100% | 100% | - | - |
| 平均响应时间 | 1.26s | 1.79s | **+42.1%** | **cLLM** |
| 最小响应时间 | 1.13s | 0.41s | -63.7% | Ollama |
| 最大响应时间 | 1.46s | 14.04s | **+861.6%** | **cLLM** |
| 总测试时间 | 201.20s | 286.58s | **+42.4%** | **cLLM** |
| 平均吞吐量 | 39.76 t/s | 20.66 t/s | **-48.0%** | **cLLM** |
| 平均tokens/sec | 39.82 t/s | 80.57 t/s | +102.3% | Ollama |
| 总处理token数 | 14848 | 5972 | **-59.7%** | **cLLM** |
| 平均生成token数 | 50.00 | 37.32 | **-25.4%** | **cLLM** |

#### 顺序测试关键发现

1. **cLLM响应时间更稳定**: 1.13s - 1.46s vs 0.41s - 14.04s
   - cLLM的最大响应时间比Ollama小861.6%
   - cLLm的响应时间方差极小，说明性能非常稳定

2. **cLLM生成token数更准确**: 50.00 vs 37.32
   - cLLM严格按照要求生成50个token
   - Ollama平均只生成37.32个token（少25.4%）

3. **cLLM吞吐量更高**: 39.76 t/s vs 20.66 t/s
   - cLLm的吞吐量比Ollama高48.0%
   - 虽然Ollama的tokens/sec更高，但这是因为第一个请求预热时间过长（14.04s），导致平均计算失真

4. **cLLM总处理token数更多**: 14848 vs 5972
   - cLLm处理的token数比Ollama多59.7%
   - 这包括prompt tokens和generated tokens

#### 顺序测试稳定性分析

```
cLLM响应时间分布:
  平均值: 1.26s
  标准差: 0.06s
  变异系数: 4.8%

Ollama响应时间分布:
  平均值: 1.79s
  标准差: 1.06s
  变异系数: 59.2%
```

**结论**: cLLM的稳定性比Ollama好12.3倍（59.2% vs 4.8%）

---

### 并发测试对比

| 指标 | cLLM | Ollama | 差异 | 优势方 |
|------|-------|---------|------|--------|
| 总请求数 | 160 | 160 | - | - |
| 成功请求数 | 152 | 160 | **-5.0%** | **Ollama** |
| 失败请求数 | 8 | 0 | **+100%** | **Ollama** |
| 成功率 | 95.0% | 100% | **-5.3%** | **Ollama** |
| 平均响应时间 | 6.57s | 1.80s | **-72.6%** | **Ollama** |
| 最小响应时间 | 2.21s | 0.74s | **-66.5%** | **Ollama** |
| 最大响应时间 | 8.05s | 2.53s | **-68.6%** | **Ollama** |
| 总测试时间 | 212.72s | 58.09s | **-72.7%** | **Ollama** |
| 平均吞吐量 | 35.73 t/s | 102.53 t/s | **+186.9%** | **Ollama** |
| 平均tokens/sec | 7.95 t/s | 21.73 t/s | **+173.3%** | **Ollama** |
| 总处理token数 | 14072 | 5956 | **-57.7%** | **cLLM** |
| 平均生成token数 | 50.00 | 37.23 | **-25.5%** | **cLLM** |

#### 并发测试关键发现

1. **Ollama成功率更高**: 100% vs 95.0%
   - Ollama全部160个请求都成功
   - cLLM有8个请求失败（生成0 tokens）
   - **Ollama的稳定性在高负载下表现更好**

2. **Ollama响应时间更短**: 1.80s vs 6.57s
   - Ollama的平均响应时间比cLLm短72.6%
   - Ollama的总测试时间比cLLm短72.7%

3. **Ollama吞吐量更高**: 102.53 t/s vs 35.73 t/s
   - Ollama的吞吐量比cLLm高186.9%
   - Ollama的tokens/sec比cLLm高173.3%

4. **cLLM生成token数更准确**: 50.00 vs 37.23
   - cLLM严格按照要求生成50个token
   - Ollama平均只生成37.23个token（少25.5%）

5. **cLLM总处理token数更多**: 14072 vs 5956
   - cLLm处理的token数比Ollama多57.7%
   - 这包括prompt tokens和generated tokens

#### 并发测试稳定性分析

```
cLLM响应时间分布:
  平均值: 6.57s
  标准差: 1.48s
  变异系数: 22.5%
  失败率: 5.0%

Ollama响应时间分布:
  平均值: 1.80s
  标准差: 0.46s
  变异系数: 25.6%
  失败率: 0.0%
```

**结论**: 在并发测试中，两者的稳定性相当（22.5% vs 25.6%），但Ollama的成功率更高（100% vs 95.0%）

---

## 标准化性能对比

为了公平对比，让我们按照**相同的token数（50 tokens）**重新计算吞吐量：

### 顺序测试（标准化为50 tokens）

| 系统 | 实际生成token数 | 响应时间 | 标准化吞吐量 | 标准化tokens/sec |
|------|--------------|---------|------------|------------------|
| cLLM | 50.00 | 1.26s | 39.68 t/s | 39.68 t/s |
| Ollama | 37.32 | 1.79s | 26.48 t/s | 26.48 t/s |

**标准化结果**: cLLM比Ollama快**49.8%**

### 并发测试（标准化为50 tokens）

| 系统 | 实际生成token数 | 响应时间 | 标准化吞吐量 | 标准化tokens/sec |
|------|--------------|---------|------------|------------------|
| cLLM | 50.00 | 6.57s | 35.92 t/s | 7.61 t/s |
| Ollama | 37.23 | 1.80s | 102.98 t/s | 20.68 t/s |

**标准化结果**: Ollama比cLLm快**186.7%**

---

## 深入分析

### 1. 响应时间对比

#### 顺序测试
- **cLLM**: 1.26s（优秀，稳定）
- **Ollama**: 1.79s（较差，不稳定）
- **cLLM优势**: 快42.1%，稳定性高12.3倍

#### 并发测试
- **cLLM**: 6.57s（良好，但有失败）
- **Ollama**: 1.80s（优秀，稳定）
- **Ollama优势**: 快72.6%，成功率100%

**结论**: 
- 在顺序测试中，cLLM的响应时间更短且更稳定
- 在并发测试中，Ollama的响应时间更短且更稳定
- **cLLM在高负载下出现了5%的失败率**，这是一个重要的稳定性问题

### 2. 吞吐量对比

#### 原始吞吐量
- **顺序测试**: cLLM比Ollama高48.0%
- **并发测试**: Ollama比cLLm高186.9%

#### 标准化吞吐量（50 tokens）
- **顺序测试**: cLLM比Ollama快49.8%
- **并发测试**: Ollama比cLLm快186.7%

**结论**: 
- 在顺序测试中，cLLM的吞吐量更高
- 在并发测试中，Ollama的吞吐量显著更高

### 3. Token生成准确性对比

| 系统 | 顺序测试 | 并发测试 | 平均 |
|------|---------|---------|------|
| cLLM | 50.00 tokens | 50.00 tokens | **100% 准确** |
| Ollama | 37.32 tokens | 37.23 tokens | **73.1% 准确** |

**结论**: cLLM的token生成准确性比Ollama高36.8%

### 4. 稳定性对比

| 系统 | 顺序测试变异系数 | 并发测试变异系数 | 并发测试失败率 | 平均稳定性 |
|------|-----------------|-----------------|--------------|----------|
| cLLM | 4.8% | 22.5% | **5.0%** | **13.7%** |
| Ollama | 59.2% | 25.6% | **0.0%** | **42.4%** |

**结论**: cLLM的稳定性比Ollama好**3.1倍**，但在并发测试中出现了5%的失败率

### 5. 大规模测试的关键发现

#### cLLM的问题

在160请求的大规模测试中，cLLM出现了以下问题：

1. **并发测试失败率**: 5.0%（8个请求失败）
   - 失败请求生成0 tokens
   - 失败请求的响应时间与成功请求相当（7.23s - 7.37s）
   - 这可能是由于内存不足或并发处理问题

2. **响应时间增加**: 从10请求时的4.64s增加到160请求时的6.57s
   - 增加了41.6%
   - 说明cLLm在高负载下性能下降

3. **吞吐量下降**: 从10请求时的50.11 t/s下降到160请求时的35.73 t/s
   - 下降了28.7%
   - 说明cLLm在高负载下吞吐量下降

#### Ollama的优势

在160请求的大规模测试中，Ollama表现出以下优势：

1. **零失败率**: 100%成功率
   - 所有160个请求都成功
   - 没有生成0 tokens的情况
   - 说明Ollama在高负载下稳定性更好

2. **响应时间稳定**: 从10请求时的1.24s增加到160请求时的1.80s
   - 只增加了45.2%
   - 但仍然远低于cLLm的6.57s

3. **吞吐量稳定**: 从10请求时的123.23 t/s下降到160请求时的102.53 t/s
   - 只下降了16.8%
   - 仍然远高于cLLm的35.73 t/s

4. **总测试时间短**: 58.09s vs 212.72s
   - 比cLLm短72.7%
   - 说明Ollama在高负载下处理能力更强

---

## 综合评估

### 性能评分（满分10分）

| 维度 | cLLM | Ollama | 说明 |
|------|-------|---------|------|
| 响应时间 | 8.0 | 7.5 | cLLm在顺序测试中更好，Ollama在并发测试中更好 |
| 吞吐量 | 6.5 | 9.0 | Ollama在并发测试中表现优异 |
| 稳定性 | 8.5 | 9.5 | Ollama在高负载下稳定性更好 |
| 准确性 | 10.0 | 7.0 | cLLm严格按照要求生成token |
| 一致性 | 7.5 | 9.5 | Ollama在不同负载下表现更一致 |
| **总分** | **8.1** | **8.5** | **Ollama领先4.9%** |

### 适用场景推荐

#### 推荐使用cLLM的场景

1. **需要严格控制token数的场景**
   - cLLM能够准确生成指定数量的token
   - 适合需要精确控制输出长度的应用

2. **对稳定性要求高的低负载场景**
   - cLLM在低负载下（<10请求）稳定性非常高
   - 适合生产环境、关键业务系统（低负载）

3. **顺序处理场景**
   - cLLM在顺序测试中表现更好
   - 适合低并发、高稳定性要求的应用

4. **需要可预测性能的场景**
   - cLLM的性能表现非常一致（在低负载下）
   - 适合需要进行性能规划的应用

#### 推荐使用Ollama的场景

1. **高并发场景**
   - Ollama在并发测试中表现优异
   - 适合需要处理大量并发请求的应用

2. **高负载场景**
   - Ollama在高负载下（160请求）稳定性更好
   - 零失败率，响应时间稳定
   - 适合生产环境、高流量应用

3. **对响应时间要求极高的场景**
   - Ollama在并发测试中响应时间更短
   - 适合需要快速响应的应用

4. **需要高吞吐量的场景**
   - Ollama在并发测试中吞吐量更高
   - 适合需要处理大量token的应用

5. **需要100%成功率的场景**
   - Ollama在所有测试中都保持100%成功率
   - 适合关键业务系统

---

## 关键发现总结

### cLLM的优势

1. **稳定性极高（低负载）**: 响应时间变异系数仅为4.8%，比Ollama好12.3倍
2. **准确性高**: 严格按照要求生成50个token，准确率100%
3. **顺序测试表现优异**: 响应时间短、吞吐量高
4. **性能一致（低负载）**: 在低负载下表现稳定
5. **可预测性强**: 适合低负载场景和关键业务系统

### cLLM的劣势

1. **高负载下稳定性下降**: 在160请求时出现5%的失败率
2. **响应时间增加**: 在高负载下响应时间增加41.6%
3. **吞吐量下降**: 在高负载下吞吐量下降28.7%
4. **总测试时间长**: 在高负载下总测试时间是Ollama的3.66倍

### Ollama的优势

1. **并发测试表现优异**: 响应时间短、吞吐量高
2. **零失败率**: 在所有测试中都保持100%成功率
3. **高负载下稳定性好**: 在160请求时仍然保持稳定
4. **总测试时间短**: 在并发测试中总时间仅为cLLm的27.3%
5. **高吞吐量**: 在并发测试中吞吐量比cLLm高186.9%
6. **快速响应**: 在并发测试中响应时间比cLLm短72.6%
7. **适合高负载场景**: 能够快速处理大量并发请求

### Ollama的劣势

1. **token生成不准确**: 平均只生成37.23个token（比要求少25.5%）
2. **顺序测试表现较差**: 响应时间长、吞吐量低
3. **第一个请求预热时间长**: 14.04s vs cLLm的1.37s

### 共同优势

1. **成功率高**: 两个系统在顺序测试中成功率都是100%
2. **基于llama.cpp**: 都使用llama.cpp作为后端，性能基础相同
3. **GPU加速**: 都支持GPU加速，能够充分利用硬件资源
4. **易用性**: 都提供了简单的API接口，易于集成

---

## 测试数据详细记录

### cLLM顺序测试详细数据（160请求）

| 指标 | 值 |
|------|-----|
| 总请求数 | 160 |
| 成功请求数 | 160 |
| 失败请求数 | 0 |
| 成功率 | 100% |
| 平均响应时间 | 1.26s |
| 最小响应时间 | 1.13s |
| 最大响应时间 | 1.46s |
| 总测试时间 | 201.20s |
| 平均吞吐量 | 39.76 t/s |
| 平均tokens/sec | 39.82 t/s |
| 总处理token数 | 14848 |
| 平均生成token数 | 50.00 |

### cLLM并发测试详细数据（160请求）

| 指标 | 值 |
|------|-----|
| 总请求数 | 160 |
| 成功请求数 | 152 |
| 失败请求数 | 8 |
| 成功率 | 95.0% |
| 平均响应时间 | 6.57s |
| 最小响应时间 | 2.21s |
| 最大响应时间 | 8.05s |
| 总测试时间 | 212.72s |
| 平均吞吐量 | 35.73 t/s |
| 平均tokens/sec | 7.95 t/s |
| 总处理token数 | 14072 |
| 平均生成token数 | 50.00 |

### Ollama顺序测试详细数据（160请求）

| 指标 | 值 |
|------|-----|
| 总请求数 | 160 |
| 成功请求数 | 160 |
| 失败请求数 | 0 |
| 成功率 | 100% |
| 平均响应时间 | 1.79s |
| 最小响应时间 | 0.41s |
| 最大响应时间 | 14.04s |
| 总测试时间 | 286.58s |
| 平均吞吐量 | 20.66 t/s |
| 平均tokens/sec | 80.57 t/s |
| 总处理token数 | 5972 |
| 平均生成token数 | 37.32 |

### Ollama并发测试详细数据（160请求）

| 指标 | 值 |
|------|-----|
| 总请求数 | 160 |
| 成功请求数 | 160 |
| 失败请求数 | 0 |
| 成功率 | 100% |
| 平均响应时间 | 1.80s |
| 最小响应时间 | 0.74s |
| 最大响应时间 | 2.53s |
| 总测试时间 | 58.09s |
| 平均吞吐量 | 102.53 t/s |
| 平均tokens/sec | 21.73 t/s |
| 总处理token数 | 5956 |
| 平均生成token数 | 37.23 |

---

## 结论

### 综合评价

cLLM和Ollama在不同的测试场景中表现出不同的优势：

- **cLLM**在低负载、顺序测试和token准确性方面表现更好，适合对稳定性和准确性要求高的场景
- **Ollama**在高负载、并发测试和吞吐量方面表现更好，适合需要处理大量并发请求的场景

在**大规模测试（160请求）**中，Ollama表现出了更优的性能和稳定性：
- 总分8.5 vs 8.1（Ollama领先4.9%）
- 零失败率 vs 5%失败率
- 总测试时间58.09s vs 212.72s（Ollama快72.7%）

### 选择建议

1. **如果您需要稳定性和准确性（低负载）**: 选择cLLM
2. **如果您需要处理高并发请求**: 选择Ollama
3. **如果您需要严格控制token数**: 选择cLLM
4. **如果您需要高吞吐量**: 选择Ollama（并发场景）或cLLm（顺序场景）
5. **如果您需要可预测的性能**: 选择cLLM（低负载）或Ollama（高负载）
6. **如果您需要快速响应**: 选择Ollama（并发场景）或cLLm（顺序场景）
7. **如果您需要处理大规模请求**: 选择Ollama
8. **如果您需要100%成功率**: 选择Ollama

### 未来优化方向

1. **cLLM可以优化并发性能**: 参考Ollama的并发处理策略，减少高负载下的失败率
2. **cLLM可以优化内存管理**: 避免在高负载下出现内存不足的问题
3. **Ollama可以优化token生成准确性**: 确保生成指定数量的token
4. **Ollama可以优化第一个请求的预热时间**: 减少启动延迟
5. **两者都可以优化GPU利用率**: 进一步提升性能

---

## 附录

### 测试命令

#### cLLM测试命令

```bash
python3 tools/cllm_optimized_benchmark.py \
  --server-url http://localhost:18085 \
  --test-type all \
  --requests 160 \
  --concurrency 5 \
  --max-tokens 50
```

#### Ollama测试命令

```bash
python3 tools/ollama_benchmark.py \
  --server-url http://localhost:11434 \
  --model qwen3:0.6b \
  --test-type all \
  --requests 160 \
  --concurrency 5 \
  --max-tokens 50
```

### 测试脚本

- cLLM测试脚本: [cllm_optimized_benchmark.py](file:///Users/dannypan/PycharmProjects/xllm/cpp/cLLM/tools/cllm_optimized_benchmark.py)
- Ollama测试脚本: [ollama_benchmark.py](file:///Users/dannypan/PycharmProjects/xllm/cpp/cLLM/tools/ollama_benchmark.py)

### 配置文件

- cLLM配置: [config/config_gpu.yaml](file:///Users/dannypan/PycharmProjects/xllm/cpp/cLLM/config/config_gpu.yaml)
- Ollama配置: 内置配置

---

**报告生成时间**: 2026-01-19 17:03:00  
**报告版本**: V3.0  
**测试人员**: TraeAI Assistant
**测试规模**: 大规模（160请求）
