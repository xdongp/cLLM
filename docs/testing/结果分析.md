两个压测脚本，我的结论如下（含证据位置）：

1) Ollama 并发好很多的主要原因（从脚本与报告可推断）

请求负载更轻：ollama_benchmark.py 固定短英文 prompt "Hello, how are you?"，而 cllm_optimized_benchmark.py 使用多条较长中文 prompt 列表。长 prompt 会显著增加 prefill 开销与显存压力，在并发时更容易放大延迟与排队时间。
cllm_optimized_benchmark.pyL215-L221
prompts = [
    "人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器。",
    ...
]
ollama_benchmark.pyL41-L48
payload = {
    "model": self.model,
    "prompt": "Hello, how are you?",
    "stream": False,
    "options": {
        "num_predict": max_tokens
    }
}
并发场景中输出 token 更少：报告里并发测试 Ollama 平均生成约 37 tokens，而 cLLM 固定 50 tokens。更少的生成量意味着更少的解码计算，吞吐量更高、响应更快是可预期的。
cllm_vs_ollama_comparison_report_v3.mdL116-L119
| 平均吞吐量 | 35.73 t/s | 102.53 t/s | **+186.9%** | **Ollama** |
| 平均tokens/sec | 7.95 t/s | 21.73 t/s | **+173.3%** | **Ollama** |
| 总处理token数 | 14072 | 5956 | **-57.7%** | **cLLM** |
| 平均生成token数 | 50.00 | 37.23 | **-25.5%** | **cLLM** |
Ollama 在并发下响应时间显著更短：报告显示并发平均响应 1.80s vs 6.57s，且最大响应 2.53s vs 8.05s，这意味着其服务端在高并发下调度/缓存/队列处理更高效（或请求负载更轻导致）。
cllm_vs_ollama_comparison_report_v3.mdL111-L115
| 平均响应时间 | 6.57s | 1.80s | **-72.6%** | **Ollama** |
| 最小响应时间 | 2.21s | 0.74s | **-66.5%** | **Ollama** |
| 最大响应时间 | 8.05s | 2.53s | **-68.6%** | **Ollama** |
2) cLLM 并发失败的可能原因（与脚本/报告一致）

失败定义来自 HTTP 异常/非 200：cllm_optimized_benchmark.py 只有在 HTTP 非 200 或异常时才算失败，因此 8 个失败多半来自“服务端拒绝/内部错误/连接异常”。这与报告中“失败率 5%”一致。
cllm_optimized_benchmark.pyL44-L91
response = requests.post(..., timeout=600)
...
if response.status_code == 200:
    ...
    return {"success": True, ...}
else:
    return {"success": False, "error": f"HTTP {response.status_code}"}
except Exception as e:
    return {"success": False, "error": str(e)}
cllm_vs_ollama_comparison_report_v3.mdL109-L112
| 成功请求数 | 152 | 160 | **-5.0%** | **Ollama** |
| 失败请求数 | 8 | 0 | **+100%** | **Ollama** |
| 成功率 | 95.0% | 100% | **-5.3%** | **Ollama** |
并发下资源/队列压力更大：cLLM 的配置在报告里使用 n_batch=1024, n_seq_max=16, GPU 全层等高负载参数。在并发 + 长 prompt 场景下，容易触发显存/上下文槽位/请求队列的资源瓶颈，导致服务端返回非 200 或连接中断。
cllm_vs_ollama_comparison_report_v3.mdL24-L33
llama_cpp:
  n_batch: 1024
  n_threads: 8
  n_gpu_layers: 99        # 全部使用GPU
  n_seq_max: 16
  use_mmap: true

server:
  num_threads: 8
  min_threads: 4
失败请求“生成 0 tokens”的现象：报告里指出失败请求生成 0 tokens，这在 cLLM 端通常意味着服务端内部错误或返回空结果（客户端若仍拿到 200 也会被算成功；因此如果报告把空输出记为失败，说明确实存在服务端侧异常或空响应）。
cllm_vs_ollama_comparison_report_v3.mdL243-L248
1. **并发测试失败率**: 5.0%（8个请求失败）
   - 失败请求生成0 tokens
   - 失败请求的响应时间与成功请求相当（7.23s - 7.37s）
   - 这可能是由于内存不足或并发处理问题
3) 关键差异点（建议作为“原因链”的结论）

负载不等价：cLLM 脚本使用长中文 prompt；Ollama 使用短英文 prompt；并发性能差异可能很大程度来自“工作量”而非单纯引擎差异。
输出长度不一致：Ollama 实际生成 token 明显更少（约 37），自然更快；cLLM 严格 50 tokens 使得吞吐量与延迟被拉高。
cLLM 失败是服务端异常/过载：失败只在 HTTP/异常发生时统计，说明是系统层面的拒绝/崩溃/连接错误，而非客户端统计偏差。
## 最新发现：Logits 全为 0 的严重问题

在最新日志中发现了一个**关键问题**：

```
[2026-01-19 18:38:24.208] [warning] Request 0 - WARNING: All logits are zero! This will cause uniform sampling.
```

### 问题分析

1. **Logits 全为 0 的含义**
   - Logits 是模型输出的原始分数，用于计算每个 token 的概率
   - 全为 0 意味着模型没有产生有意义的输出
   - 采样器会均匀随机选择 token，导致生成乱码或无意义文本

2. **与失败请求的关联**
   - 报告中 8 个失败请求生成 0 tokens，可能与此问题直接相关
   - 当 logits 全为 0 时，采样器可能无法生成有效 token
   - 虽然日志显示请求完成（isCompleted=1），但实际生成的内容可能无效

3. **问题根源（参考 ZERO_LOGITS_ANALYSIS.md）**
   - **Logits 提取逻辑问题**：`getLogitsForRequest` 中的 offset 计算可能错误
   - **Logits 数组初始化问题**：从推理引擎返回的 logitsTensor 可能未正确初始化
   - **内存访问错误**：如果 offset 计算错误，可能访问到错误的内存位置

4. **对性能的影响**
   - 即使请求成功完成，生成的文本质量也会严重下降
   - 乱码生成可能导致客户端解析失败
   - 这解释了为什么 cLLM 在并发测试中表现较差

### 与原始分析的关系

原始分析中的**三个原因仍然成立**，但需要增加第四个根本原因：

**4) cLLM 存在 Logits 提取/初始化的严重 Bug**

- 并发场景下，logits 提取逻辑可能失效
- 导致生成质量严重下降，甚至生成 0 tokens
- 这是**比资源限制更严重的代码层面问题**

### 建议的下一步行动

1. **立即修复 Logits 问题**：
   - 检查 `src/scheduler/batch_processor.cpp` 中的 `getLogitsForRequest` 函数
   - 验证 logitsOffset 计算逻辑
   - 添加调试日志，查看实际的 logits 值

2. **修复后重新测试**：
   - 先修复 Logits Bug
   - 再调整 n_seq_max 等配置参数
   - 重新运行 160 请求的并发测试

3. **长期优化**：
   - 增加 n_seq_max 到 32 或更高
   - 优化序列 ID 回收机制
   - 改进批处理调度策略

---

如果你希望我立即开始修复 Logits 问题，请告诉我。我可以：
- 查看 `batch_processor.cpp` 中的具体实现
- 分析 `getLogitsForRequest` 的 offset 计算逻辑
- 提出修复方案并实施
