# åŠ¨æ€ Batch Size è°ƒæ•´æœºåˆ¶è®¾è®¡æ–‡æ¡£

## æ–‡æ¡£ä¿¡æ¯
- **æ–‡æ¡£ç‰ˆæœ¬**: v1.0
- **åˆ›å»ºæ—¥æœŸ**: 2026-01-22
- **è®¾è®¡äºº**: cLLM Technical Team
- **çŠ¶æ€**: è®¾è®¡å®Œæˆï¼Œå¾…å®ç°

---

## 1. è®¾è®¡åŸç†

### 1.1 æ ¸å¿ƒé—®é¢˜åˆ†æ

å½“å‰ç³»ç»Ÿä½¿ç”¨é™æ€çš„ `MIN_BATCH_SIZE_FOR_ACCUMULATION = 8`ï¼Œå­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š

1. **GPU èµ„æºåˆ©ç”¨ä¸è¶³**
   - ä¸åŒ GPU å‹å·ï¼ˆNVIDIA A100ã€Apple M3ã€AMD MI250ç­‰ï¼‰çš„å†…å­˜å’Œè®¡ç®—èƒ½åŠ›å·®å¼‚å·¨å¤§
   - å›ºå®šçš„ batch size æ— æ³•å……åˆ†å‘æŒ¥é«˜ç«¯ GPU çš„æ€§èƒ½
   - å° batch size å¯¼è‡´ GPU è®¡ç®—å•å…ƒç©ºé—²ï¼Œååé‡ä¸‹é™

2. **æ‰¹å¤„ç†ç´¯ç§¯ç­–ç•¥çš„å±€é™æ€§**
   ```cpp
   // å½“å‰å®ç°ï¼ˆscheduler.cpp:428-430ï¼‰
   constexpr size_t MIN_BATCH_SIZE_FOR_ACCUMULATION = 8;
   constexpr size_t MAX_WAIT_MS_FOR_BATCH = 50;
   ```
   - é™æ€é˜ˆå€¼æ— æ³•é€‚åº”åŠ¨æ€è´Ÿè½½å˜åŒ–
   - é«˜å³°æœŸå¯èƒ½ç­‰å¾…è¿‡ä¹…ï¼Œä½å³°æœŸå¯èƒ½ç­‰å¾…ä¸è¶³
   - æœªè€ƒè™‘ GPU å†…å­˜å’Œè®¡ç®—èƒ½åŠ›çš„çº¦æŸ

3. **æ€§èƒ½è¯„ä¼°æŒ‡æ ‡ç¼ºå¤±**
   - æ²¡æœ‰é‡åŒ–æŒ‡æ ‡è¯„ä¼°å½“å‰ batch size çš„åˆç†æ€§
   - æ— æ³•åˆ¤æ–­æ˜¯å†…å­˜å—é™è¿˜æ˜¯è®¡ç®—å—é™
   - ç¼ºä¹è‡ªé€‚åº”è°ƒæ•´çš„åé¦ˆæœºåˆ¶

### 1.2 æ ¸å¿ƒç®—æ³•è®¾è®¡

#### 1.2.1 æ€§èƒ½è¯„ä¼°æŒ‡æ ‡ä½“ç³»

**ä¸€çº§æŒ‡æ ‡ï¼ˆæ ¸å¿ƒï¼‰**:
- **ååé‡ (Throughput)**: tokens/secondï¼ˆé¦–è¦ä¼˜åŒ–ç›®æ ‡ï¼‰
- **GPU åˆ©ç”¨ç‡ (GPU Utilization)**: è®¡ç®—å•å…ƒå ç”¨ç‡
- **å†…å­˜å¸¦å®½åˆ©ç”¨ç‡ (Memory Bandwidth Utilization)**: HBM/VRAM å¸¦å®½ä½¿ç”¨æƒ…å†µ

**äºŒçº§æŒ‡æ ‡ï¼ˆè¾…åŠ©ï¼‰**:
- **æ‰¹å¤„ç†æ—¶é—´ (Batch Processing Time)**: å•æ¬¡æ¨ç†å»¶è¿Ÿ
- **å†…å­˜å ç”¨ (Memory Footprint)**: å³°å€¼å†…å­˜ä½¿ç”¨é‡
- **è®¡ç®—å¯†åº¦ (Compute Density)**: FLOPs/byteï¼ˆè®¡ç®—å¯†é›†å‹ vs å†…å­˜å¯†é›†å‹ï¼‰

**è¯„ä¼°å…¬å¼**:
```
Score = Î± * Throughput + Î² * GPU_Utilization + Î³ * (1 - Memory_Waste)

å…¶ä¸­:
- Î± + Î² + Î³ = 1ï¼ˆæƒé‡ç³»æ•°ï¼‰
- Memory_Waste = (Peak_Memory - Used_Memory) / Peak_Memory
- ç›®æ ‡: Score â†’ æœ€å¤§å€¼
```

#### 1.2.2 ç®€åŒ–ä¸ºä¸‰ç§æœºåˆ¶

**staticï¼ˆé™æ€ï¼Œå½“å‰æœºåˆ¶ï¼‰**
- é‡‡ç”¨ç°æœ‰è°ƒåº¦ä¸æ‰¹å¤„ç†ç­–ç•¥ï¼Œä¸å¯ç”¨è°ƒè°å™¨ã€‚
- batch size æ¥è‡ªé…ç½®æˆ–ç°æœ‰å¯å‘å¼ï¼Œä¸åšåŠ¨æ€è°ƒæ•´ã€‚

**dynamicï¼ˆåŠ¨æ€ï¼‰**
- ç›®æ ‡ï¼šåœ¨è¿è¡Œæ—¶æŒç»­æ‰¾åˆ°æ›´ä¼˜ batch sizeã€‚
- ç®—æ³•ï¼šæŒ‡æ•°å¢åŠ ã€äºŒåˆ†ä¸‹é™ï¼Œå¾ªç¯åŠ¨æ€è°ƒæ•´ã€‚
- è¿‡ç¨‹ï¼š
  1. ä» `min_batch_size` å¼€å§‹æŒ‡æ•°å¢é•¿ï¼ˆ1 â†’ 2 â†’ 4 â†’ 8 ...ï¼‰ï¼Œç›´åˆ°æ€§èƒ½ä¸‹é™æˆ–è§¦è¾¾ä¸Šé™ã€‚
  2. åœ¨â€œæœ€åä¸€æ¬¡æ€§èƒ½æå‡â€ä¸â€œé¦–æ¬¡ä¸‹é™â€ä¹‹é—´åšäºŒåˆ†æœç´¢ï¼Œç¡®å®šæœ€ä¼˜åŒºé—´å†…çš„ batch sizeã€‚
  3. è¿›å…¥åœ¨çº¿è°ƒæ•´é˜¶æ®µï¼šæŒ‰å›ºå®šå‘¨æœŸè¯„ä¼°åå/æ—¶é—´æŒ‡æ ‡ï¼Œè‹¥æ€§èƒ½ä¸‹é™åˆ™å›é€€å¹¶ç¼©å°ï¼Œè‹¥æå‡åˆ™å°è¯•æ”¾å¤§ï¼ŒæŒç»­è¿­ä»£ã€‚

**hybirdï¼ˆæ··åˆï¼‰**
- ç›®æ ‡ï¼šå…ˆæ‰¾åˆ°æœ€ä¼˜ batch sizeï¼Œå†ä¿æŒç¨³å®šã€‚
- ç®—æ³•ï¼šåŒ dynamic çš„â€œæŒ‡æ•°å¢åŠ  + äºŒåˆ†ä¸‹é™â€ï¼Œä½†åªåœ¨å¯åŠ¨æˆ–æ‰‹åŠ¨è§¦å‘æ—¶æ‰§è¡Œã€‚
- æ‰¾åˆ°æœ€ä¼˜å€¼åé”å®š batch sizeï¼Œä¸å†è¿›è¡Œåœ¨çº¿åŠ¨æ€è°ƒæ•´ã€‚

#### 1.2.3 è®¾è®¡ç®€åŒ–è¯´æ˜
- ç»Ÿä¸€â€œæ¢æµ‹é€»è¾‘â€ä¸ºæŒ‡æ•°å¢åŠ  + äºŒåˆ†ä¸‹é™ã€‚
- dynamic ä¸ hybird çš„åŒºåˆ«åªåœ¨äºâ€œæ˜¯å¦æŒç»­åœ¨çº¿è°ƒæ•´â€ã€‚
- static å®Œå…¨æ²¿ç”¨å½“å‰æœºåˆ¶ï¼Œä¾¿äºå›é€€ä¸å¯¹æ¯”ã€‚

#### 1.2.4 æœºåˆ¶æ˜ å°„è¯´æ˜
- æ–‡æ¡£å†…çš„â€œåˆå§‹æ¢æµ‹â€å¯¹åº” dynamic/hybird å…±äº«æµç¨‹ã€‚
- â€œåŠ¨æ€è°ƒæ•´â€ä»…åœ¨ dynamic ç­–ç•¥å¯ç”¨ï¼›hybird åœ¨æ¢æµ‹å®Œæˆåé”å®šã€‚
- è‹¥é…ç½® `strategy=static`ï¼Œè°ƒè°å™¨å®Œå…¨ä¸æ¥å…¥è°ƒåº¦æµç¨‹ã€‚

---

## 2. å®ç°æ–¹æ¡ˆ

### 2.1 ä»£ç ä¿®æ”¹ä½ç½®

#### 2.1.1 æ–°å¢æ–‡ä»¶

**æ–‡ä»¶ 1: `include/cllm/scheduler/dynamic_batch_tuner.h`**
```cpp
/**
 * @file dynamic_batch_tuner.h
 * @brief åŠ¨æ€ Batch Size è°ƒè°å™¨ï¼ˆç®€åŒ–ç‰ˆï¼‰
 * 
 * è´Ÿè´£:
 * - æŒ‡æ•°å¢åŠ  + äºŒåˆ†ä¸‹é™æ¢æµ‹
 * - åŠ¨æ€/æ··åˆç­–ç•¥è°ƒæ•´
 * - æ‰¹å¤„ç†è€—æ—¶åé¦ˆ
 */

namespace cllm {

class DynamicBatchTuner {
private:
    // çŠ¶æ€ç®¡ç†
    enum class TuningPhase {
        INITIAL_PROBING,  // åˆå§‹æ¢æµ‹é˜¶æ®µ
        DYNAMIC_ADJUSTMENT, // åŠ¨æ€è°ƒæ•´é˜¶æ®µ
        STABLE_RUNNING     // ç¨³å®šè¿è¡Œé˜¶æ®µ
    };
    
    TuningPhase currentPhase_;
    std::atomic<size_t> currentBatchSize_;
    
    // æ€§èƒ½æŒ‡æ ‡
    struct PerformanceMetrics {
        double throughput;           // tokens/sec
        double gpuUtilization;       // 0-1
        double memoryUtilization;    // 0-1
        double processingTimeMs;     // æ‰¹å¤„ç†æ—¶é—´
        size_t peakMemoryMb;         // å³°å€¼å†…å­˜
        std::chrono::steady_clock::time_point timestamp;
    };
    
    std::deque<PerformanceMetrics> metricsHistory_;
    std::mutex metricsMutex_;
    
    // é…ç½®å‚æ•°
    struct TunerConfig {
        size_t minBatchSize;
        size_t maxBatchSize;
        size_t initialBatchSize;
        
        // æ¢æµ‹é˜¶æ®µå‚æ•°
        size_t maxProbingAttempts;
        double probingGrowthFactor;
        
        // è°ƒæ•´é˜¶æ®µå‚æ•°
        double performanceThreshold;  // Îµ
        double growthCoefficient;     // Î±
        double decayCoefficient;      // Î²
        double oscillationCoefficient; // Î´
        
        // ç¨³å®šé˜¶æ®µå‚æ•°
        size_t stabilizationCycles;   // N
        double batchSizeChangeThreshold; // Î¸
        double performanceFluctuationThreshold; // Ï†
        
        // å®‰å…¨å‚æ•°
        size_t memorySafetyMarginMb;
        double memoryUsageLimit;
    };
    
    TunerConfig config_;
    
    // GPU ä¿¡æ¯
    struct GPUInfo {
        std::string model;
        size_t totalMemoryMb;
        size_t availableMemoryMb;
        size_t computeUnits;
        double memoryBandwidthGbPerSec;
        double theoreticalPeakTflops;
    };
    
    GPUInfo gpuInfo_;
    
    // å†å²æœ€ä½³
    size_t bestBatchSize_;
    double bestScore_;
    
    // å®‰å…¨æœºåˆ¶
    bool isInSafeZone_;
    size_t consecutiveFailures_;
    
public:
    DynamicBatchTuner();
    ~DynamicBatchTuner();
    
    // åˆå§‹åŒ–
    void initialize(const TunerConfig& config);
    void detectGPU();
    
    // æ ¸å¿ƒæ¥å£
    size_t getOptimalBatchSize();
    void reportBatchCompletion(const PerformanceMetrics& metrics);
    
    // çŠ¶æ€æŸ¥è¯¢
    TuningPhase getCurrentPhase() const { return currentPhase_; }
    size_t getCurrentBatchSize() const { return currentBatchSize_.load(); }
    bool isStabilized() const { return currentPhase_ == TuningPhase::STABLE_RUNNING; }
    
    // é…ç½®æ¥å£
    void updateConfig(const TunerConfig& config);
    TunerConfig getConfig() const { return config_; }
    
    // è°ƒè¯•æ¥å£
    std::string getStatusReport() const;
    
private:
    // é˜¶æ®µå®ç°
    void runInitialProbing();
    void runDynamicAdjustment(const PerformanceMetrics& metrics);
    void runStableRunning(const PerformanceMetrics& metrics);
    
    // è¾…åŠ©æ–¹æ³•
    double calculateScore(const PerformanceMetrics& metrics) const;
    bool shouldIncreaseBatchSize(const PerformanceMetrics& metrics) const;
    bool shouldDecreaseBatchSize(const PerformanceMetrics& metrics) const;
    bool checkStabilizationCondition() const;
    
    // å®‰å…¨æ£€æŸ¥
    bool isMemorySafe(size_t batchSize) const;
    void handleMemoryOverflow();
    void resetToSafeState();
    
    // å·¥å…·æ–¹æ³•
    size_t estimateBatchSizeFromMemory(size_t availableMemoryMb) const;
    size_t estimateBatchSizeFromCompute() const;
};

} // namespace cllm
```

**æ–‡ä»¶ 2: `src/scheduler/dynamic_batch_tuner.cpp`**
- å®ç°è°ƒè°å™¨æ ¸å¿ƒé€»è¾‘ä¸çŠ¶æ€æœº
- æŒ‡æ•°å¢åŠ  + äºŒåˆ†ä¸‹é™æ¢æµ‹
- dynamic çš„åœ¨çº¿è°ƒæ•´ä¸ hybird çš„ç¨³å®šé”å®š
- åŸºç¡€è¾¹ç•Œä¸å®‰å…¨æ£€æŸ¥

#### 2.1.2 ä¿®æ”¹ç°æœ‰æ–‡ä»¶

**ä¿®æ”¹ 1: `src/scheduler/scheduler.cpp`**

```cpp
// åœ¨ Scheduler ç±»ä¸­æ·»åŠ æˆå‘˜ï¼ˆscheduler.hï¼‰
private:
    std::unique_ptr<DynamicBatchTuner> batchTuner_;
    std::atomic<size_t> tunedMaxBatchSize_;

// åœ¨ processRequests æ–¹æ³•ä¸­ä¿®æ”¹ï¼ˆscheduler.cpp:428ï¼‰
void Scheduler::processRequests() {
    // ... å‰ç½®æ£€æŸ¥ ...
    
    // ğŸ”¥ å…³é”®ä¿®æ”¹: ä½¿ç”¨åŠ¨æ€ batch size
    size_t minBatchSize = batchTuner_->getCurrentBatchSize();
    constexpr size_t MAX_WAIT_MS_FOR_BATCH = 50;
    
    if (queueSize < minBatchSize && runningCount == 0) {
        CLLM_DEBUG("[Scheduler::processRequests] Queue size (%zu) < %zu (dynamic), waiting for more requests (max %dms)",
                  queueSize, minBatchSize, MAX_WAIT_MS_FOR_BATCH);
        
        // ç­‰å¾…é€»è¾‘ä¿æŒä¸å˜
        std::unique_lock<std::mutex> lock(queueMutex_);
        auto waitStart = std::chrono::steady_clock::now();
        
        queueCondition_.wait_for(
            lock,
            std::chrono::milliseconds(MAX_WAIT_MS_FOR_BATCH),
            [this, minBatchSize]() {
                return requestQueue_.getQueueSize() >= minBatchSize || !running_;
            }
        );
        
        // ... åç»­å¤„ç† ...
    }
    
    // ... æ‰¹å¤„ç†å½¢æˆ ...
    
    // ğŸ”¥ æ–°å¢: è®°å½•æ‰¹å¤„ç†æ€§èƒ½æŒ‡æ ‡
    auto batchStart = std::chrono::steady_clock::now();
    
    // æ‰§è¡Œæ‰¹å¤„ç†
    SchedulerBatchProcessor processor(this, modelExecutor_, kvCache_, &batchManager_);
    processor.processBatch(activeBatch);
    
    auto batchEnd = std::chrono::steady_clock::now();
    auto processingTime = std::chrono::duration_cast<std::chrono::milliseconds>(batchEnd - batchStart).count();
    
    // æ”¶é›†æ€§èƒ½æŒ‡æ ‡
    DynamicBatchTuner::PerformanceMetrics metrics;
    metrics.throughput = calculateThroughput(activeBatch, processingTime);
    metrics.gpuUtilization = queryGPUUtilization();
    metrics.memoryUtilization = queryMemoryUtilization();
    metrics.processingTimeMs = processingTime;
    metrics.peakMemoryMb = queryPeakMemoryUsage();
    metrics.timestamp = batchEnd;
    
    // æŠ¥å‘Šç»™è°ƒè°å™¨
    batchTuner_->reportBatchCompletion(metrics);
}

// åœ¨ Scheduler æ„é€ å‡½æ•°ä¸­åˆå§‹åŒ–
Scheduler::Scheduler() {
    // ... ç°æœ‰åˆå§‹åŒ– ...
    
    // åˆå§‹åŒ–åŠ¨æ€æ‰¹å¤„ç†è°ƒè°å™¨
    DynamicBatchTuner::TunerConfig tunerConfig;
    tunerConfig.minBatchSize = 1;
    tunerConfig.maxBatchSize = 256;
    tunerConfig.initialBatchSize = 8;
    tunerConfig.maxProbingAttempts = 10;
    tunerConfig.probingGrowthFactor = 2.0;
    tunerConfig.performanceThreshold = 0.05;  // 5%
    tunerConfig.growthCoefficient = 0.2;
    tunerConfig.decayCoefficient = 0.3;
    tunerConfig.oscillationCoefficient = 0.05;
    tunerConfig.stabilizationCycles = 10;
    tunerConfig.batchSizeChangeThreshold = 0.10;  // 10%
    tunerConfig.performanceFluctuationThreshold = 0.03;  // 3%
    tunerConfig.memorySafetyMarginMb = 512;
    tunerConfig.memoryUsageLimit = 0.90;  // 90%
    
    batchTuner_ = std::make_unique<DynamicBatchTuner>();
    batchTuner_->initialize(tunerConfig);
    batchTuner_->detectGPU();
}
```

**ä¿®æ”¹ 2: `include/cllm/scheduler/scheduler.h`**
- æ·»åŠ  `DynamicBatchTuner` å‰å‘å£°æ˜
- æ·»åŠ  `batchTuner_` æˆå‘˜å˜é‡å£°æ˜
- æ·»åŠ ç›¸å…³çš„è¾…åŠ©æ–¹æ³•å£°æ˜

**ä¿®æ”¹ 3: `config/config.yaml`**
```yaml
# åŠ¨æ€ Batch Size è°ƒè°å™¨é…ç½®
dynamic_batch_tuner:
  enabled: true                    # æ€»å¼€å…³
  strategy: "dynamic"              # å¯é€‰: static | dynamic | hybird

  # static ä¸“ç”¨ï¼ˆä¸ç°æœ‰æœºåˆ¶ä¸€è‡´ï¼‰
  fixed_batch_size: 0              # 0 è¡¨ç¤ºæ²¿ç”¨ç°æœ‰ batch è®¡ç®—é€»è¾‘

  # dynamic / hybird åŸºç¡€é…ç½®
  min_batch_size: 1                # æœ€å° batch size
  max_batch_size: 256              # æœ€å¤§ batch size
  initial_batch_size: 8            # åˆå§‹ batch size

  # æŒ‡æ•°å¢åŠ  + äºŒåˆ†ä¸‹é™é…ç½®
  probing_growth_factor: 2.0       # æŒ‡æ•°å¢é•¿å› å­
  max_probing_attempts: 10         # æœ€å¤§æ¢æµ‹æ¬¡æ•°

  # dynamic åœ¨çº¿è°ƒæ•´å‚æ•°ï¼ˆhybird ä¸ä½¿ç”¨ï¼‰
  performance_threshold: 0.05      # æ€§èƒ½å˜åŒ–é˜ˆå€¼
  adjustment_factor: 0.3           # è°ƒæ•´å¹…åº¦ï¼ˆä¸Šè°ƒ/ä¸‹è°ƒï¼‰

  # å®‰å…¨å‚æ•°
  memory_usage_limit: 0.90         # å†…å­˜ä½¿ç”¨é™åˆ¶ (90%)
  max_consecutive_failures: 3      # æœ€å¤§è¿ç»­å¤±è´¥æ¬¡æ•°
```

### 2.2 å…³é”®å‡½æ•°è®¾è®¡

#### 2.2.1 GPU æ€§èƒ½æ¢æµ‹å‡½æ•°

```cpp
void DynamicBatchTuner::detectGPU() {
    // 1. è¯†åˆ« GPU å‹å·
    #ifdef __APPLE__
        // Apple Silicon: ä½¿ç”¨ Metal API
        gpuInfo_.model = detectAppleGPUModel();
        gpuInfo_.totalMemoryMb = queryAppleGPUMemory();
        gpuInfo_.computeUnits = queryAppleGPUComputeUnits();
        gpuInfo_.memoryBandwidthGbPerSec = estimateAppleGPUMemoryBandwidth();
        gpuInfo_.theoreticalPeakTflops = calculateAppleGPUPerformance();
    #elif defined(__CUDA__)
        // NVIDIA GPU: ä½¿ç”¨ CUDA API
        int deviceCount;
        cudaGetDeviceCount(&deviceCount);
        if (deviceCount > 0) {
            cudaDeviceProp props;
            cudaGetDeviceProperties(&props, 0);
            gpuInfo_.model = props.name;
            gpuInfo_.totalMemoryMb = props.totalGlobalMem / (1024 * 1024);
            gpuInfo_.computeUnits = props.multiProcessorCount;
            gpuInfo_.memoryBandwidthGbPerSec = 
                (props.memoryBusWidth * props.memoryClockRate * 2) / 1e6;
            gpuInfo_.theoreticalPeakTflops = 
                calculateNVIDIAPeakPerformance(props);
        }
    #elif defined(__HIP_PLATFORM_HCC__)
        // AMD GPU: ä½¿ç”¨ HIP API
        // ç±»ä¼¼ CUDA çš„å®ç°
    #else
        // CPU fallback
        gpuInfo_.model = "CPU Only";
        gpuInfo_.totalMemoryMb = querySystemMemory();
        gpuInfo_.computeUnits = std::thread::hardware_concurrency();
    #endif
    
    CLLM_INFO("[DynamicBatchTuner] Detected GPU: %s", gpuInfo_.model.c_str());
    CLLM_INFO("[DynamicBatchTuner] Memory: %zu MB, Compute Units: %zu", 
              gpuInfo_.totalMemoryMb, gpuInfo_.computeUnits);
    CLLM_INFO("[DynamicBatchTuner] Memory Bandwidth: %.2f GB/s, Peak Performance: %.2f TFLOPS",
              gpuInfo_.memoryBandwidthGbPerSec, gpuInfo_.theoreticalPeakTflops);
}
```

#### 2.2.2 æ€§èƒ½æŒ‡æ ‡æ”¶é›†å‡½æ•°

```cpp
void Scheduler::collectPerformanceMetrics(
    const std::vector<RequestState>& batch,
    double processingTimeMs,
    DynamicBatchTuner::PerformanceMetrics& metrics) {
    
    // 1. è®¡ç®—ååé‡
    size_t totalTokens = 0;
    for (const auto& request : batch) {
        totalTokens += request.generatedTokens.size();
    }
    metrics.throughput = totalTokens / (processingTimeMs / 1000.0);
    
    // 2. æŸ¥è¯¢ GPU åˆ©ç”¨ç‡
    #ifdef __APPLE__
        metrics.gpuUtilization = queryAppleGPUUtilization();
    #elif defined(__CUDA__)
        metrics.gpuUtilization = queryNVIDIAGPUUtilization();
    #else
        metrics.gpuUtilization = 0.5; // é»˜è®¤å€¼
    #endif
    
    // 3. æŸ¥è¯¢å†…å­˜åˆ©ç”¨ç‡
    size_t usedMemory = queryCurrentMemoryUsage();
    metrics.memoryUtilization = 
        static_cast<double>(usedMemory) / gpuInfo_.totalMemoryMb;
    
    // 4. è®°å½•æ‰¹å¤„ç†æ—¶é—´
    metrics.processingTimeMs = processingTimeMs;
    
    // 5. æŸ¥è¯¢å³°å€¼å†…å­˜
    metrics.peakMemoryMb = queryPeakMemoryUsage();
    
    // 6. è®°å½•æ—¶é—´æˆ³
    metrics.timestamp = std::chrono::steady_clock::now();
}
```

#### 2.2.3 Batch Size è®¡ç®—å‡½æ•°

```cpp
size_t DynamicBatchTuner::getOptimalBatchSize() {
    std::lock_guard<std::mutex> lock(metricsMutex_);
    
    switch (currentPhase_) {
        case TuningPhase::INITIAL_PROBING:
            return getProbingBatchSize();
            
        case TuningPhase::DYNAMIC_ADJUSTMENT:
            return currentBatchSize_.load();
            
        case TuningPhase::STABLE_RUNNING:
            return currentBatchSize_.load();
            
        default:
            return config_.initialBatchSize;
    }
}

size_t DynamicBatchTuner::getProbingBatchSize() {
    // æŒ‡æ•°å¢é•¿æ¢æµ‹
    static size_t attempt = 0;
    size_t probingSize = config_.initialBatchSize * 
        std::pow(config_.probingGrowthFactor, attempt);
    
    probingSize = std::min(probingSize, config_.maxBatchSize);
    probingSize = std::max(probingSize, config_.minBatchSize);
    
    return probingSize;
}
```

### 2.3 ä¸ç°æœ‰è°ƒåº¦ç³»ç»Ÿçš„é›†æˆ

#### 2.3.1 æ•°æ®æµå›¾

```
ç”¨æˆ·è¯·æ±‚
    â†“
RequestQueue (æ— é”é˜Ÿåˆ—)
    â†“
Scheduler::processRequests()
    â†“
DynamicBatchTuner::getOptimalBatchSize()
    â†“
[ç­‰å¾…ç´¯ç§¯] æˆ– [ç«‹å³å¤„ç†]
    â†“
BatchManager::formBatch()
    â†“
BatchProcessor::processBatch()
    â†“
InferenceEngine::forwardBatch()
    â†“
LlamaCppBackend / LibTorchBackend / KylinBackend
    â†“
GPU æ‰§è¡Œæ¨ç†
    â†“
æ”¶é›†æ€§èƒ½æŒ‡æ ‡
    â†“
DynamicBatchTuner::reportBatchCompletion()
    â†“
æ›´æ–°å†å²è®°å½• â†’ è°ƒæ•´ batch size â†’ å¾ªç¯
```

#### 2.3.2 é›†æˆç‚¹è¯´æ˜

**é›†æˆç‚¹ 1: Batch Size æŸ¥è¯¢**
- **ä½ç½®**: `scheduler.cpp:428`
- **è°ƒç”¨**: `batchTuner_->getOptimalBatchSize()`
- **é¢‘ç‡**: æ¯æ¬¡ `processRequests()` è°ƒç”¨æ—¶
- **å¼€é”€**: < 1Î¼sï¼ˆåŸå­æ“ä½œï¼‰

**é›†æˆç‚¹ 2: æ€§èƒ½æŒ‡æ ‡æŠ¥å‘Š**
- **ä½ç½®**: `scheduler.cpp:700`ï¼ˆæ‰¹å¤„ç†å®Œæˆåï¼‰
- **è°ƒç”¨**: `batchTuner_->reportBatchCompletion(metrics)`
- **é¢‘ç‡**: æ¯æ¬¡æ‰¹å¤„ç†å®Œæˆæ—¶
- **å¼€é”€**: ~10Î¼sï¼ˆæŒ‡æ ‡æ”¶é›†å’Œåˆ†æï¼‰

**é›†æˆç‚¹ 3: GPU çŠ¶æ€æŸ¥è¯¢**
- **ä½ç½®**: `llama_cpp_backend.cpp` / `libtorch_backend.cpp`
- **è°ƒç”¨**: `queryGPUUtilization()`, `queryMemoryUtilization()`
- **é¢‘ç‡**: æ¯ N ä¸ªæ‰¹å¤„ç†ï¼ˆå¯é…ç½®ï¼‰
- **å¼€é”€**: ~100Î¼sï¼ˆç³»ç»Ÿè°ƒç”¨ï¼‰

**é›†æˆç‚¹ 4: é…ç½®åŠ è½½**
- **ä½ç½®**: `config/config.cpp`
- **è°ƒç”¨**: è§£æ `dynamic_batch_tuner` é…ç½®èŠ‚
- **é¢‘ç‡**: ç³»ç»Ÿå¯åŠ¨æ—¶
- **å¼€é”€**: ä¸€æ¬¡æ€§

---

## 3. è‡ªé€‚åº”æœºåˆ¶

### 3.1 GPU æ€§èƒ½æ¢æµ‹æµç¨‹

#### 3.1.1 åˆå§‹æ¢æµ‹é˜¶æ®µ

**ç›®æ ‡**: åœ¨ç³»ç»Ÿå¯åŠ¨åçš„å‰ N ä¸ªæ‰¹å¤„ç†ä¸­ï¼Œå¿«é€Ÿæ‰¾åˆ°å¯è¡Œçš„ batch size èŒƒå›´

**æµç¨‹å›¾**:

```
å¼€å§‹
  â†“
è·å– GPU ä¿¡æ¯ï¼ˆå‹å·ã€å†…å­˜ã€è®¡ç®—å•å…ƒï¼‰
  â†“
ä¼°ç®—åˆå§‹ batch size
  â†“
for attempt = 0 to maxProbingAttempts:
  â†“
  ä½¿ç”¨æŒ‡æ•°å¢é•¿çš„ batch size æ‰§è¡Œæ¨ç†
  1, 2, 4, 8, 16, 32, ...
  â†“
  æ”¶é›†æ€§èƒ½æŒ‡æ ‡
  - ååé‡
  - GPU åˆ©ç”¨ç‡
  - å†…å­˜å ç”¨
  - æ‰¹å¤„ç†æ—¶é—´
  â†“
  æ£€æŸ¥æ˜¯å¦æˆåŠŸ:
  - å†…å­˜æ˜¯å¦æº¢å‡º?
  - æ€§èƒ½æ˜¯å¦ä¸‹é™?
  - æ˜¯å¦è¾¾åˆ°ç¨³å®š?
  â†“
  if æˆåŠŸ:
      è®°å½•ä¸ºå¯è¡Œå€¼
      ç»§ç»­å¢å¤§
  else:
      è®°å½•ä¸ºä¸Šé™
      ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾ç²¾ç¡®å®šä½
      break
  â†“
end for
  â†“
ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾ç²¾ç¡®å®šä½æœ€ä¼˜å€¼
  â†“
è®°å½•æœ€ä½³ batch size
  â†“
è¿›å…¥åŠ¨æ€è°ƒæ•´é˜¶æ®µ
```

**å…³é”®ä»£ç **:

```cpp
void DynamicBatchTuner::runInitialProbing() {
    CLLM_INFO("[DynamicBatchTuner] Starting initial probing phase");
    
    size_t lowerBound = config_.minBatchSize;
    size_t upperBound = config_.maxBatchSize;
    size_t bestSize = config_.initialBatchSize;
    double bestScore = 0.0;
    
    // é˜¶æ®µ 1: æŒ‡æ•°å¢é•¿æ¢æµ‹
    for (size_t attempt = 0; attempt < config_.maxProbingAttempts; ++attempt) {
        size_t probingSize = config_.initialBatchSize * 
            std::pow(config_.probingGrowthFactor, attempt);
        probingSize = std::min(probingSize, config_.maxBatchSize);
        
        if (!isMemorySafe(probingSize)) {
            upperBound = probingSize;
            break;
        }
        
        // æ‰§è¡Œæ¢æµ‹æ¨ç†ï¼ˆå®é™…ä¼šåœ¨ processBatch ä¸­æ‰§è¡Œï¼‰
        currentBatchSize_ = probingSize;
        
        // ç­‰å¾…æ€§èƒ½æŒ‡æ ‡ï¼ˆå¼‚æ­¥ï¼‰
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
        
        // æ£€æŸ¥æœ€æ–°æŒ‡æ ‡
        if (!metricsHistory_.empty()) {
            const auto& latest = metricsHistory_.back();
            double score = calculateScore(latest);
            
            if (score > bestScore) {
                bestScore = score;
                bestSize = probingSize;
            }
            
            // æ£€æŸ¥æ€§èƒ½æ˜¯å¦ä¸‹é™
            if (attempt > 2 && score < bestScore * 0.8) {
                CLLM_DEBUG("[DynamicBatchTuner] Performance degraded at batch size %zu", probingSize);
                upperBound = probingSize;
                break;
            }
        }
        
        lowerBound = std::max(lowerBound, probingSize);
    }
    
    // é˜¶æ®µ 2: äºŒåˆ†æŸ¥æ‰¾ç²¾ç¡®å®šä½
    CLLM_DEBUG("[DynamicBatchTuner] Binary search between %zu and %zu", lowerBound, upperBound);
    
    for (size_t i = 0; i < 5; ++i) { // æœ€å¤š 5 æ¬¡äºŒåˆ†
        if (upperBound - lowerBound <= 1) {
            break;
        }
        
        size_t mid = (lowerBound + upperBound) / 2;
        
        if (!isMemorySafe(mid)) {
            upperBound = mid;
            continue;
        }
        
        currentBatchSize_ = mid;
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
        
        if (!metricsHistory_.empty()) {
            const auto& latest = metricsHistory_.back();
            double score = calculateScore(latest);
            
            if (score > bestScore * 0.95) {
                bestScore = score;
                bestSize = mid;
                lowerBound = mid;
            } else {
                upperBound = mid;
            }
        }
    }
    
    // ç¡®å®šæœ€ç»ˆå€¼
    currentBatchSize_ = bestSize;
    bestBatchSize_ = bestSize;
    bestScore_ = bestScore;
    
    CLLM_INFO("[DynamicBatchTuner] Initial probing complete. Best batch size: %zu (score: %.3f)", 
              bestSize, bestScore);
    
    // è¿›å…¥åŠ¨æ€è°ƒæ•´é˜¶æ®µ
    currentPhase_ = TuningPhase::DYNAMIC_ADJUSTMENT;
}
```

#### 3.1.2 åŠ¨æ€è°ƒæ•´é˜¶æ®µ

**ç›®æ ‡**: åœ¨è¿è¡Œæ—¶æŒç»­ä¼˜åŒ– batch sizeï¼Œé€‚åº”è´Ÿè½½å˜åŒ–

**è°ƒæ•´ç®—æ³•**:

```cpp
void DynamicBatchTuner::runDynamicAdjustment(const PerformanceMetrics& metrics) {
    double currentScore = calculateScore(metrics);
    
    // 1. æ£€æŸ¥æ˜¯å¦åº”è¯¥å¢å¤§ batch size
    if (shouldIncreaseBatchSize(metrics)) {
        size_t newSize = currentBatchSize_ * (1 + config_.growthCoefficient);
        newSize = std::min(newSize, config_.maxBatchSize);
        newSize = std::min(newSize, bestBatchSize_ * 2); // ä¸è¶…è¿‡å†å²æœ€ä½³çš„ 2 å€
        
        if (isMemorySafe(newSize)) {
            CLLM_DEBUG("[DynamicBatchTuner] Increasing batch size: %zu â†’ %zu", 
                      currentBatchSize_, newSize);
            currentBatchSize_ = newSize;
            
            if (currentScore > bestScore_) {
                bestScore_ = currentScore;
                bestBatchSize_ = newSize;
            }
        }
    }
    
    // 2. æ£€æŸ¥æ˜¯å¦åº”è¯¥å‡å° batch size
    else if (shouldDecreaseBatchSize(metrics)) {
        size_t newSize = currentBatchSize_ * (1 - config_.decayCoefficient);
        newSize = std::max(newSize, config_.minBatchSize);
        
        CLLM_DEBUG("[DynamicBatchTuner] Decreasing batch size: %zu â†’ %zu", 
                  currentBatchSize_, newSize);
        currentBatchSize_ = newSize;
    }
    
    // 3. å°å¹…éœ‡è¡æ¢ç´¢
    else {
        double oscillation = config_.oscillationCoefficient * 
            (std::rand() % 2 == 0 ? 1 : -1);
        size_t newSize = currentBatchSize_ * (1 + oscillation);
        newSize = std::clamp(newSize, config_.minBatchSize, config_.maxBatchSize);
        
        if (isMemorySafe(newSize)) {
            currentBatchSize_ = newSize;
        }
    }
    
    // 4. æ£€æŸ¥æ˜¯å¦è¿›å…¥ç¨³å®šé˜¶æ®µ
    if (checkStabilizationCondition()) {
        CLLM_INFO("[DynamicBatchTuner] Entering stable running phase. Batch size: %zu", 
                  currentBatchSize_);
        currentPhase_ = TuningPhase::STABLE_RUNNING;
    }
}

bool DynamicBatchTuner::shouldIncreaseBatchSize(const PerformanceMetrics& metrics) const {
    // æ¡ä»¶ 1: æ€§èƒ½æå‡
    double currentScore = calculateScore(metrics);
    if (currentScore > bestScore_ * (1 + config_.performanceThreshold)) {
        return true;
    }
    
    // æ¡ä»¶ 2: GPU åˆ©ç”¨ç‡ä½
    if (metrics.gpuUtilization < 0.5) {
        return true;
    }
    
    // æ¡ä»¶ 3: å†…å­˜æœ‰å¯Œä½™
    if (metrics.memoryUtilization < 0.7) {
        return true;
    }
    
    return false;
}

bool DynamicBatchTuner::shouldDecreaseBatchSize(const PerformanceMetrics& metrics) const {
    // æ¡ä»¶ 1: æ€§èƒ½ä¸‹é™
    double currentScore = calculateScore(metrics);
    if (currentScore < bestScore_ * (1 - config_.performanceThreshold)) {
        return true;
    }
    
    // æ¡ä»¶ 2: å†…å­˜æ¥è¿‘ä¸Šé™
    if (metrics.memoryUtilization > config_.memoryUsageLimit) {
        return true;
    }
    
    // æ¡ä»¶ 3: æ‰¹å¤„ç†æ—¶é—´è¿‡é•¿
    if (metrics.processingTimeMs > 1000) { // > 1s
        return true;
    }
    
    return false;
}
```

#### 3.1.3 ç¨³å®šè¿è¡Œé˜¶æ®µ

**ç›®æ ‡**: ä¿æŒæœ€ä¼˜ batch sizeï¼Œé¿å…é¢‘ç¹æ³¢åŠ¨

**ç¨³å®šç­–ç•¥**:

```cpp
void DynamicBatchTuner::runStableRunning(const PerformanceMetrics& metrics) {
    // 1. ç›‘æ§æ€§èƒ½æ³¢åŠ¨
    double currentScore = calculateScore(metrics);
    
    // 2. æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è°ƒæ•´
    if (currentScore < bestScore_ * 0.8) {
        // æ€§èƒ½ä¸‹é™è¶…è¿‡ 20%ï¼Œé‡æ–°è¿›å…¥è°ƒæ•´é˜¶æ®µ
        CLLM_WARN("[DynamicBatchTuner] Performance dropped significantly. Re-entering adjustment phase");
        currentPhase_ = TuningPhase::DYNAMIC_ADJUSTMENT;
        return;
    }
    
    // 3. å®šæœŸæ¢ç´¢ï¼ˆé˜²æ­¢é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼‰
    static size_t stableCycles = 0;
    stableCycles++;
    
    if (stableCycles >= config_.stabilizationCycles * 10) {
        // æ¯ 10 ä¸ªç¨³å®šå‘¨æœŸï¼Œè¿›è¡Œä¸€æ¬¡æ¢ç´¢
        stableCycles = 0;
        
        size_t explorationSize = currentBatchSize_ * (1 + config_.oscillationCoefficient);
        explorationSize = std::min(explorationSize, config_.maxBatchSize);
        
        if (isMemorySafe(explorationSize)) {
            CLLM_DEBUG("[DynamicBatchTuner] Exploration: trying batch size %zu", explorationSize);
            currentBatchSize_ = explorationSize;
        }
    }
    
    // 4. è´Ÿè½½å˜åŒ–æ£€æµ‹
    if (isLoadChangedSignificantly()) {
        CLLM_DEBUG("[DynamicBatchTuner] Load changed significantly. Re-entering adjustment phase");
        currentPhase_ = TuningPhase::DYNAMIC_ADJUSTMENT;
    }
}

bool DynamicBatchTuner::checkStabilizationCondition() const {
    if (metricsHistory_.size() < config_.stabilizationCycles) {
        return false;
    }
    
    // 1. æ£€æŸ¥ batch size å˜åŒ–
    size_t firstSize = metricsHistory_[0].batchSize;
    size_t lastSize = metricsHistory_.back().batchSize;
    double sizeChange = std::abs(static_cast<double>(lastSize - firstSize) / firstSize);
    
    if (sizeChange > config_.batchSizeChangeThreshold) {
        return false;
    }
    
    // 2. æ£€æŸ¥æ€§èƒ½æ³¢åŠ¨
    double sumScore = 0.0;
    double maxScore = 0.0;
    double minScore = std::numeric_limits<double>::max();
    
    for (const auto& m : metricsHistory_) {
        double score = calculateScore(m);
        sumScore += score;
        maxScore = std::max(maxScore, score);
        minScore = std::min(minScore, score);
    }
    
    double avgScore = sumScore / metricsHistory_.size();
    double fluctuation = (maxScore - minScore) / avgScore;
    
    if (fluctuation > config_.performanceFluctuationThreshold) {
        return false;
    }
    
    // 3. æ£€æŸ¥è¿ç»­å‘¨æœŸæ•°
    if (metricsHistory_.size() < config_.stabilizationCycles) {
        return false;
    }
    
    return true;
}
```

### 3.2 çŠ¶æ€è½¬æ¢å›¾

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                                      â”‚
                    â–¼                                      â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
    â”‚  INITIAL_PROBING     â”‚                               â”‚
    â”‚  (åˆå§‹æ¢æµ‹é˜¶æ®µ)       â”‚â”€â”€ æ¢æµ‹å®Œæˆ â”€â”€â–º                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
           â”‚                                               â”‚
           â”‚ æ¢æµ‹å¤±è´¥                                       â”‚
           â–¼                                               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
    â”‚  SAFE_FALLBACK       â”‚                               â”‚
    â”‚  (å®‰å…¨å›é€€)          â”‚â”€â”€ æ¢å¤æ­£å¸¸ â”€â”€â–º                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
                                                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
    â”‚ DYNAMIC_ADJUSTMENT   â”‚                               â”‚
    â”‚ (åŠ¨æ€è°ƒæ•´é˜¶æ®µ)        â”‚â”€â”€ æ»¡è¶³ç¨³å®šæ¡ä»¶ â”€â”€â–º             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
           â”‚                                               â”‚
           â”‚ æ€§èƒ½ä¸‹é™ / è´Ÿè½½å˜åŒ–                           â”‚
           â–¼                                               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
    â”‚  RE-EVALUATION       â”‚                               â”‚
    â”‚  (é‡æ–°è¯„ä¼°)          â”‚â”€â”€ è¯„ä¼°å®Œæˆ â”€â”€â–º                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
                                                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
    â”‚ STABLE_RUNNING       â”‚                               â”‚
    â”‚ (ç¨³å®šè¿è¡Œé˜¶æ®µ)        â”‚                               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
           â”‚                                               â”‚
           â”‚ å®šæœŸæ¢ç´¢ / æ€§èƒ½ä¸‹é™                           â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º DYNAMIC_ADJUSTMENT          â”‚
```

---

## 4. å®‰å…¨æœºåˆ¶

### 4.1 è¾¹ç•Œæ£€æŸ¥

#### 4.1.1 Batch Size è¾¹ç•Œ

```cpp
size_t DynamicBatchTuner::clampBatchSize(size_t batchSize) const {
    return std::clamp(batchSize, 
                      config_.minBatchSize, 
                      config_.maxBatchSize);
}

bool DynamicBatchTuner::isBatchSizeValid(size_t batchSize) const {
    return batchSize >= config_.minBatchSize && 
           batchSize <= config_.maxBatchSize;
}
```

#### 4.1.2 å†…å­˜è¾¹ç•Œ

```cpp
bool DynamicBatchTuner::isMemorySafe(size_t batchSize) const {
    // ä¼°ç®—å†…å­˜éœ€æ±‚
    size_t estimatedMemoryMb = estimateMemoryUsage(batchSize);
    
    // æ£€æŸ¥æ˜¯å¦åœ¨å®‰å…¨èŒƒå›´å†…
    size_t availableMemory = gpuInfo_.totalMemoryMb - config_.memorySafetyMarginMb;
    
    if (estimatedMemoryMb > availableMemory * config_.memoryUsageLimit) {
        return false;
    }
    
    return true;
}

size_t DynamicBatchTuner::estimateMemoryUsage(size_t batchSize) const {
    // ç®€åŒ–ä¼°ç®—ï¼š
    // å†…å­˜ â‰ˆ batch_size Ã— avg_seq_len Ã— memory_per_token
    // 
    // å¯¹äº qwen3-0.6b:
    // - avg_seq_len â‰ˆ 100 tokens
    // - memory_per_token â‰ˆ 2 MB
    // 
    // å› æ­¤: memory â‰ˆ batch_size Ã— 200 MB
    
    return batchSize * 200; // MB
}
```

#### 4.1.3 æ€§èƒ½è¾¹ç•Œ

```cpp
bool DynamicBatchTuner::isPerformanceAcceptable(const PerformanceMetrics& metrics) const {
    // 1. ååé‡æ£€æŸ¥
    if (metrics.throughput < 10) { // < 10 tokens/sec
        CLLM_WARN("[DynamicBatchTuner] Throughput too low: %.2f tokens/sec", metrics.throughput);
        return false;
    }
    
    // 2. æ‰¹å¤„ç†æ—¶é—´æ£€æŸ¥
    if (metrics.processingTimeMs > 5000) { // > 5s
        CLLM_WARN("[DynamicBatchTuner] Processing time too long: %.2f ms", metrics.processingTimeMs);
        return false;
    }
    
    // 3. GPU åˆ©ç”¨ç‡æ£€æŸ¥
    if (metrics.gpuUtilization < 0.1) { // < 10%
        CLLM_WARN("[DynamicBatchTuner] GPU utilization too low: %.2f%%", 
                  metrics.gpuUtilization * 100);
        return false;
    }
    
    return true;
}
```

### 4.2 å¼‚å¸¸å¤„ç†

#### 4.2.1 å†…å­˜æº¢å‡ºå¤„ç†

```cpp
void DynamicBatchTuner::handleMemoryOverflow() {
    CLLM_ERROR("[DynamicBatchTuner] Memory overflow detected! Current batch size: %zu", 
               currentBatchSize_);
    
    // 1. è®°å½•å¤±è´¥æ¬¡æ•°
    consecutiveFailures_++;
    
    // 2. ç«‹å³å‡å° batch size
    size_t newSize = currentBatchSize_ / 2;
    newSize = std::max(newSize, config_.minBatchSize);
    
    CLLM_WARN("[DynamicBatchTuner] Reducing batch size to %zu", newSize);
    currentBatchSize_ = newSize;
    
    // 3. æ£€æŸ¥æ˜¯å¦éœ€è¦å›é€€åˆ°å®‰å…¨çŠ¶æ€
    if (consecutiveFailures_ >= config_.maxConsecutiveFailures) {
        resetToSafeState();
    }
    
    // 4. é‡æ–°è¿›å…¥æ¢æµ‹é˜¶æ®µ
    currentPhase_ = TuningPhase::INITIAL_PROBING;
}

void DynamicBatchTuner::resetToSafeState() {
    CLLM_ERROR("[DynamicBatchTuner] Too many consecutive failures. Resetting to safe state");
    
    // é‡ç½®åˆ°æœ€å° batch size
    currentBatchSize_ = config_.minBatchSize;
    
    // æ¸…ç©ºå†å²è®°å½•
    metricsHistory_.clear();
    
    // é‡ç½®å¤±è´¥è®¡æ•°
    consecutiveFailures_ = 0;
    
    // é‡ç½®æœ€ä½³è®°å½•
    bestBatchSize_ = config_.initialBatchSize;
    bestScore_ = 0.0;
    
    // è¿›å…¥åˆå§‹æ¢æµ‹é˜¶æ®µ
    currentPhase_ = TuningPhase::INITIAL_PROBING;
    
    CLLM_INFO("[DynamicBatchTuner] Reset to safe state. Batch size: %zu", 
              currentBatchSize_);
}
```

#### 4.2.2 æ€§èƒ½å¼‚å¸¸å¤„ç†

```cpp
void DynamicBatchTuner::handlePerformanceAnomaly(const PerformanceMetrics& metrics) {
    double currentScore = calculateScore(metrics);
    
    // æ£€æŸ¥æ˜¯å¦å¼‚å¸¸
    if (currentScore < bestScore_ * 0.5) { // æ€§èƒ½ä¸‹é™è¶…è¿‡ 50%
        CLLM_WARN("[DynamicBatchTuner] Performance anomaly detected! Score dropped from %.3f to %.3f", 
                  bestScore_, currentScore);
        
        // 1. æ£€æŸ¥æ˜¯å¦æ˜¯ä¸´æ—¶æ³¢åŠ¨
        if (isTemporaryFluctuation()) {
            CLLM_DEBUG("[DynamicBatchTuner] Likely temporary fluctuation, ignoring");
            return;
        }
        
        // 2. å°è¯•æ¢å¤åˆ°å†å²æœ€ä½³
        if (bestBatchSize_ != currentBatchSize_) {
            CLLM_DEBUG("[DynamicBatchTuner] Reverting to best batch size: %zu", bestBatchSize_);
            currentBatchSize_ = bestBatchSize_;
            return;
        }
        
        // 3. é‡æ–°è¯„ä¼°
        currentPhase_ = TuningPhase::DYNAMIC_ADJUSTMENT;
    }
}

bool DynamicBatchTuner::isTemporaryFluctuation() const {
    if (metricsHistory_.size() < 3) {
        return false;
    }
    
    // æ£€æŸ¥æœ€è¿‘ 3 ä¸ªæŒ‡æ ‡çš„è¶‹åŠ¿
    const auto& m1 = metricsHistory_[metricsHistory_.size() - 3];
    const auto& m2 = metricsHistory_[metricsHistory_.size() - 2];
    const auto& m3 = metricsHistory_.back();
    
    double s1 = calculateScore(m1);
    double s2 = calculateScore(m2);
    double s3 = calculateScore(m3);
    
    // å¦‚æœæ˜¯ V å‹æ³¢åŠ¨ï¼ˆä¸‹é™åç«‹å³ä¸Šå‡ï¼‰ï¼Œå¯èƒ½æ˜¯ä¸´æ—¶çš„
    if (s2 < s1 * 0.8 && s3 > s2 * 1.2) {
        return true;
    }
    
    return false;
}
```

### 4.3 å›é€€ç­–ç•¥

#### 4.3.1 å¤šçº§å›é€€æœºåˆ¶

```cpp
enum class FallbackLevel {
    NONE,           // æ— å›é€€
    MINOR_ADJUSTMENT, // å°å¹…è°ƒæ•´
    MODERATE_FALLBACK, // ä¸­åº¦å›é€€
    MAJOR_FALLBACK,    // å¤§å¹…å›é€€
    SAFE_MODE          // å®‰å…¨æ¨¡å¼
};

FallbackLevel DynamicBatchTuner::determineFallbackLevel(const PerformanceMetrics& metrics) const {
    double currentScore = calculateScore(metrics);
    
    // è®¡ç®—æ€§èƒ½ä¸‹é™æ¯”ä¾‹
    double scoreDrop = 1.0 - (currentScore / bestScore_);
    
    if (scoreDrop < 0.1) {
        return FallbackLevel::NONE; // < 10% ä¸‹é™
    }
    else if (scoreDrop < 0.3) {
        return FallbackLevel::MINOR_ADJUSTMENT; // 10-30% ä¸‹é™
    }
    else if (scoreDrop < 0.5) {
        return FallbackLevel::MODERATE_FALLBACK; // 30-50% ä¸‹é™
    }
    else if (scoreDrop < 0.8) {
        return FallbackLevel::MAJOR_FALLBACK; // 50-80% ä¸‹é™
    }
    else {
        return FallbackLevel::SAFE_MODE; // > 80% ä¸‹é™
    }
}

void DynamicBatchTuner::executeFallback(FallbackLevel level) {
    switch (level) {
        case FallbackLevel::NONE:
            // æ— æ“ä½œ
            break;
            
        case FallbackLevel::MINOR_ADJUSTMENT:
            // å°å¹…è°ƒæ•´: å‡å° 10%
            currentBatchSize_ = currentBatchSize_ * 0.9;
            CLLM_DEBUG("[DynamicBatchTuner] Minor adjustment: batch size %zu", currentBatchSize_);
            break;
            
        case FallbackLevel::MODERATE_FALLBACK:
            // ä¸­åº¦å›é€€: å‡å° 30%ï¼Œæ¢å¤åˆ°å†å²æœ€ä½³
            currentBatchSize_ = std::min(currentBatchSize_ * 0.7, bestBatchSize_);
            CLLM_WARN("[DynamicBatchTuner] Moderate fallback: batch size %zu", currentBatchSize_);
            break;
            
        case FallbackLevel::MAJOR_FALLBACK:
            // å¤§å¹…å›é€€: å‡å° 50%ï¼Œé‡æ–°è¿›å…¥è°ƒæ•´é˜¶æ®µ
            currentBatchSize_ = currentBatchSize_ * 0.5;
            currentPhase_ = TuningPhase::DYNAMIC_ADJUSTMENT;
            CLLM_WARN("[DynamicBatchTuner] Major fallback: batch size %zu", currentBatchSize_);
            break;
            
        case FallbackLevel::SAFE_MODE:
            // å®‰å…¨æ¨¡å¼: é‡ç½®åˆ°æœ€å° batch size
            resetToSafeState();
            CLLM_ERROR("[DynamicBatchTuner] Entering safe mode");
            break;
    }
}
```

#### 4.3.2 è‡ªåŠ¨æ¢å¤æœºåˆ¶

```cpp
void DynamicBatchTuner::attemptRecovery() {
    if (currentPhase_ != TuningPhase::STABLE_RUNNING) {
        return;
    }
    
    // æ£€æŸ¥æ˜¯å¦å¯ä»¥æ¢å¤
    if (isPerformanceRecovering()) {
        CLLM_INFO("[DynamicBatchTuner] Performance is recovering. Attempting to increase batch size");
        
        // é€æ­¥å¢å¤§ batch size
        size_t targetSize = currentBatchSize_ * 1.1;
        targetSize = std::min(targetSize, bestBatchSize_);
        
        if (isMemorySafe(targetSize)) {
            currentBatchSize_ = targetSize;
        }
    }
}

bool DynamicBatchTuner::isPerformanceRecovering() const {
    if (metricsHistory_.size() < 5) {
        return false;
    }
    
    // æ£€æŸ¥æœ€è¿‘ 5 ä¸ªæŒ‡æ ‡çš„è¶‹åŠ¿
    std::vector<double> recentScores;
    for (size_t i = metricsHistory_.size() - 5; i < metricsHistory_.size(); ++i) {
        recentScores.push_back(calculateScore(metricsHistory_[i]));
    }
    
    // è®¡ç®—æ–œç‡
    double slope = calculateTrendSlope(recentScores);
    
    // å¦‚æœæ–œç‡ä¸ºæ­£ä¸”å¤§äºé˜ˆå€¼ï¼Œè¯´æ˜åœ¨æ¢å¤
    return slope > 0.01; // æ¯æ­¥æå‡ > 1%
}
```

---

## 5. æ€§èƒ½éªŒè¯

### 5.1 æµ‹è¯•æ–¹æ¡ˆ

#### 5.1.1 ä¸åŒ GPU å‹å·éªŒè¯

**æµ‹è¯•çŸ©é˜µ**:

| GPU å‹å· | å†…å­˜ | ç†è®ºæ€§èƒ½ | é¢„æœŸ batch size | æµ‹è¯•åœºæ™¯ |
|---------|------|---------|----------------|--------|
| **NVIDIA A100 40GB** | 40GB | 312 TFLOPS | 64-128 | é«˜å¹¶å‘æ¨ç† |
| **NVIDIA A10 24GB** | 24GB | 19.5 TFLOPS | 32-64 | ä¸­ç­‰å¹¶å‘ |
| **NVIDIA T4 16GB** | 16GB | 8.1 TFLOPS | 16-32 | ä½å¹¶å‘ |
| **Apple M3 Ultra** | 128GB | ~100 TOPS | 32-64 | æ··åˆè´Ÿè½½ |
| **Apple M3 Pro** | 36GB | ~40 TOPS | 16-32 | æ ‡å‡†è´Ÿè½½ |
| **AMD MI250X** | 64GB | 819 TFLOPS | 128-256 | è¶…é«˜å¹¶å‘ |
| **Intel Arc A770** | 16GB | 21 TFLOPS | 16-32 | å…¥é—¨çº§ |

#### 5.1.2 æµ‹è¯•ç”¨ä¾‹è®¾è®¡

**æµ‹è¯•ç”¨ä¾‹ 1: åˆå§‹æ¢æµ‹é˜¶æ®µéªŒè¯**

```python
def test_initial_probing_phase():
    """éªŒè¯åˆå§‹æ¢æµ‹é˜¶æ®µèƒ½å¦æ­£ç¡®æ‰¾åˆ°å¯è¡Œçš„ batch size èŒƒå›´"""
    
    # æ­¥éª¤ 1: å¯åŠ¨æœåŠ¡å™¨ï¼Œå¯ç”¨åŠ¨æ€æ‰¹å¤„ç†
    server = start_cllm_server(enable_dynamic_batch=True)
    
    # æ­¥éª¤ 2: å‘é€ä¸€ç³»åˆ—è¯·æ±‚
    for i in range(20):
        send_request(prompt="Hello", max_tokens=50)
    
    # æ­¥éª¤ 3: æ£€æŸ¥è°ƒè°å™¨çŠ¶æ€
    tuner_status = server.get_tuner_status()
    
    # æ–­è¨€ 1: åº”è¯¥å®Œæˆåˆå§‹æ¢æµ‹
    assert tuner_status.phase == "DYNAMIC_ADJUSTMENT" or tuner_status.phase == "STABLE_RUNNING"
    
    # æ–­è¨€ 2: åº”è¯¥æ‰¾åˆ°åˆç†çš„ batch size
    assert tuner_status.current_batch_size >= 1
    assert tuner_status.current_batch_size <= 256
    
    # æ–­è¨€ 3: åº”è¯¥æœ‰å†å²æœ€ä½³è®°å½•
    assert tuner_status.best_batch_size > 0
    assert tuner_status.best_score > 0
    
    print(f"âœ“ Initial probing phase completed. Best batch size: {tuner_status.best_batch_size}")
```

**æµ‹è¯•ç”¨ä¾‹ 2: åŠ¨æ€è°ƒæ•´é˜¶æ®µéªŒè¯**

```python
def test_dynamic_adjustment_under_load():
    """éªŒè¯åŠ¨æ€è°ƒæ•´é˜¶æ®µèƒ½å¦é€‚åº”è´Ÿè½½å˜åŒ–"""
    
    # æ­¥éª¤ 1: å¯åŠ¨æœåŠ¡å™¨
    server = start_cllm_server(enable_dynamic_batch=True)
    
    # æ­¥éª¤ 2: ä½è´Ÿè½½è¿è¡Œ
    print("Phase 1: Low load (2 concurrent requests)")
    for i in range(10):
        send_concurrent_requests(count=2, max_tokens=50)
    
    low_load_batch_size = server.get_tuner_status().current_batch_size
    
    # æ­¥éª¤ 3: é«˜è´Ÿè½½è¿è¡Œ
    print("Phase 2: High load (32 concurrent requests)")
    for i in range(10):
        send_concurrent_requests(count=32, max_tokens=50)
    
    high_load_batch_size = server.get_tuner_status().current_batch_size
    
    # æ–­è¨€: é«˜è´Ÿè½½ä¸‹åº”è¯¥å¢å¤§ batch size
    assert high_load_batch_size >= low_load_batch_size * 0.8, \
        f"Batch size should not decrease significantly under high load"
    
    print(f"âœ“ Dynamic adjustment works: {low_load_batch_size} â†’ {high_load_batch_size}")
```

**æµ‹è¯•ç”¨ä¾‹ 3: å†…å­˜æº¢å‡ºå¤„ç†éªŒè¯**

```python
def test_memory_overflow_handling():
    """éªŒè¯å†…å­˜æº¢å‡ºæ—¶çš„å®‰å…¨æœºåˆ¶"""
    
    # æ­¥éª¤ 1: é™åˆ¶ GPU å†…å­˜
    server = start_cllm_server(
        enable_dynamic_batch=True,
        max_gpu_memory_mb=4096  # é™åˆ¶ä¸º 4GB
    )
    
    # æ­¥éª¤ 2: å‘é€å¤§è¯·æ±‚è§¦å‘å†…å­˜æº¢å‡º
    try:
        send_request(prompt="Hello " * 1000, max_tokens=1000)
    except MemoryOverflowError:
        pass
    
    # æ­¥éª¤ 3: æ£€æŸ¥è°ƒè°å™¨çŠ¶æ€
    tuner_status = server.get_tuner_status()
    
    # æ–­è¨€ 1: åº”è¯¥å‡å° batch size
    assert tuner_status.current_batch_size < tuner_status.best_batch_size
    
    # æ–­è¨€ 2: åº”è¯¥è®°å½•å¤±è´¥
    assert tuner_status.consecutive_failures > 0
    
    # æ–­è¨€ 3: ç³»ç»Ÿåº”è¯¥ç»§ç»­è¿è¡Œ
    assert server.is_running()
    
    print("âœ“ Memory overflow handling works correctly")
```

**æµ‹è¯•ç”¨ä¾‹ 4: ç¨³å®šè¿è¡Œé˜¶æ®µéªŒè¯**

```python
def test_stable_running_phase():
    """éªŒè¯ç¨³å®šè¿è¡Œé˜¶æ®µèƒ½å¦ä¿æŒæœ€ä¼˜ batch size"""
    
    # æ­¥éª¤ 1: å¯åŠ¨æœåŠ¡å™¨å¹¶è¿è¡Œä¸€æ®µæ—¶é—´
    server = start_cllm_server(enable_dynamic_batch=True)
    
    # æ­¥éª¤ 2: æŒç»­å‘é€è¯·æ±‚ç›´åˆ°è¿›å…¥ç¨³å®šé˜¶æ®µ
    batch_sizes = []
    for i in range(50):
        send_concurrent_requests(count=16, max_tokens=50)
        
        status = server.get_tuner_status()
        batch_sizes.append(status.current_batch_size)
        
        if status.phase == "STABLE_RUNNING":
            print(f"Entered stable running phase after {i+1} batches")
            break
    
    # æ­¥éª¤ 3: ç»§ç»­è¿è¡Œï¼Œæ£€æŸ¥ batch size æ˜¯å¦ç¨³å®š
    stable_batch_sizes = []
    for i in range(20):
        send_concurrent_requests(count=16, max_tokens=50)
        status = server.get_tuner_status()
        stable_batch_sizes.append(status.current_batch_size)
    
    # æ–­è¨€: batch size åº”è¯¥ä¿æŒç›¸å¯¹ç¨³å®š
    max_variation = max(stable_batch_sizes) - min(stable_batch_sizes)
    assert max_variation <= 4, \
        f"Batch size variation too large: {max_variation}"
    
    print(f"âœ“ Stable running phase maintained. Batch size variation: {max_variation}")
```

### 5.2 æ€§èƒ½æå‡è¯„ä¼°æŒ‡æ ‡

#### 5.2.1 æ ¸å¿ƒæŒ‡æ ‡

```python
class PerformanceMetrics:
    def __init__(self):
        # ååé‡æŒ‡æ ‡
        self.throughput_improvement = 0.0  # ç›¸å¯¹äºé™æ€é…ç½®çš„æå‡æ¯”ä¾‹
        self.max_throughput = 0.0          # æœ€å¤§ååé‡
        self.avg_throughput = 0.0          # å¹³å‡ååé‡
        
        # å»¶è¿ŸæŒ‡æ ‡
        self.p50_latency = 0.0             # P50 å»¶è¿Ÿ
        self.p95_latency = 0.0             # P95 å»¶è¿Ÿ
        self.p99_latency = 0.0             # P99 å»¶è¿Ÿ
        
        # èµ„æºåˆ©ç”¨ç‡æŒ‡æ ‡
        self.avg_gpu_utilization = 0.0     # å¹³å‡ GPU åˆ©ç”¨ç‡
        self.peak_gpu_utilization = 0.0    # å³°å€¼ GPU åˆ©ç”¨ç‡
        self.avg_memory_utilization = 0.0  # å¹³å‡å†…å­˜åˆ©ç”¨ç‡
        
        # ç¨³å®šæ€§æŒ‡æ ‡
        self.success_rate = 0.0            # è¯·æ±‚æˆåŠŸç‡
        self.batch_size_stability = 0.0    # batch size ç¨³å®šæ€§ï¼ˆæ ‡å‡†å·®ï¼‰
        self.consecutive_failures = 0      # è¿ç»­å¤±è´¥æ¬¡æ•°
        
        # æ”¶æ•›æŒ‡æ ‡
        self.convergence_time = 0.0        # æ”¶æ•›æ—¶é—´ï¼ˆç§’ï¼‰
        self.convergence_iterations = 0    # æ”¶æ•›è¿­ä»£æ¬¡æ•°
        
    def calculate_overall_score(self):
        """è®¡ç®—ç»¼åˆè¯„åˆ†ï¼ˆ0-100ï¼‰"""
        score = 0.0
        
        # ååé‡æƒé‡: 40%
        score += min(self.throughput_improvement * 100, 40)
        
        # GPU åˆ©ç”¨ç‡æƒé‡: 30%
        score += self.avg_gpu_utilization * 30
        
        # ç¨³å®šæ€§æƒé‡: 20%
        score += self.success_rate * 20
        
        # æ”¶æ•›é€Ÿåº¦æƒé‡: 10%
        if self.convergence_time < 60:  # < 1min
            score += 10
        elif self.convergence_time < 120:  # < 2min
            score += 5
        
        return score
```

#### 5.2.2 ä¸é™æ€è®¾ç½®çš„å¯¹æ¯”

**å¯¹æ¯”æµ‹è¯•è„šæœ¬**:

```python
def compare_static_vs_dynamic():
    """å¯¹æ¯”é™æ€ batch size å’ŒåŠ¨æ€è°ƒæ•´çš„æ€§èƒ½"""
    
    # æµ‹è¯•é…ç½®
    test_configs = [
        {"name": "Static Batch Size 8", "dynamic": False, "batch_size": 8},
        {"name": "Static Batch Size 16", "dynamic": False, "batch_size": 16},
        {"name": "Static Batch Size 32", "dynamic": False, "batch_size": 32},
        {"name": "Static Batch Size 64", "dynamic": False, "batch_size": 64},
        {"name": "Dynamic Batch Tuner", "dynamic": True, "batch_size": None},
    ]
    
    results = []
    
    for config in test_configs:
        print(f"\n{'='*60}")
        print(f"Testing: {config['name']}")
        print(f"{'='*60}")
        
        # å¯åŠ¨æœåŠ¡å™¨
        server = start_cllm_server(
            enable_dynamic_batch=config['dynamic'],
            static_batch_size=config['batch_size']
        )
        
        # è¿è¡Œæµ‹è¯•
        metrics = run_benchmark(
            server,
            concurrent_requests=[8, 16, 24, 32],
            total_requests=72,
            max_tokens=50
        )
        
        results.append({
            'config': config['name'],
            'throughput': metrics['avg_throughput'],
            'gpu_utilization': metrics['avg_gpu_utilization'],
            'success_rate': metrics['success_rate'],
            'p95_latency': metrics['p95_latency'],
        })
        
        server.stop()
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    print("\n" + "="*80)
    print("STATIC vs DYNAMIC BATCH SIZE COMPARISON")
    print("="*80)
    
    print(f"{'Configuration':<30} {'Throughput':>12} {'GPU Util':>12} {'Success Rate':>12} {'P95 Latency':>12}")
    print("-"*80)
    
    for result in results:
        print(f"{result['config']:<30} "
              f"{result['throughput']:>12.2f} "
              f"{result['gpu_utilization']*100:>11.1f}% "
              f"{result['success_rate']*100:>11.1f}% "
              f"{result['p95_latency']:>12.2f}s")
    
    # è®¡ç®—åŠ¨æ€è°ƒæ•´çš„ä¼˜åŠ¿
    static_results = [r for r in results if 'Static' in r['config']]
    dynamic_result = next(r for r in results if 'Dynamic' in r['config'])
    
    best_static_throughput = max(r['throughput'] for r in static_results)
    throughput_improvement = (dynamic_result['throughput'] - best_static_throughput) / best_static_throughput * 100
    
    print("\n" + "="*80)
    print(f"Dynamic Batch Tunerä¼˜åŠ¿: +{throughput_improvement:.1f}% ååé‡")
    print("="*80)
```

### 5.3 æ€§èƒ½ç›®æ ‡

#### 5.3.1 å¿…é¡»æ»¡è¶³çš„ç›®æ ‡

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | è¯´æ˜ |
|-----|--------|------|
| **ååé‡æå‡** | â‰¥ 20% | ç›¸å¯¹äºæœ€ä¼˜é™æ€é…ç½® |
| **GPU åˆ©ç”¨ç‡** | â‰¥ 70% | å¹³å‡åˆ©ç”¨ç‡ |
| **è¯·æ±‚æˆåŠŸç‡** | â‰¥ 99.9% | æ— å†…å­˜æº¢å‡ºå¯¼è‡´çš„å¤±è´¥ |
| **æ”¶æ•›æ—¶é—´** | < 2åˆ†é’Ÿ | ä»å¯åŠ¨åˆ°ç¨³å®š |
| **Batch Size ç¨³å®šæ€§** | â‰¤ 10% | æ ‡å‡†å·®/å‡å€¼ |
| **ç³»ç»Ÿå¼€é”€** | < 5% | è°ƒè°å™¨æœ¬èº«çš„å¼€é”€ |

#### 5.3.2 æœŸæœ›è¾¾åˆ°çš„ç›®æ ‡

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | è¯´æ˜ |
|-----|--------|------|
| **ååé‡æå‡** | â‰¥ 40% | ç›¸å¯¹äºæœ€ä¼˜é™æ€é…ç½® |
| **GPU åˆ©ç”¨ç‡** | â‰¥ 85% | å¹³å‡åˆ©ç”¨ç‡ |
| **æ”¶æ•›æ—¶é—´** | < 1åˆ†é’Ÿ | ä»å¯åŠ¨åˆ°ç¨³å®š |
| **è·¨ GPU å…¼å®¹æ€§** | æ”¯æŒ â‰¥ 5 ç§ GPU | NVIDIA, AMD, Apple, Intel |
| **è‡ªé€‚åº”èƒ½åŠ›** | å“åº”æ—¶é—´ < 10 æ‰¹ | å¯¹è´Ÿè½½å˜åŒ–çš„å“åº” |

#### 5.3.3 æ€§èƒ½å›å½’æµ‹è¯•

```python
def run_performance_regression_test():
    """è¿è¡Œæ€§èƒ½å›å½’æµ‹è¯•ï¼Œç¡®ä¿ä¼˜åŒ–ä¸ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™"""
    
    # åŸºå‡†æ€§èƒ½ï¼ˆæ¥è‡ªå†å²æœ€ä½³ï¼‰
    baseline_metrics = {
        'throughput': 137.73,  # tokens/sec
        'gpu_utilization': 0.75,
        'success_rate': 0.995,
        'p95_latency': 5.36,  # seconds
    }
    
    # è¿è¡Œæµ‹è¯•
    current_metrics = run_benchmark(
        server=None,  # ä½¿ç”¨é»˜è®¤é…ç½®
        concurrent_requests=[8, 16, 24, 32],
        total_requests=72,
        max_tokens=50
    )
    
    # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ€§èƒ½è¦æ±‚
    issues = []
    
    if current_metrics['throughput'] < baseline_metrics['throughput'] * 0.9:
        issues.append(f"ååé‡ä¸‹é™è¶…è¿‡ 10%: {current_metrics['throughput']:.2f} vs {baseline_metrics['throughput']:.2f}")
    
    if current_metrics['gpu_utilization'] < baseline_metrics['gpu_utilization'] * 0.8:
        issues.append(f"GPU åˆ©ç”¨ç‡ä¸‹é™è¶…è¿‡ 20%: {current_metrics['gpu_utilization']*100:.1f}% vs {baseline_metrics['gpu_utilization']*100:.1f}%")
    
    if current_metrics['success_rate'] < 0.99:
        issues.append(f"æˆåŠŸç‡ä½äº 99%: {current_metrics['success_rate']*100:.1f}%")
    
    if current_metrics['p95_latency'] > baseline_metrics['p95_latency'] * 1.2:
        issues.append(f"P95 å»¶è¿Ÿå¢åŠ è¶…è¿‡ 20%: {current_metrics['p95_latency']:.2f}s vs {baseline_metrics['p95_latency']:.2f}s")
    
    # ç”ŸæˆæŠ¥å‘Š
    if issues:
        print("âŒ æ€§èƒ½å›å½’æµ‹è¯•å¤±è´¥:")
        for issue in issues:
            print(f"  - {issue}")
        return False
    else:
        print("âœ… æ€§èƒ½å›å½’æµ‹è¯•é€šè¿‡")
        return True
```

---

## 6. é…ç½®æ¥å£

### 6.1 é…ç½®å‚æ•°è®¾è®¡

#### 6.1.1 æ ¸å¿ƒé…ç½®å‚æ•°

```yaml
# åŠ¨æ€ Batch Size è°ƒè°å™¨é…ç½®
dynamic_batch_tuner:
  # åŸºç¡€å¼€å…³
  enabled: true                    # æ˜¯å¦å¯ç”¨åŠ¨æ€è°ƒæ•´
  
  # Batch Size èŒƒå›´
  min_batch_size: 1                # æœ€å° batch size
  max_batch_size: 256              # æœ€å¤§ batch size
  initial_batch_size: 8            # åˆå§‹ batch size
  
  # æ¢æµ‹é˜¶æ®µå‚æ•°
  max_probing_attempts: 10         # æœ€å¤§æ¢æµ‹æ¬¡æ•°
  probing_growth_factor: 2.0       # æ¢æµ‹å¢é•¿å› å­ (1.5-3.0)
  probing_timeout_ms: 5000         # æ¢æµ‹è¶…æ—¶æ—¶é—´
  
  # è°ƒæ•´é˜¶æ®µå‚æ•°
  performance_threshold: 0.05      # æ€§èƒ½å˜åŒ–é˜ˆå€¼ (Îµ: 0.01-0.20)
  growth_coefficient: 0.2          # å¢é•¿ç³»æ•° (Î±: 0.1-0.5)
  decay_coefficient: 0.3           # è¡°å‡ç³»æ•° (Î²: 0.2-0.5)
  oscillation_coefficient: 0.05    # éœ‡è¡ç³»æ•° (Î´: 0.01-0.10)
  adjustment_interval_ms: 100      # è°ƒæ•´é—´éš”æ—¶é—´
  
  # ç¨³å®šé˜¶æ®µå‚æ•°
  stabilization_cycles: 10         # ç¨³å®šå‘¨æœŸæ•° (N: 5-20)
  batch_size_change_threshold: 0.10  # batch size å˜åŒ–é˜ˆå€¼ (Î¸: 0.05-0.20)
  performance_fluctuation_threshold: 0.03  # æ€§èƒ½æ³¢åŠ¨é˜ˆå€¼ (Ï†: 0.01-0.10)
  exploration_interval: 100        # å®šæœŸæ¢ç´¢é—´éš”ï¼ˆæ‰¹æ¬¡æ•°ï¼‰
  
  # å®‰å…¨å‚æ•°
  memory_safety_margin_mb: 512     # å†…å­˜å®‰å…¨ä½™é‡ (256-2048 MB)
  memory_usage_limit: 0.90         # å†…å­˜ä½¿ç”¨é™åˆ¶ (0.70-0.95)
  max_consecutive_failures: 3      # æœ€å¤§è¿ç»­å¤±è´¥æ¬¡æ•° (2-5)
  fallback_level: "moderate"       # å›é€€çº§åˆ« (none/minor/moderate/major/safe)
  
  # æ€§èƒ½æƒé‡ï¼ˆÎ± + Î² + Î³ = 1.0ï¼‰
  throughput_weight: 0.5           # ååé‡æƒé‡ (Î±: 0.3-0.7)
  gpu_utilization_weight: 0.3      # GPU åˆ©ç”¨ç‡æƒé‡ (Î²: 0.2-0.5)
  memory_efficiency_weight: 0.2    # å†…å­˜æ•ˆç‡æƒé‡ (Î³: 0.1-0.3)
  
  # æ€§èƒ½æŒ‡æ ‡
  min_acceptable_throughput: 10    # æœ€å°å¯æ¥å—ååé‡ (tokens/sec)
  max_acceptable_latency_ms: 5000  # æœ€å¤§å¯æ¥å—å»¶è¿Ÿ (ms)
  target_gpu_utilization: 0.80     # ç›®æ ‡ GPU åˆ©ç”¨ç‡ (0.60-0.95)
  
  # é«˜çº§å‚æ•°
  enable_oscillation: true         # æ˜¯å¦å¯ç”¨éœ‡è¡æ¢ç´¢
  enable_auto_recovery: true       # æ˜¯å¦å¯ç”¨è‡ªåŠ¨æ¢å¤
  recovery_sensitivity: 0.5        # æ¢å¤çµæ•åº¦ (0.1-1.0)
  load_change_threshold: 0.30      # è´Ÿè½½å˜åŒ–é˜ˆå€¼ (0.20-0.50)
  
  # è°ƒè¯•å‚æ•°
  enable_debug_logging: false      # æ˜¯å¦å¯ç”¨è°ƒè¯•æ—¥å¿—
  metrics_history_size: 100        # å†å²è®°å½•å¤§å° (50-200)
  report_interval_sec: 10          # çŠ¶æ€æŠ¥å‘Šé—´éš” (5-30 sec)
```

#### 6.1.2 å‚æ•°è¯´æ˜ä¸æ¨èå€¼

**Batch Size èŒƒå›´å‚æ•°**:

| å‚æ•° | æ¨èå€¼ | èŒƒå›´ | è¯´æ˜ |
|-----|--------|------|------|
| `min_batch_size` | 1 | 1-8 | æœ€å° batch sizeï¼Œç¡®ä¿å³ä½¿åœ¨ä½è´Ÿè½½ä¸‹ä¹Ÿèƒ½å¤„ç†è¯·æ±‚ |
| `max_batch_size` | 256 | 64-512 | æœ€å¤§ batch sizeï¼Œé˜²æ­¢å†…å­˜æº¢å‡ºã€‚æ ¹æ® GPU å†…å­˜è°ƒæ•´ |
| `initial_batch_size` | 8 | 4-16 | åˆå§‹ batch sizeï¼Œç”¨äºæ¢æµ‹é˜¶æ®µçš„èµ·ç‚¹ |

**æ¢æµ‹é˜¶æ®µå‚æ•°**:

| å‚æ•° | æ¨èå€¼ | èŒƒå›´ | è¯´æ˜ |
|-----|--------|------|------|
| `max_probing_attempts` | 10 | 5-20 | æœ€å¤§æ¢æµ‹æ¬¡æ•°ã€‚æ¬¡æ•°è¶Šå¤šï¼Œæ¢æµ‹è¶Šå……åˆ†ï¼Œä½†å¯åŠ¨æ—¶é—´è¶Šé•¿ |
| `probing_growth_factor` | 2.0 | 1.5-3.0 | æ¢æµ‹å¢é•¿å› å­ã€‚2.0 è¡¨ç¤ºæ¯æ¬¡ç¿»å€ (1â†’2â†’4â†’8...) |
| `probing_timeout_ms` | 5000 | 3000-10000 | æ¢æµ‹è¶…æ—¶æ—¶é—´ï¼Œé˜²æ­¢æ¢æµ‹é˜¶æ®µå¡ä½ |

**è°ƒæ•´é˜¶æ®µå‚æ•°**:

| å‚æ•° | æ¨èå€¼ | èŒƒå›´ | è¯´æ˜ |
|-----|--------|------|------|
| `performance_threshold` | 0.05 | 0.01-0.20 | æ€§èƒ½å˜åŒ–é˜ˆå€¼ (Îµ)ã€‚å€¼è¶Šå°ï¼Œå¯¹æ€§èƒ½å˜åŒ–è¶Šæ•æ„Ÿ |
| `growth_coefficient` | 0.2 | 0.1-0.5 | å¢é•¿ç³»æ•° (Î±)ã€‚0.2 è¡¨ç¤ºæ¯æ¬¡å¢åŠ  20% |
| `decay_coefficient` | 0.3 | 0.2-0.5 | è¡°å‡ç³»æ•° (Î²)ã€‚0.3 è¡¨ç¤ºæ¯æ¬¡å‡å°‘ 30% |
| `oscillation_coefficient` | 0.05 | 0.01-0.10 | éœ‡è¡ç³»æ•° (Î´)ã€‚0.05 è¡¨ç¤º Â±5% çš„éšæœºæ³¢åŠ¨ |
| `adjustment_interval_ms` | 100 | 50-500 | è°ƒæ•´é—´éš”æ—¶é—´ã€‚å¤ªå°ä¼šå¯¼è‡´é¢‘ç¹è°ƒæ•´ |

**ç¨³å®šé˜¶æ®µå‚æ•°**:

| å‚æ•° | æ¨èå€¼ | èŒƒå›´ | è¯´æ˜ |
|-----|--------|------|------|
| `stabilization_cycles` | 10 | 5-20 | ç¨³å®šå‘¨æœŸæ•° (N)ã€‚éœ€è¦è¿ç»­ N ä¸ªå‘¨æœŸæ»¡è¶³ç¨³å®šæ¡ä»¶ |
| `batch_size_change_threshold` | 0.10 | 0.05-0.20 | batch size å˜åŒ–é˜ˆå€¼ (Î¸)ã€‚å˜åŒ– < 10% è®¤ä¸ºç¨³å®š |
| `performance_fluctuation_threshold` | 0.03 | 0.01-0.10 | æ€§èƒ½æ³¢åŠ¨é˜ˆå€¼ (Ï†)ã€‚æ³¢åŠ¨ < 3% è®¤ä¸ºç¨³å®š |
| `exploration_interval` | 100 | 50-200 | å®šæœŸæ¢ç´¢é—´éš”ã€‚æ¯å¤„ç† 100 ä¸ªæ‰¹æ¬¡åè¿›è¡Œä¸€æ¬¡æ¢ç´¢ |

**å®‰å…¨å‚æ•°**:

| å‚æ•° | æ¨èå€¼ | èŒƒå›´ | è¯´æ˜ |
|-----|--------|------|------|
| `memory_safety_margin_mb` | 512 | 256-2048 | å†…å­˜å®‰å…¨ä½™é‡ã€‚ä¿ç•™è¶³å¤Ÿçš„å†…å­˜é˜²æ­¢æº¢å‡º |
| `memory_usage_limit` | 0.90 | 0.70-0.95 | å†…å­˜ä½¿ç”¨é™åˆ¶ã€‚ä½¿ç”¨ 90% çš„å†…å­˜æ—¶å¼€å§‹é™åˆ¶ |
| `max_consecutive_failures` | 3 | 2-5 | æœ€å¤§è¿ç»­å¤±è´¥æ¬¡æ•°ã€‚è¶…è¿‡åè§¦å‘å®‰å…¨å›é€€ |
| `fallback_level` | "moderate" | none/minor/moderate/major/safe | å›é€€çº§åˆ«ã€‚"moderate" è¡¨ç¤ºä¸­åº¦å›é€€ |

**æ€§èƒ½æƒé‡å‚æ•°**:

| å‚æ•° | æ¨èå€¼ | èŒƒå›´ | è¯´æ˜ |
|-----|--------|------|------|
| `throughput_weight` | 0.5 | 0.3-0.7 | ååé‡æƒé‡ (Î±)ã€‚é¦–è¦ä¼˜åŒ–ç›®æ ‡ |
| `gpu_utilization_weight` | 0.3 | 0.2-0.5 | GPU åˆ©ç”¨ç‡æƒé‡ (Î²) |
| `memory_efficiency_weight` | 0.2 | 0.1-0.3 | å†…å­˜æ•ˆç‡æƒé‡ (Î³) |

### 6.2 é…ç½®åŠ è½½ä¸æ›´æ–°

#### 6.2.1 é…ç½®åŠ è½½ä»£ç 

> âœ… å½“å‰å®ç°ä½¿ç”¨ `Config::dynamicBatchTunerConfig()` ç›´æ¥è¯»å–é…ç½®ã€‚  
> ä¸‹é¢æ—§å¼åŠ è½½ç¤ºä¾‹ä»…ä½œä¸ºå†å²å‚è€ƒï¼Œä¸å†ä½œä¸ºå®ç°ä¾æ®ã€‚

```cpp
// scheduler.cpp
auto tunerConfig = Config::instance().dynamicBatchTunerConfig();
if (tunerConfig.enabled) {
    // strategy: static | dynamic | hybird
    // å‚æ•°ç”± Config ç»Ÿä¸€è§£æå¹¶æä¾›é»˜è®¤å€¼
}
```

```cpp
// config.cpp
#include "cllm/config/config.h"
#include "cllm/scheduler/dynamic_batch_tuner.h"

namespace cllm {

void Config::loadDynamicBatchTunerConfig(const YAML::Node& config) {
    if (!config["dynamic_batch_tuner"]) {
        CLLM_WARN("[Config] dynamic_batch_tuner config not found, using defaults");
        return;
    }
    
    const auto& tunerConfig = config["dynamic_batch_tuner"];
    
    // åŸºç¡€å¼€å…³
    if (tunerConfig["enabled"]) {
        dynamicBatchTunerEnabled_ = tunerConfig["enabled"].as<bool>();
    }
    
    // Batch Size èŒƒå›´
    if (tunerConfig["min_batch_size"]) {
        minBatchSize_ = tunerConfig["min_batch_size"].as<size_t>();
    }
    
    if (tunerConfig["max_batch_size"]) {
        maxBatchSize_ = tunerConfig["max_batch_size"].as<size_t>();
    }
    
    if (tunerConfig["initial_batch_size"]) {
        initialBatchSize_ = tunerConfig["initial_batch_size"].as<size_t>();
    }
    
    // æ¢æµ‹é˜¶æ®µå‚æ•°
    if (tunerConfig["max_probing_attempts"]) {
        maxProbingAttempts_ = tunerConfig["max_probing_attempts"].as<size_t>();
    }
    
    if (tunerConfig["probing_growth_factor"]) {
        probingGrowthFactor_ = tunerConfig["probing_growth_factor"].as<double>();
    }
    
    // è°ƒæ•´é˜¶æ®µå‚æ•°
    if (tunerConfig["performance_threshold"]) {
        performanceThreshold_ = tunerConfig["performance_threshold"].as<double>();
    }
    
    if (tunerConfig["growth_coefficient"]) {
        growthCoefficient_ = tunerConfig["growth_coefficient"].as<double>();
    }
    
    if (tunerConfig["decay_coefficient"]) {
        decayCoefficient_ = tunerConfig["decay_coefficient"].as<double>();
    }
    
    // ç¨³å®šé˜¶æ®µå‚æ•°
    if (tunerConfig["stabilization_cycles"]) {
        stabilizationCycles_ = tunerConfig["stabilization_cycles"].as<size_t>();
    }
    
    if (tunerConfig["batch_size_change_threshold"]) {
        batchSizeChangeThreshold_ = tunerConfig["batch_size_change_threshold"].as<double>();
    }
    
    // å®‰å…¨å‚æ•°
    if (tunerConfig["memory_safety_margin_mb"]) {
        memorySafetyMarginMb_ = tunerConfig["memory_safety_margin_mb"].as<size_t>();
    }
    
    if (tunerConfig["memory_usage_limit"]) {
        memoryUsageLimit_ = tunerConfig["memory_usage_limit"].as<double>();
    }
    
    // æ€§èƒ½æƒé‡
    if (tunerConfig["throughput_weight"]) {
        throughputWeight_ = tunerConfig["throughput_weight"].as<double>();
    }
    
    if (tunerConfig["gpu_utilization_weight"]) {
        gpuUtilizationWeight_ = tunerConfig["gpu_utilization_weight"].as<double>();
    }
    
    if (tunerConfig["memory_efficiency_weight"]) {
        memoryEfficiencyWeight_ = tunerConfig["memory_efficiency_weight"].as<double>();
    }
    
    CLLM_INFO("[Config] Dynamic batch tuner config loaded: enabled=%s, min_batch_size=%zu, max_batch_size=%zu",
              dynamicBatchTunerEnabled_ ? "true" : "false",
              minBatchSize_, maxBatchSize_);
}

void Config::updateDynamicBatchTunerConfig(const DynamicBatchTuner::TunerConfig& config) {
    // è¿è¡Œæ—¶æ›´æ–°é…ç½®
    std::lock_guard<std::mutex> lock(configMutex_);
    
    // æ›´æ–°é…ç½®å‚æ•°
    minBatchSize_ = config.minBatchSize;
    maxBatchSize_ = config.maxBatchSize;
    initialBatchSize_ = config.initialBatchSize;
    
    // ... æ›´æ–°å…¶ä»–å‚æ•° ...
    
    CLLM_INFO("[Config] Dynamic batch tuner config updated at runtime");
}

} // namespace cllm
```

#### 6.2.2 é…ç½®éªŒè¯

```cpp
bool Config::validateDynamicBatchTunerConfig() const {
    // éªŒè¯å‚æ•°èŒƒå›´
    if (minBatchSize_ > maxBatchSize_) {
        CLLM_ERROR("[Config] minBatchSize (%zu) > maxBatchSize (%zu)", 
                   minBatchSize_, maxBatchSize_);
        return false;
    }
    
    if (initialBatchSize_ < minBatchSize_ || initialBatchSize_ > maxBatchSize_) {
        CLLM_ERROR("[Config] initialBatchSize (%zu) out of range [%zu, %zu]", 
                   initialBatchSize_, minBatchSize_, maxBatchSize_);
        return false;
    }
    
    if (probingGrowthFactor_ < 1.5 || probingGrowthFactor_ > 3.0) {
        CLLM_WARN("[Config] probingGrowthFactor (%.2f) outside recommended range [1.5, 3.0]", 
                  probingGrowthFactor_);
    }
    
    if (performanceThreshold_ < 0.01 || performanceThreshold_ > 0.20) {
        CLLM_WARN("[Config] performanceThreshold (%.2f) outside recommended range [0.01, 0.20]", 
                  performanceThreshold_);
    }
    
    if (growthCoefficient_ < 0.1 || growthCoefficient_ > 0.5) {
        CLLM_WARN("[Config] growthCoefficient (%.2f) outside recommended range [0.1, 0.5]", 
                  growthCoefficient_);
    }
    
    if (decayCoefficient_ < 0.2 || decayCoefficient_ > 0.5) {
        CLLM_WARN("[Config] decayCoefficient (%.2f) outside recommended range [0.2, 0.5]", 
                  decayCoefficient_);
    }
    
    if (memoryUsageLimit_ < 0.70 || memoryUsageLimit_ > 0.95) {
        CLLM_WARN("[Config] memoryUsageLimit (%.2f) outside recommended range [0.70, 0.95]", 
                  memoryUsageLimit_);
    }
    
    // éªŒè¯æƒé‡å’Œä¸º 1.0
    double weightSum = throughputWeight_ + gpuUtilizationWeight_ + memoryEfficiencyWeight_;
    if (std::abs(weightSum - 1.0) > 0.01) {
        CLLM_WARN("[Config] Performance weights sum to %.2f (should be 1.0)", weightSum);
    }
    
    CLLM_INFO("[Config] Dynamic batch tuner config validation passed");
    return true;
}
```

### 6.3 è¿è¡Œæ—¶é…ç½®æ›´æ–°æ¥å£

#### 6.3.1 HTTP API æ¥å£

```cpp
// scheduler_http_api.cpp
#include "cllm/scheduler/scheduler.h"
#include "cllm/config/config.h"

namespace cllm {

class SchedulerHttpApi {
private:
    Scheduler* scheduler_;
    Config* config_;
    
public:
    // è·å–è°ƒè°å™¨çŠ¶æ€
    crow::json::wvalue getTunerStatus() {
        crow::json::wvalue response;
        
        if (!scheduler_->batchTuner_) {
            response["error"] = "Dynamic batch tuner not enabled";
            return response;
        }
        
        auto* tuner = scheduler_->batchTuner_.get();
        
        response["enabled"] = true;
        response["phase"] = getPhaseName(tuner->getCurrentPhase());
        response["current_batch_size"] = static_cast<size_t>(tuner->getCurrentBatchSize());
        response["best_batch_size"] = tuner->getBestBatchSize();
        response["best_score"] = tuner->getBestScore();
        response["is_stabilized"] = tuner->isStabilized();
        response["consecutive_failures"] = tuner->getConsecutiveFailures();
        
        // æ€§èƒ½æŒ‡æ ‡
        auto metrics = tuner->getLatestMetrics();
        response["throughput"] = metrics.throughput;
        response["gpu_utilization"] = metrics.gpuUtilization;
        response["memory_utilization"] = metrics.memoryUtilization;
        response["processing_time_ms"] = metrics.processingTimeMs;
        
        // GPU ä¿¡æ¯
        auto gpuInfo = tuner->getGPUInfo();
        response["gpu_model"] = gpuInfo.model;
        response["gpu_memory_mb"] = gpuInfo.totalMemoryMb;
        
        return response;
    }
    
    // æ›´æ–°è°ƒè°å™¨é…ç½®
    crow::json::wvalue updateTunerConfig(const crow::json::rvalue& request) {
        crow::json::wvalue response;
        
        if (!scheduler_->batchTuner_) {
            response["error"] = "Dynamic batch tuner not enabled";
            return response;
        }
        
        try {
            DynamicBatchTuner::TunerConfig config = tuner->getConfig();
            
            // æ›´æ–°è¯·æ±‚ä¸­çš„å‚æ•°
            if (request.has("min_batch_size")) {
                config.minBatchSize = request["min_batch_size"].i();
            }
            
            if (request.has("max_batch_size")) {
                config.maxBatchSize = request["max_batch_size"].i();
            }
            
            if (request.has("performance_threshold")) {
                config.performanceThreshold = request["performance_threshold"].d();
            }
            
            if (request.has("growth_coefficient")) {
                config.growthCoefficient = request["growth_coefficient"].d();
            }
            
            if (request.has("decay_coefficient")) {
                config.decayCoefficient = request["decay_coefficient"].d();
            }
            
            // éªŒè¯é…ç½®
            if (!validateConfigUpdate(config)) {
                response["error"] = "Invalid configuration";
                return response;
            }
            
            // æ›´æ–°é…ç½®
            tuner->updateConfig(config);
            
            response["success"] = true;
            response["message"] = "Configuration updated successfully";
            response["new_config"] = serializeConfig(config);
            
            CLLM_INFO("[SchedulerHttpApi] Tuner config updated at runtime");
            
        } catch (const std::exception& e) {
            response["error"] = std::string("Failed to update config: ") + e.what();
            CLLM_ERROR("[SchedulerHttpApi] Failed to update tuner config: %s", e.what());
        }
        
        return response;
    }
    
    // é‡ç½®è°ƒè°å™¨
    crow::json::wvalue resetTuner() {
        crow::json::wvalue response;
        
        if (!scheduler_->batchTuner_) {
            response["error"] = "Dynamic batch tuner not enabled";
            return response;
        }
        
        try {
            scheduler_->batchTuner_->resetToSafeState();
            
            response["success"] = true;
            response["message"] = "Tuner reset to safe state";
            
            CLLM_INFO("[SchedulerHttpApi] Tuner reset via HTTP API");
            
        } catch (const std::exception& e) {
            response["error"] = std::string("Failed to reset tuner: ") + e.what();
        }
        
        return response;
    }
    
    // æ‰‹åŠ¨è®¾ç½® batch size
    crow::json::wvalue setBatchSize(const crow::json::rvalue& request) {
        crow::json::wvalue response;
        
        if (!request.has("batch_size")) {
            response["error"] = "batch_size parameter required";
            return response;
        }
        
        size_t batchSize = request["batch_size"].i();
        
        if (!scheduler_->batchTuner_) {
            response["error"] = "Dynamic batch tuner not enabled";
            return response;
        }
        
        try {
            scheduler_->batchTuner_->setBatchSize(batchSize);
            
            response["success"] = true;
            response["message"] = "Batch size set successfully";
            response["batch_size"] = batchSize;
            
            CLLM_INFO("[SchedulerHttpApi] Batch size set to %zu via HTTP API", batchSize);
            
        } catch (const std::exception& e) {
            response["error"] = std::string("Failed to set batch size: ") + e.what();
        }
        
        return response;
    }
    
private:
    std::string getPhaseName(DynamicBatchTuner::TuningPhase phase) {
        switch (phase) {
            case DynamicBatchTuner::TuningPhase::INITIAL_PROBING:
                return "INITIAL_PROBING";
            case DynamicBatchTuner::TuningPhase::DYNAMIC_ADJUSTMENT:
                return "DYNAMIC_ADJUSTMENT";
            case DynamicBatchTuner::TuningPhase::STABLE_RUNNING:
                return "STABLE_RUNNING";
            default:
                return "UNKNOWN";
        }
    }
    
    bool validateConfigUpdate(const DynamicBatchTuner::TunerConfig& config) {
        if (config.minBatchSize > config.maxBatchSize) {
            return false;
        }
        
        if (config.performanceThreshold < 0.01 || config.performanceThreshold > 0.20) {
            return false;
        }
        
        return true;
    }
    
    crow::json::wvalue serializeConfig(const DynamicBatchTuner::TunerConfig& config) {
        crow::json::wvalue json;
        json["min_batch_size"] = config.minBatchSize;
        json["max_batch_size"] = config.maxBatchSize;
        json["initial_batch_size"] = config.initialBatchSize;
        json["performance_threshold"] = config.performanceThreshold;
        json["growth_coefficient"] = config.growthCoefficient;
        json["decay_coefficient"] = config.decayCoefficient;
        return json;
    }
};

} // namespace cllm
```

#### 6.3.2 API ä½¿ç”¨ç¤ºä¾‹

```bash
# 1. è·å–è°ƒè°å™¨çŠ¶æ€
curl -X GET http://localhost:8080/api/scheduler/tuner/status

å“åº”ç¤ºä¾‹:
{
  "enabled": true,
  "phase": "STABLE_RUNNING",
  "current_batch_size": 16,
  "best_batch_size": 16,
  "best_score": 0.85,
  "is_stabilized": true,
  "consecutive_failures": 0,
  "throughput": 132.73,
  "gpu_utilization": 0.75,
  "memory_utilization": 0.68,
  "processing_time_ms": 125.5,
  "gpu_model": "Apple M3 Pro",
  "gpu_memory_mb": 36864
}

# 2. æ›´æ–°è°ƒè°å™¨é…ç½®
curl -X POST http://localhost:8080/api/scheduler/tuner/config \
  -H "Content-Type: application/json" \
  -d '{
    "min_batch_size": 4,
    "max_batch_size": 128,
    "performance_threshold": 0.03,
    "growth_coefficient": 0.15,
    "decay_coefficient": 0.25
  }'

å“åº”ç¤ºä¾‹:
{
  "success": true,
  "message": "Configuration updated successfully",
  "new_config": {
    "min_batch_size": 4,
    "max_batch_size": 128,
    "initial_batch_size": 8,
    "performance_threshold": 0.03,
    "growth_coefficient": 0.15,
    "decay_coefficient": 0.25
  }
}

# 3. é‡ç½®è°ƒè°å™¨
curl -X POST http://localhost:8080/api/scheduler/tuner/reset

å“åº”ç¤ºä¾‹:
{
  "success": true,
  "message": "Tuner reset to safe state"
}

# 4. æ‰‹åŠ¨è®¾ç½® batch size
curl -X POST http://localhost:8080/api/scheduler/tuner/batch-size \
  -H "Content-Type: application/json" \
  -d '{"batch_size": 32}'

å“åº”ç¤ºä¾‹:
{
  "success": true,
  "message": "Batch size set successfully",
  "batch_size": 32
}
```

### 6.4 é…ç½®æ–‡ä»¶ç¤ºä¾‹

#### 6.4.1 é’ˆå¯¹ä¸åŒ GPU çš„æ¨èé…ç½®

**é…ç½® 1: NVIDIA A100 40GBï¼ˆé«˜æ€§èƒ½åœºæ™¯ï¼‰**

```yaml
dynamic_batch_tuner:
  enabled: true
  
  min_batch_size: 4
  max_batch_size: 256
  initial_batch_size: 16
  
  max_probing_attempts: 15
  probing_growth_factor: 2.5
  
  performance_threshold: 0.03
  growth_coefficient: 0.25
  decay_coefficient: 0.35
  
  stabilization_cycles: 15
  batch_size_change_threshold: 0.08
  performance_fluctuation_threshold: 0.02
  
  memory_safety_margin_mb: 2048
  memory_usage_limit: 0.95
  
  throughput_weight: 0.6
  gpu_utilization_weight: 0.3
  memory_efficiency_weight: 0.1
```

**é…ç½® 2: Apple M3 Pro 36GBï¼ˆæ ‡å‡†åœºæ™¯ï¼‰**

```yaml
dynamic_batch_tuner:
  enabled: true
  
  min_batch_size: 2
  max_batch_size: 64
  initial_batch_size: 8
  
  max_probing_attempts: 10
  probing_growth_factor: 2.0
  
  performance_threshold: 0.05
  growth_coefficient: 0.20
  decay_coefficient: 0.30
  
  stabilization_cycles: 10
  batch_size_change_threshold: 0.10
  performance_fluctuation_threshold: 0.03
  
  memory_safety_margin_mb: 512
  memory_usage_limit: 0.90
  
  throughput_weight: 0.5
  gpu_utilization_weight: 0.3
  memory_efficiency_weight: 0.2
```

**é…ç½® 3: NVIDIA T4 16GBï¼ˆå…¥é—¨åœºæ™¯ï¼‰**

```yaml
dynamic_batch_tuner:
  enabled: true
  
  min_batch_size: 1
  max_batch_size: 32
  initial_batch_size: 4
  
  max_probing_attempts: 8
  probing_growth_factor: 1.8
  
  performance_threshold: 0.08
  growth_coefficient: 0.15
  decay_coefficient: 0.25
  
  stabilization_cycles: 8
  batch_size_change_threshold: 0.15
  performance_fluctuation_threshold: 0.05
  
  memory_safety_margin_mb: 1024
  memory_usage_limit: 0.85
  
  throughput_weight: 0.4
  gpu_utilization_weight: 0.4
  memory_efficiency_weight: 0.2
```

**é…ç½® 4: ç¦ç”¨åŠ¨æ€è°ƒæ•´ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰**

```yaml
dynamic_batch_tuner:
  enabled: false
  
  # ä½¿ç”¨é™æ€é…ç½®
  min_batch_size: 8
  max_batch_size: 8
  initial_batch_size: 8
```

---

## 7. å®ç°è·¯çº¿å›¾

### 7.1 åˆ†é˜¶æ®µå®ç°è®¡åˆ’

#### Phase 1: åŸºç¡€æ¡†æ¶ï¼ˆ1-2 å‘¨ï¼‰

**ç›®æ ‡**: å®ç°æ ¸å¿ƒåŠŸèƒ½ï¼Œèƒ½å¤Ÿè¿è¡Œ

**ä»»åŠ¡åˆ—è¡¨**:
1. âœ… è®¾è®¡å®Œæˆï¼ˆæœ¬æ–‡æ¡£ï¼‰
2. åˆ›å»º `dynamic_batch_tuner.h` å’Œ `dynamic_batch_tuner.cpp`
3. å®ç° GPU ä¿¡æ¯æ¢æµ‹åŠŸèƒ½
4. å®ç°åˆå§‹æ¢æµ‹é˜¶æ®µï¼ˆç®€åŒ–ç‰ˆï¼‰
5. é›†æˆåˆ° `scheduler.cpp`
6. å®ç°åŸºç¡€é…ç½®åŠ è½½
7. ç¼–å†™å•å…ƒæµ‹è¯•

**éªŒæ”¶æ ‡å‡†**:
- èƒ½å¤Ÿæ­£ç¡®è¯†åˆ« GPU å‹å·å’Œå†…å­˜
- èƒ½å¤Ÿå®Œæˆåˆå§‹æ¢æµ‹ï¼Œæ‰¾åˆ°å¯è¡Œçš„ batch size
- ç³»ç»Ÿèƒ½å¤Ÿæ­£å¸¸è¿è¡Œï¼Œæ— å´©æºƒ

#### Phase 2: åŠ¨æ€è°ƒæ•´ï¼ˆ2-3 å‘¨ï¼‰

**ç›®æ ‡**: å®ç°å®Œæ•´çš„è‡ªé€‚åº”è°ƒæ•´ç®—æ³•

**ä»»åŠ¡åˆ—è¡¨**:
1. å®ç°åŠ¨æ€è°ƒæ•´é˜¶æ®µï¼ˆçˆ¬å±±ç®—æ³•ï¼‰
2. å®ç°ç¨³å®šè¿è¡Œé˜¶æ®µ
3. å®ç°æ€§èƒ½æŒ‡æ ‡æ”¶é›†å’Œåˆ†æ
4. å®ç°çŠ¶æ€è½¬æ¢é€»è¾‘
5. ä¼˜åŒ–è°ƒæ•´ç®—æ³•
6. ç¼–å†™é›†æˆæµ‹è¯•

**éªŒæ”¶æ ‡å‡†**:
- èƒ½å¤Ÿé€‚åº”è´Ÿè½½å˜åŒ–ï¼Œè‡ªåŠ¨è°ƒæ•´ batch size
- èƒ½å¤Ÿè¿›å…¥ç¨³å®šé˜¶æ®µï¼Œä¿æŒæœ€ä¼˜ batch size
- ååé‡æå‡ â‰¥ 10%ï¼ˆç›¸å¯¹äºé™æ€é…ç½®ï¼‰

#### Phase 3: å®‰å…¨æœºåˆ¶ï¼ˆ1-2 å‘¨ï¼‰

**ç›®æ ‡**: å®Œå–„å®‰å…¨æœºåˆ¶ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®š

**ä»»åŠ¡åˆ—è¡¨**:
1. å®ç°è¾¹ç•Œæ£€æŸ¥
2. å®ç°å¼‚å¸¸å¤„ç†ï¼ˆå†…å­˜æº¢å‡ºã€æ€§èƒ½å¼‚å¸¸ï¼‰
3. å®ç°å¤šçº§å›é€€ç­–ç•¥
4. å®ç°è‡ªåŠ¨æ¢å¤æœºåˆ¶
5. ç¼–å†™å‹åŠ›æµ‹è¯•å’Œæ•…éšœæ³¨å…¥æµ‹è¯•

**éªŒæ”¶æ ‡å‡†**:
- å†…å­˜æº¢å‡ºæ—¶èƒ½å¤Ÿè‡ªåŠ¨å›é€€ï¼Œä¸å´©æºƒ
- è¿ç»­å¤±è´¥æ—¶èƒ½å¤Ÿè¿›å…¥å®‰å…¨æ¨¡å¼
- æ•…éšœæ¢å¤åèƒ½å¤Ÿè‡ªåŠ¨æ¢å¤æ€§èƒ½

#### Phase 4: é…ç½®ä¸ç›‘æ§ï¼ˆ1 å‘¨ï¼‰

**ç›®æ ‡**: æä¾›å®Œå–„çš„é…ç½®æ¥å£å’Œç›‘æ§èƒ½åŠ›

**ä»»åŠ¡åˆ—è¡¨**:
1. å®ç°å®Œæ•´çš„é…ç½®å‚æ•°ä½“ç³»
2. å®ç° HTTP API æ¥å£
3. å®ç°å®æ—¶ç›‘æ§å’ŒçŠ¶æ€æŠ¥å‘Š
4. å®ç°è¿è¡Œæ—¶é…ç½®æ›´æ–°
5. ç¼–å†™ API æ–‡æ¡£

**éªŒæ”¶æ ‡å‡†**:
- èƒ½å¤Ÿé€šè¿‡é…ç½®æ–‡ä»¶çµæ´»è°ƒæ•´å‚æ•°
- èƒ½å¤Ÿé€šè¿‡ HTTP API æŸ¥è¯¢çŠ¶æ€å’Œæ›´æ–°é…ç½®
- èƒ½å¤Ÿå®æ—¶ç›‘æ§è°ƒè°å™¨è¿è¡ŒçŠ¶æ€

#### Phase 5: ä¼˜åŒ–ä¸éªŒè¯ï¼ˆ2-3 å‘¨ï¼‰

**ç›®æ ‡**: ä¼˜åŒ–æ€§èƒ½ï¼Œå®Œæˆå¤š GPU éªŒè¯

**ä»»åŠ¡åˆ—è¡¨**:
1. ä¼˜åŒ–ç®—æ³•æ€§èƒ½ï¼ˆé™ä½å¼€é”€ < 5%ï¼‰
2. åœ¨å¤šç§ GPU ä¸Šæµ‹è¯•ï¼ˆNVIDIAã€AMDã€Appleã€Intelï¼‰
3. æ€§èƒ½å¯¹æ¯”æµ‹è¯•ï¼ˆé™æ€ vs åŠ¨æ€ï¼‰
4. ç¼–å†™æ€§èƒ½æŠ¥å‘Š
5. æ–‡æ¡£å®Œå–„

**éªŒæ”¶æ ‡å‡†**:
- ç³»ç»Ÿå¼€é”€ < 5%
- ååé‡æå‡ â‰¥ 20%ï¼ˆç›¸å¯¹äºæœ€ä¼˜é™æ€é…ç½®ï¼‰
- æ”¯æŒ â‰¥ 5 ç§ GPU å‹å·
- å®Œæ•´çš„æµ‹è¯•æŠ¥å‘Šå’Œæ–‡æ¡£

### 7.2 å…³é”®é‡Œç¨‹ç¢‘

| é‡Œç¨‹ç¢‘ | æ—¶é—´ | äº¤ä»˜ç‰© | éªŒæ”¶æ ‡å‡† |
|-------|------|--------|--------|
| **M1: è®¾è®¡å®Œæˆ** | Day 3 | è®¾è®¡æ–‡æ¡£ | è¯„å®¡é€šè¿‡ |
| **M2: åŸºç¡€æ¡†æ¶å®Œæˆ** | Day 10 | å¯è¿è¡Œä»£ç  | èƒ½å¤Ÿå®Œæˆåˆå§‹æ¢æµ‹ |
| **M3: åŠ¨æ€è°ƒæ•´å®Œæˆ** | Day 24 | å®Œæ•´ç®—æ³•å®ç° | ååé‡æå‡ â‰¥ 10% |
| **M4: å®‰å…¨æœºåˆ¶å®Œæˆ** | Day 38 | ç¨³å®šçš„ç³»ç»Ÿ | æ— å´©æºƒï¼Œè‡ªåŠ¨æ¢å¤ |
| **M5: é…ç½®ç›‘æ§å®Œæˆ** | Day 45 | API å’Œæ–‡æ¡£ | å¯é…ç½®ã€å¯ç›‘æ§ |
| **M6: ä¼˜åŒ–éªŒè¯å®Œæˆ** | Day 60 | ç”Ÿäº§å°±ç»ªç‰ˆæœ¬ | ååé‡æå‡ â‰¥ 20%ï¼Œæ”¯æŒå¤š GPU |

### 7.3 æŠ€æœ¯é£é™©ä¸åº”å¯¹

| é£é™© | å½±å“ | åº”å¯¹æªæ–½ |
|-----|------|--------|
| **GPU ä¿¡æ¯æ¢æµ‹å¤±è´¥** | æ— æ³•æ­£ç¡®ä¼°ç®— batch size | æä¾›é»˜è®¤é…ç½®ï¼Œé™çº§åˆ°é™æ€æ¨¡å¼ |
| **è°ƒæ•´ç®—æ³•æŒ¯è¡** | batch size é¢‘ç¹æ³¢åŠ¨ | å¢åŠ é˜»å°¼ç³»æ•°ï¼Œå»¶é•¿ç¨³å®šå‘¨æœŸ |
| **å†…å­˜ä¼°ç®—ä¸å‡†ç¡®** | å¯èƒ½å¯¼è‡´æº¢å‡º | ä¿å®ˆä¼°ç®—ï¼Œé¢„ç•™å……è¶³ä½™é‡ |
| **æ€§èƒ½æŒ‡æ ‡æ”¶é›†å¼€é”€å¤§** | å½±å“ç³»ç»Ÿæ€§èƒ½ | é‡‡æ ·æ”¶é›†ï¼Œé™ä½é¢‘ç‡ |
| **è·¨ GPU å…¼å®¹æ€§é—®é¢˜** | æŸäº› GPU æ— æ³•å·¥ä½œ | æŠ½è±¡ GPU æ¥å£ï¼Œæä¾›å¹³å°ç‰¹å®šå®ç° |
| **ä¸ç°æœ‰è°ƒåº¦ç³»ç»Ÿå†²çª** | è°ƒåº¦é€»è¾‘æ··ä¹± | ä»”ç»†è®¾è®¡é›†æˆç‚¹ï¼Œå……åˆ†æµ‹è¯• |

---

## 8. æ€»ç»“

### 8.1 è®¾è®¡äº®ç‚¹

1. **ä¸‰é˜¶æ®µè‡ªé€‚åº”ç®—æ³•**
   - åˆå§‹æ¢æµ‹é˜¶æ®µï¼šå¿«é€Ÿæ‰¾åˆ°å¯è¡ŒèŒƒå›´
   - åŠ¨æ€è°ƒæ•´é˜¶æ®µï¼šæŒç»­ä¼˜åŒ–é€‚åº”å˜åŒ–
   - ç¨³å®šè¿è¡Œé˜¶æ®µï¼šä¿æŒæœ€ä¼˜é¿å…æ³¢åŠ¨

2. **å¤šç»´åº¦æ€§èƒ½è¯„ä¼°**
   - ååé‡ã€GPU åˆ©ç”¨ç‡ã€å†…å­˜æ•ˆç‡ä¸‰ç»´è¯„ä¼°
   - å¯é…ç½®çš„æƒé‡ç³»æ•°ï¼Œé€‚åº”ä¸åŒåœºæ™¯
   - é‡åŒ–çš„æ€§èƒ½è¯„åˆ†ä½“ç³»

3. **å®Œå–„çš„å®‰å…¨æœºåˆ¶**
   - å¤šçº§å›é€€ç­–ç•¥ï¼ˆminor â†’ moderate â†’ major â†’ safeï¼‰
   - è‡ªåŠ¨æ¢å¤æœºåˆ¶
   - è¾¹ç•Œæ£€æŸ¥å’Œå¼‚å¸¸å¤„ç†

4. **çµæ´»çš„é…ç½®æ¥å£**
   - ä¸°å¯Œçš„é…ç½®å‚æ•°ï¼ˆ30+ï¼‰
   - è¿è¡Œæ—¶æ›´æ–°èƒ½åŠ›
   - HTTP API æ¥å£

5. **è·¨ GPU å¹³å°å…¼å®¹æ€§**
   - æ”¯æŒ NVIDIAã€AMDã€Appleã€Intel GPU
   - è‡ªåŠ¨æ¢æµ‹ GPU ä¿¡æ¯
   - å¹³å°ç‰¹å®šä¼˜åŒ–

### 8.2 é¢„æœŸæ”¶ç›Š

| æŒ‡æ ‡ | é™æ€é…ç½® | åŠ¨æ€è°ƒæ•´ | æå‡ |
|-----|---------|---------|------|
| **ååé‡** | 80-120 t/s | 100-160 t/s | **+20-40%** |
| **GPU åˆ©ç”¨ç‡** | 50-70% | 70-90% | **+20-30%** |
| **å†…å­˜æ•ˆç‡** | 60-80% | 80-95% | **+20-25%** |
| **æ”¶æ•›æ—¶é—´** | N/A | < 2 min | - |
| **ç³»ç»Ÿå¼€é”€** | 0% | < 5% | - |

### 8.3 åç»­å·¥ä½œ

1. **ç«‹å³å¼€å§‹**
   - Phase 1: åŸºç¡€æ¡†æ¶å®ç°
   - é¢„è®¡ 1-2 å‘¨

2. **å¹¶è¡Œè¿›è¡Œ**
   - æµ‹è¯•ç¯å¢ƒå‡†å¤‡ï¼ˆå¤šç§ GPUï¼‰
   - æµ‹è¯•ç”¨ä¾‹è®¾è®¡
   - æ€§èƒ½åŸºå‡†æµ‹è¯•

3. **é•¿æœŸä¼˜åŒ–**
   - æ”¯æŒå¤š GPU ååŒ
   - æ”¯æŒæ¨¡å‹åŠ¨æ€åˆ‡æ¢
   - æ”¯æŒè´Ÿè½½é¢„æµ‹

---

## é™„å½•

### A. å‚è€ƒæ–‡çŒ®

1. **Roofline Model**: Williams et al., "Roofline: An Insightful Visual Performance Model for Floating-Point Programs and Multicore Architectures", 2009
2. **Amdahl's Law**: Gene Amdahl, "Validity of the single processor approach to achieving large scale computing capabilities", 1967
3. **Hill-Climbing Algorithm**: Stuart Russell and Peter Norvig, "Artificial Intelligence: A Modern Approach", 2010
4. **Llama.cpp Batch Processing**: https://github.com/ggerganov/llama.cpp
5. **CUDA Best Practices Guide**: NVIDIA Corporation

### B. æœ¯è¯­è¡¨

| æœ¯è¯­ | è‹±æ–‡ | è¯´æ˜ |
|-----|------|------|
| **Batch Size** | Batch Size | æ‰¹å¤„ç†å¤§å°ï¼Œä¸€æ¬¡æ¨ç†å¤„ç†çš„è¯·æ±‚æ•° |
| **ååé‡** | Throughput | æ¯ç§’å¤„ç†çš„ tokens æ•° |
| **GPU åˆ©ç”¨ç‡** | GPU Utilization | GPU è®¡ç®—å•å…ƒçš„å ç”¨ç‡ |
| **å†…å­˜å¸¦å®½** | Memory Bandwidth | GPU å†…å­˜çš„æ•°æ®ä¼ è¾“é€Ÿç‡ |
| **è°ƒè°å™¨** | Tuner | åŠ¨æ€è°ƒæ•´ batch size çš„ç»„ä»¶ |
| **æ¢æµ‹é˜¶æ®µ** | Probing Phase | ç³»ç»Ÿå¯åŠ¨æ—¶çš„ batch size æ¢ç´¢é˜¶æ®µ |
| **è°ƒæ•´é˜¶æ®µ** | Adjustment Phase | è¿è¡Œæ—¶çš„ batch size ä¼˜åŒ–é˜¶æ®µ |
| **ç¨³å®šé˜¶æ®µ** | Stable Phase | ä¿æŒæœ€ä¼˜ batch size çš„é˜¶æ®µ |

### C. è”ç³»æ–¹å¼

- **æŠ€æœ¯è´Ÿè´£äºº**: cLLM Technical Team
- **æ–‡æ¡£ç‰ˆæœ¬**: v1.0
- **æ›´æ–°æ—¥æœŸ**: 2026-01-22
- **åé¦ˆé‚®ç®±**: tech@cllm.ai

---

**æ–‡æ¡£ç»“æŸ**

*æœ¬æ–‡æ¡£æè¿°äº† cLLM åŠ¨æ€ Batch Size è°ƒæ•´æœºåˆ¶çš„å®Œæ•´è®¾è®¡æ–¹æ¡ˆã€‚*  
*æ‰€æœ‰ä»£ç ç¤ºä¾‹å‡ä¸ºä¼ªä»£ç ï¼Œå®é™…å®ç°å¯èƒ½ä¼šæœ‰æ‰€ä¸åŒã€‚*  
*æœ¬è®¾è®¡æ–¹æ¡ˆéµå¾ª cLLM é¡¹ç›®çš„æ¶æ„åŸåˆ™å’Œç¼–ç è§„èŒƒã€‚*