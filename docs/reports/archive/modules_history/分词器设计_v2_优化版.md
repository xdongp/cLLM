# cLLM åˆ†è¯å™¨æ¨¡å—è®¾è®¡ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

## 1. è®¾è®¡ç›®æ ‡

### 1.1 æ ¸å¿ƒåŠŸèƒ½
- æ–‡æœ¬ â†” Token ID åŒå‘è½¬æ¢
- æ”¯æŒä¸»æµæ¨¡å‹æ ¼å¼ï¼ˆHuggingFace/SentencePiece/Qwen/DeepSeek/Llamaç­‰ï¼‰
- ç‰¹æ®Š Token å¤„ç†ï¼ˆBOS/EOS/PAD/FIMç­‰ï¼‰
- æµå¼å¤„ç†æ”¯æŒ
- é«˜æ€§èƒ½åˆ†è¯ç®—æ³•ï¼ˆåŸºäºllama.cppå®ç°ï¼‰

### 1.2 æ€§èƒ½æŒ‡æ ‡
- ç¼–ç é€Ÿåº¦ â‰¥ 50MB/s
- å†…å­˜å ç”¨ â‰¤ 50MB
- åŠ è½½æ—¶é—´ â‰¤ 100ms
- æ”¯æŒå¤šç§é¢„å¤„ç†å™¨ï¼ˆDeepSeekã€Qwenç­‰ç‰¹å®šé¢„å¤„ç†ï¼‰

### 1.3 å…¼å®¹æ€§ç›®æ ‡
- æ”¯æŒQwenç³»åˆ—æ¨¡å‹ï¼ˆQwenã€Qwen2ç­‰ï¼‰
- æ”¯æŒDeepSeekç³»åˆ—æ¨¡å‹ï¼ˆDeepSeek-LLMã€DeepSeek-Coderã€DeepSeek3ç­‰ï¼‰
- æ”¯æŒLlamaç³»åˆ—æ¨¡å‹
- å‘åå…¼å®¹ç°æœ‰SentencePieceæ¨¡å‹

## 2. æ¶æ„è®¾è®¡

### 2.1 æ¨¡å—ç»„æˆ
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            TokenizerManager         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚         ITokenizer             â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚
â”‚  â”‚  â”‚   SentencePieceEngine   â”‚   â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚
â”‚  â”‚  â”‚    llamaCppEngine       â”‚   â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚
â”‚  â”‚  â”‚     JsonTokenizer       â”‚   â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚        ModelDetector           â”‚ â”‚
â”‚  â”‚  Detect model type (Qwen,     â”‚ â”‚
â”‚  â”‚  DeepSeek, Llama, etc.)      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 ç±»å…³ç³»å›¾
```mermaid
classDiagram
    class ITokenizer {
        <<interface>>
        +encode(text: str): int[]
        +decode(ids: int[]): str
        +getVocabSize(): int
        +getModelType(): ModelType
    }
    
    class SentencePieceTokenizer {
        -processor: SentencePieceProcessor
        +loadModel(path: str)
    }
    
    class LlamaCppTokenizer {
        -vocab: llama_vocab*
        -modelType: ModelType
        +initialize(modelPath: str)
    }
    
    class QwenTokenizer {
        -processor: SentencePieceProcessor
        -specialTokens: map[str, int]
        +encodeWithFim(text: str)
    }
    
    class DeepSeekTokenizer {
        -processor: llama_vocab*
        -regex_patterns: vector[str]
        +applyPreprocessing(text: str)
    }
    }
    
    class DeepSeekTokenizer {
        -processor: llama_vocab*
        -regex_patterns: vector[str]
        +applyPreprocessing(text: str)
    }
    
    class TokenizerManager {
        -tokenizers: Map<string, ITokenizer>
        +getTokenizer(modelType: str): ITokenizer
        +detectModelType(configPath: str): ModelType
    }
    
    ITokenizer <|-- SentencePieceTokenizer
    ITokenizer <|-- LlamaCppTokenizer
    ITokenizer <|-- QwenTokenizer
    ITokenizer <|-- DeepSeekTokenizer
    TokenizerManager --> ITokenizer
```

## 3. è¯¦ç»†è®¾è®¡

### 3.1 æ¥å£å®šä¹‰

```cpp
enum class ModelType {
    AUTO,           // è‡ªåŠ¨æ£€æµ‹
    QWEN,           // Qwenç³»åˆ—æ¨¡å‹
    QWEN2,          // Qwen2ç³»åˆ—æ¨¡å‹
    DEEPSEEK_LLM,   // DeepSeek LLMæ¨¡å‹
    DEEPSEEK_CODER, // DeepSeek Coderæ¨¡å‹
    DEEPSEEK3_LLM,  // DeepSeek3 LLMæ¨¡å‹
    LLAMA,          // Llamaç³»åˆ—æ¨¡å‹
    BERT,           // BERTç³»åˆ—æ¨¡å‹
    GPT2,           // GPT2ç³»åˆ—æ¨¡å‹
    SPM,            // SentencePieceæ¨¡å‹
    BPE,            // BPEæ¨¡å‹
    WPM             // WordPieceæ¨¡å‹
};

class ITokenizer {
public:
    virtual ~ITokenizer() = default;
    
    // æ ¸å¿ƒåŠŸèƒ½
    virtual std::vector<int> encode(
        const std::string& text, 
        bool addSpecialTokens = true
    ) = 0;
    
    virtual std::string decode(
        const std::vector<int>& ids,
        bool skipSpecialTokens = true
    ) = 0;
    
    // è¯æ±‡è¡¨æ“ä½œ
    virtual int getVocabSize() const = 0;
    virtual std::string idToToken(int id) const = 0;
    virtual int tokenToId(const std::string& token) const = 0;
    
    // ç‰¹æ®ŠToken
    virtual int getBosId() const = 0;
    virtual int getEosId() const = 0;
    virtual int getPadId() const = 0;
    virtual int getUnkId() const = 0;
    
    // æ¨¡å‹ç±»å‹
    virtual ModelType getModelType() const = 0;
    
    // åŠ è½½æ¨¡å‹
    virtual bool load(const std::string& modelPath) = 0;
};
```

### 3.2 llama.cpp é›†æˆå®ç°

#### 3.2.1 LlamaCppTokenizer å®ç°

```cpp
class LlamaCppTokenizer : public ITokenizer {
private:
    llama_vocab* vocab_;
    ModelType modelType_;
    std::unordered_map<std::string, int> specialTokens_;
    std::unordered_map<int, std::string> idToTokenMap_;
    
    // ç‰¹æ®Štoken ID
    int bosId_{-1};
    int eosId_{-1};
    int padId_{-1};
    int unkId_{-1};
    
public:
    explicit LlamaCppTokenizer(ModelType modelType);
    ~LlamaCppTokenizer() override;
    
    bool load(const std::string& modelPath) override;
    std::vector<int> encode(const std::string& text, bool addSpecialTokens = true) override;
    std::string decode(const std::vector<int>& ids, bool skipSpecialTokens = true) override;
    
    int getVocabSize() const override;
    std::string idToToken(int id) const override;
    int tokenToId(const std::string& token) const override;
    
    int getBosId() const override { return bosId_; }
    int getEosId() const override { return eosId_; }
    int getPadId() const override { return padId_; }
    int getUnkId() const override { return unkId_; }
    
    ModelType getModelType() const override { return modelType_; }
    
private:
    void loadModelConfig(const std::string& configPath);
    void loadSpecialTokens(const std::string& configPath);
    void initializeRegexPatterns();
};
```

#### 3.2.2 DeepSeek åˆ†è¯å™¨å®ç°

```cpp
class DeepSeekTokenizer : public LlamaCppTokenizer {
public:
    explicit DeepSeekTokenizer(ModelType modelType) : LlamaCppTokenizer(modelType) {}
    
    std::vector<int> encode(const std::string& text, bool addSpecialTokens = true) override {
        // åº”ç”¨DeepSeekç‰¹å®šçš„é¢„å¤„ç†
        std::string processedText = applyDeepSeekPreprocessing(text);
        return LlamaCppTokenizer::encode(processedText, addSpecialTokens);
    }
    
private:
    std::string applyDeepSeekPreprocessing(const std::string& text) {
        // DeepSeekç‰¹å®šçš„é¢„å¤„ç†é€»è¾‘
        // æ ¹æ®æ¨¡å‹ç±»å‹åº”ç”¨ä¸åŒçš„æ­£åˆ™è¡¨è¾¾å¼
        switch(getModelType()) {
            case ModelType::DEEPSEEK_LLM:
                return applyDeepSeekLLMPreprocessing(text);
            case ModelType::DEEPSEEK_CODER:
                return applyDeepSeekCoderPreprocessing(text);
            case ModelType::DEEPSEEK3_LLM:
                return applyDeepSeek3Preprocessing(text);
            default:
                return text;
        }
    }
    
    std::string applyDeepSeekLLMPreprocessing(const std::string& text) {
        // DeepSeek LLMä½¿ç”¨çš„æ­£åˆ™è¡¨è¾¾å¼
        // "[\r\n]", "\\s?[A-Za-zÂµÃ€-Ã–Ã˜-Ã¶Ã¸-ÆºÆ¼-Æ¿Ç„-Ê“Ê•-Ê¯Í°-Í³Í¶Í·Í»-Í½Í¿Î†Îˆ-ÎŠÎŒÎ-Î¡Î£-ÏµÏ·-ÒÒŠ-Ô¯Ô±-Õ–á‚ -áƒ…á -áµá¸-á½á²-á²ºá²½-á²¿á´€-á´«áµ«-áµ·áµ¹-á¶šá¸€-á¼•á¼˜-á¼á¼ -á½…á½ˆ-á½á½-á½—á½™á½›á½á½Ÿ-Ïá¾€-á¾´á¾¶-á¾¼Î¹á¿‚-á¿„á¿†-á¿Œá¿-Îá¿–-ÎŠá¿ -á¿¬á¿²-á¿´á¿¶-á¿¼â„‚â„‡â„Š-â„“â„•â„™-â„â„¤Î©â„¨K-â„­â„¯-â„´â„¹â„¼-â„¿â……-â…‰â…â†ƒâ†„â°€-â±»â±¾-â³¤â³«-â³®â³²â³³ê™€-ê™­êš€-êš›êœ¢-ê¯ê±-ê‡ê‹-êê­°-ê®¿ï¬€-ï¬†ï¬“-ï¬—ï¼¡-ï¼ºï½-ï½šğ€-ğ‘ğ’°-ğ““ğ“˜-ğ“»ğ²€-ğ²²ğ³€-ğ³²ğ‘¢ -ğ‘£Ÿğ¤€-ğ¥ƒ]+",
        // "\\s?[!-/:-~ï¼-ï¼ï¼š-ï½â€˜-â€Ÿã€€-ã€‚]+", "\\s+$", "[ä¸€-é¾¥à €-ä¸€ê°€-íŸ¿]+", "\\p{N}+"
        return text; // å®é™…å®ç°å°†åœ¨cppæ–‡ä»¶ä¸­
    }
    
    std::string applyDeepSeekCoderPreprocessing(const std::string& text) {
        // DeepSeek Coderä½¿ç”¨çš„æ­£åˆ™è¡¨è¾¾å¼
        // "[\r\n]", "\\s?\\p{L}+", "\\s?\\p{P}+", "[ä¸€-é¾¥à €-ä¸€ê°€-íŸ¿]+", "\\p{N}"
        return text; // å®é™…å®ç°å°†åœ¨cppæ–‡ä»¶ä¸­
    }
    
    std::string applyDeepSeek3Preprocessing(const std::string& text) {
        // DeepSeek3ä½¿ç”¨çš„æ­£åˆ™è¡¨è¾¾å¼
        // "\\p{N}{1,3}", "[ä¸€-é¾¥ã€-ã‚Ÿã‚ -ãƒ¿]+", "[!\"#$%&'()*+,\\-./:;<=>?@\\[\\\\\\]^_`{|}~][A-Za-z]+|[^\r\n\\p{L}\\p{P}\\p{S}]?[\\p{L}\\p{M}]+| ?[\\p{P}\\p{S}]+[\r\n]*|\\s*[\r\n]+|\\s+(?!\\S)|\\s+"
        return text; // å®é™…å®ç°å°†åœ¨cppæ–‡ä»¶ä¸­
    }
};
```

#### 3.2.3 Qwen åˆ†è¯å™¨å®ç°

```cpp
class QwenTokenizer : public LlamaCppTokenizer {
public:
    explicit QwenTokenizer() : LlamaCppTokenizer(ModelType::QWEN) {}
    
    std::vector<int> encode(const std::string& text, bool addSpecialTokens = true) override {
        // Qwenç‰¹å®šçš„FIMï¼ˆFill-in-the-Middleï¼‰å¤„ç†
        if (needsFimProcessing(text)) {
            return encodeWithFim(text, addSpecialTokens);
        }
        return LlamaCppTokenizer::encode(text, addSpecialTokens);
    }
    
private:
    bool needsFimProcessing(const std::string& text) {
        // æ£€æŸ¥æ˜¯å¦éœ€è¦FIMå¤„ç†
        return text.find("<|endoftext|>") != std::string::npos || 
               text.find("") != std::string::npos;
    }
    
    std::vector<int> encodeWithFim(const std::string& text, bool addSpecialTokens) {
        // Qwençš„FIMå¤„ç†é€»è¾‘
        // è¿™é‡Œå®ç°Qwenç‰¹æœ‰çš„FIMï¼ˆFill-in-the-Middleï¼‰åˆ†è¯é€»è¾‘
        std::vector<int> result;
        
        // å®é™…çš„FIMå¤„ç†å°†åœ¨cppæ–‡ä»¶ä¸­å®ç°
        return LlamaCppTokenizer::encode(text, addSpecialTokens);
    }
    
    std::string applyQwenPreprocessing(const std::string& text) {
        // Qwen2ä½¿ç”¨çš„æ­£åˆ™è¡¨è¾¾å¼
        // "(?:'[sS]|'[tT]|'[rR][eE]|'[vV][eE]|'[mM]|'[lL][lL]|'[dD])|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
        return text; // å®é™…å®ç°å°†åœ¨cppæ–‡ä»¶ä¸­
    }
};
```

### 3.3 æ€§èƒ½ä¼˜åŒ–

#### 3.3.1 å†…å­˜ç¼“å­˜è®¾è®¡

```cpp
class TokenCache {
private:
    std::unordered_map<std::string, std::vector<int>> encodeCache_;
    std::unordered_map<std::vector<int>, std::string, VectorHash> decodeCache_;
    mutable std::shared_mutex mutex_;
    size_t maxSize_;
    
public:
    TokenCache(size_t maxSize = 10000) : maxSize_(maxSize) {}
    
    void putEncode(const std::string& text, const std::vector<int>& tokens) {
        std::unique_lock lock(mutex_);
        if (encodeCache_.size() >= maxSize_) {
            encodeCache_.erase(encodeCache_.begin()); // ç®€å•çš„LRUç­–ç•¥
        }
        encodeCache_[text] = tokens;
    }
    
    std::optional<std::vector<int>> getEncode(const std::string& text) const {
        std::shared_lock lock(mutex_);
        auto it = encodeCache_.find(text);
        return it != encodeCache_.end() ? std::make_optional(it->second) : std::nullopt;
    }
    
    void putDecode(const std::vector<int>& tokens, const std::string& text) {
        std::unique_lock lock(mutex_);
        if (decodeCache_.size() >= maxSize_) {
            decodeCache_.erase(decodeCache_.begin()); // ç®€å•çš„LRUç­–ç•¥
        }
        decodeCache_[tokens] = text;
    }
    
    std::optional<std::string> getDecode(const std::vector<int>& tokens) const {
        std::shared_lock lock(mutex_);
        auto it = decodeCache_.find(tokens);
        return it != decodeCache_.end() ? std::make_optional(it->second) : std::nullopt;
    }
};
```

#### 3.3.2 æ‰¹å¤„ç†æ¥å£

```cpp
class BatchTokenizer {
public:
    struct BatchResult {
        std::vector<std::vector<int>> tokenized;
        std::vector<bool> success;
        std::vector<std::string> errors;
    };
    
    static BatchResult batchEncode(
        ITokenizer* tokenizer,
        const std::vector<std::string>& texts,
        bool addSpecialTokens = true,
        int maxParallel = 4
    );
    
    static std::vector<std::string> batchDecode(
        ITokenizer* tokenizer,
        const std::vector<std::vector<int>>& tokenLists,
        bool skipSpecialTokens = true,
        int maxParallel = 4
    );
};
```

## 4. æ¨¡å‹å…¼å®¹æ€§

### 4.1 æ”¯æŒæ ¼å¼
| æ ¼å¼ç±»å‹ | æ–‡ä»¶ç¤ºä¾‹ | åŠ è½½æ–¹å¼ | é€‚ç”¨æ¨¡å‹ |
|---------|---------|---------|----------|
| SentencePiece | model.spm | ç›´æ¥åŠ è½½ | Llamaã€æ—©æœŸæ¨¡å‹ |
| HuggingFace | tokenizer.json | è§£æé…ç½® | å¤§å¤šæ•°ç°ä»£æ¨¡å‹ |
| GGUF | model.gguf | llama.cppåŠ è½½ | llama.cppæ¨¡å‹ |
| TikToken | qwen.tiktoken | è‡ªå®šä¹‰è§£æ | Qwenç³»åˆ— |

### 4.2 è‡ªåŠ¨æ£€æµ‹æœºåˆ¶

```cpp
class ModelDetector {
public:
    static ModelType detectModelType(const std::string& configPath) {
        try {
            auto config = readJson(configPath);
            
            // æ£€æŸ¥tokenizer_classå­—æ®µ
            if (config.contains("tokenizer_class")) {
                std::string tokenizerClass = config["tokenizer_class"].get<std::string>();
                
                if (tokenizerClass.find("Qwen") != std::string::npos) {
                    return ModelType::QWEN;
                } else if (tokenizerClass.find("DeepSeek") != std::string::npos) {
                    if (tokenizerClass.find("DeepSeek3") != std::string::npos) {
                        return ModelType::DEEPSEEK3_LLM;
                    } else if (tokenizerClass.find("Coder") != std::string::npos) {
                        return ModelType::DEEPSEEK_CODER;
                    } else {
                        return ModelType::DEEPSEEK_LLM;
                    }
                } else if (tokenizerClass.find("Llama") != std::string::npos) {
                    return ModelType::LLAMA;
                } else if (tokenizerClass.find("Bert") != std::string::npos) {
                    return ModelType::BERT;
                } else if (tokenizerClass.find("GPT2") != std::string::npos) {
                    return ModelType::GPT2;
                }
            }
            
            // æ£€æŸ¥chat_templateå­—æ®µ
            if (config.contains("chat_template")) {
                std::string chatTemplate = config["chat_template"].get<std::string>();
                
                if (chatTemplate.find("qwen") != std::string::npos) {
                    return ModelType::QWEN;
                } else if (chatTemplate.find("deepseek") != std::string::npos) {
                    if (chatTemplate.find("deepseek3") != std::string::npos) {
                        return ModelType::DEEPSEEK3_LLM;
                    } else if (chatTemplate.find("coder") != std::string::npos) {
                        return ModelType::DEEPSEEK_CODER;
                    } else {
                        return ModelType::DEEPSEEK_LLM;
                    }
                }
            }
            
            // æ£€æŸ¥model_typeå­—æ®µ
            if (config.contains("model_type")) {
                std::string modelType = config["model_type"].get<std::string>();
                
                if (modelType.find("qwen") != std::string::npos) {
                    return ModelType::QWEN;
                } else if (modelType.find("deepseek") != std::string::npos) {
                    if (modelType.find("deepseek3") != std::string::npos) {
                        return ModelType::DEEPSEEK3_LLM;
                    } else if (modelType.find("coder") != std::string::npos) {
                        return ModelType::DEEPSEEK_CODER;
                    } else {
                        return ModelType::DEEPSEEK_LLM;
                    }
                }
            }
            
            // æ£€æŸ¥ç‰¹æ®Štokenåç§°æ¨¡å¼
            if (config.contains("added_tokens_decoder")) {
                auto tokens = config["added_tokens_decoder"];
                for (auto& item : tokens.items()) {
                    if (item.value().contains("content")) {
                        std::string content = item.value()["content"];
                        if (content == "" || content == "" || content == "" || 
                            content == "" || content == "") {
                            return ModelType::QWEN;  // Qwenç‰¹æœ‰çš„FIM tokens
                        } else if (content.find("deepseek") != std::string::npos) {
                            return ModelType::DEEPSEEK_LLM;
                        }
                    }
                }
            }
            
            // é»˜è®¤è¿”å›SPM
            return ModelType::SPM;
        } catch (const std::exception& e) {
            // å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤ç±»å‹
            return ModelType::SPM;
        }
    }
};
```

### 4.3 ç‰¹æ®ŠTokenå¤„ç†

```cpp
// ä»é…ç½®æ–‡ä»¶åŠ è½½ç‰¹æ®ŠToken
void loadSpecialTokens(ITokenizer* tokenizer, const std::string& configPath) {
    auto json = readJson(configPath);
    
    // æ ‡å‡†ç‰¹æ®ŠToken
    if (json.contains("bos_token_id")) {
        // è®¾ç½®BOS ID
    }
    if (json.contains("eos_token_id")) {
        // è®¾ç½®EOS ID
    }
    if (json.contains("pad_token_id")) {
        // è®¾ç½®PAD ID
    }
    if (json.contains("unk_token_id")) {
        // è®¾ç½®UNK ID
    }
    
    // æ¨¡å‹ç‰¹å®šçš„ç‰¹æ®ŠToken
    if (json.contains("additional_special_tokens")) {
        // å¤„ç†é¢å¤–çš„ç‰¹æ®ŠToken
    }
}
```

## 5. å®ç°ç­–ç•¥

### 5.1 llama.cpp é›†æˆæ–¹æ¡ˆ
- ä½¿ç”¨llama.cppçš„`llama_vocab`ç»“æ„è¿›è¡Œåº•å±‚åˆ†è¯
- å°è£…llama.cppçš„åˆ†è¯ç®—æ³•å®ç°
- ä¿æŒä¸ç°æœ‰APIçš„å…¼å®¹æ€§
- å®ç°llama.cppé¢„å¤„ç†å™¨ç±»å‹çš„æ”¯æŒ

### 5.2 é€æ­¥è¿ç§»ç­–ç•¥
1. **ç¬¬ä¸€é˜¶æ®µ**: å®ç°åŸºç¡€llama.cppåˆ†è¯å™¨å°è£…
2. **ç¬¬äºŒé˜¶æ®µ**: æ·»åŠ DeepSeekå’ŒQwenæ¨¡å‹æ”¯æŒ
3. **ç¬¬ä¸‰é˜¶æ®µ**: ä¼˜åŒ–æ€§èƒ½å’Œå†…å­˜ä½¿ç”¨
4. **ç¬¬å››é˜¶æ®µ**: é›†æˆåˆ°cLLMä¸»æµç¨‹

### 5.3 æ€§èƒ½ä¼˜åŒ–è€ƒè™‘
- å®ç°tokenç¼“å­˜æœºåˆ¶
- ä¼˜åŒ–å­—ç¬¦ä¸²å¤„ç†
- ä½¿ç”¨é«˜æ•ˆçš„æ­£åˆ™è¡¨è¾¾å¼å¼•æ“
- é¢„åˆ†é…å†…å­˜å‡å°‘åŠ¨æ€åˆ†é…å¼€é”€
- å¹¶è¡Œæ‰¹å¤„ç†æ”¯æŒ

## 6. æµ‹è¯•æ–¹æ¡ˆ

### 6.1 å•å…ƒæµ‹è¯•

```cpp
TEST(LlamaCppTokenizerTest, QwenEncodeDecode) {
    LlamaCppTokenizer tokenizer(ModelType::QWEN);
    ASSERT_TRUE(tokenizer.load("test_models/qwen/tokenizer.json"));
    
    std::string text = "Hello, world!";
    auto ids = tokenizer.encode(text);
    ASSERT_FALSE(ids.empty());
    
    std::string decoded = tokenizer.decode(ids);
    EXPECT_EQ(decoded, text);
}

TEST(DeepSeekTokenizerTest, DeepSeekCoderPreprocessing) {
    DeepSeekTokenizer tokenizer(ModelType::DEEPSEEK_CODER);
    ASSERT_TRUE(tokenizer.load("test_models/deepseek-coder/config.json"));
    
    std::string text = "def hello_world():\n    print('Hello')";
    auto ids = tokenizer.encode(text);
    ASSERT_FALSE(ids.empty());
}
```

### 6.2 æ€§èƒ½æµ‹è¯•

```bash
# è¿è¡Œæ€§èƒ½æµ‹è¯•
./bin/tokenizer_benchmark \
    --model=test.qwen \
    --text=sample.txt \
    --iterations=1000 \
    --batch-size=32
```

## 7. æ¼”è¿›è§„åˆ’

### 7.1 çŸ­æœŸç›®æ ‡ï¼ˆ1-2ä¸ªæœˆï¼‰
- å®ç°åŸºç¡€llama.cppåˆ†è¯å™¨æ”¯æŒ
- æ·»åŠ Qwenå’ŒDeepSeekæ¨¡å‹æ”¯æŒ
- å®Œæˆæ€§èƒ½åŸºå‡†æµ‹è¯•

### 7.2 ä¸­æœŸç›®æ ‡ï¼ˆ3-6ä¸ªæœˆï¼‰
- æ”¯æŒæ›´å¤šæ¨¡å‹æ ¼å¼ï¼ˆLlama3ã€Mixtralç­‰ï¼‰
- ä¼˜åŒ–æ‰¹å¤„ç†æ€§èƒ½
- æ·»åŠ é‡åŒ–æ”¯æŒ

### 7.3 é•¿æœŸç›®æ ‡ï¼ˆ6-12ä¸ªæœˆï¼‰
- åŠ¨æ€åˆ†è¯å™¨åŠ è½½
- è®­ç»ƒæ”¯æŒ
- ç¡¬ä»¶åŠ é€Ÿï¼ˆGPU/TPUï¼‰
- è‡ªé€‚åº”åˆ†è¯ç®—æ³•

## 8. æ€»ç»“

æœ¬æ–‡æ¡£æå‡ºäº†ä¸€ä¸ªç°ä»£åŒ–ã€å¯æ‰©å±•çš„åˆ†è¯å™¨è®¾è®¡æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåŸºäºllama.cppçš„å¼ºå¤§åˆ†è¯èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†ä¸cLLMé¡¹ç›®çš„å…¼å®¹æ€§ã€‚é€šè¿‡ç»Ÿä¸€çš„æ¥å£è®¾è®¡å’Œæ¨¡å‹è‡ªåŠ¨æ£€æµ‹æœºåˆ¶ï¼Œè¯¥æ–¹æ¡ˆèƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒQwenã€DeepSeekç­‰å¤šç§æ¨¡å‹ï¼Œä¸ºcLLMé¡¹ç›®æä¾›å¼ºå¤§çš„æ–‡æœ¬å¤„ç†èƒ½åŠ›ã€‚

è¯¥è®¾è®¡å……åˆ†è€ƒè™‘äº†æ€§èƒ½ã€å¯ç»´æŠ¤æ€§å’Œæ‰©å±•æ€§ï¼Œä¸ºcLLMé¡¹ç›®çš„æœªæ¥å‘å±•å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚