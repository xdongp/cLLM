# cLLM 项目分词器设计文档

## 1. 概述

本文档详细描述了cLLM项目中分词器模块的设计方案。分词器是自然语言处理流水线中的关键组件，负责将原始文本转换为模型能够理解的token序列，或将模型输出的token序列转换回可读文本。

## 2. 设计目标

### 2.1 功能目标
- **多模型支持**: 支持Qwen、DeepSeek等多种主流大语言模型的分词需求
- **高效分词**: 提供高性能的文本分词和解分词功能
- **标准化接口**: 提供统一的分词器接口，便于扩展和维护
- **兼容性**: 与现有的cLLM架构完全兼容

### 2.2 性能目标
- 高吞吐量的分词处理能力
- 低延迟的实时分词响应
- 最小化内存占用

## 3. 现状分析

### 3.1 当前实现
cLLM项目目前使用基于SentencePiece的分词器，具体为`Qwen2Tokenizer`，主要特点：
- 基于SentencePiece库实现
- 针对Qwen模型进行了优化
- 提供基础的编码/解码功能

### 3.2 存在的问题
- 缺乏对其他模型（如DeepSeek）的支持
- 架构较为固定，不易扩展
- 无法充分利用llama.cpp中先进的分词算法

### 3.3 llama.cpp 分词器优势
根据调研，llama.cpp提供了以下优势：
- 支持多种分词算法（BPE、SPM、WPM等）
- 针对不同模型优化的预处理器（DeepSeek、Qwen等）
- 高性能的实现和优化
- 丰富的正则表达式预处理规则

## 4. 架构设计

### 4.1 分层架构
```
+------------------+     +------------------+     +------------------+
|   Application    | <-> |  Tokenizer API   | <-> |  Implementation  |
|    Layer         |     |   Abstraction    |     |   Layer          |
+------------------+     +------------------+     +------------------+
```

### 4.2 核心组件

#### 4.2.1 TokenizerBase (抽象基类)
```cpp
class TokenizerBase {
public:
    virtual ~TokenizerBase() = default;
    virtual std::vector<int> encode(const std::string& text, bool addSpecialTokens = false) = 0;
    virtual std::string decode(const std::vector<int>& tokenIds, bool skipSpecialTokens = true) = 0;
    virtual int getVocabSize() const = 0;
    virtual std::string getTokenText(int tokenId) const = 0;
    virtual bool isSpecialToken(int tokenId) const = 0;
};
```

#### 4.2.2 UnifiedTokenizer (统一分词器)
统一管理多种模型类型的分词器实现：
- 自动检测模型类型
- 根据模型类型选择合适的分词算法
- 提供一致的接口

#### 4.2.3 Model-Specific Tokenizers (模型特定分词器)
- QwenTokenizer: 专门处理Qwen模型的分词
- DeepSeekTokenizer: 专门处理DeepSeek模型的分词
- LlamaCppTokenizer: 基于llama.cpp的通用分词器

## 5. 详细设计

### 5.1 模型类型枚举
```cpp
enum ModelType {
    AUTO,           // 自动检测
    QWEN,           // Qwen系列模型
    DEEPSEEK_LLM,   // DeepSeek LLM模型
    DEEPSEEK_CODER, // DeepSeek Coder模型
    DEEPSEEK3_LLM,  // DeepSeek3 LLM模型
    BPE,            // 通用BPE模型
    SPM,            // 通用SentencePiece模型
    WPM             // 通用WordPiece模型
};
```

### 5.2 自动检测机制
分词器应能自动检测模型类型，检测顺序如下：
1. 检查 `tokenizer_class` 字段
2. 检查 `chat_template` 字段
3. 检查 `model_type` 字段
4. 检查特殊token名称模式
5. 回退到默认类型

### 5.3 DeepSeek 分词器实现
基于llama.cpp的DeepSeek分词器配置：

#### 5.3.1 DeepSeek LLM 正则表达式
```cpp
case LLAMA_VOCAB_PRE_TYPE_DEEPSEEK_LLM:
    regex_exprs = {
        "[\r\n]",
        "\\s?[A-Za-zµÀ-ÖØ-öø-ƺƼ-ƿǄ-ʓʕ-ʯͰ-ͳͶͷͻ-ͽͿΆΈ-ΊΌΎ-ΡΣ-ϵϷ-ҁҊ-ԯԱ-ՖႠ-ჅᎠ-Ᏽᏸ-ᏽᲐ-ᲺᲽ-Ჿᴀ-ᴫᵫ-ᵷᵹ-ᶚḀ-ἕἘ-Ἕἠ-ὅὈ-Ὅὐ-ὗὙὛὝὟ-ώᾀ-ᾴᾶ-ᾼιῂ-ῄῆ-ῌῐ-ΐῖ-Ίῠ-Ῥῲ-ῴῶ-ῼℂℇℊ-ℓℕℙ-ℝℤΩℨK-ℭℯ-ℴℹℼ-ℿⅅ-ⅉⅎↃↄⰀ-ⱻⱾ-ⳤⳫ-ⳮⳲⳳꙀ-ꙭꚀ-ꚛꜢ-ꝯꝱ-ꞇꞋ-ꞎꭰ-ꮿﬀ-ﬆﬓ-ﬗＡ-Ｚａ-ｚ𐐀-𐑏𐒰-𐓓𐓘-𐓻𐲀-𐲲𐳀-𐳲𑢠-𑣟𞤀-𞥃]+",
        "\\s?[!-/:-~！-／：-～‘-‟　-。]+",
        "\\s+$",
        "[一-龥ࠀ-一가-퟿]+",
        "\\p{N}+",
    };
```

#### 5.3.2 DeepSeek Coder 正则表达式
```cpp
case LLAMA_VOCAB_PRE_TYPE_DEEPSEEK_CODER:
    regex_exprs = {
        "[\r\n]",
        "\\s?\\p{L}+",
        "\\s?\\p{P}+",
        "[一-龥ࠀ-一가-퟿]+",
        "\\p{N}",
    };
```

### 5.4 Qwen 分词器实现
基于llama.cpp的Qwen分词器配置：

#### 5.4.1 Qwen2 正则表达式
```cpp
case LLAMA_VOCAB_PRE_TYPE_QWEN2:
    regex_exprs = {
        "(?:'[sS]|'[tT]|'[rR][eE]|'[vV][eE]|'[mM]|'[lL][lL]|'[dD])|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+",
    };
```

#### 5.4.2 Qwen 特殊 Tokens
- `FIM_PRE`: "" (prefix token)
- `FIM_MID`: "" (middle token) 
- `FIM_SUF`: "" (suffix token)
- `FIM_PAD`: "" (pad token)
- `FIM_REP`: "" (repo token)
- `FIM_SEP`: "" (separator token)

## 6. 接口设计

### 6.1 主要接口
```cpp
class UnifiedTokenizer : public TokenizerBase {
public:
    // 构造函数，支持自动检测或指定模型类型
    explicit UnifiedTokenizer(const std::string& modelPath, ModelType modelType = AUTO);
    
    // 基础分词功能
    std::vector<int> encode(const std::string& text, bool addSpecialTokens = false) override;
    std::string decode(const std::vector<int>& tokenIds, bool skipSpecialTokens = true) override;
    
    // 元数据查询
    int getVocabSize() const override;
    std::string getTokenText(int tokenId) const override;
    bool isSpecialToken(int tokenId) const override;
    
    // 特殊token管理
    void setBosToken(int tokenId);
    void setEosToken(int tokenId);
    void setPadToken(int tokenId);
    void setUnkToken(int tokenId);
    
    int getBosToken() const;
    int getEosToken() const;
    int getPadToken() const;
    int getUnkToken() const;
    
    // 工具方法
    static ModelType detectModelType(const std::string& configPath);
};
```

### 6.2 配置文件处理
分词器应能从以下配置文件中提取必要信息：
- `config.json`: 模型配置
- `tokenizer.json`: 分词器配置
- `tokenizer_config.json`: 分词器参数配置

## 7. 实现策略

### 7.1 逐步迁移策略
1. **第一阶段**: 实现基于llama.cpp的分词器封装
2. **第二阶段**: 添加DeepSeek和Qwen模型支持
3. **第三阶段**: 优化性能和内存使用
4. **第四阶段**: 集成到cLLM主流程

### 7.2 llama.cpp 集成方案
- 封装llama.cpp的`llama_vocab`结构
- 使用llama.cpp的分词算法实现
- 保持与现有API的兼容性

### 7.3 性能优化考虑
- 实现token缓存机制
- 优化字符串处理
- 使用高效的正则表达式引擎
- 预分配内存减少动态分配开销

## 8. 测试策略

### 8.1 单元测试
- 针对每种模型类型的基本功能测试
- 边界条件和异常处理测试
- 性能基准测试

### 8.2 集成测试
- 与推理引擎的集成测试
- 不同输入长度的处理能力测试
- 并发访问的安全性测试

## 9. 部署和维护

### 9.1 部署要求
- llama.cpp库的正确链接
- 相关模型文件的可用性
- 充足的内存资源

### 9.2 维护指南
- 新增模型类型的扩展指南
- 性能监控和调优建议
- 常见问题排查手册

## 10. 未来扩展

### 10.1 模型支持扩展
- 支持更多开源模型的分词器
- 自定义分词器插件机制

### 10.2 功能增强
- 分词结果的可视化工具
- 分词性能分析工具
- 在线分词质量评估

## 11. 总结

本文档提出了一个现代化、可扩展的分词器设计方案，该方案基于llama.cpp的强大分词能力，同时保持了与cLLM项目的兼容性。通过统一的接口设计和模型自动检测机制，该方案能够有效支持Qwen、DeepSeek等多种模型，为cLLM项目提供强大的文本处理能力。

该设计充分考虑了性能、可维护性和扩展性，为cLLM项目的未来发展奠定了坚实的基础。