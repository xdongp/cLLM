# cLLM 分词器实现方案调研报告

## 1. 项目背景与目标

### 1.1 调研目标
为 cLLM 项目选择一个简单、稳定、高性能的分词器实现方案，需要：
- 与主流模型格式兼容（Qwen、Llama 等）
- 性能高效，适合生产环境
- 易于集成和维护
- 支持流式生成场景

### 1.2 cLLM 现有架构要求
根据设计文档，分词器需要满足：
- 文本编码/解码能力
- Token ID 转换
- 特殊 Token 处理
- 与 ModelExecutor 和 Scheduler 集成

---

## 2. 现有实现分析

### 2.1 当前代码结构
```
src/tokenizer/
├── tokenizer.cpp       # SentencePiece 分词器
├── tokenizer.h         # 分词器接口
├── qwen2_tokenizer.cpp # Qwen2 自定义分词器
├── json_tokenizer.cpp  # 简单 JSON 分词器
├── manager.cpp         # 分词器管理器
├── generator.cpp       # 流式生成器
└── token.cpp           # Token 数据结构
```

### 2.2 现有问题
1. **Tokenizer.cpp** 使用 SentencePiece，但：
   - 实现相对简单
   - 未完整处理特殊 Token
   - 与 HuggingFace transformers 兼容性可能存在问题

2. **Qwen2Tokenizer.cpp** 存在：
   - 手写的分词逻辑，性能可能不佳
   - 特殊 Token 处理不够完整
   - 子词分词算法需要优化

3. **JsonTokenizer.cpp**：
   - 基于空格分词，不适合生产环境
   - 仅作为简单演示

---

## 3. 主流分词方案对比

### 3.1 方案概览

| 方案 | 类型 | 优点 | 缺点 | 适用场景 |
|------|------|------|------|----------|
| **SentencePiece** | 独立库 | 成熟稳定、跨平台、无依赖 | 功能相对基础 | 通用场景 |
| **tokenizers-cpp** | HuggingFace 绑定 | 功能完整、兼容性好、性能高 | 依赖 Rust 环境 | 需要 HuggingFace 兼容性 |
| **llama.cpp 内置** | 内置实现 | 与 llama.cpp 集成好、GGUF 兼容 | 特定于 llama.cpp 生态 | 已有 llama.cpp 项目迁移 |
| **自定义实现** | 手写 | 完全可控、可定制 | 开发维护成本高 | 特殊需求场景 |

### 3.2 SentencePiece 方案

**核心文件**: `sentencepiece_processor.h`

**支持的分词算法**:
- BPE (Byte Pair Encoding)
- Unigram Language Model
- WordPiece
- Word (空格分词)

**特点**:
```
优点:
- Google 官方维护，稳定性高
- 纯 C++ 实现，无外部依赖
- 支持多种分词算法
- 模型文件体积小
- 内存占用可控

缺点:
- 功能相对基础
- HuggingFace transformers 兼容性需要额外处理
- 特殊 Token 处理需要手动实现
```

**性能表现**:
- 编码速度：约 10-50 MB/s（取决于文本）
- 内存占用：约 2-5 MB（基础库）
- 模型加载：约 10-50 ms

### 3.3 tokenizers-cpp 方案

**项目地址**: https://github.com/mlc-ai/tokenizers-cpp

**架构设计**:
```
┌─────────────────────────────────────┐
│       C++ API (tokenizers-cpp)      │
├─────────────────────────────────────┤
│    HuggingFace    │    SentencePiece │
│    Tokenizers     │    Processor      │
├─────────────────────────────────────┤
│          Rust Core Engine           │
└─────────────────────────────────────┘
```

**支持的分词器类型**:
- BPE (GPT-2, Llama 等)
- WordPiece (BERT)
- Unigram (SentencePiece)
- Byte-level BPE

**特点**:
```
优点:
- 与 HuggingFace transformers 完全兼容
- 支持训练和推理
- 性能优秀（Rust 实现）
- 支持流式处理
- 标准化输出格式

缺点:
- 需要 Rust 工具链编译
- 依赖 msgpack 库
- 跨平台编译复杂度较高
- 库文件体积较大
```

**性能表现**:
- 编码速度：约 50-200 MB/s
- 内存占用：约 10-20 MB
- 模型加载：约 50-100 ms

### 3.4 llama.cpp 内置分词器

**核心文件**: `llama-vocab.cpp` (约 3901 行)

**支持的分词器类型**:
- SPM (SentencePiece 兼容)
- BPE
- WPM (WordPiece)
- UGM (Unigram)
- RWKV
- PLaMo2

**特点**:
```
优点:
- 与 llama.cpp 完全集成
- 支持 GGUF 格式
- 经过大规模验证
- 性能优秀

缺点:
- 特定于 llama.cpp 生态
- API 不够通用
- 独立使用复杂度高
```

---

## 4. 推荐方案：SentencePiece + HuggingFace 兼容层

### 4.1 选择理由

1. **简单稳定**: SentencePiece 是成熟稳定的库，维护良好
2. **跨平台**: 纯 C++ 实现，无外部依赖
3. **性能适中**: 满足 cLLM 的性能需求
4. **兼容性好**: 支持大多数主流模型格式
5. **易于集成**: CMake 集成简单

### 4.2 方案设计

**推荐架构**:
```
┌─────────────────────────────────────────────────────────┐
│                    Tokenizer 接口层                      │
│              (统一接口，抽象分词器实现)                   │
├─────────────────────────────────────────────────────────┤
│                  SentencePiece 分词器                    │
│                   + 兼容层实现                           │
├─────────────────────────────────────────────────────────┤
│                   SentencePiece 库                       │
└─────────────────────────────────────────────────────────┘
```

### 4.3 核心功能要求

**必需功能**:
1. **编码 (encode)**
   - 输入：文本字符串
   - 输出：Token ID 列表
   - 支持添加特殊 Token (BOS/EOS)

2. **解码 (decode)**
   - 输入：Token ID 列表
   - 输出：文本字符串
   - 支持跳过特殊 Token

3. **词汇表查询**
   - 获取词汇表大小
   - ID 到 Token 转换
   - Token 到 ID 转换

4. **特殊 Token 处理**
   - BOS (Begin of Sequence)
   - EOS (End of Sequence)
   - PAD (Padding)
   - UNK (Unknown)
   - 自定义特殊 Token

### 4.4 HuggingFace 兼容性实现

**关键点**:

1. **特殊 Token ID 获取**:
   ```cpp
   // 从 tokenizer.json 或模型配置中读取
   int bos_id = config["bos_token_id"].get<int>();
   int eos_id = config["eos_token_id"].get<int>();
   int pad_id = config["pad_token_id"].get<int>();
   int unk_id = config["unk_token_id"].get<int>();
   ```

2. **Chat Template 支持**:
   ```cpp
   // 对于 Qwen 等模型，需要支持 chat template
   std::string applyChatTemplate(const std::string& messages);
   ```

3. **Attention Mask 处理**:
   ```cpp
   // 生成 attention mask
   std::vector<int> createAttentionMask(const std::vector<int>& tokenIds);
   ```

---

## 5. 实现方案详细设计

### 5.1 统一接口设计

**头文件**: `include/cllm/tokenizer/tokenizer.h`

```cpp
namespace cllm {

class Tokenizer {
public:
    virtual ~Tokenizer() = default;
    
    // 编码
    virtual std::vector<int> encode(
        const std::string& text,
        bool addSpecialTokens = true
    ) = 0;
    
    // 解码
    virtual std::string decode(
        const std::vector<int>& tokenIds,
        bool skipSpecialTokens = true
    ) = 0;
    
    // 词汇表操作
    virtual int getVocabSize() const = 0;
    virtual std::string idToToken(int id) const = 0;
    virtual int tokenToId(const std::string& token) const = 0;
    
    // 特殊 Token
    virtual bool isSpecialToken(int id) const = 0;
    virtual int getBosTokenId() const = 0;
    virtual int getEosTokenId() const = 0;
    virtual int getPadTokenId() const = 0;
    virtual int getUnkTokenId() const = 0;
    
    // 模型加载
    virtual void load(const std::string& modelPath) = 0;
    virtual void unload() = 0;
    virtual bool isLoaded() const = 0;
};

} // namespace cllm
```

### 5.2 SentencePiece 实现

**头文件**: `include/cllm/tokenizer/sentencepiece_tokenizer.h`

```cpp
#pragma once

#include "tokenizer.h"
#include <sentencepiece_processor.h>

namespace cllm {

class SentencePieceTokenizer : public Tokenizer {
public:
    explicit SentencePieceTokenizer(const std::string& modelPath);
    ~SentencePieceTokenizer() override;
    
    // 禁止拷贝
    SentencePieceTokenizer(const SentencePieceTokenizer&) = delete;
    SentencePieceTokenizer& operator=(const SentencePieceTokenizer&) = delete;
    
    // 允许移动
    SentencePieceTokenizer(SentencePieceTokenizer&&) noexcept;
    SentencePieceTokenizer& operator=(SentencePieceTokenizer&&) noexcept;
    
    // Tokenizer 接口实现
    std::vector<int> encode(
        const std::string& text,
        bool addSpecialTokens = true
    ) override;
    
    std::string decode(
        const std::vector<int>& tokenIds,
        bool skipSpecialTokens = true
    ) override;
    
    int getVocabSize() const override;
    std::string idToToken(int id) const override;
    int tokenToId(const std::string& token) const override;
    
    bool isSpecialToken(int id) const override;
    int getBosTokenId() const override;
    int getEosTokenId() const override;
    int getPadTokenId() const override;
    int getUnkTokenId() const override;
    
    void load(const std::string& modelPath) override;
    void unload() override;
    bool isLoaded() const override;
    
    // HuggingFace 兼容性
    void loadHuggingFace(const std::string& tokenizerDir);
    void loadModelFile(const std::string& modelPath);
    
private:
    void _loadSpecialTokens(const nlohmann::json& config);
    
    std::unique_ptr<sentencepiece::SentencePieceProcessor> processor_;
    
    // HuggingFace 兼容的特殊 Token
    int bosTokenId_ = -1;
    int eosTokenId_ = -1;
    int padTokenId_ = -1;
    int unkTokenId_ = -1;
    
    // 特殊 Token 集合
    std::unordered_set<int> specialTokenIds_;
    
    std::string modelPath_;
    bool loaded_ = false;
};

} // namespace cllm
```

### 5.3 性能优化策略

**1. 预分配内存**:
```cpp
// 在 encode/decode 时预分配 vector 容量
std::vector<int> encode(const std::string& text, bool addSpecialTokens) {
    std::vector<int> tokenIds;
    tokenIds.reserve(text.length() / 3); // 预估容量
    
    // ... 编码逻辑
    
    return tokenIds;
}
```

**2. 缓存优化**:
```cpp
// 缓存常用 Token 的 ID
std::unordered_map<std::string, int> tokenToIdCache_;

// 在 tokenToId 中使用缓存
int tokenToId(const std::string& token) const override {
    auto it = tokenToIdCache_.find(token);
    if (it != tokenToIdCache_.end()) {
        return it->second;
    }
    int id = processor_->PieceToId(token);
    tokenToIdCache_[token] = id;
    return id;
}
```

**3. 并发安全**:
```cpp
// 对于只读访问，使用 shared_lock
mutable std::shared_mutex mutex_;

std::vector<int> encode(const std::string& text, bool addSpecialTokens) {
    std::shared_lock<std::shared_mutex> lock(mutex_);
    // ... 编码逻辑
}
```

---

## 6. 模型格式兼容性

### 6.1 SentencePiece 模型 (.model)
```bash
# 使用 sentencepiece 训练
spm_train --input=data.txt --model_prefix=mymodel --vocab_size=5000

# 支持的参数
--model_type=bpe|unigram|word|wordpiece
--character_coverage=1.0
--input_sentence_size=1000000
```

### 6.2 HuggingFace 格式
```
tokenizer.json      # 分词器配置
vocab.json          # 词汇表映射
merges.txt          # BPE 合并规则 (可选)
special_tokens_map.json  # 特殊 Token 配置
tokenizer_config.json    # 分词器配置
```

### 6.3 Qwen2 模型兼容
对于 Qwen2 系列模型，需要：
1. 加载 `vocab.json` 和 `merges.txt`
2. 读取 `tokenizer_config.json` 中的特殊 Token
3. 实现 Qwen2 的 Chat Template

---

## 7. 测试方案

### 7.1 单元测试
```cpp
// tests/test_tokenizer.cpp

TEST(TokenizerTest, EncodeDecode) {
    Tokenizer tokenizer("path/to/tokenizer.model");
    
    std::string text = "Hello, world!";
    auto tokenIds = tokenizer.encode(text);
    std::string decoded = tokenizer.decode(tokenIds);
    
    // 验证 round-trip
    EXPECT_EQ(text, decoded);
}

TEST(TokenizerTest, SpecialTokens) {
    Tokenizer tokenizer("path/to/tokenizer.model");
    
    // 验证特殊 Token 存在
    EXPECT_GE(tokenizer.getBosTokenId(), 0);
    EXPECT_GE(tokenizer.getEosTokenId(), 0);
}

TEST(TokenizerTest, Performance) {
    Tokenizer tokenizer("path/to/tokenizer.model");
    
    std::string text = "Hello, world! " * 1000;
    
    // 性能测试
    auto start = std::chrono::high_resolution_clock::now();
    for (int i = 0; i < 100; ++i) {
        tokenizer.encode(text);
    }
    auto end = std::chrono::high_resolution_clock::now();
    
    // 验证性能 (should be < 10ms per encode)
    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);
    EXPECT_LT(duration.count() / 100, 10000);
}
```

### 7.2 集成测试
```cpp
// tests/test_tokenizer_integration.cpp

TEST(TokenizerIntegration, Qwen2Model) {
    Tokenizer tokenizer("path/to/qwen2/tokenizer");
    
    // 测试 Qwen2 特定功能
    auto tokenIds = tokenizer.encode("你好世界");
    EXPECT_FALSE(tokenIds.empty());
}
```

---

## 8. 实施计划

### 8.1 第一阶段：基础实现
- [ ] 实现 SentencePieceTokenizer 类
- [ ] 实现基础 encode/decode 功能
- [ ] 实现特殊 Token 处理
- [ ] 编写单元测试

### 8.2 第二阶段：兼容性增强
- [ ] 添加 HuggingFace 格式支持
- [ ] 添加 Qwen2 特定支持
- [ ] 实现 Chat Template
- [ ] 性能优化

### 8.3 第三阶段：完善与优化
- [ ] 完善错误处理
- [ ] 添加日志记录
- [ ] 编写集成测试
- [ ] 文档完善

---

## 9. 风险与注意事项

### 9.1 潜在风险
1. **性能风险**: 手写分词器可能性能不足
   - 解决：使用 SentencePiece 库
   - 后续优化：考虑迁移到 tokenizers-cpp

2. **兼容风险**: HuggingFace 兼容性可能存在问题
   - 解决：实现完整的特殊 Token 处理
   - 测试：覆盖主流模型格式

3. **维护风险**: 自定义代码增加维护成本
   - 解决：保持代码简洁，添加详细注释
   - 长期：考虑使用标准库替代

### 9.2 注意事项
1. 确保跨平台兼容性（Linux/macOS/Windows）
2. 注意内存管理，避免内存泄漏
3. 处理异常情况，提供清晰的错误信息
4. 保持 API 稳定性，避免频繁变更

---

## 10. 参考资源

### 10.1 相关项目
- [SentencePiece](https://github.com/google/sentencepiece)
- [tokenizers-cpp](https://github.com/mlc-ai/tokenizers-cpp)
- [llama.cpp](https://github.com/ggerganov/llama.cpp)
- [HuggingFace tokenizers](https://github.com/huggingface/tokenizers)

### 10.2 文档
- [SentencePiece 文档](https://github.com/google/sentencepiece/blob/master/doc/index.md)
- [tokenizers-cpp 示例](https://github.com/mlc-ai/tokenizers-cpp/tree/main/example)
- [HuggingFace Tokenizers 文档](https://huggingface.co/docs/tokenizers)

---

## 11. 总结

### 推荐方案：SentencePiece + 兼容层

**选择理由**:
1. ✅ 简单稳定：SentencePiece 是成熟库
2. ✅ 跨平台：纯 C++ 实现，无外部依赖
3. ✅ 性能适中：满足 cLLM 性能需求
4. ✅ 兼容性好：支持大多数主流模型
5. ✅ 易于集成：CMake 集成简单

**下一步行动**:
1. 实现 SentencePieceTokenizer 类
2. 添加 HuggingFace 兼容层
3. 完善 Qwen2 特定支持
4. 编写完整的测试用例
5. 持续优化性能

**备选方案**:
如果性能成为瓶颈，可考虑迁移到 tokenizers-cpp。