# Tokenizeræ¨¡å—ReviewæŠ¥å‘Š

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**å®¡æŸ¥æ—¥æœŸ**: 2026-01-10  
**å®¡æŸ¥èŒƒå›´**: Tokenizeræ¨¡å—è®¾è®¡æ–‡æ¡£ã€ä»£ç å®ç°ã€é›†æˆæ–¹æ¡ˆ  
**å®¡æŸ¥äºº**: cLLM Team

---

## ğŸ“‹ ç›®å½•

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [è®¾è®¡æ–‡æ¡£ä¸ä»£ç ä¸€è‡´æ€§åˆ†æ](#è®¾è®¡æ–‡æ¡£ä¸ä»£ç ä¸€è‡´æ€§åˆ†æ)
3. [tokenizers-cppé›†æˆæ–¹æ¡ˆåˆ†æ](#tokenizers-cppé›†æˆæ–¹æ¡ˆåˆ†æ)
4. [CTokenizerå®ç°è¯„ä¼°](#ctokenizerå®ç°è¯„ä¼°)
5. [å…³é”®é—®é¢˜è¯†åˆ«](#å…³é”®é—®é¢˜è¯†åˆ«)
6. [æ”¹è¿›å»ºè®®](#æ”¹è¿›å»ºè®®)
7. [æ€»ç»“](#æ€»ç»“)

---

## æ¦‚è¿°

### å®¡æŸ¥ç›®æ ‡

æœ¬æ¬¡å®¡æŸ¥æ—¨åœ¨è¯„ä¼°Tokenizeræ¨¡å—çš„è®¾è®¡åˆç†æ€§ã€å®ç°å®Œæ•´æ€§ä»¥åŠtokenizers-cppé›†æˆæ–¹æ¡ˆçš„å¯è¡Œæ€§ï¼Œè¯†åˆ«æ½œåœ¨é—®é¢˜å¹¶æä¾›æ”¹è¿›å»ºè®®ã€‚

### å®¡æŸ¥èŒƒå›´

- **è®¾è®¡æ–‡æ¡£**: [Tokenizeræ¨¡å—è®¾è®¡.md](../modules/Tokenizeræ¨¡å—è®¾è®¡.md)
- **CTokenizerè®¾è®¡**: [CTokenizeråˆ†è¯è®¾è®¡.md](../modules/CTokenizeråˆ†è¯è®¾è®¡.md)
- **ä»£ç å®ç°**: include/cllm/tokenizer/ å’Œ src/tokenizer/ ç›®å½•
- **æµ‹è¯•ä»£ç **: tests/test_tokenizer.cpp
- **æ„å»ºé…ç½®**: CMakeLists.txt

### å®¡æŸ¥æ–¹æ³•

- è®¾è®¡æ–‡æ¡£ä¸ä»£ç å®ç°çš„ä¸€è‡´æ€§æ£€æŸ¥
- ä»£ç ç»“æ„åˆ†æ
- æ¥å£å®Œæ•´æ€§éªŒè¯
- å®ç°çŠ¶æ€è¯„ä¼°
- æµ‹è¯•è¦†ç›–åº¦åˆ†æ

---

## è®¾è®¡æ–‡æ¡£ä¸ä»£ç ä¸€è‡´æ€§åˆ†æ

### è®¾è®¡æ–‡æ¡£æ¦‚è§ˆ

[Tokenizeræ¨¡å—è®¾è®¡.md](../modules/Tokenizeræ¨¡å—è®¾è®¡.md)æå‡ºäº†åŒæ–¹æ¡ˆç­–ç•¥ï¼š

1. **tokenizers-cppï¼ˆæ¨èï¼‰** - Hugging Face tokenizerçš„C++å®ç°
2. **è‡ªç ”CTokenizer** - åŸºäºSentencePieceçš„è‡ªç ”åˆ†è¯å™¨

### æ¶æ„è®¾è®¡

è®¾è®¡æ–‡æ¡£å®šä¹‰äº†ä»¥ä¸‹æ ¸å¿ƒæ¥å£ï¼š

#### 3.1 ITokenizeræ¥å£

```cpp
class ITokenizer {
public:
    virtual ~ITokenizer() {}
    
    virtual std::vector<int> encode(const std::string& text, bool addSpecialTokens = false) = 0;
    virtual std::string decode(const std::vector<int>& tokenIds, bool skipSpecialTokens = true) = 0;
    
    virtual int getVocabSize() const = 0;
    virtual std::string getTokenText(int tokenId) const = 0;
    
    virtual void loadModel(const std::string& modelPath) = 0;
    virtual void unloadModel() = 0;
    
    virtual bool isLoaded() const = 0;
};
```

#### 3.2 TokenizerManageræ¥å£

```cpp
class ITokenizerManager {
public:
    virtual ~ITokenizerManager() {}
    
    virtual std::vector<int> encode(const std::string& text) = 0;
    virtual std::string decode(const std::vector<int>& tokenIds) = 0;
    
    virtual std::string generate(
        const std::string& requestId,
        const std::string& prompt,
        int maxTokens = 100,
        float temperature = 0.7f,
        float topP = 0.9f
    ) = 0;
    
    virtual std::vector<GenerationResponse> generateStream(
        const std::string& requestId,
        const std::string& prompt,
        int maxTokens = 100,
        float temperature = 0.7f,
        float topP = 0.9f
    ) = 0;
    
    virtual TokenizerStats getStats() const = 0;
    virtual void resetStats() = 0;
};
```

### ä»£ç å®ç°åˆ†æ

#### å®é™…æ¥å£å®šä¹‰

ä»£ç ä¸­å­˜åœ¨å¤šä¸ªåŸºç±»å®šä¹‰ï¼š

1. **CTokenizeråŸºç±»** ([include/cllm/CTokenizer/tokenizer.h](../include/cllm/CTokenizer/tokenizer.h))
   ```cpp
   class CTokenizer {
   public:
       virtual ~CTokenizer() = default;
       
       virtual std::vector<llama_token> encode(
           const std::string& text, 
           bool addSpecialTokens = true
       ) = 0;
       
       virtual std::string decode(
           const std::vector<llama_token>& ids,
           bool skipSpecialTokens = true
       ) = 0;
       
       // ... å…¶ä»–æ¥å£
   };
   ```

2. **TokenizerBaseåŸºç±»** ([include/cllm/tokenizer/tokenizer_base.h](../include/cllm/tokenizer/tokenizer_base.h))
   ```cpp
   class TokenizerBase {
   public:
       virtual ~TokenizerBase() = default;
       
       virtual std::vector<int> encode(const std::string& text, bool addSpecialTokens = false) = 0;
       virtual std::string decode(const std::vector<int>& tokenIds, bool skipSpecialTokens = true) = 0;
       virtual int getVocabSize() const = 0;
       virtual std::string getTokenText(int tokenId) const = 0;
       virtual bool isSpecialToken(int tokenId) const = 0;
   };
   ```

3. **ITokenizeræ¥å£** - ä»…å­˜åœ¨äºè®¾è®¡æ–‡æ¡£ä¸­ï¼Œæœªæ‰¾åˆ°å®é™…å¤´æ–‡ä»¶

#### ä¸€è‡´æ€§é—®é¢˜

| è®¾è®¡æ–‡æ¡£ | ä»£ç å®ç° | ä¸€è‡´æ€§ |
|---------|---------|--------|
| ITokenizer | CTokenizer, TokenizerBase | âŒ ä¸ä¸€è‡´ |
| ITokenizerManager | TokenizerManager | âš ï¸ éƒ¨åˆ†ä¸€è‡´ |
| StreamGenerator | æœªæ‰¾åˆ°å®ç° | âŒ ç¼ºå¤± |

### ModelTypeæšä¸¾ä¸ä¸€è‡´

#### CTokenizerä¸­çš„ModelType

[include/cllm/CTokenizer/tokenizer.h](../include/cllm/CTokenizer/tokenizer.h#L14)

```cpp
enum class ModelType {
    AUTO,           // è‡ªåŠ¨æ£€æµ‹
    QWEN,           // Qwenç³»åˆ—æ¨¡å‹
    QWEN2,          // Qwen2ç³»åˆ—æ¨¡å‹
    DEEPSEEK_LLM,   // DeepSeek LLMæ¨¡å‹
    DEEPSEEK_CODER, // DeepSeek Coderæ¨¡å‹
    DEEPSEEK3_LLM,  // DeepSeek3 LLMæ¨¡å‹
    LLAMA,          // Llamaç³»åˆ—æ¨¡å‹
    BERT,           // BERTç³»åˆ—æ¨¡å‹
    GPT2,           // GPT2ç³»åˆ—æ¨¡å‹
    SPM,            // SentencePieceæ¨¡å‹
    BPE,            // BPEæ¨¡å‹
    WPM             // WordPieceæ¨¡å‹
};
```

#### UnifiedTokenizerä¸­çš„ModelType

[include/cllm/tokenizer/unified_tokenizer.h](../include/cllm/tokenizer/unified_tokenizer.h#L17)

```cpp
enum ModelType {
    AUTO,           ///< è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹
    QWEN,           ///< Qwenæ¨¡å‹
    DEEPSEEK_LLM,   ///< DeepSeek LLMæ¨¡å‹
    DEEPSEEK_CODER, ///< DeepSeek Coderæ¨¡å‹
    DEEPSEEK3_LLM,  ///< DeepSeek3 LLMæ¨¡å‹
    BPE,            ///< BPEæ¨¡å‹ï¼ˆé€šç”¨ï¼‰
    SPM,            ///< SentencePieceæ¨¡å‹ï¼ˆé€šç”¨ï¼‰
    WPM             ///< WordPieceæ¨¡å‹ï¼ˆé€šç”¨ï¼‰
};
```

**å·®å¼‚**: UnifiedTokenizerç¼ºå°‘ QWEN2, LLAMA, BERT, GPT2 ç±»å‹

---

## tokenizers-cppé›†æˆæ–¹æ¡ˆåˆ†æ

### é›†æˆæ¶æ„

#### HFTokenizerå®ç°

[include/cllm/tokenizer/hf_tokenizer.h](../include/cllm/tokenizer/hf_tokenizer.h)

```cpp
class HFTokenizer : public ITokenizer {
public:
    explicit HFTokenizer(ModelType modelType);
    ~HFTokenizer() override;

    // ITokenizeræ¥å£å®ç°
    bool load(const std::string& modelPath) override;
    std::vector<int> encode(const std::string& text, bool addSpecialTokens) override;
    std::string decode(const std::vector<int>& ids, bool skipSpecialTokens) override;
    
    int getVocabSize() const override;
    std::string idToToken(int id) const override;
    int tokenToId(const std::string& token) const override;
    
    int getBosId() const override;
    int getEosId() const override;
    int getPadId() const override;
    int getUnkId() const override;
    
    ModelType getModelType() const override;

private:
    void loadSpecialTokens(const std::string& configPath);

    std::unique_ptr<tokenizers::Tokenizer> tokenizer_;
    ModelType modelType_;
    
    // ç‰¹æ®ŠToken IDs
    int bosId_ = -1;
    int eosId_ = -1;
    int padId_ = -1;
    int unkId_ = -1;
};
```

#### å®ç°åˆ†æ

[src/tokenizer/hf_tokenizer.cpp](../src/tokenizer/hf_tokenizer.cpp)

```cpp
bool HFTokenizer::load(const std::string& modelPath) {
    try {
        // åŠ è½½tokenizer.json
        tokenizer_ = tokenizers::Tokenizer::FromFile(modelPath + "/tokenizer.json");
        
        // åŠ è½½ç‰¹æ®Štokené…ç½®
        loadSpecialTokens(modelPath + "/config.json");
        return true;
    } catch (const std::exception& e) {
        // è®°å½•é”™è¯¯æ—¥å¿—
        return false;
    }
}

std::vector<int> HFTokenizer::encode(const std::string& text, bool addSpecialTokens) {
    auto encoding = tokenizer_->Encode(text, addSpecialTokens);
    return encoding.GetIds();
}

std::string HFTokenizer::decode(const std::vector<int>& ids, bool skipSpecialTokens) {
    return tokenizer_->Decode(ids, skipSpecialTokens);
}
```

### æ„å»ºé…ç½®

[CMakeLists.txt](../CMakeLists.txt)

```cmake
option(USE_TOKENIZERS_CPP "Use tokenizers-cpp library" ON)

if(USE_TOKENIZERS_CPP)
    find_package(tokenizers_cpp REQUIRED)
    target_link_libraries(cllm PRIVATE tokenizers_cpp::tokenizers_cpp)
    target_compile_definitions(cllm PRIVATE USE_TOKENIZERS_CPP)
endif()
```

### è¯„ä¼°ç»“æœ

| è¯„ä¼°é¡¹ | çŠ¶æ€ | è¯´æ˜ |
|-------|------|------|
| æ¥å£è®¾è®¡ | âœ… è‰¯å¥½ | æ¸…æ™°çš„æŠ½è±¡æ¥å£ |
| å®ç°å®Œæ•´æ€§ | âš ï¸ éƒ¨åˆ† | åŸºæœ¬åŠŸèƒ½å·²å®ç° |
| é”™è¯¯å¤„ç† | âš ï¸ åŸºç¡€ | æœ‰å¼‚å¸¸æ•è·ä½†æ—¥å¿—ä¸å®Œæ•´ |
| æµ‹è¯•è¦†ç›– | âŒ ä¸è¶³ | æµ‹è¯•è¢«è·³è¿‡ |
| æ–‡æ¡£å®Œæ•´æ€§ | âœ… è‰¯å¥½ | æœ‰è¯¦ç»†æ³¨é‡Š |

---

## CTokenizerå®ç°è¯„ä¼°

### æ¶æ„è®¾è®¡

#### ç»§æ‰¿å±‚æ¬¡

```
CTokenizer (åŸºç±»)
â”œâ”€â”€ SentencePieceTokenizer
â”‚   â”œâ”€â”€ QwenTokenizer
â”‚   â””â”€â”€ DeepSeekTokenizer
â””â”€â”€ LlamaTokenizer
```

#### æ ¸å¿ƒç»„ä»¶

1. **CTokenizeråŸºç±»** ([include/cllm/CTokenizer/tokenizer.h](../include/cllm/CTokenizer/tokenizer.h))
   - å®šä¹‰äº†ç»Ÿä¸€çš„åˆ†è¯å™¨æ¥å£
   - æ”¯æŒå¤šç§æ¨¡å‹ç±»å‹
   - æä¾›ç‰¹æ®Štokenç®¡ç†

2. **SentencePieceTokenizer** ([include/cllm/CTokenizer/sentencepiece_tokenizer.h](../include/cllm/CTokenizer/sentencepiece_tokenizer.h))
   - åŸºäºSentencePieceçš„é€šç”¨åˆ†è¯å™¨
   - æ”¯æŒBPEã€Unigramã€WordPieceç®—æ³•
   - æä¾›æ¨¡å‹é…ç½®åŠ è½½

3. **QwenTokenizer** ([include/cllm/CTokenizer/qwen_tokenizer.h](../include/cllm/CTokenizer/qwen_tokenizer.h))
   - Qwenæ¨¡å‹ä¸“ç”¨åˆ†è¯å™¨
   - æ”¯æŒFIMï¼ˆFill-in-the-Middleï¼‰å¤„ç†
   - ç‰¹æ®Šé¢„å¤„ç†é€»è¾‘

4. **DeepSeekTokenizer** ([include/cllm/CTokenizer/deepseek_tokenizer.h](../include/cllm/CTokenizer/deepseek_tokenizer.h))
   - DeepSeekæ¨¡å‹ä¸“ç”¨åˆ†è¯å™¨
   - æ”¯æŒLLMã€Coderã€DeepSeek3ä¸‰ç§å˜ä½“
   - æ¨¡å‹ç‰¹å®šçš„é¢„å¤„ç†

5. **LlamaTokenizer** ([include/cllm/CTokenizer/llama_tokenizer.h](../include/cllm/CTokenizer/llama_tokenizer.h))
   - Llamaæ¨¡å‹ä¸“ç”¨åˆ†è¯å™¨
   - åŸºäºllama.cppçš„è¯æ±‡è¡¨

### å®ç°çŠ¶æ€

| ç»„ä»¶ | å¤´æ–‡ä»¶ | å®ç°æ–‡ä»¶ | çŠ¶æ€ |
|-----|--------|---------|------|
| CTokenizer | âœ… | - | âœ… åŸºç±» |
| SentencePieceTokenizer | âœ… | âŒ | âš ï¸ ç¼ºå®ç° |
| QwenTokenizer | âœ… | âœ… | âš ï¸ éƒ¨åˆ†å®ç° |
| DeepSeekTokenizer | âœ… | âœ… | âš ï¸ éƒ¨åˆ†å®ç° |
| LlamaTokenizer | âœ… | âŒ | âš ï¸ ç¼ºå®ç° |

### åŠŸèƒ½ç‰¹æ€§

#### QwenTokenizerçš„FIMå¤„ç†

[src/CTokenizer/qwen_tokenizer.cpp](../src/CTokenizer/qwen_tokenizer.cpp#L18)

```cpp
bool QwenTokenizer::needsFimProcessing(const std::string& text) {
    // æ£€æŸ¥æ˜¯å¦éœ€è¦FIMå¤„ç†
    // Qwenæ¨¡å‹ç‰¹æœ‰çš„FIM tokens: <|fim_begin|>, <|fim_end|>, 
    return text.find("<|fim_begin|>") != std::string::npos || 
           text.find("<|fim_end|>") != std::string::npos ||
           text.find("``") != std::string::npos ||
           text.find("<|fim_suf|>") != std::string::npos ||
           text.find("<|fim_pre|>") != std::string::npos;
}

std::vector<llama_token> QwenTokenizer::encodeWithFim(const std::string& text, bool addSpecialTokens) {
    // å®ç°Qwençš„FIMï¼ˆFill-in-the-Middleï¼‰å¤„ç†é€»è¾‘
    // è¿™é‡Œéœ€è¦è¯†åˆ«FIMç›¸å…³çš„ç‰¹æ®Šæ ‡è®°å¹¶è¿›è¡Œç›¸åº”å¤„ç†
    
    // æŸ¥æ‰¾FIMæ ‡è®°
    std::string fim_begin = "<|fim_begin|>";
    std::string fim_suffix = "<|fim_suf|>";
    std::string fim_end = "<|fim_end|>";
    
    // åœ¨Qwenæ¨¡å‹ä¸­ï¼ŒFIMæ ¼å¼é€šå¸¸æ˜¯ï¼š``...```
    std::string fim_prefix = "<|fim_pre|>";
    std::string fim_middle = "``";
    
    // ... FIMå¤„ç†é€»è¾‘
}
```

#### DeepSeekTokenizerçš„å¤šæ¨¡å‹æ”¯æŒ

[src/CTokenizer/deepseek_tokenizer.cpp](../src/CTokenizer/deepseek_tokenizer.cpp#L8)

```cpp
std::string DeepSeekTokenizer::applyDeepSeekPreprocessing(const std::string& text) {
    // DeepSeekç‰¹å®šçš„é¢„å¤„ç†é€»è¾‘
    // æ ¹æ®æ¨¡å‹ç±»å‹åº”ç”¨ä¸åŒçš„æ­£åˆ™è¡¨è¾¾å¼
    switch(getModelType()) {
        case ModelType::DEEPSEEK_LLM:
            return applyDeepSeekLLMPreprocessing(text);
        case ModelType::DEEPSEEK_CODER:
            return applyDeepSeekCoderPreprocessing(text);
        case ModelType::DEEPSEEK3_LLM:
            return applyDeepSeek3Preprocessing(text);
        default:
            return text;
    }
}
```

### è¯„ä¼°ç»“æœ

| è¯„ä¼°é¡¹ | çŠ¶æ€ | è¯´æ˜ |
|-------|------|------|
| æ¶æ„è®¾è®¡ | âœ… ä¼˜ç§€ | æ¸…æ™°çš„ç»§æ‰¿å±‚æ¬¡ |
| æ¨¡å‹æ”¯æŒ | âœ… è‰¯å¥½ | æ”¯æŒQwenã€DeepSeekã€Llama |
| ç‰¹æ®ŠåŠŸèƒ½ | âœ… ä¼˜ç§€ | FIMã€å¤šæ¨¡å‹é¢„å¤„ç† |
| å®ç°å®Œæ•´æ€§ | âš ï¸ ä¸è¶³ | å¤šä¸ªç»„ä»¶ç¼ºå°‘å®ç° |
| ä»£ç è´¨é‡ | âœ… è‰¯å¥½ | ç»“æ„æ¸…æ™°ï¼Œæ³¨é‡Šå®Œæ•´ |

---

## å…³é”®é—®é¢˜è¯†åˆ«

### ä¸¥é‡é—®é¢˜ï¼ˆP0ï¼‰

#### 1. ITokenizeræ¥å£ç¼ºå¤± âš ï¸

**é—®é¢˜æè¿°**:
- [NativeTokenizer](../include/cllm/tokenizer/native_tokenizer.h)å’Œ[HFTokenizer](../include/cllm/tokenizer/hf_tokenizer.h)éƒ½ç»§æ‰¿è‡ªITokenizer
- ä½†ITokenizeræ¥å£åªå­˜åœ¨äºè®¾è®¡æ–‡æ¡£ä¸­ï¼Œæ²¡æœ‰å®é™…å¤´æ–‡ä»¶
- å¯¼è‡´ç¼–è¯‘é”™è¯¯

**å½±å“**: æ— æ³•ç¼–è¯‘ï¼Œé˜»å¡å¼€å‘

**ç›¸å…³æ–‡ä»¶**:
- include/cllm/tokenizer/native_tokenizer.h
- include/cllm/tokenizer/hf_tokenizer.h
- docs/modules/Tokenizeræ¨¡å—è®¾è®¡.md

#### 2. ModelTypeæšä¸¾ä¸ä¸€è‡´ âš ï¸

**é—®é¢˜æè¿°**:
- CTokenizerçš„ModelTypeåŒ…å«12ç§ç±»å‹
- UnifiedTokenizerçš„ModelTypeåªåŒ…å«8ç§ç±»å‹
- ç¼ºå°‘QWEN2, LLAMA, BERT, GPT2

**å½±å“**: ç±»å‹ä¸åŒ¹é…ï¼ŒåŠŸèƒ½å—é™

**ç›¸å…³æ–‡ä»¶**:
- include/cllm/CTokenizer/tokenizer.h
- include/cllm/tokenizer/unified_tokenizer.h

#### 3. UnifiedTokenizerå®ç°ä¸å®Œæ•´ âš ï¸

**é—®é¢˜æè¿°**:
- [encode](../src/tokenizer/unified_tokenizer.cpp#L239)å’Œ[decode](../src/tokenizer/unified_tokenizer.cpp#L261)æ–¹æ³•ä½¿ç”¨æ¨¡æ‹Ÿå®ç°
- å£°æ˜äº†llama.cppçš„å‡½æ•°ä½†æœªçœŸæ­£è°ƒç”¨
- è¿”å›æ¨¡æ‹Ÿæ•°æ®è€ŒéçœŸå®åˆ†è¯ç»“æœ

**å½±å“**: æ— æ³•æ­£å¸¸å·¥ä½œ

**ç›¸å…³æ–‡ä»¶**:
- src/tokenizer/unified_tokenizer.cpp

### ä¸­ç­‰é—®é¢˜ï¼ˆP1ï¼‰

#### 4. TokenizerManagerå®ç°ä¸å®Œæ•´

**é—®é¢˜æè¿°**:
- [manager.cpp](../src/tokenizer/manager.cpp#L45)è°ƒç”¨äº†loadStopTokensä½†æœªå®ç°
- åœæ­¢è¯åŠŸèƒ½ç¼ºå¤±

**å½±å“**: åŠŸèƒ½ä¸å®Œæ•´

**ç›¸å…³æ–‡ä»¶**:
- src/tokenizer/manager.cpp

#### 5. CTokenizerå®ç°æ–‡ä»¶ç¼ºå¤±

**é—®é¢˜æè¿°**:
- [LlamaTokenizer](../include/cllm/CTokenizer/llama_tokenizer.h)å’Œ[SentencePieceTokenizer](../include/cllm/CTokenizer/sentencepiece_tokenizer.h)åªæœ‰å¤´æ–‡ä»¶
- ç¼ºå°‘å¯¹åº”çš„.cppå®ç°æ–‡ä»¶

**å½±å“**: æ— æ³•ä½¿ç”¨è¿™äº›åˆ†è¯å™¨

**ç›¸å…³æ–‡ä»¶**:
- include/cllm/CTokenizer/llama_tokenizer.h
- include/cllm/CTokenizer/sentencepiece_tokenizer.h

#### 6. æ¨¡å‹ç‰¹å®šåˆ†è¯å™¨å®ç°ä¸å®Œæ•´

**é—®é¢˜æè¿°**:
- [QwenTokenizer](../src/CTokenizer/qwen_tokenizer.cpp)çš„applyQwenPreprocessingä¸ºç©º
- [DeepSeekTokenizer](../src/CTokenizer/deepseek_tokenizer.cpp)çš„é¢„å¤„ç†é€»è¾‘ä¸ºç©º
- æ­£åˆ™è¡¨è¾¾å¼æœªå®ç°

**å½±å“**: æ¨¡å‹ç‰¹å®šåŠŸèƒ½ç¼ºå¤±

**ç›¸å…³æ–‡ä»¶**:
- src/CTokenizer/qwen_tokenizer.cpp
- src/CTokenizer/deepseek_tokenizer.cpp

### ä½ä¼˜å…ˆçº§é—®é¢˜ï¼ˆP2ï¼‰

#### 7. æ¶æ„è®¾è®¡å†—ä½™

**é—®é¢˜æè¿°**:
- å­˜åœ¨å¤šä¸ªåŸºç±»ï¼šCTokenizerã€TokenizerBaseã€ITokenizerï¼ˆè®¾è®¡æ–‡æ¡£ï¼‰
- æ¥å£å®šä¹‰é‡å¤ï¼ŒèŒè´£ä¸æ¸…

**å½±å“**: ç»´æŠ¤å¤æ‚åº¦å¢åŠ 

**ç›¸å…³æ–‡ä»¶**:
- include/cllm/CTokenizer/tokenizer.h
- include/cllm/tokenizer/tokenizer_base.h
- docs/modules/Tokenizeræ¨¡å—è®¾è®¡.md

#### 8. æµ‹è¯•è¦†ç›–ä¸è¶³

**é—®é¢˜æè¿°**:
- [test_tokenizer.cpp](../tests/test_tokenizer.cpp)ä¸­å¤§é‡ä½¿ç”¨GTEST_SKIP()
- ç¼ºå°‘å®é™…æ¨¡å‹æ–‡ä»¶è¿›è¡Œæµ‹è¯•
- æ— æ³•éªŒè¯åŠŸèƒ½æ­£ç¡®æ€§

**å½±å“**: è´¨é‡ä¿è¯ä¸è¶³

**ç›¸å…³æ–‡ä»¶**:
- tests/test_tokenizer.cpp

---

## æ”¹è¿›å»ºè®®

### ç«‹å³ä¿®å¤ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰

#### 1. åˆ›å»ºITokenizeræ¥å£å¤´æ–‡ä»¶

**å»ºè®®æ–‡ä»¶**: `include/cllm/tokenizer/i_tokenizer.h`

```cpp
#pragma once

#include <string>
#include <vector>

namespace cllm {

class ITokenizer {
public:
    virtual ~ITokenizer() = default;
    
    virtual bool load(const std::string& modelPath) = 0;
    virtual std::vector<int> encode(const std::string& text, bool addSpecialTokens) = 0;
    virtual std::string decode(const std::vector<int>& ids, bool skipSpecialTokens) = 0;
    
    virtual int getVocabSize() const = 0;
    virtual std::string idToToken(int id) const = 0;
    virtual int tokenToId(const std::string& token) const = 0;
    
    virtual int getBosId() const = 0;
    virtual int getEosId() const = 0;
    virtual int getPadId() const = 0;
    virtual int getUnkId() const = 0;
    
    virtual ModelType getModelType() const = 0;
};

} // namespace cllm
```

#### 2. ç»Ÿä¸€ModelTypeæšä¸¾å®šä¹‰

**å»ºè®®æ–‡ä»¶**: `include/cllm/tokenizer/model_type.h`

```cpp
#pragma once

namespace cllm {

enum class ModelType {
    AUTO,
    QWEN,
    QWEN2,
    DEEPSEEK_LLM,
    DEEPSEEK_CODER,
    DEEPSEEK3_LLM,
    LLAMA,
    BERT,
    GPT2,
    SPM,
    BPE,
    WPM
};

} // namespace cllm
```

**ä¿®æ”¹æ–‡ä»¶**:
- include/cllm/CTokenizer/tokenizer.h
- include/cllm/tokenizer/unified_tokenizer.h
- include/cllm/tokenizer/native_tokenizer.h
- include/cllm/tokenizer/hf_tokenizer.h

#### 3. å®ç°UnifiedTokenizerçš„çœŸå®åˆ†è¯é€»è¾‘

**ä¿®æ”¹æ–‡ä»¶**: `src/tokenizer/unified_tokenizer.cpp`

```cpp
std::vector<int> UnifiedTokenizer::encode(const std::string& text, bool addSpecialTokens) {
    if (!tokenizerImpl_ || !tokenizerImpl_->vocab) {
        throw std::runtime_error("Tokenizer not initialized");
    }
    
    std::vector<int> tokens;
    tokens.resize(text.length() * 4); // é¢„åˆ†é…è¶³å¤Ÿç©ºé—´
    
    int n_tokens = llama_tokenize(
        tokenizerImpl_->vocab,
        text.c_str(),
        text.length(),
        tokens.data(),
        tokens.size(),
        addSpecialTokens,
        true
    );
    
    if (n_tokens < 0) {
        throw std::runtime_error("Tokenization failed");
    }
    
    tokens.resize(n_tokens);
    return tokens;
}

std::string UnifiedTokenizer::decode(const std::vector<int>& tokenIds, bool skipSpecialTokens) {
    if (!tokenizerImpl_ || !tokenizerImpl_->vocab) {
        throw std::runtime_error("Tokenizer not initialized");
    }
    
    std::string text;
    text.resize(tokenIds.size() * 4); // é¢„åˆ†é…è¶³å¤Ÿç©ºé—´
    
    int n_chars = llama_detokenize(
        tokenizerImpl_->vocab,
        tokenIds.data(),
        tokenIds.size(),
        text.data(),
        text.size(),
        skipSpecialTokens,
        true
    );
    
    if (n_chars < 0) {
        throw std::runtime_error("Detokenization failed");
    }
    
    text.resize(n_chars);
    return text;
}
```

### çŸ­æœŸæ”¹è¿›ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰

#### 4. å®ç°TokenizerManagerçš„loadStopTokens

**ä¿®æ”¹æ–‡ä»¶**: `src/tokenizer/manager.cpp`

```cpp
void TokenizerManager::loadStopTokens(const std::string& modelPath) {
    std::string configPath = modelPath + "/config.json";
    std::ifstream f(configPath);
    if (!f.is_open()) return;
    
    auto config = nlohmann::json::parse(f);
    
    if (config.contains("stop_tokens")) {
        stopTokens_ = config["stop_tokens"].get<std::vector<std::string>>();
    }
    
    // è½¬æ¢ä¸ºtoken IDs
    for (const auto& stopToken : stopTokens_) {
        int tokenId = tokenizer_->tokenToId(stopToken);
        if (tokenId >= 0) {
            stopTokenIds_.push_back(tokenId);
        }
    }
}
```

#### 5. å®ç°LlamaTokenizerå’ŒSentencePieceTokenizer

**åˆ›å»ºæ–‡ä»¶**: `src/CTokenizer/llama_tokenizer.cpp`

```cpp
#include "cllm/CTokenizer/llama_tokenizer.h"
#include <llama.h>

namespace cllm {

LlamaTokenizer::LlamaTokenizer(ModelType modelType) 
    : modelType_(modelType), vocab_(nullptr), context_(nullptr) {
    bosId_ = -1;
    eosId_ = -1;
    padId_ = -1;
    unkId_ = -1;
}

LlamaTokenizer::~LlamaTokenizer() {
    // æ¸…ç†llama_vocabå’Œllama_context
}

bool LlamaTokenizer::load(const std::string& modelPath) {
    // å®ç°llama_vocabåŠ è½½é€»è¾‘
    // è¿™é‡Œéœ€è¦è°ƒç”¨llama.cppçš„ç›¸åº”å‡½æ•°
    return true;
}

std::vector<llama_token> LlamaTokenizer::encode(
    const std::string& text, 
    bool addSpecialTokens
) {
    // å®ç°ç¼–ç é€»è¾‘
    return {};
}

std::string LlamaTokenizer::decode(
    const std::vector<llama_token>& ids,
    bool skipSpecialTokens
) {
    // å®ç°è§£ç é€»è¾‘
    return "";
}

int LlamaTokenizer::getVocabSize() const {
    return vocab_ ? llama_vocab_n_tokens(vocab_) : 0;
}

std::string LlamaTokenizer::idToToken(llama_token id) const {
    return vocab_ ? llama_vocab_get_text(vocab_, id) : "";
}

llama_token LlamaTokenizer::tokenToId(const std::string& token) const {
    // å®ç°tokenåˆ°IDçš„æ˜ å°„
    return -1;
}

} // namespace cllm
```

**åˆ›å»ºæ–‡ä»¶**: `src/CTokenizer/sentencepiece_tokenizer.cpp`

```cpp
#include "cllm/CTokenizer/sentencepiece_tokenizer.h"
#include <sentencepiece_processor.h>

namespace cllm {

SentencePieceTokenizer::SentencePieceTokenizer(ModelType modelType) 
    : modelType_(modelType), bosId_(-1), eosId_(-1), padId_(-1), unkId_(-1) {
    processor_ = std::make_unique<sentencepiece::SentencePieceProcessor>();
}

SentencePieceTokenizer::~SentencePieceTokenizer() = default;

bool SentencePieceTokenizer::load(const std::string& modelPath) {
    // åŠ è½½SentencePieceæ¨¡å‹
    std::string spModelPath = modelPath;
    if (spModelPath.back() != '/') spModelPath += '/';
    spModelPath += "tokenizer.model";
    
    auto status = processor_->Load(spModelPath);
    if (!status.ok()) {
        return false;
    }
    
    // åŠ è½½ç‰¹æ®Štokené…ç½®
    loadSpecialTokens(modelPath + "/config.json");
    
    // åˆå§‹åŒ–æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
    initializeRegexPatterns();
    
    return true;
}

std::vector<llama_token> SentencePieceTokenizer::encode(
    const std::string& text, 
    bool addSpecialTokens
) {
    std::vector<int> ids;
    auto status = processor_->Encode(text, &ids);
    if (!status.ok()) return {};
    
    if (addSpecialTokens) {
        if (bosId_ >= 0) ids.insert(ids.begin(), bosId_);
        if (eosId_ >= 0) ids.push_back(eosId_);
    }
    
    return ids;
}

std::string SentencePieceTokenizer::decode(
    const std::vector<llama_token>& ids,
    bool skipSpecialTokens
) {
    std::vector<int> filteredIds;
    for (int id : ids) {
        if (!skipSpecialTokens || (id != bosId_ && id != eosId_ && id != padId_)) {
            filteredIds.push_back(id);
        }
    }
    
    std::string text;
    auto status = processor_->Decode(filteredIds, &text);
    return status.ok() ? text : "";
}

int SentencePieceTokenizer::getVocabSize() const {
    return processor_ ? processor_->GetPieceSize() : 0;
}

std::string SentencePieceTokenizer::idToToken(llama_token id) const {
    return processor_ ? processor_->IdToPiece(id) : "[UNK]";
}

llama_token SentencePieceTokenizer::tokenToId(const std::string& token) const {
    return processor_ ? processor_->PieceToId(token) : unkId_;
}

void SentencePieceTokenizer::loadModelConfig(const std::string& configPath) {
    std::ifstream f(configPath);
    if (!f.is_open()) return;
    
    auto config = nlohmann::json::parse(f);
    // åŠ è½½æ¨¡å‹é…ç½®
}

void SentencePieceTokenizer::loadSpecialTokens(const std::string& configPath) {
    std::ifstream f(configPath);
    if (!f.is_open()) return;
    
    auto config = nlohmann::json::parse(f);
    
    if (config.contains("bos_token_id")) bosId_ = config["bos_token_id"];
    if (config.contains("eos_token_id")) eosId_ = config["eos_token_id"];
    if (config.contains("pad_token_id")) padId_ = config["pad_token_id"];
    if (config.contains("unk_token_id")) unkId_ = config["unk_token_id"];
    
    // ä»added_tokens_decoderåŠ è½½ç‰¹æ®Štoken
    if (config.contains("added_tokens_decoder")) {
        auto tokens = config["added_tokens_decoder"];
        for (auto& item : tokens.items()) {
            int tokenId = std::stoi(item.key());
            if (item.value().contains("content")) {
                std::string content = item.value()["content"];
                specialTokens_[content] = tokenId;
                idToTokenMap_[tokenId] = content;
            }
        }
    }
}

void SentencePieceTokenizer::initializeRegexPatterns() {
    // æ ¹æ®æ¨¡å‹ç±»å‹åˆå§‹åŒ–æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
    switch(modelType_) {
        case ModelType::QWEN:
            // Qwençš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
            break;
        case ModelType::DEEPSEEK_LLM:
        case ModelType::DEEPSEEK_CODER:
        case ModelType::DEEPSEEK3_LLM:
            // DeepSeekçš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
            break;
        default:
            // é»˜è®¤æ¨¡å¼
            break;
    }
}

} // namespace cllm
```

#### 6. å®Œå–„æ¨¡å‹ç‰¹å®šåˆ†è¯å™¨çš„é¢„å¤„ç†é€»è¾‘

**ä¿®æ”¹æ–‡ä»¶**: `src/CTokenizer/qwen_tokenizer.cpp`

```cpp
std::string QwenTokenizer::applyQwenPreprocessing(const std::string& text) {
    // Qwen2ä½¿ç”¨çš„é¢„å¤„ç†é€»è¾‘
    // æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼š
    // - "(?:'[sS]|'[tT]|'[rR][eE]|'[vV][eE]|'[mM]|'[lL][lL]|'[dD])": åŒ¹é…è‹±è¯­ç¼©å†™
    // - "[^\r\n\p{L}\p{N}]?\p{L}+": åŒ¹é…å­—æ¯åºåˆ—
    // - "\p{N}": åŒ¹é…æ•°å­—
    // - å¤æ‚çš„ç©ºç™½å’Œæ ‡ç‚¹å¤„ç†æ¨¡å¼
    
    std::string result = text;
    
    // å®ç°Qwenç‰¹å®šçš„æ­£åˆ™è¡¨è¾¾å¼é¢„å¤„ç†
    // è¿™é‡Œéœ€è¦æ ¹æ®Qwen2çš„å®é™…å®ç°æ¥å®Œå–„
    
    return result;
}
```

**ä¿®æ”¹æ–‡ä»¶**: `src/CTokenizer/deepseek_tokenizer.cpp`

```cpp
std::string DeepSeekTokenizer::applyDeepSeekLLMPreprocessing(const std::string& text) {
    // DeepSeek LLMä½¿ç”¨çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼š
    // - "[\r\n]": åŒ¹é…æ¢è¡Œç¬¦
    // - "\\s?[A-Za-z...]": åŒ¹é…å­—æ¯å­—ç¬¦
    // - "\\s?[!-/:-~...]": åŒ¹é…æ ‡ç‚¹ç¬¦å·
    // - "[ä¸€-é¾¥...]": åŒ¹é…ä¸­æ–‡å­—ç¬¦
    // - "\\p{N}+": åŒ¹é…æ•°å­—
    
    std::string result = text;
    
    // å®ç°DeepSeek LLMç‰¹å®šçš„æ­£åˆ™è¡¨è¾¾å¼é¢„å¤„ç†
    
    return result;
}

std::string DeepSeekTokenizer::applyDeepSeekCoderPreprocessing(const std::string& text) {
    // DeepSeek Coderä½¿ç”¨çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼š
    // - "[\r\n]": åŒ¹é…æ¢è¡Œç¬¦
    // - "\\s?\\p{L}+": åŒ¹é…å­—æ¯
    // - "\\s?\\p{P}+": åŒ¹é…æ ‡ç‚¹
    // - "[ä¸€-é¾¥...]": åŒ¹é…ä¸­æ–‡å­—ç¬¦
    // - "\\p{N}": åŒ¹é…æ•°å­—
    
    std::string result = text;
    
    // å®ç°DeepSeek Coderç‰¹å®šçš„æ­£åˆ™è¡¨è¾¾å¼é¢„å¤„ç†
    
    return result;
}

std::string DeepSeekTokenizer::applyDeepSeek3Preprocessing(const std::string& text) {
    // DeepSeek3ä½¿ç”¨çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼š
    // - "\\p{N}{1,3}": åŒ¹é…1-3ä½æ•°å­—
    // - "[ä¸€-é¾¥...]": åŒ¹é…ä¸­æ–‡å­—ç¬¦
    // - å¤æ‚çš„æ··åˆæ¨¡å¼ç”¨äºåŒ¹é…å„ç§å­—ç¬¦ç»„åˆ
    
    std::string result = text;
    
    // å®ç°DeepSeek3ç‰¹å®šçš„æ­£åˆ™è¡¨è¾¾å¼é¢„å¤„ç†
    
    return result;
}
```

### é•¿æœŸä¼˜åŒ–ï¼ˆä½ä¼˜å…ˆçº§ï¼‰

#### 7. ç®€åŒ–æ¶æ„è®¾è®¡

**å»ºè®®**:
- ç»Ÿä¸€ä½¿ç”¨CTokenizerä½œä¸ºåŸºç±»
- ç§»é™¤TokenizerBaseå’ŒITokenizerçš„å†—ä½™å®šä¹‰
- å»ºç«‹æ¸…æ™°çš„ç»§æ‰¿å±‚æ¬¡

**ç›®æ ‡æ¶æ„**:
```
CTokenizer (ç»Ÿä¸€åŸºç±»)
â”œâ”€â”€ NativeTokenizer (è‡ªç ”å®ç°)
â”‚   â”œâ”€â”€ SentencePieceTokenizer
â”‚   â”œâ”€â”€ QwenTokenizer
â”‚   â””â”€â”€ DeepSeekTokenizer
â”œâ”€â”€ HFTokenizer (tokenizers-cppå®ç°)
â””â”€â”€ UnifiedTokenizer (ç»Ÿä¸€æ¥å£)
```

#### 8. å®Œå–„æµ‹è¯•è¦†ç›–

**å»ºè®®**:
- æä¾›æµ‹è¯•æ¨¡å‹æ–‡ä»¶
- å®ç°å®Œæ•´çš„å•å…ƒæµ‹è¯•
- æ·»åŠ é›†æˆæµ‹è¯•
- æ·»åŠ æ€§èƒ½æµ‹è¯•

**æµ‹è¯•æ–‡ä»¶**: `tests/test_tokenizer.cpp`

```cpp
// ç§»é™¤GTEST_SKIPï¼Œå®ç°çœŸå®æµ‹è¯•
TEST_F(NativeTokenizerTest, LoadModel) {
    ASSERT_TRUE(tokenizer_->load("path/to/model"));
    EXPECT_GT(tokenizer_->getVocabSize(), 0);
}

TEST_F(NativeTokenizerTest, EncodeDecode) {
    std::string text = "Hello, world!";
    auto ids = tokenizer_->encode(text, false);
    EXPECT_FALSE(ids.empty());
    
    std::string decoded = tokenizer_->decode(ids, false);
    EXPECT_EQ(text, decoded);
}
```

#### 9. æ›´æ–°è®¾è®¡æ–‡æ¡£

**å»ºè®®**:
- åŒæ­¥æ–‡æ¡£ä¸ä»£ç å®ç°
- æä¾›æ¸…æ™°çš„æ¶æ„å›¾
- æä¾›ä½¿ç”¨ç¤ºä¾‹
- æ·»åŠ APIæ–‡æ¡£

---

## æ€»ç»“

### CTokenizeråˆç†æ€§è¯„ä¼°

#### âœ… åˆç†çš„è®¾è®¡ç‚¹

1. **åŒæ–¹æ¡ˆç­–ç•¥** - tokenizers-cpp + è‡ªç ”CTokenizerï¼Œæä¾›äº†çµæ´»æ€§å’Œå¤‡é€‰æ–¹æ¡ˆ
2. **æ¨¡å‹ç‰¹å®šå¤„ç†** - QwenTokenizeræ”¯æŒFIMï¼ŒDeepSeekTokenizeræ”¯æŒå¤šæ¨¡å‹
3. **ç»Ÿä¸€æ¥å£** - CTokenizeræä¾›äº†æ¸…æ™°çš„æŠ½è±¡æ¥å£
4. **è‡ªåŠ¨æ£€æµ‹** - UnifiedTokenizerçš„detectModelTypeåŠŸèƒ½
5. **æ¶æ„æ¸…æ™°** - ç»§æ‰¿å±‚æ¬¡åˆç†ï¼ŒèŒè´£åˆ†æ˜

#### âš ï¸ éœ€è¦æ”¹è¿›çš„ç‚¹

1. **å®ç°ä¸å®Œæ•´** - å¤šä¸ªå…³é”®ç»„ä»¶åªæœ‰å¤´æ–‡ä»¶æ²¡æœ‰å®ç°
2. **æ¥å£ä¸ä¸€è‡´** - ModelTypeæšä¸¾å’Œæ¥å£å®šä¹‰ä¸ç»Ÿä¸€
3. **ä¾èµ–å…³ç³»å¤æ‚** - UnifiedTokenizerä¾èµ–Qwen2Tokenizerä½†å®ç°ä¸æ˜ç¡®
4. **æµ‹è¯•è¦†ç›–ä¸è¶³** - æ— æ³•éªŒè¯åŠŸèƒ½æ­£ç¡®æ€§
5. **æ¶æ„å†—ä½™** - å­˜åœ¨å¤šä¸ªåŸºç±»ï¼ŒèŒè´£ä¸æ¸…

### æ€»ä½“è¯„ä»·

**è®¾è®¡ç†å¿µ**: âœ… **åˆç†**  
CTokenizerçš„è®¾è®¡ç†å¿µæ˜¯åˆç†çš„ï¼Œæä¾›äº†æ¸…æ™°çš„æ¶æ„å’Œçµæ´»çš„æ‰©å±•æ€§ã€‚åŒæ–¹æ¡ˆç­–ç•¥å’Œæ¨¡å‹ç‰¹å®šå¤„ç†ä½“ç°äº†è‰¯å¥½çš„å·¥ç¨‹å®è·µã€‚

**å®ç°çŠ¶æ€**: âš ï¸ **ä¸å®Œæ•´**  
å½“å‰å®ç°çŠ¶æ€ä¸å®Œæ•´ï¼Œå­˜åœ¨å¤šä¸ªå…³é”®é—®é¢˜éœ€è¦è§£å†³ã€‚ç‰¹åˆ«æ˜¯ITokenizeræ¥å£ç¼ºå¤±ã€ModelTypeä¸ä¸€è‡´ã€UnifiedTokenizerå®ç°ä¸å®Œæ•´ç­‰é—®é¢˜å½±å“åŸºæœ¬åŠŸèƒ½ã€‚

**å»ºè®®ä¼˜å…ˆçº§**: ğŸ”´ **é«˜**  
å»ºè®®ä¼˜å…ˆè§£å†³é«˜ä¼˜å…ˆçº§é—®é¢˜ï¼Œç¡®ä¿åŸºæœ¬åŠŸèƒ½å¯ç”¨åå†è¿›è¡Œä¼˜åŒ–ã€‚

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç«‹å³æ‰§è¡Œ**ï¼ˆæœ¬å‘¨ï¼‰:
   - åˆ›å»ºITokenizeræ¥å£å¤´æ–‡ä»¶
   - ç»Ÿä¸€ModelTypeæšä¸¾å®šä¹‰
   - å®ç°UnifiedTokenizerçš„çœŸå®åˆ†è¯é€»è¾‘

2. **çŸ­æœŸæ‰§è¡Œ**ï¼ˆ2å‘¨å†…ï¼‰:
   - å®ç°TokenizerManagerçš„loadStopTokens
   - å®ç°LlamaTokenizerå’ŒSentencePieceTokenizer
   - å®Œå–„æ¨¡å‹ç‰¹å®šåˆ†è¯å™¨çš„é¢„å¤„ç†é€»è¾‘

3. **é•¿æœŸä¼˜åŒ–**ï¼ˆ1ä¸ªæœˆå†…ï¼‰:
   - ç®€åŒ–æ¶æ„è®¾è®¡
   - å®Œå–„æµ‹è¯•è¦†ç›–
   - æ›´æ–°è®¾è®¡æ–‡æ¡£

### é™„å½•

#### ç›¸å…³æ–‡ä»¶æ¸…å•

**è®¾è®¡æ–‡æ¡£**:
- docs/modules/Tokenizeræ¨¡å—è®¾è®¡.md
- docs/modules/CTokenizeråˆ†è¯è®¾è®¡.md
- docs/research/åˆ†è¯å™¨æŠ€æœ¯è°ƒç ”.md

**å¤´æ–‡ä»¶**:
- include/cllm/tokenizer/tokenizer.h
- include/cllm/tokenizer/tokenizer_base.h
- include/cllm/tokenizer/unified_tokenizer.h
- include/cllm/tokenizer/native_tokenizer.h
- include/cllm/tokenizer/hf_tokenizer.h
- include/cllm/tokenizer/manager.h
- include/cllm/CTokenizer/tokenizer.h
- include/cllm/CTokenizer/sentencepiece_tokenizer.h
- include/cllm/CTokenizer/qwen_tokenizer.h
- include/cllm/CTokenizer/deepseek_tokenizer.h
- include/cllm/CTokenizer/llama_tokenizer.h

**å®ç°æ–‡ä»¶**:
- src/tokenizer/unified_tokenizer.cpp
- src/tokenizer/native_tokenizer.cpp
- src/tokenizer/hf_tokenizer.cpp
- src/tokenizer/manager.cpp
- src/CTokenizer/qwen_tokenizer.cpp
- src/CTokenizer/deepseek_tokenizer.cpp

**æµ‹è¯•æ–‡ä»¶**:
- tests/test_tokenizer.cpp

**æ„å»ºé…ç½®**:
- CMakeLists.txt

---

**æ–‡æ¡£ç»“æŸ**
