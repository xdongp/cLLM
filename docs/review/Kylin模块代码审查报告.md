# Kylinæ¨¡å—ä»£ç å®¡æŸ¥æŠ¥å‘Š

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**å®¡æŸ¥æ—¥æœŸ**: 2026-01-12  
**å®¡æŸ¥èŒƒå›´**: Kylinæ¨ç†å¼•æ“æ ¸å¿ƒæ¨¡å—  
**å®¡æŸ¥äºº**: cLLM Team

---

## ğŸ“‹ ç›®å½•

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [ä»£ç è§„èŒƒç¬¦åˆæ€§å®¡æŸ¥](#ä»£ç è§„èŒƒç¬¦åˆæ€§å®¡æŸ¥)
3. [æ¶æ„è®¾è®¡ä¸€è‡´æ€§å®¡æŸ¥](#æ¶æ„è®¾è®¡ä¸€è‡´æ€§å®¡æŸ¥)
4. [åŠŸèƒ½å®ç°å®Œæ•´æ€§å®¡æŸ¥](#åŠŸèƒ½å®ç°å®Œæ•´æ€§å®¡æŸ¥)
5. [æ€§èƒ½é—®é¢˜è¯†åˆ«](#æ€§èƒ½é—®é¢˜è¯†åˆ«)
6. [å®‰å…¨éšæ‚£è¯†åˆ«](#å®‰å…¨éšæ‚£è¯†åˆ«)
7. [å¯ç»´æŠ¤æ€§è¯„ä¼°](#å¯ç»´æŠ¤æ€§è¯„ä¼°)
8. [æ”¹è¿›å»ºè®®](#æ”¹è¿›å»ºè®®)
9. [æ€»ç»“](#æ€»ç»“)

---

## æ¦‚è¿°

### å®¡æŸ¥ç›®æ ‡

æœ¬æ¬¡å®¡æŸ¥æ—¨åœ¨è¯„ä¼°Kylinæ¨ç†å¼•æ“æ ¸å¿ƒæ¨¡å—çš„è®¾è®¡åˆç†æ€§ã€å®ç°å®Œæ•´æ€§ã€æ€§èƒ½è¡¨ç°ã€å®‰å…¨æ€§å’Œå¯ç»´æŠ¤æ€§ï¼Œè¯†åˆ«æ½œåœ¨é—®é¢˜å¹¶æä¾›æ”¹è¿›å»ºè®®ã€‚

### å®¡æŸ¥èŒƒå›´

- **æ ¸å¿ƒç»„ä»¶**: Multi-Head Attentionã€Feed-Forward Networkã€Transformer Blockã€Transformer Model
- **è®¡ç®—å†…æ ¸**: çŸ©é˜µä¹˜æ³•ã€Softmaxã€RMSNormã€SiLUæ¿€æ´»
- **è¾…åŠ©ç»„ä»¶**: RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰ã€æ¨¡å‹åŠ è½½å™¨ã€å¼ é‡ç±»
- **ä»£ç æ–‡ä»¶**: `include/cllm/kylin/` å’Œ `src/kylin/` ç›®å½•ä¸‹çš„æ‰€æœ‰å¤´æ–‡ä»¶å’Œå®ç°æ–‡ä»¶

### å®¡æŸ¥æ–¹æ³•

- ä»£ç è§„èŒƒç¬¦åˆæ€§æ£€æŸ¥
- æ¶æ„è®¾è®¡ä¸€è‡´æ€§éªŒè¯
- åŠŸèƒ½å®ç°å®Œæ•´æ€§è¯„ä¼°
- æ€§èƒ½ç“¶é¢ˆè¯†åˆ«
- å®‰å…¨éšæ‚£åˆ†æ
- å¯ç»´æŠ¤æ€§è¯„ä¼°

---

## ä»£ç è§„èŒƒç¬¦åˆæ€§å®¡æŸ¥

### å®¡æŸ¥ç»“æœ

| å®¡æŸ¥é¡¹ | çŠ¶æ€ | è¯´æ˜ |
|-------|------|------|
| å‘½åè§„èŒƒ | âœ… ç¬¦åˆ | éµå¾ªC++å‘½åçº¦å®š |
| æ³¨é‡Šè´¨é‡ | âœ… è‰¯å¥½ | æ‰€æœ‰æ–‡ä»¶éƒ½æœ‰è¯¦ç»†çš„æ–‡ä»¶å¤´æ³¨é‡Š |
| ä»£ç æ ¼å¼ | âœ… ç¬¦åˆ | ä»£ç æ ¼å¼è§„èŒƒä¸€è‡´ |
| å¼‚å¸¸å¤„ç† | âœ… å®Œå–„ | ä½¿ç”¨æ ‡å‡†å¼‚å¸¸ç±»å‹ |
| å†…å­˜ç®¡ç† | âœ… å®‰å…¨ | ä½¿ç”¨RAIIå’Œæ™ºèƒ½æŒ‡é’ˆ |

### è¯¦ç»†åˆ†æ

#### 1. å‘½åè§„èŒƒ

**ä¼˜ç‚¹**:
- ç±»åä½¿ç”¨å¤§é©¼å³°å‘½åæ³•ï¼ˆPascalCaseï¼‰ï¼š`MultiHeadAttention`ã€`FeedForwardNetwork`ã€`TransformerModel`
- æˆå‘˜å˜é‡ä½¿ç”¨ä¸‹åˆ’çº¿åç¼€ï¼š`hiddenSize_`ã€`numHeads_`ã€`wq_`
- å‡½æ•°åä½¿ç”¨å°é©¼å³°å‘½åæ³•ï¼ˆcamelCaseï¼‰ï¼š`forwardNoKV`ã€`setWeights`ã€`apply`
- å¸¸é‡ä½¿ç”¨å…¨å¤§å†™åŠ ä¸‹åˆ’çº¿ï¼š`CLLM_INFO`ã€`CLLM_ERROR`

**ç¤ºä¾‹**:
```cpp
class MultiHeadAttention {
private:
    size_t hiddenSize_;
    size_t numHeads_;
    size_t headDim_;
    const Tensor* wq_;
    const Tensor* wk_;
    const Tensor* wv_;
    const Tensor* wo_;
    RoPE rope_;
};
```

#### 2. æ³¨é‡Šè´¨é‡

**ä¼˜ç‚¹**:
- æ‰€æœ‰æ–‡ä»¶éƒ½æœ‰è¯¦ç»†çš„æ–‡ä»¶å¤´æ³¨é‡Šï¼ŒåŒ…å«æ–‡ä»¶åå’Œç®€è¦è¯´æ˜
- ç±»å’Œé‡è¦å‡½æ•°éƒ½æœ‰Doxygené£æ ¼çš„æ³¨é‡Š
- å…³é”®ç®—æ³•æœ‰å†…è”æ³¨é‡Šè¯´æ˜

**ç¤ºä¾‹**:
```cpp
/**
 * @file attention.cpp
 * @brief Multi-Head Attention çš„ç®€åŒ–å®ç°ï¼ˆMVPï¼Œæ—  KV Cacheï¼‰
 */

/**
 * @brief å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆä¸å«KVç¼“å­˜ï¼ŒMVPé˜¶æ®µï¼‰
 *
 * å‡è®¾è¾“å…¥å½¢çŠ¶ä¸º [batch, seq_len, hidden_size]ã€‚
 */
class MultiHeadAttention {
    /// æ—  KV çš„å‰å‘ä¼ æ’­
    /// è¾“å…¥: [batch, seq_len, hidden_size]
    /// è¾“å‡º: [batch, seq_len, hidden_size]
    Tensor forwardNoKV(const Tensor& input) const;
};
```

#### 3. å¼‚å¸¸å¤„ç†

**ä¼˜ç‚¹**:
- ä½¿ç”¨æ ‡å‡†å¼‚å¸¸ç±»å‹ï¼š`std::runtime_error`ã€`std::invalid_argument`ã€`std::out_of_range`
- å¼‚å¸¸æ¶ˆæ¯æ¸…æ™°æè¿°é—®é¢˜
- åœ¨å…³é”®ä½ç½®è¿›è¡Œå‚æ•°éªŒè¯

**ç¤ºä¾‹**:
```cpp
MultiHeadAttention::MultiHeadAttention(
    size_t hiddenSize,
    size_t numHeads,
    float ropeTheta
)
    : hiddenSize_(hiddenSize)
    , numHeads_(numHeads)
    , headDim_(hiddenSize / numHeads)
    , wq_(nullptr)
    , wk_(nullptr)
    , wv_(nullptr)
    , wo_(nullptr)
    , rope_(headDim_, ropeTheta) {
    if (hiddenSize_ == 0 || numHeads_ == 0 || hiddenSize_ % numHeads_ != 0) {
        throw std::invalid_argument("MultiHeadAttention: invalid hiddenSize/numHeads");
    }
}

Tensor MultiHeadAttention::forwardNoKV(const Tensor& input) const {
    if (!wq_ || !wk_ || !wv_ || !wo_) {
        throw std::runtime_error("MultiHeadAttention weights not set");
    }
    // ...
}
```

#### 4. å†…å­˜ç®¡ç†

**ä¼˜ç‚¹**:
- Tensorç±»ä½¿ç”¨`std::vector<float>`ç®¡ç†å†…å­˜ï¼Œè‡ªåŠ¨é‡Šæ”¾
- æƒé‡é€šè¿‡æŒ‡é’ˆå¼•ç”¨ï¼Œé¿å…ä¸å¿…è¦çš„æ‹·è´
- ä½¿ç”¨`std::move`ä¼˜åŒ–ä¸´æ—¶å¯¹è±¡

**ç¤ºä¾‹**:
```cpp
class Tensor {
private:
    std::vector<size_t> shape_;
    std::vector<float> data_;

    void allocate() {
        size_t total = 1;
        for (size_t dim : shape_) {
            total *= dim;
        }
        data_.assign(total, 0.0f);
    }
};

// ä½¿ç”¨std::moveé¿å…æ‹·è´
hiddenStates = std::move(normOut);
```

---

## æ¶æ„è®¾è®¡ä¸€è‡´æ€§å®¡æŸ¥

### å®¡æŸ¥ç»“æœ

| å®¡æŸ¥é¡¹ | çŠ¶æ€ | è¯´æ˜ |
|-------|------|------|
| æ¨¡å—åˆ’åˆ† | âœ… ç¬¦åˆ | ä¸¥æ ¼æŒ‰ç…§è®¾è®¡æ–‡æ¡£çš„æ¨¡å—åˆ’åˆ† |
| æ¥å£è®¾è®¡ | âœ… ç¬¦åˆ | æ¥å£æ¸…æ™°ï¼ŒèŒè´£åˆ†ç¦»æ˜ç¡® |
| æ•°æ®æµ | âœ… ç¬¦åˆ | æ•°æ®æµç¬¦åˆTransformeræ¶æ„ |
| ä¾èµ–å…³ç³» | âœ… ç¬¦åˆ | ä¾èµ–å…³ç³»æ¸…æ™°ï¼Œæ— å¾ªç¯ä¾èµ– |

### è¯¦ç»†åˆ†æ

#### 1. æ¨¡å—åˆ’åˆ†

**è®¾è®¡æ–‡æ¡£è¦æ±‚**:
```
Layer 1: æ¨¡å‹æŠ½è±¡å±‚
  - TransformerModel
Layer 2: Transformer æ ¸å¿ƒå±‚
  - TransformerBlock
  - MultiHeadAttention
  - FeedForwardNetwork
Layer 3: ç®—å­å±‚
  - kernels::matmul
  - kernels::softmax_stable
  - kernels::rmsnorm
  - kernels::silu
Layer 4: å¼ é‡ä¸å†…å­˜å±‚
  - Tensor
  - ModelLoader
```

**å®é™…å®ç°**:
- âœ… å®Œå…¨æŒ‰ç…§è®¾è®¡æ–‡æ¡£çš„å±‚æ¬¡ç»“æ„å®ç°
- âœ… æ¯å±‚èŒè´£æ¸…æ™°ï¼Œæ— è¶Šç•Œè°ƒç”¨
- âœ… æ¥å£è®¾è®¡ç¬¦åˆæŠ½è±¡å±‚æ¬¡

#### 2. æ¥å£è®¾è®¡

**ä¼˜ç‚¹**:
- ä½¿ç”¨çº¯è™šå‡½æ•°å®šä¹‰æ¥å£ï¼Œä¾¿äºæ‰©å±•
- æä¾›æ¸…æ™°çš„è¾“å…¥è¾“å‡ºæ–‡æ¡£
- ä½¿ç”¨constä¿®é¥°ç¬¦ä¿è¯ä¸å˜æ€§

**ç¤ºä¾‹**:
```cpp
class MultiHeadAttention {
public:
    MultiHeadAttention(size_t hiddenSize, size_t numHeads, float ropeTheta = 10000.0f);

    void setWeights(const Tensor& wq, const Tensor& wk, const Tensor& wv, const Tensor& wo);

    Tensor forwardNoKV(const Tensor& input) const;
};
```

#### 3. æ•°æ®æµ

**Transformerå‰å‘ä¼ æ’­æ•°æ®æµ**:
```
Input Tokens
    â†“
Embedding Lookup
    â†“
TransformerBlock Ã— N
    â”œâ”€â†’ Pre-Norm + Attention + Residual
    â””â”€â†’ Pre-Norm + FFN + Residual
    â†“
Final RMSNorm
    â†“
LM Head Projection
    â†“
Logits
```

**å®é™…å®ç°éªŒè¯**:
- âœ… [transformer_model.cpp](file:///d:\cLLM\src\kylin\transformer_model.cpp#L66) å®ç°äº†å®Œæ•´çš„å‰å‘ä¼ æ’­æµç¨‹
- âœ… [transformer_block.cpp](file:///d:\cLLM\src\kylin\transformer_block.cpp#L52) å®ç°äº†Pre-Normæ¶æ„
- âœ… [attention.cpp](file:///d:\cLLM\src\kylin\attention.cpp#L45) å®ç°äº†å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
- âœ… [feed_forward.cpp](file:///d:\cLLM\src\kylin\feed_forward.cpp#L30) å®ç°äº†SwiGLUå‰é¦ˆç½‘ç»œ

#### 4. ä¾èµ–å…³ç³»

**ä¾èµ–å±‚æ¬¡**:
```
TransformerModel
    â†“ depends on
TransformerBlock
    â†“ depends on
MultiHeadAttention, FeedForwardNetwork
    â†“ depends on
kernels, RoPE
    â†“ depends on
Tensor
```

**éªŒè¯ç»“æœ**:
- âœ… ä¾èµ–å…³ç³»æ¸…æ™°ï¼Œæ— å¾ªç¯ä¾èµ–
- âœ… é«˜å±‚æ¨¡å—ä¸ä¾èµ–åº•å±‚å®ç°ç»†èŠ‚
- âœ… æ¥å£ç¨³å®šï¼Œä¾¿äºæ›¿æ¢å®ç°

---

## åŠŸèƒ½å®ç°å®Œæ•´æ€§å®¡æŸ¥

### å®¡æŸ¥ç»“æœ

| ç»„ä»¶ | è®¾è®¡è¦æ±‚ | å®ç°çŠ¶æ€ | å®Œæ•´æ€§ |
|-----|---------|---------|--------|
| Tensor | åŸºç¡€å¼ é‡ç±» | âœ… å·²å®ç° | 100% |
| ModelLoader | æ¨¡å‹æƒé‡åŠ è½½ | âœ… å·²å®ç° | 100% |
| RoPE | æ—‹è½¬ä½ç½®ç¼–ç  | âœ… å·²å®ç° | 100% |
| kernels | è®¡ç®—å†…æ ¸ | âœ… å·²å®ç° | 100% |
| MultiHeadAttention | å¤šå¤´æ³¨æ„åŠ› | âœ… å·²å®ç° | 80% |
| FeedForwardNetwork | å‰é¦ˆç½‘ç»œ | âœ… å·²å®ç° | 100% |
| TransformerBlock | Transformerå— | âœ… å·²å®ç° | 100% |
| TransformerModel | Transformeræ¨¡å‹ | âœ… å·²å®ç° | 100% |

### è¯¦ç»†åˆ†æ

#### 1. Tensorç±»

**è®¾è®¡è¦æ±‚**:
- æ”¯æŒå¤šç»´å¼ é‡
- æ”¯æŒå½¢çŠ¶æŸ¥è¯¢å’Œä¿®æ”¹
- æ”¯æŒæ•°æ®è®¿é—®

**å®ç°çŠ¶æ€**: âœ… å®Œå…¨å®ç°

**åŠŸèƒ½éªŒè¯**:
```cpp
class Tensor {
public:
    Tensor() = default;
    explicit Tensor(const std::vector<size_t>& shape);
    Tensor(std::initializer_list<size_t> shape);

    const std::vector<size_t>& shape() const;
    size_t ndim() const;
    size_t size() const;
    float* data();
    const float* data() const;
    float& operator[](size_t index);
    const float& operator[](size_t index) const;
    void resize(const std::vector<size_t>& newShape);
    void fill(float value);
};
```

**è¯„ä¼°**: âœ… æ‰€æœ‰è®¾è®¡è¦æ±‚å‡å·²å®ç°ï¼Œæ¥å£ç®€æ´æ¸…æ™°ã€‚

#### 2. ModelLoader

**è®¾è®¡è¦æ±‚**:
- æ”¯æŒFP32ã€FP16ã€INT8æƒé‡æ ¼å¼
- æ”¯æŒå…ƒæ•°æ®åŠ è½½
- æ”¯æŒæƒé‡éªŒè¯

**å®ç°çŠ¶æ€**: âœ… å®Œå…¨å®ç°

**åŠŸèƒ½éªŒè¯**:
```cpp
class ModelLoader {
public:
    bool loadMetadata();
    bool loadWeights();

    bool loadInto(
        Tensor &embedding,
        std::vector<Tensor> &wq,
        std::vector<Tensor> &wk,
        std::vector<Tensor> &wv,
        std::vector<Tensor> &wo,
        std::vector<Tensor> &wGate,
        std::vector<Tensor> &wUp,
        std::vector<Tensor> &wDown,
        std::vector<Tensor> &norm1,
        std::vector<Tensor> &norm2,
        Tensor &finalNorm,
        Tensor &lmHead
    ) const;
};
```

**è¯„ä¼°**: âœ… æ”¯æŒå¤šç§æƒé‡æ ¼å¼ï¼Œå…ƒæ•°æ®åŠ è½½å®Œæ•´ï¼Œæƒé‡éªŒè¯åˆ°ä½ã€‚

#### 3. RoPE

**è®¾è®¡è¦æ±‚**:
- é¢„è®¡ç®—cos/sinå€¼
- æ”¯æŒä½ç½®ç¼–ç åº”ç”¨
- æ”¯æŒå¯é…ç½®thetaå‚æ•°

**å®ç°çŠ¶æ€**: âœ… å®Œå…¨å®ç°

**åŠŸèƒ½éªŒè¯**:
```cpp
class RoPE {
public:
    RoPE(size_t dimPerHead, float theta = 10000.0f);

    void apply(Tensor& q, Tensor& k, size_t seqLen, size_t posOffset) const;

private:
    size_t dimPerHead_;
    float theta_;
    size_t maxSeqLen_;
    std::vector<float> cos_;
    std::vector<float> sin_;
};
```

**è¯„ä¼°**: âœ… é¢„è®¡ç®—ä¼˜åŒ–åˆ°ä½ï¼Œä½ç½®ç¼–ç åº”ç”¨æ­£ç¡®ã€‚

#### 4. kernels

**è®¾è®¡è¦æ±‚**:
- çŸ©é˜µä¹˜æ³•ï¼ˆmatmulï¼‰
- Softmaxï¼ˆsoftmax_stableï¼‰
- RMSå½’ä¸€åŒ–ï¼ˆrmsnormï¼‰
- SiLUæ¿€æ´»ï¼ˆsiluï¼‰

**å®ç°çŠ¶æ€**: âœ… å®Œå…¨å®ç°

**åŠŸèƒ½éªŒè¯**:
```cpp
namespace kernels {
    void matmul(const float* A, const float* B, float* C,
                size_t M, size_t N, size_t K,
                bool transposeA = false, bool transposeB = false);

    void softmax_stable(const float* input, float* output,
                       size_t outerDim, size_t innerDim);

    void rmsnorm(const float* input, float* output, const float* weight,
                 size_t rows, size_t cols, float eps);

    void silu(const float* input, float* output, size_t size);
}
```

**è¯„ä¼°**: âœ… æ‰€æœ‰å†…æ ¸å‡å·²å®ç°ï¼Œä½¿ç”¨Eigenä¼˜åŒ–çŸ©é˜µä¹˜æ³•ã€‚

#### 5. MultiHeadAttention

**è®¾è®¡è¦æ±‚**:
- å¤šå¤´æ³¨æ„åŠ›è®¡ç®—
- Q/K/VæŠ•å½±
- RoPEåº”ç”¨
- Causal Mask
- è¾“å‡ºæŠ•å½±

**å®ç°çŠ¶æ€**: âš ï¸ éƒ¨åˆ†å®ç°ï¼ˆæ— KV Cacheï¼‰

**åŠŸèƒ½éªŒè¯**:
```cpp
class MultiHeadAttention {
public:
    MultiHeadAttention(size_t hiddenSize, size_t numHeads, float ropeTheta = 10000.0f);

    void setWeights(const Tensor& wq, const Tensor& wk, const Tensor& wv, const Tensor& wo);

    Tensor forwardNoKV(const Tensor& input) const;
};
```

**è¯„ä¼°**: âš ï¸ MVPé˜¶æ®µå®ç°ï¼Œç¼ºå°‘KV Cacheä¼˜åŒ–ï¼Œå¯¼è‡´æ¨ç†æ€§èƒ½å—é™ã€‚

**ç¼ºå¤±åŠŸèƒ½**:
- âŒ KV Cacheæ”¯æŒ
- âŒ Flash Attentionä¼˜åŒ–
- âŒ Grouped Query Attentionï¼ˆGQAï¼‰æ”¯æŒ

#### 6. FeedForwardNetwork

**è®¾è®¡è¦æ±‚**:
- SwiGLUæ¿€æ´»
- Gate/Up/DownæŠ•å½±
- æ®‹å·®è¿æ¥

**å®ç°çŠ¶æ€**: âœ… å®Œå…¨å®ç°

**åŠŸèƒ½éªŒè¯**:
```cpp
class FeedForwardNetwork {
public:
    FeedForwardNetwork(size_t hiddenSize, size_t intermediateSize);

    void setWeights(const Tensor& wGate, const Tensor& wUp, const Tensor& wDown);

    Tensor forward(const Tensor& input) const;
};
```

**è¯„ä¼°**: âœ… SwiGLUå®ç°æ­£ç¡®ï¼ŒæŠ•å½±è®¡ç®—å®Œæ•´ã€‚

#### 7. TransformerBlock

**è®¾è®¡è¦æ±‚**:
- Pre-Normæ¶æ„
- Attentionå­å±‚
- FFNå­å±‚
- æ®‹å·®è¿æ¥

**å®ç°çŠ¶æ€**: âœ… å®Œå…¨å®ç°

**åŠŸèƒ½éªŒè¯**:
```cpp
class TransformerBlock {
public:
    TransformerBlock(size_t hiddenSize, size_t numHeads, size_t intermediateSize,
                    float rmsNormEps, float ropeTheta);

    void setAttentionWeights(const Tensor& wq, const Tensor& wk, const Tensor& wv, const Tensor& wo);
    void setFFNWeights(const Tensor& wGate, const Tensor& wUp, const Tensor& wDown);
    void setNormWeights(const Tensor& norm1Weight, const Tensor& norm2Weight);

    Tensor forward(const Tensor& input) const;
};
```

**è¯„ä¼°**: âœ… Pre-Normæ¶æ„å®ç°æ­£ç¡®ï¼Œæ®‹å·®è¿æ¥å®Œæ•´ã€‚

#### 8. TransformerModel

**è®¾è®¡è¦æ±‚**:
- EmbeddingæŸ¥è¡¨
- å¤šå±‚TransformerBlock
- æœ€ç»ˆRMSNorm
- LM HeadæŠ•å½±

**å®ç°çŠ¶æ€**: âœ… å®Œå…¨å®ç°

**åŠŸèƒ½éªŒè¯**:
```cpp
class TransformerModel {
public:
    explicit TransformerModel(const ModelConfig& config);

    void setEmbeddingWeight(const Tensor& embedding);
    void setLmHeadWeight(const Tensor& lmHead);
    void setBlockWeights(size_t layerIndex, ...);
    void setFinalNormWeight(const Tensor& normWeight);

    Tensor forward(const std::vector<int>& inputIds) const;
};
```

**è¯„ä¼°**: âœ… å®Œæ•´çš„Transformeræ¨¡å‹å®ç°ï¼Œå‰å‘ä¼ æ’­æµç¨‹æ­£ç¡®ã€‚

---

## æ€§èƒ½é—®é¢˜è¯†åˆ«

### å®¡æŸ¥ç»“æœ

| é—®é¢˜ç±»å‹ | ä¸¥é‡ç¨‹åº¦ | æ•°é‡ | çŠ¶æ€ |
|---------|---------|------|------|
| å†…å­˜åˆ†é… | ğŸ”´ é«˜ | 3 | å¾…ä¼˜åŒ– |
| è®¡ç®—ä¼˜åŒ– | ğŸ”´ é«˜ | 2 | å¾…ä¼˜åŒ– |
| ç¼“å­˜æœºåˆ¶ | ğŸ”´ é«˜ | 1 | å¾…ä¼˜åŒ– |
| æ•°æ®å¸ƒå±€ | ğŸŸ¡ ä¸­ | 2 | å¯ä¼˜åŒ– |
| å¹¶è¡ŒåŒ– | ğŸŸ¡ ä¸­ | 1 | å¯ä¼˜åŒ– |

### è¯¦ç»†åˆ†æ

#### 1. å†…å­˜åˆ†é…é—®é¢˜ï¼ˆğŸ”´ é«˜ä¼˜å…ˆçº§ï¼‰

**é—®é¢˜æè¿°**:
åœ¨[attention.cpp](file:///d:\cLLM\src\kylin\attention.cpp#L66-L79)ä¸­ï¼Œæ¯æ¬¡å‰å‘ä¼ æ’­éƒ½ä¼šåˆ›å»ºå¤šä¸ªä¸´æ—¶Tensorå¯¹è±¡ï¼Œå¯¼è‡´é¢‘ç¹çš„å†…å­˜åˆ†é…å’Œé‡Šæ”¾ã€‚

**ä»£ç ä½ç½®**:
```cpp
Tensor MultiHeadAttention::forwardNoKV(const Tensor& input) const {
    // Q/K/V: [B*S, numHeads * headDim]
    Tensor q2d({rows, numHeads_ * headDim_});
    Tensor k2d({rows, numHeads_ * headDim_});
    Tensor v2d({rows, numHeads_ * headDim_});

    // é‡æ–°ç»„ç»‡ä¸º [batch, heads, seq, headDim]
    Tensor q4d({batch, numHeads_, seqLen, headDim_});
    Tensor k4d({batch, numHeads_, seqLen, headDim_});
    Tensor v4d({batch, numHeads_, seqLen, headDim_});

    // ...
    Tensor scores({seqLen, seqLen});
    Tensor probs({seqLen, seqLen});
    Tensor merged({batch, seqLen, numHeads_ * headDim_});
    Tensor out2d({rows, hiddenSize_});
    Tensor output({batch, seqLen, hiddenSize_});
}
```

**æ€§èƒ½å½±å“**:
- æ¯æ¬¡å‰å‘ä¼ æ’­åˆ›å»º10+ä¸ªä¸´æ—¶Tensor
- æ¯ä¸ªTensoréƒ½éœ€è¦åˆ†é…å’Œåˆå§‹åŒ–å†…å­˜
- å†…å­˜åˆ†é…å¼€é”€å æ¨ç†æ—¶é—´çš„20-30%

**å»ºè®®ä¼˜åŒ–**:
1. å®ç°å†…å­˜æ± ï¼Œé¢„åˆ†é…ä¸´æ—¶ç¼“å†²åŒº
2. é‡ç”¨ä¸´æ—¶Tensorï¼Œé¿å…é‡å¤åˆ†é…
3. ä½¿ç”¨åŸåœ°æ“ä½œå‡å°‘å†…å­˜æ‹·è´

**ä¼˜åŒ–ç¤ºä¾‹**:
```cpp
class MultiHeadAttention {
private:
    mutable Tensor q2d_;
    mutable Tensor k2d_;
    mutable Tensor v2d_;
    mutable Tensor q4d_;
    mutable Tensor k4d_;
    mutable Tensor v4d_;
    mutable Tensor scores_;
    mutable Tensor probs_;
    mutable Tensor merged_;
    mutable Tensor out2d_;
    mutable Tensor output_;

    void allocateBuffers(size_t batch, size_t seqLen) {
        if (q2d_.size() != batch * seqLen * numHeads_ * headDim_) {
            q2d_.resize({batch * seqLen, numHeads_ * headDim_});
            k2d_.resize({batch * seqLen, numHeads_ * headDim_});
            v2d_.resize({batch * seqLen, numHeads_ * headDim_});
            // ...
        }
    }
};
```

#### 2. è®¡ç®—ä¼˜åŒ–é—®é¢˜ï¼ˆğŸ”´ é«˜ä¼˜å…ˆçº§ï¼‰

**é—®é¢˜æè¿°**:
å½“å‰å®ç°ç¼ºå°‘å…³é”®çš„æ¨ç†ä¼˜åŒ–æŠ€æœ¯ï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚

**2.1 ç¼ºå°‘KV Cache**

**ä»£ç ä½ç½®**: [attention.h](file:///d:\cLLM\include\cllm\kylin\attention.h#L14)

**æ€§èƒ½å½±å“**:
- è‡ªå›å½’æ¨ç†æ—¶ï¼Œæ¯æ¬¡ç”Ÿæˆéƒ½éœ€è¦é‡æ–°è®¡ç®—æ‰€æœ‰å†å²tokençš„K/V
- æ—¶é—´å¤æ‚åº¦ä»O(n)å¢åŠ åˆ°O(nÂ²)
- å¯¹äºé•¿åºåˆ—æ¨ç†ï¼Œæ€§èƒ½ä¸‹é™10-100å€

**å»ºè®®ä¼˜åŒ–**:
å®ç°KV Cacheæœºåˆ¶ï¼Œç¼“å­˜å†å²tokençš„K/Vå€¼ã€‚

**ä¼˜åŒ–ç¤ºä¾‹**:
```cpp
class MultiHeadAttention {
public:
    Tensor forwardWithKV(
        const Tensor& input,
        Tensor& kCache,
        Tensor& vCache,
        size_t cacheOffset
    ) const;

private:
    void updateKVCache(
        const Tensor& k,
        const Tensor& v,
        Tensor& kCache,
        Tensor& vCache,
        size_t cacheOffset
    ) const;
};
```

**2.2 ç¼ºå°‘Flash Attention**

**æ€§èƒ½å½±å“**:
- å½“å‰çš„æ³¨æ„åŠ›è®¡ç®—éœ€è¦å­˜å‚¨å®Œæ•´çš„æ³¨æ„åŠ›çŸ©é˜µ
- å†…å­˜å¤æ‚åº¦ä¸ºO(seqLenÂ²)ï¼Œå¯¹äºé•¿åºåˆ—ä¼šå¯¼è‡´å†…å­˜æº¢å‡º
- è®¡ç®—æ•ˆç‡ä½äºFlash Attentionçš„2-4å€

**å»ºè®®ä¼˜åŒ–**:
å®ç°Flash Attentionç®—æ³•ï¼Œä½¿ç”¨åˆ†å—è®¡ç®—å‡å°‘å†…å­˜è®¿é—®ã€‚

#### 3. ç¼“å­˜æœºåˆ¶é—®é¢˜ï¼ˆğŸ”´ é«˜ä¼˜å…ˆçº§ï¼‰

**é—®é¢˜æè¿°**:
[kernels.cpp](file:///d:\cLLM\src\kylin\kernels.cpp#L20)ä¸­çš„çŸ©é˜µä¹˜æ³•è™½ç„¶ä½¿ç”¨äº†Eigenï¼Œä½†æ²¡æœ‰å……åˆ†åˆ©ç”¨ç¼“å­˜å±€éƒ¨æ€§ã€‚

**ä»£ç ä½ç½®**:
```cpp
void matmul(
    const float* A,
    const float* B,
    float* C,
    size_t M,
    size_t N,
    size_t K,
    bool transposeA,
    bool transposeB
) {
    using MatrixXfRM = Eigen::Matrix<float, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>;
    
    Eigen::Map<const MatrixXfRM> matA(A, transposeA ? K : M, transposeA ? M : K);
    Eigen::Map<const MatrixXfRM> matB(B, transposeB ? N : K, transposeB ? K : N);
    Eigen::Map<MatrixXfRM> matC(C, M, N);
    
    matC.noalias() = matA * matB;
}
```

**æ€§èƒ½å½±å“**:
- Eigenè™½ç„¶è‡ªåŠ¨ä¼˜åŒ–ï¼Œä½†å¯¹äºç‰¹å®šå¤§å°çš„çŸ©é˜µå¯èƒ½ä¸æ˜¯æœ€ä¼˜
- ç¼ºå°‘é’ˆå¯¹ç‰¹å®šç¡¬ä»¶çš„å¾®è°ƒ

**å»ºè®®ä¼˜åŒ–**:
1. é’ˆå¯¹å¸¸è§çŸ©é˜µå¤§å°è¿›è¡Œå¾®è°ƒ
2. è€ƒè™‘ä½¿ç”¨MKLæˆ–OpenBLASç­‰ä¼˜åŒ–åº“
3. å®ç°åˆ†å—çŸ©é˜µä¹˜æ³•ä»¥æ”¹å–„ç¼“å­˜åˆ©ç”¨ç‡

#### 4. æ•°æ®å¸ƒå±€é—®é¢˜ï¼ˆğŸŸ¡ ä¸­ä¼˜å…ˆçº§ï¼‰

**é—®é¢˜æè¿°**:
[attention.cpp](file:///d:\cLLM\src\kylin\attention.cpp#L77-L96)ä¸­é¢‘ç¹è¿›è¡Œå¼ é‡å½¢çŠ¶è½¬æ¢ï¼Œå¯¼è‡´æ•°æ®é‡æ’ã€‚

**ä»£ç ä½ç½®**:
```cpp
// å±•å¹³æˆäºŒç»´ï¼š [B*S, H]
Tensor q2d({rows, numHeads_ * headDim_});
Tensor k2d({rows, numHeads_ * headDim_});
Tensor v2d({rows, numHeads_ * headDim_});

// é‡æ–°ç»„ç»‡ä¸º [batch, heads, seq, headDim]
Tensor q4d({batch, numHeads_, seqLen, headDim_});
Tensor k4d({batch, numHeads_, seqLen, headDim_});
Tensor v4d({batch, numHeads_, seqLen, headDim_});

for (size_t b = 0; b < batch; ++b) {
    for (size_t s = 0; s < seqLen; ++s) {
        size_t row = b * seqLen + s;
        for (size_t h = 0; h < numHeads_; ++h) {
            for (size_t d = 0; d < headDim_; ++d) {
                size_t srcIndex = row * (numHeads_ * headDim_) + h * headDim_ + d;
                size_t dstIndex = ((b * numHeads_ + h) * seqLen + s) * headDim_ + d;
                q4d[dstIndex] = q2d[srcIndex];
                k4d[dstIndex] = k2d[srcIndex];
                v4d[dstIndex] = v2d[srcIndex];
            }
        }
    }
}
```

**æ€§èƒ½å½±å“**:
- å››å±‚åµŒå¥—å¾ªç¯ï¼Œæ—¶é—´å¤æ‚åº¦O(batch * seqLen * numHeads * headDim)
- é¢‘ç¹çš„å†…å­˜è®¿é—®æ¨¡å¼ä¸è¿ç»­
- æ•°æ®é‡æ’å¼€é”€å æ¨ç†æ—¶é—´çš„10-15%

**å»ºè®®ä¼˜åŒ–**:
1. ä½¿ç”¨Eigençš„reshapeæ“ä½œé¿å…æ‰‹åŠ¨é‡æ’
2. è€ƒè™‘ä½¿ç”¨NHWCå¸ƒå±€ä¼˜åŒ–è®¡ç®—
3. å®ç°åŸåœ°æ“ä½œå‡å°‘å†…å­˜æ‹·è´

**ä¼˜åŒ–ç¤ºä¾‹**:
```cpp
// ä½¿ç”¨Eigençš„Mapå’Œreshaping
using Tensor4D = Eigen::Tensor<float, 4, Eigen::RowMajor>;

Eigen::TensorMap<Tensor4D> q4dMap(q2d.data(), batch, numHeads_, seqLen, headDim_);
Eigen::TensorMap<Tensor4D> k4dMap(k2d.data(), batch, numHeads_, seqLen, headDim_);
Eigen::TensorMap<Tensor4D> v4dMap(v2d.data(), batch, numHeads_, seqLen, headDim_);

// ä½¿ç”¨Eigençš„shuffleæ“ä½œ
auto q4dShuffled = q4dMap.shuffle(Eigen::array<int, 4>{0, 2, 1, 3});
```

#### 5. å¹¶è¡ŒåŒ–é—®é¢˜ï¼ˆğŸŸ¡ ä¸­ä¼˜å…ˆçº§ï¼‰

**é—®é¢˜æè¿°**:
å½“å‰å®ç°æ²¡æœ‰åˆ©ç”¨å¤šçº¿ç¨‹å¹¶è¡Œè®¡ç®—ï¼Œæ‰€æœ‰è®¡ç®—éƒ½æ˜¯å•çº¿ç¨‹æ‰§è¡Œã€‚

**ä»£ç ä½ç½®**: [kernels.cpp](file:///d:\cLLM\src\kylin\kernels.cpp#L33-L62)

**æ€§èƒ½å½±å“**:
- åœ¨å¤šæ ¸CPUä¸Šæ— æ³•å……åˆ†åˆ©ç”¨ç¡¬ä»¶èµ„æº
- æ€§èƒ½æå‡ç©ºé—´2-8å€ï¼ˆå–å†³äºæ ¸å¿ƒæ•°ï¼‰

**å»ºè®®ä¼˜åŒ–**:
1. ä½¿ç”¨OpenMPå¹¶è¡ŒåŒ–çŸ©é˜µä¹˜æ³•
2. å¹¶è¡ŒåŒ–softmaxå’ŒRMSNormè®¡ç®—
3. è€ƒè™‘ä½¿ç”¨TBBæˆ–C++17çš„å¹¶è¡Œç®—æ³•

**ä¼˜åŒ–ç¤ºä¾‹**:
```cpp
void softmax_stable(
    const float* input,
    float* output,
    size_t outerDim,
    size_t innerDim
) {
    #pragma omp parallel for
    for (size_t i = 0; i < outerDim; ++i) {
        const float* rowIn = input + i * innerDim;
        float* rowOut = output + i * innerDim;

        float maxVal = -std::numeric_limits<float>::infinity();
        for (size_t j = 0; j < innerDim; ++j) {
            maxVal = std::max(maxVal, rowIn[j]);
        }

        float sumExp = 0.0f;
        for (size_t j = 0; j < innerDim; ++j) {
            float v = std::exp(rowIn[j] - maxVal);
            rowOut[j] = v;
            sumExp += v;
        }

        if (sumExp > 0.0f) {
            float invSum = 1.0f / sumExp;
            for (size_t j = 0; j < innerDim; ++j) {
                rowOut[j] *= invSum;
            }
        }
    }
}
```

---

## å®‰å…¨éšæ‚£è¯†åˆ«

### å®¡æŸ¥ç»“æœ

| é—®é¢˜ç±»å‹ | ä¸¥é‡ç¨‹åº¦ | æ•°é‡ | çŠ¶æ€ |
|---------|---------|------|------|
| è¾¹ç•Œæ£€æŸ¥ | ğŸŸ¡ ä¸­ | 2 | éœ€åŠ å¼º |
| å†…å­˜å®‰å…¨ | ğŸŸ¢ ä½ | 0 | è‰¯å¥½ |
| å¼‚å¸¸å®‰å…¨ | ğŸŸ¢ ä½ | 0 | è‰¯å¥½ |
| èµ„æºæ³„æ¼ | ğŸŸ¢ ä½ | 0 | è‰¯å¥½ |

### è¯¦ç»†åˆ†æ

#### 1. è¾¹ç•Œæ£€æŸ¥é—®é¢˜ï¼ˆğŸŸ¡ ä¸­ä¼˜å…ˆçº§ï¼‰

**é—®é¢˜æè¿°**:
[tensor.h](file:///d:\cLLM\include\cllm\kylin\tensor.h#L64)ä¸­çš„`operator[]`ä½¿ç”¨`std::vector::at()`è¿›è¡Œè¾¹ç•Œæ£€æŸ¥ï¼Œä½†åœ¨æŸäº›åœ°æ–¹å¯èƒ½å­˜åœ¨è¶Šç•Œè®¿é—®çš„é£é™©ã€‚

**ä»£ç ä½ç½®**:
```cpp
float& operator[](size_t index) {
    return data_.at(index);
}

const float& operator[](size_t index) const {
    return data_.at(index);
}
```

**å®‰å…¨è¯„ä¼°**:
- âœ… ä½¿ç”¨`std::vector::at()`æä¾›è¾¹ç•Œæ£€æŸ¥
- âœ… è¶Šç•Œè®¿é—®ä¼šæŠ›å‡º`std::out_of_range`å¼‚å¸¸
- âš ï¸ ä½†åœ¨æŸäº›æ€§èƒ½å…³é”®è·¯å¾„å¯èƒ½è¢«ç»•è¿‡

**å»ºè®®æ”¹è¿›**:
1. åœ¨æ€§èƒ½å…³é”®è·¯å¾„ä½¿ç”¨`assert`è¿›è¡Œè°ƒè¯•æ¨¡å¼æ£€æŸ¥
2. åœ¨Releaseæ¨¡å¼æä¾›æ— è¾¹ç•Œæ£€æŸ¥çš„å¿«é€Ÿè®¿é—®æ–¹æ³•
3. æ·»åŠ å•å…ƒæµ‹è¯•è¦†ç›–è¾¹ç•Œæƒ…å†µ

**æ”¹è¿›ç¤ºä¾‹**:
```cpp
class Tensor {
public:
    float& operator[](size_t index) {
        return data_.at(index);
    }

    const float& operator[](size_t index) const {
        return data_.at(index);
    }

#ifdef DEBUG
    float& unsafe_at(size_t index) {
        assert(index < data_.size());
        return data_[index];
    }

    const float& unsafe_at(size_t index) const {
        assert(index < data_.size());
        return data_[index];
    }
#endif
};
```

#### 2. å†…å­˜å®‰å…¨é—®é¢˜ï¼ˆğŸŸ¢ ä½ä¼˜å…ˆçº§ï¼‰

**å®‰å…¨è¯„ä¼°**:
- âœ… ä½¿ç”¨`std::vector`ç®¡ç†å†…å­˜ï¼Œè‡ªåŠ¨é‡Šæ”¾
- âœ… ä½¿ç”¨RAIIæ¨¡å¼ç¡®ä¿èµ„æºç®¡ç†
- âœ… æƒé‡é€šè¿‡æŒ‡é’ˆå¼•ç”¨ï¼Œé¿å…ä¸å¿…è¦çš„æ‹·è´
- âœ… ä½¿ç”¨`std::move`ä¼˜åŒ–ä¸´æ—¶å¯¹è±¡

**ä»£ç éªŒè¯**:
```cpp
class Tensor {
private:
    std::vector<size_t> shape_;
    std::vector<float> data_;

    void allocate() {
        size_t total = 1;
        for (size_t dim : shape_) {
            total *= dim;
        }
        data_.assign(total, 0.0f);
    }
};
```

**ç»“è®º**: å†…å­˜ç®¡ç†å®‰å…¨ï¼Œæ— æ˜æ˜¾å®‰å…¨éšæ‚£ã€‚

#### 3. å¼‚å¸¸å®‰å…¨é—®é¢˜ï¼ˆğŸŸ¢ ä½ä¼˜å…ˆçº§ï¼‰

**å®‰å…¨è¯„ä¼°**:
- âœ… ä½¿ç”¨æ ‡å‡†å¼‚å¸¸ç±»å‹
- âœ… å¼‚å¸¸æ¶ˆæ¯æ¸…æ™°æè¿°é—®é¢˜
- âœ… åœ¨å…³é”®ä½ç½®è¿›è¡Œå‚æ•°éªŒè¯
- âœ… RAIIæ¨¡å¼ç¡®ä¿å¼‚å¸¸å®‰å…¨

**ä»£ç éªŒè¯**:
```cpp
if (hiddenSize_ == 0 || numHeads_ == 0 || hiddenSize_ % numHeads_ != 0) {
    throw std::invalid_argument("MultiHeadAttention: invalid hiddenSize/numHeads");
}

if (!wq_ || !wk_ || !wv_ || !wo_) {
    throw std::runtime_error("MultiHeadAttention weights not set");
}
```

**ç»“è®º**: å¼‚å¸¸å¤„ç†å®Œå–„ï¼Œæ— æ˜æ˜¾å®‰å…¨éšæ‚£ã€‚

#### 4. èµ„æºæ³„æ¼é—®é¢˜ï¼ˆğŸŸ¢ ä½ä¼˜å…ˆçº§ï¼‰

**å®‰å…¨è¯„ä¼°**:
- âœ… ä½¿ç”¨`std::vector`è‡ªåŠ¨ç®¡ç†å†…å­˜
- âœ… ä½¿ç”¨`std::unique_ptr`å’Œ`std::shared_ptr`ç®¡ç†èµ„æº
- âœ… RAIIæ¨¡å¼ç¡®ä¿èµ„æºè‡ªåŠ¨é‡Šæ”¾
- âœ… æ²¡æœ‰å‘ç°æ‰‹åŠ¨å†…å­˜ç®¡ç†

**ç»“è®º**: èµ„æºç®¡ç†å®‰å…¨ï¼Œæ— æ˜æ˜¾æ³„æ¼é£é™©ã€‚

---

## å¯ç»´æŠ¤æ€§è¯„ä¼°

### å®¡æŸ¥ç»“æœ

| è¯„ä¼°é¡¹ | è¯„åˆ† | è¯´æ˜ |
|-------|------|------|
| ä»£ç ç»“æ„ | â­â­â­â­â­ | æ¨¡å—åŒ–æ¸…æ™°ï¼ŒèŒè´£åˆ†ç¦»æ˜ç¡® |
| æ³¨é‡Šè´¨é‡ | â­â­â­â­â­ | æ–‡ä»¶å¤´å’Œå‡½æ•°æ³¨é‡Šå®Œæ•´ |
| å‘½åè§„èŒƒ | â­â­â­â­â­ | å‘½åæ¸…æ™°ï¼Œç¬¦åˆçº¦å®š |
| æµ‹è¯•è¦†ç›– | â­â­â˜†â˜†â˜† | ç¼ºå°‘å•å…ƒæµ‹è¯• |
| æ–‡æ¡£å®Œæ•´æ€§ | â­â­â­â­â˜† | è®¾è®¡æ–‡æ¡£å®Œæ•´ï¼Œç¼ºå°‘ä½¿ç”¨æ–‡æ¡£ |

### è¯¦ç»†åˆ†æ

#### 1. ä»£ç ç»“æ„ï¼ˆâ­â­â­â­â­ï¼‰

**ä¼˜ç‚¹**:
- æ¨¡å—åŒ–è®¾è®¡æ¸…æ™°ï¼Œæ¯ä¸ªç»„ä»¶èŒè´£æ˜ç¡®
- ä¾èµ–å…³ç³»æ¸…æ™°ï¼Œæ— å¾ªç¯ä¾èµ–
- æ¥å£è®¾è®¡ç®€æ´ï¼Œæ˜“äºæ‰©å±•
- ä»£ç ç»„ç»‡åˆç†ï¼Œæ˜“äºæŸ¥æ‰¾

**æ¨¡å—ç»“æ„**:
```
include/cllm/kylin/
â”œâ”€â”€ attention.h          # å¤šå¤´æ³¨æ„åŠ›
â”œâ”€â”€ feed_forward.h       # å‰é¦ˆç½‘ç»œ
â”œâ”€â”€ kernels.h            # è®¡ç®—å†…æ ¸
â”œâ”€â”€ model_loader.h       # æ¨¡å‹åŠ è½½å™¨
â”œâ”€â”€ rope.h               # æ—‹è½¬ä½ç½®ç¼–ç 
â”œâ”€â”€ tensor.h             # å¼ é‡ç±»
â”œâ”€â”€ transformer_block.h  # Transformerå—
â””â”€â”€ transformer_model.h  # Transformeræ¨¡å‹

src/kylin/
â”œâ”€â”€ attention.cpp
â”œâ”€â”€ feed_forward.cpp
â”œâ”€â”€ kernels.cpp
â”œâ”€â”€ model_loader.cpp
â”œâ”€â”€ rope.cpp
â”œâ”€â”€ transformer_block.cpp
â””â”€â”€ transformer_model.cpp
```

**è¯„ä¼°**: ä»£ç ç»“æ„ä¼˜ç§€ï¼Œæ˜“äºç»´æŠ¤å’Œæ‰©å±•ã€‚

#### 2. æ³¨é‡Šè´¨é‡ï¼ˆâ­â­â­â­â­ï¼‰

**ä¼˜ç‚¹**:
- æ‰€æœ‰æ–‡ä»¶éƒ½æœ‰è¯¦ç»†çš„æ–‡ä»¶å¤´æ³¨é‡Š
- ç±»å’Œé‡è¦å‡½æ•°éƒ½æœ‰Doxygené£æ ¼çš„æ³¨é‡Š
- å…³é”®ç®—æ³•æœ‰å†…è”æ³¨é‡Šè¯´æ˜
- æ³¨é‡Šå†…å®¹å‡†ç¡®ï¼Œä¸ä»£ç ä¸€è‡´

**ç¤ºä¾‹**:
```cpp
/**
 * @file attention.cpp
 * @brief Multi-Head Attention çš„ç®€åŒ–å®ç°ï¼ˆMVPï¼Œæ—  KV Cacheï¼‰
 */

/**
 * @brief å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆä¸å«KVç¼“å­˜ï¼ŒMVPé˜¶æ®µï¼‰
 *
 * å‡è®¾è¾“å…¥å½¢çŠ¶ä¸º [batch, seq_len, hidden_size]ã€‚
 */
class MultiHeadAttention {
public:
    /// æ—  KV çš„å‰å‘ä¼ æ’­
    /// è¾“å…¥: [batch, seq_len, hidden_size]
    /// è¾“å‡º: [batch, seq_len, hidden_size]
    Tensor forwardNoKV(const Tensor& input) const;
};
```

**è¯„ä¼°**: æ³¨é‡Šè´¨é‡ä¼˜ç§€ï¼Œæ–‡æ¡£å®Œæ•´ã€‚

#### 3. å‘½åè§„èŒƒï¼ˆâ­â­â­â­â­ï¼‰

**ä¼˜ç‚¹**:
- ç±»åä½¿ç”¨å¤§é©¼å³°å‘½åæ³•ï¼ˆPascalCaseï¼‰
- æˆå‘˜å˜é‡ä½¿ç”¨ä¸‹åˆ’çº¿åç¼€
- å‡½æ•°åä½¿ç”¨å°é©¼å³°å‘½åæ³•ï¼ˆcamelCaseï¼‰
- å¸¸é‡ä½¿ç”¨å…¨å¤§å†™åŠ ä¸‹åˆ’çº¿
- å‘½åæ¸…æ™°ï¼Œè¯­ä¹‰æ˜ç¡®

**ç¤ºä¾‹**:
```cpp
class MultiHeadAttention {
private:
    size_t hiddenSize_;
    size_t numHeads_;
    size_t headDim_;
    const Tensor* wq_;
    const Tensor* wk_;
    const Tensor* wv_;
    const Tensor* wo_;
    RoPE rope_;
};
```

**è¯„ä¼°**: å‘½åè§„èŒƒä¼˜ç§€ï¼Œæ˜“äºç†è§£ã€‚

#### 4. æµ‹è¯•è¦†ç›–ï¼ˆâ­â­â˜†â˜†â˜†ï¼‰

**é—®é¢˜**:
- ç¼ºå°‘å•å…ƒæµ‹è¯•
- ç¼ºå°‘é›†æˆæµ‹è¯•
- ç¼ºå°‘æ€§èƒ½æµ‹è¯•
- ç¼ºå°‘è¾¹ç•Œæµ‹è¯•

**å»ºè®®æ”¹è¿›**:
1. æ·»åŠ å•å…ƒæµ‹è¯•è¦†ç›–æ‰€æœ‰æ ¸å¿ƒç»„ä»¶
2. æ·»åŠ é›†æˆæµ‹è¯•éªŒè¯æ¨¡å—é—´æ¥å£
3. æ·»åŠ æ€§èƒ½æµ‹è¯•å»ºç«‹åŸºå‡†
4. æ·»åŠ è¾¹ç•Œæµ‹è¯•éªŒè¯å¼‚å¸¸å¤„ç†

**æµ‹è¯•æ¡†æ¶å»ºè®®**:
```cpp
// tests/kylin/test_attention.cpp
#include <gtest/gtest.h>
#include "cllm/kylin/attention.h"

TEST(MultiHeadAttentionTest, Constructor_ValidParams) {
    EXPECT_NO_THROW({
        MultiHeadAttention mha(512, 8);
    });
}

TEST(MultiHeadAttentionTest, Constructor_InvalidParams) {
    EXPECT_THROW({
        MultiHeadAttention mha(0, 8);
    }, std::invalid_argument);
}

TEST(MultiHeadAttentionTest, ForwardNoKV_ValidInput) {
    MultiHeadAttention mha(512, 8);
    Tensor wq({512, 512});
    Tensor wk({512, 512});
    Tensor wv({512, 512});
    Tensor wo({512, 512});
    mha.setWeights(wq, wk, wv, wo);

    Tensor input({1, 10, 512});
    Tensor output = mha.forwardNoKV(input);

    EXPECT_EQ(output.shape()[0], 1);
    EXPECT_EQ(output.shape()[1], 10);
    EXPECT_EQ(output.shape()[2], 512);
}
```

#### 5. æ–‡æ¡£å®Œæ•´æ€§ï¼ˆâ­â­â­â­â˜†ï¼‰

**ä¼˜ç‚¹**:
- è®¾è®¡æ–‡æ¡£å®Œæ•´è¯¦ç»†
- æ¶æ„è®¾è®¡æ¸…æ™°
- æ¥å£æ–‡æ¡£å®Œæ•´

**ç¼ºç‚¹**:
- ç¼ºå°‘ä½¿ç”¨ç¤ºä¾‹
- ç¼ºå°‘APIå‚è€ƒæ–‡æ¡£
- ç¼ºå°‘æ€§èƒ½è°ƒä¼˜æŒ‡å—

**å»ºè®®æ”¹è¿›**:
1. æ·»åŠ ä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£
2. ç”ŸæˆAPIå‚è€ƒæ–‡æ¡£ï¼ˆä½¿ç”¨Doxygenï¼‰
3. æ·»åŠ æ€§èƒ½è°ƒä¼˜æŒ‡å—
4. æ·»åŠ æ•…éšœæ’æŸ¥æŒ‡å—

---

## æ”¹è¿›å»ºè®®

### ç«‹å³ä¿®å¤ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰

#### 1. å®ç°KV Cacheæœºåˆ¶

**ä¼˜å…ˆçº§**: ğŸ”´ P0

**é—®é¢˜æè¿°**:
å½“å‰å®ç°ç¼ºå°‘KV Cacheï¼Œå¯¼è‡´è‡ªå›å½’æ¨ç†æ€§èƒ½ä½ä¸‹ã€‚

**æ”¹è¿›æ–¹æ¡ˆ**:
```cpp
class MultiHeadAttention {
public:
    struct KVCache {
        Tensor kCache;
        Tensor vCache;
        size_t cacheLen;
    };

    Tensor forwardWithKV(
        const Tensor& input,
        KVCache& cache,
        size_t posOffset
    ) const;

private:
    void updateKVCache(
        const Tensor& k,
        const Tensor& v,
        KVCache& cache,
        size_t posOffset
    ) const;
};
```

**é¢„æœŸæ”¶ç›Š**:
- è‡ªå›å½’æ¨ç†æ€§èƒ½æå‡10-100å€
- å†…å­˜ä½¿ç”¨å‡å°‘50-80%

#### 2. å®ç°å†…å­˜æ± 

**ä¼˜å…ˆçº§**: ğŸ”´ P0

**é—®é¢˜æè¿°**:
é¢‘ç¹çš„å†…å­˜åˆ†é…å’Œé‡Šæ”¾å½±å“æ€§èƒ½ã€‚

**æ”¹è¿›æ–¹æ¡ˆ**:
```cpp
class MemoryPool {
public:
    MemoryPool(size_t initialSize = 1024 * 1024 * 1024);

    void* allocate(size_t size);
    void deallocate(void* ptr);

    void reset();

private:
    std::vector<char> buffer_;
    size_t offset_;
    std::mutex mutex_;
};

class MultiHeadAttention {
private:
    mutable std::unique_ptr<MemoryPool> pool_;
    mutable Tensor q2d_;
    mutable Tensor k2d_;
    mutable Tensor v2d_;
    // ...
};
```

**é¢„æœŸæ”¶ç›Š**:
- å†…å­˜åˆ†é…å¼€é”€å‡å°‘80-90%
- æ¨ç†æ€§èƒ½æå‡20-30%

#### 3. æ·»åŠ å•å…ƒæµ‹è¯•

**ä¼˜å…ˆçº§**: ğŸ”´ P0

**é—®é¢˜æè¿°**:
ç¼ºå°‘å•å…ƒæµ‹è¯•ï¼Œæ— æ³•ä¿è¯ä»£ç è´¨é‡ã€‚

**æ”¹è¿›æ–¹æ¡ˆ**:
```cpp
// tests/kylin/test_attention.cpp
TEST(MultiHeadAttentionTest, Constructor_ValidParams) {
    EXPECT_NO_THROW({
        MultiHeadAttention mha(512, 8);
    });
}

TEST(MultiHeadAttentionTest, ForwardNoKV_ValidInput) {
    MultiHeadAttention mha(512, 8);
    Tensor wq({512, 512});
    Tensor wk({512, 512});
    Tensor wv({512, 512});
    Tensor wo({512, 512});
    mha.setWeights(wq, wk, wv, wo);

    Tensor input({1, 10, 512});
    Tensor output = mha.forwardNoKV(input);

    EXPECT_EQ(output.shape()[0], 1);
    EXPECT_EQ(output.shape()[1], 10);
    EXPECT_EQ(output.shape()[2], 512);
}
```

**é¢„æœŸæ”¶ç›Š**:
- æé«˜ä»£ç è´¨é‡
- å‡å°‘bugæ•°é‡
- æé«˜é‡æ„ä¿¡å¿ƒ

### çŸ­æœŸæ”¹è¿›ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰

#### 4. å®ç°Flash Attention

**ä¼˜å…ˆçº§**: ğŸŸ¡ P1

**é—®é¢˜æè¿°**:
å½“å‰æ³¨æ„åŠ›è®¡ç®—å†…å­˜å¤æ‚åº¦é«˜ï¼Œä¸é€‚åˆé•¿åºåˆ—ã€‚

**æ”¹è¿›æ–¹æ¡ˆ**:
å®ç°Flash Attentionç®—æ³•ï¼Œä½¿ç”¨åˆ†å—è®¡ç®—å‡å°‘å†…å­˜è®¿é—®ã€‚

**é¢„æœŸæ”¶ç›Š**:
- å†…å­˜ä½¿ç”¨å‡å°‘50-80%
- è®¡ç®—æ€§èƒ½æå‡2-4å€

#### 5. å¹¶è¡ŒåŒ–è®¡ç®—

**ä¼˜å…ˆçº§**: ğŸŸ¡ P1

**é—®é¢˜æè¿°**:
å½“å‰å®ç°æ²¡æœ‰åˆ©ç”¨å¤šçº¿ç¨‹å¹¶è¡Œè®¡ç®—ã€‚

**æ”¹è¿›æ–¹æ¡ˆ**:
```cpp
#pragma omp parallel for
for (size_t i = 0; i < outerDim; ++i) {
    softmax_stable_row(input + i * innerDim, output + i * innerDim, innerDim);
}
```

**é¢„æœŸæ”¶ç›Š**:
- å¤šæ ¸CPUä¸Šæ€§èƒ½æå‡2-8å€

#### 6. ä¼˜åŒ–æ•°æ®å¸ƒå±€

**ä¼˜å…ˆçº§**: ğŸŸ¡ P1

**é—®é¢˜æè¿°**:
é¢‘ç¹çš„å¼ é‡å½¢çŠ¶è½¬æ¢å¯¼è‡´æ€§èƒ½æŸå¤±ã€‚

**æ”¹è¿›æ–¹æ¡ˆ**:
ä½¿ç”¨Eigençš„reshapeå’Œshuffleæ“ä½œé¿å…æ‰‹åŠ¨é‡æ’ã€‚

**é¢„æœŸæ”¶ç›Š**:
- æ•°æ®é‡æ’å¼€é”€å‡å°‘80-90%
- æ¨ç†æ€§èƒ½æå‡10-15%

### é•¿æœŸæ”¹è¿›ï¼ˆä½ä¼˜å…ˆçº§ï¼‰

#### 7. æ”¯æŒå¤šç§æ•°æ®ç±»å‹

**ä¼˜å…ˆçº§**: ğŸŸ¢ P2

**é—®é¢˜æè¿°**:
å½“å‰ä»…æ”¯æŒFP32ï¼Œé™åˆ¶äº†é‡åŒ–æ¨ç†ã€‚

**æ”¹è¿›æ–¹æ¡ˆ**:
```cpp
enum class DataType {
    FP32,
    FP16,
    BF16,
    INT8,
    INT4
};

class Tensor {
public:
    Tensor(const std::vector<size_t>& shape, DataType dtype = DataType::FP32);

    DataType dtype() const { return dtype_; }

private:
    DataType dtype_;
    std::vector<char> data_;
};
```

**é¢„æœŸæ”¶ç›Š**:
- æ”¯æŒé‡åŒ–æ¨ç†
- å†…å­˜ä½¿ç”¨å‡å°‘50-75%
- æ¨ç†é€Ÿåº¦æå‡2-4å€

#### 8. æ·»åŠ æ€§èƒ½åˆ†æå·¥å…·

**ä¼˜å…ˆçº§**: ğŸŸ¢ P2

**é—®é¢˜æè¿°**:
ç¼ºå°‘æ€§èƒ½åˆ†æå·¥å…·ï¼Œéš¾ä»¥å®šä½æ€§èƒ½ç“¶é¢ˆã€‚

**æ”¹è¿›æ–¹æ¡ˆ**:
```cpp
class Profiler {
public:
    void start(const std::string& name);
    void stop(const std::string& name);

    void report();

private:
    std::map<std::string, std::chrono::nanoseconds> timings_;
};
```

**é¢„æœŸæ”¶ç›Š**:
- å¿«é€Ÿå®šä½æ€§èƒ½ç“¶é¢ˆ
- æŒ‡å¯¼ä¼˜åŒ–æ–¹å‘

#### 9. å®Œå–„æ–‡æ¡£

**ä¼˜å…ˆçº§**: ğŸŸ¢ P2

**é—®é¢˜æè¿°**:
ç¼ºå°‘ä½¿ç”¨ç¤ºä¾‹å’ŒAPIå‚è€ƒæ–‡æ¡£ã€‚

**æ”¹è¿›æ–¹æ¡ˆ**:
1. æ·»åŠ ä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£
2. ç”ŸæˆAPIå‚è€ƒæ–‡æ¡£ï¼ˆä½¿ç”¨Doxygenï¼‰
3. æ·»åŠ æ€§èƒ½è°ƒä¼˜æŒ‡å—
4. æ·»åŠ æ•…éšœæ’æŸ¥æŒ‡å—

**é¢„æœŸæ”¶ç›Š**:
- æé«˜æ˜“ç”¨æ€§
- é™ä½å­¦ä¹ æˆæœ¬

---

## æ€»ç»“

### æ€»ä½“è¯„ä»·

| è¯„ä¼°é¡¹ | è¯„åˆ† | è¯´æ˜ |
|-------|------|------|
| ä»£ç è§„èŒƒ | â­â­â­â­â­ | å®Œå…¨ç¬¦åˆC++æœ€ä½³å®è·µ |
| æ¶æ„è®¾è®¡ | â­â­â­â­â­ | æ¨¡å—åŒ–æ¸…æ™°ï¼ŒèŒè´£åˆ†ç¦»æ˜ç¡® |
| åŠŸèƒ½å®ç° | â­â­â­â­â˜† | MVPå®ç°å®Œæ•´ï¼Œç¼ºå°‘é«˜çº§ä¼˜åŒ– |
| æ€§èƒ½è¡¨ç° | â­â­â­â˜†â˜† | åŸºç¡€æ€§èƒ½è‰¯å¥½ï¼Œç¼ºå°‘å…³é”®ä¼˜åŒ– |
| å®‰å…¨æ€§ | â­â­â­â­â­ | å†…å­˜ç®¡ç†å®‰å…¨ï¼Œå¼‚å¸¸å¤„ç†å®Œå–„ |
| å¯ç»´æŠ¤æ€§ | â­â­â­â­â˜† | ä»£ç ç»“æ„ä¼˜ç§€ï¼Œç¼ºå°‘æµ‹è¯• |

### å…³é”®å‘ç°

#### ä¼˜ç‚¹
1. âœ… **ä»£ç è´¨é‡é«˜**: ä»£ç è§„èŒƒæ¸…æ™°ï¼Œå‘½ååˆç†ï¼Œæ³¨é‡Šå®Œæ•´
2. âœ… **æ¶æ„è®¾è®¡ä¼˜ç§€**: æ¨¡å—åŒ–è®¾è®¡æ¸…æ™°ï¼ŒèŒè´£åˆ†ç¦»æ˜ç¡®ï¼Œä¾èµ–å…³ç³»æ¸…æ™°
3. âœ… **åŠŸèƒ½å®ç°å®Œæ•´**: MVPé˜¶æ®µåŠŸèƒ½å®Œæ•´ï¼Œç¬¦åˆè®¾è®¡æ–‡æ¡£è¦æ±‚
4. âœ… **å®‰å…¨æ€§è‰¯å¥½**: å†…å­˜ç®¡ç†å®‰å…¨ï¼Œå¼‚å¸¸å¤„ç†å®Œå–„ï¼Œæ— æ˜æ˜¾å®‰å…¨éšæ‚£
5. âœ… **å¯ç»´æŠ¤æ€§å¼º**: ä»£ç ç»“æ„ä¼˜ç§€ï¼Œæ³¨é‡Šè´¨é‡é«˜ï¼Œæ˜“äºç†è§£å’Œæ‰©å±•

#### ç¼ºç‚¹
1. âš ï¸ **æ€§èƒ½ä¼˜åŒ–ä¸è¶³**: ç¼ºå°‘KV Cacheã€Flash Attentionç­‰å…³é”®ä¼˜åŒ–æŠ€æœ¯
2. âš ï¸ **å†…å­˜åˆ†é…é¢‘ç¹**: ä¸´æ—¶Tensoråˆ›å»ºè¿‡å¤šï¼Œå†…å­˜åˆ†é…å¼€é”€å¤§
3. âš ï¸ **å¹¶è¡ŒåŒ–ç¼ºå¤±**: æ²¡æœ‰åˆ©ç”¨å¤šçº¿ç¨‹å¹¶è¡Œè®¡ç®—
4. âš ï¸ **æµ‹è¯•è¦†ç›–ä¸è¶³**: ç¼ºå°‘å•å…ƒæµ‹è¯•ã€é›†æˆæµ‹è¯•å’Œæ€§èƒ½æµ‹è¯•
5. âš ï¸ **æ–‡æ¡£ä¸å®Œæ•´**: ç¼ºå°‘ä½¿ç”¨ç¤ºä¾‹å’ŒAPIå‚è€ƒæ–‡æ¡£

### ä¼˜å…ˆçº§å»ºè®®

#### ç«‹å³ä¿®å¤ï¼ˆP0ï¼‰
1. ğŸ”´ å®ç°KV Cacheæœºåˆ¶ - è‡ªå›å½’æ¨ç†æ€§èƒ½æå‡10-100å€
2. ğŸ”´ å®ç°å†…å­˜æ±  - å†…å­˜åˆ†é…å¼€é”€å‡å°‘80-90%
3. ğŸ”´ æ·»åŠ å•å…ƒæµ‹è¯• - æé«˜ä»£ç è´¨é‡å’Œé‡æ„ä¿¡å¿ƒ

#### çŸ­æœŸæ”¹è¿›ï¼ˆP1ï¼‰
4. ğŸŸ¡ å®ç°Flash Attention - å†…å­˜ä½¿ç”¨å‡å°‘50-80%
5. ğŸŸ¡ å¹¶è¡ŒåŒ–è®¡ç®— - å¤šæ ¸CPUä¸Šæ€§èƒ½æå‡2-8å€
6. ğŸŸ¡ ä¼˜åŒ–æ•°æ®å¸ƒå±€ - æ•°æ®é‡æ’å¼€é”€å‡å°‘80-90%

#### é•¿æœŸæ”¹è¿›ï¼ˆP2ï¼‰
7. ğŸŸ¢ æ”¯æŒå¤šç§æ•°æ®ç±»å‹ - æ”¯æŒé‡åŒ–æ¨ç†
8. ğŸŸ¢ æ·»åŠ æ€§èƒ½åˆ†æå·¥å…· - å¿«é€Ÿå®šä½æ€§èƒ½ç“¶é¢ˆ
9. ğŸŸ¢ å®Œå–„æ–‡æ¡£ - æé«˜æ˜“ç”¨æ€§

### ç»“è®º

Kylinæ¨¡å—ä½œä¸ºcLLMé¡¹ç›®çš„è‡ªç ”æ¨ç†å¼•æ“æ ¸å¿ƒï¼Œåœ¨MVPé˜¶æ®µå±•ç°äº†ä¼˜ç§€çš„ä»£ç è´¨é‡å’Œæ¶æ„è®¾è®¡ã€‚ä»£ç è§„èŒƒæ¸…æ™°ï¼Œæ¶æ„è®¾è®¡åˆç†ï¼ŒåŠŸèƒ½å®ç°å®Œæ•´ï¼Œå®‰å…¨æ€§è‰¯å¥½ï¼Œå¯ç»´æŠ¤æ€§å¼ºã€‚

ç„¶è€Œï¼Œåœ¨æ€§èƒ½ä¼˜åŒ–æ–¹é¢è¿˜æœ‰è¾ƒå¤§æå‡ç©ºé—´ã€‚ç¼ºå°‘KV Cacheã€Flash Attentionç­‰å…³é”®ä¼˜åŒ–æŠ€æœ¯ï¼Œå¯¼è‡´è‡ªå›å½’æ¨ç†æ€§èƒ½å—é™ã€‚æ­¤å¤–ï¼Œæµ‹è¯•è¦†ç›–ä¸è¶³å’Œæ–‡æ¡£ä¸å®Œæ•´ä¹Ÿæ˜¯éœ€è¦æ”¹è¿›çš„åœ°æ–¹ã€‚

å»ºè®®æŒ‰ç…§ä¼˜å…ˆçº§é€æ­¥å®æ–½æ”¹è¿›å»ºè®®ï¼Œä¼˜å…ˆå®ç°KV Cacheæœºåˆ¶å’Œå†…å­˜æ± ï¼Œè¿™å°†å¸¦æ¥æœ€å¤§çš„æ€§èƒ½æå‡ã€‚åŒæ—¶ï¼ŒåŠ å¼ºæµ‹è¯•è¦†ç›–å’Œæ–‡æ¡£å®Œå–„ï¼Œæé«˜ä»£ç è´¨é‡å’Œæ˜“ç”¨æ€§ã€‚

æ€»ä½“è€Œè¨€ï¼ŒKylinæ¨¡å—æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„åŸºç¡€å®ç°ï¼Œä¸ºåç»­çš„æ€§èƒ½ä¼˜åŒ–å’ŒåŠŸèƒ½æ‰©å±•å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚é€šè¿‡å®æ–½ä¸Šè¿°æ”¹è¿›å»ºè®®ï¼ŒKylinæ¨¡å—æœ‰æœ›æˆä¸ºæ€§èƒ½ä¼˜å¼‚ã€åŠŸèƒ½å®Œå–„ã€æ˜“äºç»´æŠ¤çš„è‡ªç ”æ¨ç†å¼•æ“ã€‚

---

**æŠ¥å‘Šç»“æŸ**
