# Phase 4: ç³»ç»Ÿé›†æˆæµ‹è¯•é˜¶æ®µ æ‰§è¡Œè®¡åˆ’

**è´Ÿè´£Agent**: Agent-4  
**é¢„è®¡è€—æ—¶**: 24å°æ—¶  
**ä¾èµ–**: Phase 3 å®Œæˆ  
**æ‰§è¡Œæ—¶é—´**: T+44h ~ T+68h  

---

## ğŸ“‹ é˜¶æ®µç›®æ ‡

å¯¹æ•´ä¸ªç³»ç»Ÿè¿›è¡Œå…¨é¢æµ‹è¯•ï¼ŒåŒ…æ‹¬åŠŸèƒ½æµ‹è¯•ã€æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œå‹åŠ›ç¨³å®šæ€§æµ‹è¯•ã€‚

---

## ğŸ“Š ä»»åŠ¡æ¸…å•

| å­é˜¶æ®µ | ä»»åŠ¡æ•° | è€—æ—¶ | ä¾èµ– | çŠ¶æ€ |
|--------|--------|------|------|------|
| P4.1: ç³»ç»ŸåŠŸèƒ½æµ‹è¯• | 4 | 6h | P3.3 | â³ å¾…æ‰§è¡Œ |
| P4.2: æ€§èƒ½åŸºå‡†æµ‹è¯• | 4 | 10h | P4.1 | â³ å¾…æ‰§è¡Œ |
| P4.3: å‹åŠ›å’Œç¨³å®šæ€§æµ‹è¯• | 4 | 8h | P4.2 | â³ å¾…æ‰§è¡Œ |

**æ€»è®¡**: 16ä¸ªä»»åŠ¡ï¼Œ24å°æ—¶

---

## ğŸ“ è¯¦ç»†ä»»åŠ¡è¯´æ˜

### P4.1: ç³»ç»ŸåŠŸèƒ½æµ‹è¯• (6å°æ—¶)

#### æµ‹è¯•é‡ç‚¹
- Chat completion API
- Text completion API
- Streaming API
- Token counting
- APIå…¼å®¹æ€§ï¼ˆOpenAIæ ¼å¼ï¼‰
- é”™è¯¯å¤„ç†

#### ä»»åŠ¡åˆ—è¡¨

**P4.1.1: æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•** (90åˆ†é’Ÿ)

**Chat Completionæµ‹è¯•**:
```cpp
TEST(SystemFunctionality, ChatCompletion) {
  // å¯åŠ¨å®Œæ•´ç³»ç»Ÿ
  cLLMServer server;
  server.initialize("${CLLM_TEST_MODEL_PATH}");
  server.start();
  
  HTTPClient client;
  json request = {
    {"model", "qwen2-0.5b"},
    {"messages", {
      {{"role", "user"}, {"content", "What is 2+2?"}}
    }},
    {"max_tokens", 50},
    {"temperature", 0.7}
  };
  
  auto response = client.post("http://localhost:8080/v1/chat/completions", request);
  
  EXPECT_EQ(response.status_code, 200);
  
  auto result = json::parse(response.body);
  EXPECT_TRUE(result.contains("choices"));
  EXPECT_GT(result["choices"].size(), 0);
  EXPECT_TRUE(result["choices"][0].contains("message"));
  EXPECT_TRUE(result["choices"][0]["message"].contains("content"));
  
  std::string answer = result["choices"][0]["message"]["content"];
  LOG(INFO) << "Answer: " << answer;
  
  server.stop();
}
```

**Text Completionæµ‹è¯•**:
```cpp
TEST(SystemFunctionality, TextCompletion) {
  cLLMServer server;
  server.initialize("${CLLM_TEST_MODEL_PATH}");
  server.start();
  
  HTTPClient client;
  json request = {
    {"model", "qwen2-0.5b"},
    {"prompt", "Once upon a time"},
    {"max_tokens", 100},
    {"temperature", 0.8}
  };
  
  auto response = client.post("http://localhost:8080/v1/completions", request);
  
  EXPECT_EQ(response.status_code, 200);
  
  auto result = json::parse(response.body);
  EXPECT_TRUE(result.contains("choices"));
  EXPECT_FALSE(result["choices"][0]["text"].empty());
  
  server.stop();
}
```

**Streamingæµ‹è¯•**:
```cpp
TEST(SystemFunctionality, Streaming) {
  cLLMServer server;
  server.initialize("${CLLM_TEST_MODEL_PATH}");
  server.start();
  
  HTTPClient client;
  json request = {
    {"model", "qwen2-0.5b"},
    {"messages", {{{"role", "user"}, {"content", "Tell me a story."}}}},
    {"stream", true},
    {"max_tokens", 200}
  };
  
  std::vector<std::string> chunks;
  
  client.postStream("http://localhost:8080/v1/chat/completions", request, 
    [&](const std::string& chunk) {
      if (!chunk.empty() && chunk != "data: [DONE]\n\n") {
        chunks.push_back(chunk);
      }
    }
  );
  
  EXPECT_GT(chunks.size(), 0);
  
  LOG(INFO) << "Received " << chunks.size() << " chunks";
  
  server.stop();
}
```

**Token Countingæµ‹è¯•**:
```cpp
TEST(SystemFunctionality, TokenCounting) {
  cLLMServer server;
  server.initialize("${CLLM_TEST_MODEL_PATH}");
  server.start();
  
  HTTPClient client;
  json request = {
    {"text", "Hello, world! This is a test."}
  };
  
  auto response = client.post("http://localhost:8080/v1/tokenize", request);
  
  EXPECT_EQ(response.status_code, 200);
  
  auto result = json::parse(response.body);
  EXPECT_TRUE(result.contains("tokens"));
  EXPECT_TRUE(result.contains("count"));
  EXPECT_GT(result["count"], 0);
  
  server.stop();
}
```

---

**P4.1.2: API å…¼å®¹æ€§æµ‹è¯•** (90åˆ†é’Ÿ)

**OpenAIæ ¼å¼å…¼å®¹æ€§**:
```cpp
TEST(SystemFunctionality, OpenAICompatibility) {
  cLLMServer server;
  server.initialize("${CLLM_TEST_MODEL_PATH}");
  server.start();
  
  HTTPClient client;
  
  // ä½¿ç”¨æ ‡å‡†OpenAIè¯·æ±‚æ ¼å¼
  json request = {
    {"model", "qwen2-0.5b"},
    {"messages", {
      {{"role", "system"}, {"content", "You are a helpful assistant."}},
      {{"role", "user"}, {"content", "Hello!"}}
    }},
    {"temperature", 0.7},
    {"top_p", 0.9},
    {"max_tokens", 100},
    {"presence_penalty", 0.0},
    {"frequency_penalty", 0.0}
  };
  
  auto response = client.post("http://localhost:8080/v1/chat/completions", request);
  
  EXPECT_EQ(response.status_code, 200);
  
  auto result = json::parse(response.body);
  
  // éªŒè¯å“åº”æ ¼å¼ç¬¦åˆOpenAIè§„èŒƒ
  EXPECT_TRUE(result.contains("id"));
  EXPECT_TRUE(result.contains("object"));
  EXPECT_EQ(result["object"], "chat.completion");
  EXPECT_TRUE(result.contains("created"));
  EXPECT_TRUE(result.contains("model"));
  EXPECT_TRUE(result.contains("choices"));
  EXPECT_TRUE(result.contains("usage"));
  EXPECT_TRUE(result["usage"].contains("prompt_tokens"));
  EXPECT_TRUE(result["usage"].contains("completion_tokens"));
  EXPECT_TRUE(result["usage"].contains("total_tokens"));
  
  server.stop();
}
```

---

**P4.1.3: å¤šåœºæ™¯æµ‹è¯•** (90åˆ†é’Ÿ)

**åœºæ™¯1: äº‹å®é—®ç­”**:
```cpp
TEST(SystemFunctionality, FactualQA) {
  cLLMServer server;
  server.initialize("${CLLM_TEST_MODEL_PATH}");
  server.start();
  
  HTTPClient client;
  
  std::vector<std::pair<std::string, std::vector<std::string>>> qa_pairs = {
    {"What is the capital of France?", {"Paris"}},
    {"Who wrote Romeo and Juliet?", {"Shakespeare", "William Shakespeare"}},
    {"What is 15 + 27?", {"42"}}
  };
  
  for (const auto& [question, expected_keywords] : qa_pairs) {
    json request = {
      {"model", "qwen2-0.5b"},
      {"messages", {{{"role", "user"}, {"content", question}}}},
      {"max_tokens", 50}
    };
    
    auto response = client.post("http://localhost:8080/v1/chat/completions", request);
    auto result = json::parse(response.body);
    
    std::string answer = result["choices"][0]["message"]["content"];
    
    // æ£€æŸ¥ç­”æ¡ˆä¸­æ˜¯å¦åŒ…å«é¢„æœŸå…³é”®è¯
    bool found = false;
    for (const auto& keyword : expected_keywords) {
      if (answer.find(keyword) != std::string::npos) {
        found = true;
        break;
      }
    }
    
    EXPECT_TRUE(found) << "Question: " << question << ", Answer: " << answer;
  }
  
  server.stop();
}
```

**åœºæ™¯2: ä»£ç ç”Ÿæˆ**:
```cpp
TEST(SystemFunctionality, CodeGeneration) {
  cLLMServer server;
  server.initialize("${CLLM_TEST_MODEL_PATH}");
  server.start();
  
  HTTPClient client;
  json request = {
    {"model", "qwen2-0.5b"},
    {"messages", {{{"role", "user"}, {"content", "Write a Python function to calculate factorial."}}}},
    {"max_tokens", 200}
  };
  
  auto response = client.post("http://localhost:8080/v1/chat/completions", request);
  auto result = json::parse(response.body);
  
  std::string code = result["choices"][0]["message"]["content"];
  
  // éªŒè¯ä»£ç åŒ…å«å…³é”®è¦ç´ 
  EXPECT_TRUE(code.find("def") != std::string::npos || code.find("function") != std::string::npos);
  EXPECT_TRUE(code.find("factorial") != std::string::npos);
  
  LOG(INFO) << "Generated code:\n" << code;
  
  server.stop();
}
```

---

**P4.1.4: é”™è¯¯å¤„ç†æµ‹è¯•** (90åˆ†é’Ÿ)

**å„ç§é”™è¯¯æƒ…å†µ**:
```cpp
TEST(SystemFunctionality, ErrorHandling) {
  cLLMServer server;
  server.initialize("${CLLM_TEST_MODEL_PATH}");
  server.start();
  
  HTTPClient client;
  
  // é”™è¯¯1: ç¼ºå°‘å¿…è¦å­—æ®µ
  json invalid_request1 = {
    {"model", "qwen2-0.5b"}
    // ç¼ºå°‘ messages
  };
  auto response1 = client.post("http://localhost:8080/v1/chat/completions", invalid_request1);
  EXPECT_EQ(response1.status_code, 400);
  
  // é”™è¯¯2: æ— æ•ˆçš„æ¨¡å‹å
  json invalid_request2 = {
    {"model", "invalid-model"},
    {"messages", {{{"role", "user"}, {"content", "test"}}}}
  };
  auto response2 = client.post("http://localhost:8080/v1/chat/completions", invalid_request2);
  EXPECT_TRUE(response2.status_code == 400 || response2.status_code == 404);
  
  // é”™è¯¯3: max_tokens è¶…é™
  json invalid_request3 = {
    {"model", "qwen2-0.5b"},
    {"messages", {{{"role", "user"}, {"content", "test"}}}},
    {"max_tokens", 100000} // è¿œè¶…æ¨¡å‹é™åˆ¶
  };
  auto response3 = client.post("http://localhost:8080/v1/chat/completions", invalid_request3);
  EXPECT_TRUE(response3.status_code == 400 || response3.status_code == 200);
  
  // é”™è¯¯4: æ— æ•ˆçš„ temperature
  json invalid_request4 = {
    {"model", "qwen2-0.5b"},
    {"messages", {{{"role", "user"}, {"content", "test"}}}},
    {"temperature", -1.0} // æ— æ•ˆå€¼
  };
  auto response4 = client.post("http://localhost:8080/v1/chat/completions", invalid_request4);
  EXPECT_EQ(response4.status_code, 400);
  
  // ç³»ç»Ÿåº”è¯¥ä»ç„¶æ­£å¸¸å·¥ä½œ
  json valid_request = {
    {"model", "qwen2-0.5b"},
    {"messages", {{{"role", "user"}, {"content", "Are you OK?"}}}},
    {"max_tokens", 20}
  };
  auto response5 = client.post("http://localhost:8080/v1/chat/completions", valid_request);
  EXPECT_EQ(response5.status_code, 200);
  
  server.stop();
}
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æ­£å¸¸
- âœ… API å“åº”ç¬¦åˆ OpenAI æ ¼å¼
- âœ… å¤šåœºæ™¯æµ‹è¯•é€šè¿‡
- âœ… é”™è¯¯å¤„ç†å¥å£®

---

### P4.2: æ€§èƒ½åŸºå‡†æµ‹è¯• (10å°æ—¶)

#### æµ‹è¯•é‡ç‚¹
- ååé‡æµ‹è¯•
- å»¶è¿Ÿæµ‹è¯•ï¼ˆP50/P95/P99ï¼‰
- èµ„æºä½¿ç”¨æµ‹è¯•
- æ‰©å±•æ€§æµ‹è¯•

#### ä»»åŠ¡åˆ—è¡¨

**P4.2.1: ååé‡æµ‹è¯•** (150åˆ†é’Ÿ)

**å•è¯·æ±‚ååé‡**:
```cpp
TEST(PerformanceBenchmark, SingleRequestThroughput) {
  cLLMServer server;
  server.initialize("${CLLM_TEST_MODEL_PATH}");
  server.start();
  
  HTTPClient client;
  json request = {
    {"model", "qwen2-0.5b"},
    {"messages", {{{"role", "user"}, {"content", "Count from 1 to 100."}}}},
    {"max_tokens", 200}
  };
  
  auto start = std::chrono::high_resolution_clock::now();
  
  auto response = client.post("http://localhost:8080/v1/chat/completions", request);
  
  auto end = std::chrono::high_resolution_clock::now();
  
  EXPECT_EQ(response.status_code, 200);
  
  auto result = json::parse(response.body);
  int completion_tokens = result["usage"]["completion_tokens"];
  
  double duration_sec = std::chrono::duration_cast<std::chrono::milliseconds>(
    end - start
  ).count() / 1000.0;
  
  double tokens_per_sec = completion_tokens / duration_sec;
  
  LOG(INFO) << "Single Request Throughput: " << tokens_per_sec << " tokens/sec";
  LOG(INFO) << "Completion tokens: " << completion_tokens;
  LOG(INFO) << "Duration: " << duration_sec << " sec";
  
  // ç›®æ ‡: > 100 tokens/sec
  EXPECT_GT(tokens_per_sec, 100);
  
  server.stop();
}
```

**æ‰¹å¤„ç†ååé‡**:
```cpp
TEST(PerformanceBenchmark, BatchThroughput) {
  cLLMServer server;
  server.initialize("${CLLM_TEST_MODEL_PATH}");
  server.start();
  
  const int BATCH_SIZE = 8;
  std::vector<std::thread> threads;
  std::atomic<int> total_tokens{0};
  
  auto start = std::chrono::high_resolution_clock::now();
  
  for (int i = 0; i < BATCH_SIZE; ++i) {
    threads.emplace_back([&, i]() {
      HTTPClient client;
      json request = {
        {"model", "qwen2-0.5b"},
        {"messages", {{{"role", "user"}, {"content", "Test " + std::to_string(i)}}}},
        {"max_tokens", 50}
      };
      
      auto response = client.post("http://localhost:8080/v1/chat/completions", request);
      
      if (response.status_code == 200) {
        auto result = json::parse(response.body);
        total_tokens += result["usage"]["completion_tokens"].get<int>();
      }
    });
  }
  
  for (auto& thread : threads) {
    thread.join();
  }
  
  auto end = std::chrono::high_resolution_clock::now();
  
  double duration_sec = std::chrono::duration_cast<std::chrono::milliseconds>(
    end - start
  ).count() / 1000.0;
  
  double throughput = total_tokens.load() / duration_sec;
  
  LOG(INFO) << "Batch Throughput (" << BATCH_SIZE << "): " << throughput << " tokens/sec";
  LOG(INFO) << "Total tokens: " << total_tokens.load();
  LOG(INFO) << "Duration: " << duration_sec << " sec";
  
  server.stop();
}
```

---

**P4.2.2: å»¶è¿Ÿæµ‹è¯•ï¼ˆP50/P95/P99ï¼‰** (150åˆ†é’Ÿ)

```cpp
TEST(PerformanceBenchmark, LatencyDistribution) {
  cLLMServer server;
  server.initialize("${CLLM_TEST_MODEL_PATH}");
  server.start();
  
  const int NUM_REQUESTS = 100;
  std::vector<double> latencies;
  
  HTTPClient client;
  
  for (int i = 0; i < NUM_REQUESTS; ++i) {
    json request = {
      {"model", "qwen2-0.5b"},
      {"messages", {{{"role", "user"}, {"content", "Hello"}}}},
      {"max_tokens", 10}
    };
    
    auto start = std::chrono::high_resolution_clock::now();
    auto response = client.post("http://localhost:8080/v1/chat/completions", request);
    auto end = std::chrono::high_resolution_clock::now();
    
    if (response.status_code == 200) {
      double latency = std::chrono::duration_cast<std::chrono::milliseconds>(
        end - start
      ).count();
      latencies.push_back(latency);
    }
  }
  
  std::sort(latencies.begin(), latencies.end());
  
  double p50 = latencies[NUM_REQUESTS * 50 / 100];
  double p95 = latencies[NUM_REQUESTS * 95 / 100];
  double p99 = latencies[NUM_REQUESTS * 99 / 100];
  double mean = std::accumulate(latencies.begin(), latencies.end(), 0.0) / latencies.size();
  
  LOG(INFO) << "Latency Distribution:";
  LOG(INFO) << "  Mean: " << mean << " ms";
  LOG(INFO) << "  P50: " << p50 << " ms";
  LOG(INFO) << "  P95: " << p95 << " ms";
  LOG(INFO) << "  P99: " << p99 << " ms";
  
  // æ€§èƒ½ç›®æ ‡
  EXPECT_LT(p50, 50);   // P50 < 50ms
  EXPECT_LT(p95, 100);  // P95 < 100ms
  EXPECT_LT(p99, 200);  // P99 < 200ms
  
  server.stop();
}
```

---

**P4.2.3: èµ„æºä½¿ç”¨æµ‹è¯•** (150åˆ†é’Ÿ)
**P4.2.4: æ‰©å±•æ€§æµ‹è¯•** (150åˆ†é’Ÿ)

**éªŒæ”¶æ ‡å‡†**:
- âœ… ååé‡ > 100 tokens/sec
- âœ… P50 å»¶è¿Ÿ < 50ms
- âœ… P95 å»¶è¿Ÿ < 100ms
- âœ… P99 å»¶è¿Ÿ < 200ms
- âœ… å†…å­˜ä½¿ç”¨ < 8GB
- âœ… CPU ä½¿ç”¨åˆç†

---

### P4.3: å‹åŠ›å’Œç¨³å®šæ€§æµ‹è¯• (8å°æ—¶)

#### æµ‹è¯•é‡ç‚¹
- é«˜å¹¶å‘æµ‹è¯•ï¼ˆ8 å¹¶å‘ï¼‰
- é•¿æ—¶é—´è¿è¡Œï¼ˆ5 åˆ†é’Ÿï¼‰
- å¼‚å¸¸æ³¨å…¥æµ‹è¯•
- æ¢å¤æµ‹è¯•

#### ä»»åŠ¡åˆ—è¡¨

**P4.3.1: é«˜å¹¶å‘æµ‹è¯•ï¼ˆ8 å¹¶å‘ï¼‰** (8)
**P4.3.2: é•¿æ—¶é—´è¿è¡Œï¼ˆ5 åˆ†é’Ÿï¼‰** (5)


**éªŒæ”¶æ ‡å‡†**:
- âœ… 8 å¹¶å‘æ— é”™è¯¯
- âœ… é•¿æ—¶é—´è¿è¡Œç¨³å®šï¼ˆæ— å†…å­˜æ³„æ¼ï¼‰
- âœ… å¼‚å¸¸æƒ…å†µæ­£ç¡®å¤„ç†
- âœ… ç³»ç»Ÿå¯æ¢å¤

---

## âœ… æ€»ä½“éªŒæ”¶æ ‡å‡†

### å¿…é¡»å®Œæˆ

- [ ] P4.1: ç³»ç»ŸåŠŸèƒ½æµ‹è¯•é€šè¿‡
- [ ] P4.2: æ€§èƒ½åŸºå‡†è¾¾æ ‡
- [ ] P4.3: å‹åŠ›ç¨³å®šæ€§æµ‹è¯•é€šè¿‡

### è´¨é‡æŒ‡æ ‡

- [ ] æ‰€æœ‰APIæ­£å¸¸å·¥ä½œ
- [ ] æ€§èƒ½æŒ‡æ ‡å…¨éƒ¨è¾¾æ ‡
- [ ] 100 å¹¶å‘æµ‹è¯•é€šè¿‡
- [ ] é•¿æ—¶é—´è¿è¡Œæ— å´©æºƒ

---

## ğŸ“Š æ‰§è¡ŒæŠ¥å‘Š

**æ‰§è¡Œæ—¶é—´**: ________

**å®Œæˆæƒ…å†µ**:
- P4.1: â˜ å®Œæˆ / â˜ å¤±è´¥
- P4.2: â˜ å®Œæˆ / â˜ å¤±è´¥
- P4.3: â˜ å®Œæˆ / â˜ å¤±è´¥

**æ€§èƒ½æŒ‡æ ‡**:
- ååé‡: ________ tokens/sec
- P50 å»¶è¿Ÿ: ________ ms
- P95 å»¶è¿Ÿ: ________ ms
- P99 å»¶è¿Ÿ: ________ ms
- æœ€å¤§å¹¶å‘: ________ 
- é•¿æ—¶é—´è¿è¡Œ: ________ åˆ†é’Ÿ

**æ€»ä½“çŠ¶æ€**: â˜ æˆåŠŸ / â˜ éƒ¨åˆ†æˆåŠŸ / â˜ å¤±è´¥

---

## ğŸ”„ ä¸‹ä¸€æ­¥

Phase 4 å®Œæˆåï¼Œé€šçŸ¥ Agent-5 å¯åŠ¨ Phase 5:

```bash
touch /tmp/cllm_test_locks/phase4.done
echo "âœ… Phase 4 å®Œæˆï¼ŒAgent-5 å¯ä»¥å¯åŠ¨ Phase 5"
```
