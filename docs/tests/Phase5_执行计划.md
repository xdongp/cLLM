# Phase 5: E2E åœºæ™¯æµ‹è¯•é˜¶æ®µ æ‰§è¡Œè®¡åˆ’

**è´Ÿè´£Agent**: Agent-5  
**é¢„è®¡è€—æ—¶**: 6å°æ—¶  
**ä¾èµ–**: Phase 4 å®Œæˆ  
**æ‰§è¡Œæ—¶é—´**: T+68h ~ T+74h  

---

## ğŸ“‹ é˜¶æ®µç›®æ ‡

è¿›è¡Œç«¯åˆ°ç«¯çš„çœŸå®åœºæ™¯æµ‹è¯•ï¼ŒéªŒè¯ç³»ç»Ÿåœ¨å®é™…ä½¿ç”¨åœºæ™¯ä¸­çš„è¡¨ç°å’Œè´¨é‡ã€‚

---

## ğŸ“Š ä»»åŠ¡æ¸…å•

| ä»»åŠ¡ID | ä»»åŠ¡åç§° | è€—æ—¶ | ä¾èµ– | çŠ¶æ€ |
|--------|---------|------|------|------|
| P5.1.1 | å•è½®é—®ç­”åœºæ™¯ | 90min | P4.3 | â³ å¾…æ‰§è¡Œ |
| P5.1.2 | å¤šè½®å¯¹è¯åœºæ™¯ | 90min | P5.1.1 | â³ å¾…æ‰§è¡Œ |
| P5.1.3 | ä¸“ä¸šä»»åŠ¡åœºæ™¯ | 90min | P5.1.2 | â³ å¾…æ‰§è¡Œ |
| P5.1.4 | è´¨é‡è¯„ä¼° | 90min | P5.1.3 | â³ å¾…æ‰§è¡Œ |

**æ€»è®¡**: 4ä¸ªä»»åŠ¡ï¼Œ6å°æ—¶

---

## ğŸ“ è¯¦ç»†ä»»åŠ¡è¯´æ˜

### P5.1.1: å•è½®é—®ç­”åœºæ™¯ (90åˆ†é’Ÿ)

#### æµ‹è¯•é‡ç‚¹
- äº‹å®é—®ç­”
- æ¨ç†é—®ç­”
- å¸¸è¯†é—®ç­”
- æ•°å­¦é—®ç­”

#### åœºæ™¯æµ‹è¯•

**äº‹å®é—®ç­”**:
```cpp
TEST(E2EScenarios, FactualQuestions) {
  cLLMServer server;
  server.initialize("${CLLM_TEST_MODEL_PATH}");
  server.start();
  
  HTTPClient client;
  
  std::vector<std::pair<std::string, std::vector<std::string>>> test_cases = {
    {"What is the capital of China?", {"Beijing", "åŒ—äº¬"}},
    {"Who invented the telephone?", {"Bell", "Alexander"}},
    {"What is the largest planet in our solar system?", {"Jupiter", "æœ¨æ˜Ÿ"}},
    {"When did World War II end?", {"1945"}},
    {"What is the speed of light?", {"300000", "3Ã—10^8", "299792458"}}
  };
  
  int passed = 0;
  int total = test_cases.size();
  
  for (const auto& [question, expected_keywords] : test_cases) {
    json request = {
      {"model", "qwen2-0.5b"},
      {"messages", {{{"role", "user"}, {"content", question}}}},
      {"max_tokens", 100},
      {"temperature", 0.3} // ä½æ¸©åº¦ï¼Œæ›´ç¡®å®šçš„å›ç­”
    };
    
    auto response = client.post("http://localhost:8080/v1/chat/completions", request);
    ASSERT_EQ(response.status_code, 200);
    
    auto result = json::parse(response.body);
    std::string answer = result["choices"][0]["message"]["content"];
    
    LOG(INFO) << "Q: " << question;
    LOG(INFO) << "A: " << answer;
    
    // æ£€æŸ¥ç­”æ¡ˆä¸­æ˜¯å¦åŒ…å«é¢„æœŸå…³é”®è¯
    bool correct = false;
    for (const auto& keyword : expected_keywords) {
      if (answer.find(keyword) != std::string::npos) {
        correct = true;
        break;
      }
    }
    
    if (correct) {
      passed++;
      LOG(INFO) << "âœ… PASS";
    } else {
      LOG(WARNING) << "âŒ FAIL (expected keywords: " 
                   << join(expected_keywords, ", ") << ")";
    }
    LOG(INFO) << "---";
  }
  
  double accuracy = static_cast<double>(passed) / total;
  LOG(INFO) << "Factual QA Accuracy: " << accuracy * 100 << "% (" 
            << passed << "/" << total << ")";
  
  // æœŸæœ›è‡³å°‘60%æ­£ç¡®ç‡
  EXPECT_GE(accuracy, 0.6);
  
  server.stop();
}
```

**æ¨ç†é—®ç­”**:
```cpp
TEST(E2EScenarios, ReasoningQuestions) {
  cLLMServer server;
  server.initialize("${CLLM_TEST_MODEL_PATH}");
  server.start();
  
  HTTPClient client;
  
  std::vector<std::pair<std::string, std::vector<std::string>>> test_cases = {
    {
      "If A is taller than B, and B is taller than C, who is the tallest?",
      {"A"}
    },
    {
      "If all roses are flowers, and some flowers fade quickly, can we conclude all roses fade quickly?",
      {"No", "cannot", "not necessarily"}
    },
    {
      "A train leaves Station A at 60 km/h, and another train leaves Station B (120km away) at 40 km/h towards each other. When do they meet?",
      {"1.2", "72", "minutes"}
    }
  };
  
  int passed = 0;
  int total = test_cases.size();
  
  for (const auto& [question, expected_indicators] : test_cases) {
    json request = {
      {"model", "qwen2-0.5b"},
      {"messages", {{{"role", "user"}, {"content", question}}}},
      {"max_tokens", 200},
      {"temperature", 0.5}
    };
    
    auto response = client.post("http://localhost:8080/v1/chat/completions", request);
    auto result = json::parse(response.body);
    std::string answer = result["choices"][0]["message"]["content"];
    
    LOG(INFO) << "Q: " << question;
    LOG(INFO) << "A: " << answer;
    
    bool correct = false;
    for (const auto& indicator : expected_indicators) {
      if (answer.find(indicator) != std::string::npos) {
        correct = true;
        break;
      }
    }
    
    if (correct) {
      passed++;
      LOG(INFO) << "âœ… PASS";
    } else {
      LOG(WARNING) << "âŒ FAIL";
    }
    LOG(INFO) << "---";
  }
  
  double accuracy = static_cast<double>(passed) / total;
  LOG(INFO) << "Reasoning QA Accuracy: " << accuracy * 100 << "%";
  
  // æ¨ç†é—®é¢˜æ›´éš¾ï¼ŒæœŸæœ›è‡³å°‘40%æ­£ç¡®ç‡
  EXPECT_GE(accuracy, 0.4);
  
  server.stop();
}
```

**æ•°å­¦é—®ç­”**:
```cpp
TEST(E2EScenarios, MathQuestions) {
  cLLMServer server;
  server.initialize("${CLLM_TEST_MODEL_PATH}");
  server.start();
  
  HTTPClient client;
  
  std::vector<std::pair<std::string, std::string>> test_cases = {
    {"What is 15 + 27?", "42"},
    {"What is 8 Ã— 7?", "56"},
    {"What is 100 - 37?", "63"},
    {"What is 144 Ã· 12?", "12"},
    {"What is 2^10?", "1024"}
  };
  
  int passed = 0;
  
  for (const auto& [question, expected_answer] : test_cases) {
    json request = {
      {"model", "qwen2-0.5b"},
      {"messages", {{{"role", "user"}, {"content", question}}}},
      {"max_tokens", 50},
      {"temperature", 0.1}
    };
    
    auto response = client.post("http://localhost:8080/v1/chat/completions", request);
    auto result = json::parse(response.body);
    std::string answer = result["choices"][0]["message"]["content"];
    
    LOG(INFO) << "Q: " << question;
    LOG(INFO) << "A: " << answer;
    
    if (answer.find(expected_answer) != std::string::npos) {
      passed++;
      LOG(INFO) << "âœ… PASS";
    } else {
      LOG(WARNING) << "âŒ FAIL (expected: " << expected_answer << ")";
    }
    LOG(INFO) << "---";
  }
  
  double accuracy = static_cast<double>(passed) / test_cases.size();
  LOG(INFO) << "Math QA Accuracy: " << accuracy * 100 << "%";
  
  EXPECT_GE(accuracy, 0.7); // ç®€å•æ•°å­¦é¢˜æœŸæœ›70%+
  
  server.stop();
}
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… äº‹å®é—®ç­”æ­£ç¡®ç‡ > 60%
- âœ… æ¨ç†é—®ç­”æ­£ç¡®ç‡ > 40%
- âœ… æ•°å­¦é—®ç­”æ­£ç¡®ç‡ > 70%
- âœ… æ‰€æœ‰å›ç­”æ ¼å¼æ­£ç¡®

---

### P5.1.2: å¤šè½®å¯¹è¯åœºæ™¯ (90åˆ†é’Ÿ)

#### æµ‹è¯•é‡ç‚¹
- ä¸Šä¸‹æ–‡ä¿æŒ
- æŒ‡ä»£æ¶ˆè§£
- è¯é¢˜åˆ‡æ¢
- å¯¹è¯è¿è´¯æ€§

#### åœºæ™¯æµ‹è¯•

**ä¸Šä¸‹æ–‡ä¿æŒ**:
```cpp
TEST(E2EScenarios, ContextRetention) {
  cLLMServer server;
  server.initialize("${CLLM_TEST_MODEL_PATH}");
  server.start();
  
  HTTPClient client;
  
  // å¤šè½®å¯¹è¯
  std::vector<json> messages;
  
  // ç¬¬1è½®ï¼šä»‹ç»ä¿¡æ¯
  messages.push_back({{"role", "user"}, {"content", "My name is Alice and I'm 25 years old."}});
  
  json request1 = {
    {"model", "qwen2-0.5b"},
    {"messages", messages},
    {"max_tokens", 50}
  };
  
  auto response1 = client.post("http://localhost:8080/v1/chat/completions", request1);
  auto result1 = json::parse(response1.body);
  std::string answer1 = result1["choices"][0]["message"]["content"];
  
  messages.push_back({{"role", "assistant"}, {"content", answer1}});
  LOG(INFO) << "Round 1 - User: My name is Alice and I'm 25 years old.";
  LOG(INFO) << "Round 1 - Assistant: " << answer1;
  
  // ç¬¬2è½®ï¼šè¯¢é—®ä¹‹å‰æåˆ°çš„ä¿¡æ¯
  messages.push_back({{"role", "user"}, {"content", "What's my name?"}});
  
  json request2 = {
    {"model", "qwen2-0.5b"},
    {"messages", messages},
    {"max_tokens", 20}
  };
  
  auto response2 = client.post("http://localhost:8080/v1/chat/completions", request2);
  auto result2 = json::parse(response2.body);
  std::string answer2 = result2["choices"][0]["message"]["content"];
  
  LOG(INFO) << "Round 2 - User: What's my name?";
  LOG(INFO) << "Round 2 - Assistant: " << answer2;
  
  // éªŒè¯ï¼šç­”æ¡ˆä¸­åº”åŒ…å« "Alice"
  EXPECT_TRUE(answer2.find("Alice") != std::string::npos) 
    << "Context not retained: " << answer2;
  
  // ç¬¬3è½®ï¼šè¯¢é—®å¹´é¾„
  messages.push_back({{"role", "assistant"}, {"content", answer2}});
  messages.push_back({{"role", "user"}, {"content", "How old am I?"}});
  
  json request3 = {
    {"model", "qwen2-0.5b"},
    {"messages", messages},
    {"max_tokens", 20}
  };
  
  auto response3 = client.post("http://localhost:8080/v1/chat/completions", request3);
  auto result3 = json::parse(response3.body);
  std::string answer3 = result3["choices"][0]["message"]["content"];
  
  LOG(INFO) << "Round 3 - User: How old am I?";
  LOG(INFO) << "Round 3 - Assistant: " << answer3;
  
  // éªŒè¯ï¼šç­”æ¡ˆä¸­åº”åŒ…å« "25"
  EXPECT_TRUE(answer3.find("25") != std::string::npos)
    << "Context not retained: " << answer3;
  
  server.stop();
}
```

**æŒ‡ä»£æ¶ˆè§£**:
```cpp
TEST(E2EScenarios, Coreference) {
  cLLMServer server;
  server.initialize("${CLLM_TEST_MODEL_PATH}");
  server.start();
  
  HTTPClient client;
  
  std::vector<json> messages;
  
  // ç¬¬1è½®ï¼šå¼•å…¥ä¸»é¢˜
  messages.push_back({{"role", "user"}, {"content", "Tell me about Python programming language."}});
  
  json request1 = {
    {"model", "qwen2-0.5b"},
    {"messages", messages},
    {"max_tokens", 100}
  };
  
  auto response1 = client.post("http://localhost:8080/v1/chat/completions", request1);
  auto result1 = json::parse(response1.body);
  std::string answer1 = result1["choices"][0]["message"]["content"];
  
  messages.push_back({{"role", "assistant"}, {"content", answer1}});
  
  // ç¬¬2è½®ï¼šä½¿ç”¨æŒ‡ä»£è¯ "it"
  messages.push_back({{"role", "user"}, {"content", "What is it mainly used for?"}});
  
  json request2 = {
    {"model", "qwen2-0.5b"},
    {"messages", messages},
    {"max_tokens", 100}
  };
  
  auto response2 = client.post("http://localhost:8080/v1/chat/completions", request2);
  auto result2 = json::parse(response2.body);
  std::string answer2 = result2["choices"][0]["message"]["content"];
  
  LOG(INFO) << "User: What is it mainly used for?";
  LOG(INFO) << "Assistant: " << answer2;
  
  // éªŒè¯ï¼šç­”æ¡ˆåº”è¯¥ä¸Pythonç›¸å…³
  bool relevant = answer2.find("Python") != std::string::npos ||
                  answer2.find("programming") != std::string::npos ||
                  answer2.find("web") != std::string::npos ||
                  answer2.find("data") != std::string::npos ||
                  answer2.find("AI") != std::string::npos;
  
  EXPECT_TRUE(relevant) << "Failed to resolve coreference";
  
  server.stop();
}
```

**è¯é¢˜åˆ‡æ¢**:
```cpp
TEST(E2EScenarios, TopicSwitch) {
  cLLMServer server;
  server.initialize("${CLLM_TEST_MODEL_PATH}");
  server.start();
  
  HTTPClient client;
  
  std::vector<json> messages;
  
  // è¯é¢˜1ï¼šå¤©æ°”
  messages.push_back({{"role", "user"}, {"content", "What's the weather like today?"}});
  
  json request1 = {{"model", "qwen2-0.5b"}, {"messages", messages}, {"max_tokens", 50}};
  auto response1 = client.post("http://localhost:8080/v1/chat/completions", request1);
  auto result1 = json::parse(response1.body);
  messages.push_back({{"role", "assistant"}, {"content", result1["choices"][0]["message"]["content"]}});
  
  // è¯é¢˜åˆ‡æ¢åˆ°ï¼šç¼–ç¨‹
  messages.push_back({{"role", "user"}, {"content", "By the way, can you help me write a Python function?"}});
  
  json request2 = {{"model", "qwen2-0.5b"}, {"messages", messages}, {"max_tokens", 100}};
  auto response2 = client.post("http://localhost:8080/v1/chat/completions", request2);
  auto result2 = json::parse(response2.body);
  std::string answer2 = result2["choices"][0]["message"]["content"];
  
  LOG(INFO) << "Assistant (after topic switch): " << answer2;
  
  // éªŒè¯ï¼šåº”è¯¥èƒ½å¤Ÿå¤„ç†è¯é¢˜åˆ‡æ¢
  bool handled = answer2.find("Python") != std::string::npos ||
                 answer2.find("function") != std::string::npos ||
                 answer2.find("def") != std::string::npos ||
                 answer2.find("code") != std::string::npos;
  
  EXPECT_TRUE(handled) << "Failed to handle topic switch";
  
  server.stop();
}
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… ä¸Šä¸‹æ–‡ä¿æŒæ­£ç¡®
- âœ… æŒ‡ä»£æ¶ˆè§£æ­£ç¡®
- âœ… è¯é¢˜åˆ‡æ¢å¤„ç†æ­£ç¡®
- âœ… å¯¹è¯è¿è´¯æ€§è‰¯å¥½

---

### P5.1.3: ä¸“ä¸šä»»åŠ¡åœºæ™¯ (90åˆ†é’Ÿ)

#### æµ‹è¯•é‡ç‚¹
- ä»£ç ç”Ÿæˆ
- æ–‡æœ¬æ‘˜è¦
- ç¿»è¯‘
- æ–‡æ¡£æ’°å†™

#### åœºæ™¯æµ‹è¯•

**ä»£ç ç”Ÿæˆ**:
```cpp
TEST(E2EScenarios, CodeGeneration) {
  cLLMServer server;
  server.initialize("${CLLM_TEST_MODEL_PATH}");
  server.start();
  
  HTTPClient client;
  
  std::vector<std::string> prompts = {
    "Write a Python function to calculate Fibonacci numbers.",
    "Write a JavaScript function to check if a string is a palindrome.",
    "Write a C++ function to sort an array using quicksort."
  };
  
  for (const auto& prompt : prompts) {
    json request = {
      {"model", "qwen2-0.5b"},
      {"messages", {{{"role", "user"}, {"content", prompt}}}},
      {"max_tokens", 300}
    };
    
    auto response = client.post("http://localhost:8080/v1/chat/completions", request);
    auto result = json::parse(response.body);
    std::string code = result["choices"][0]["message"]["content"];
    
    LOG(INFO) << "Prompt: " << prompt;
    LOG(INFO) << "Generated code:\n" << code;
    LOG(INFO) << "---";
    
    // éªŒè¯ä»£ç åŒ…å«åŸºæœ¬è¦ç´ 
    bool has_function = code.find("def ") != std::string::npos ||
                       code.find("function ") != std::string::npos ||
                       code.find("void ") != std::string::npos;
    
    EXPECT_TRUE(has_function) << "No function definition found";
  }
  
  server.stop();
}
```

**æ–‡æœ¬æ‘˜è¦**:
```cpp
TEST(E2EScenarios, TextSummarization) {
  cLLMServer server;
  server.initialize("${CLLM_TEST_MODEL_PATH}");
  server.start();
  
  HTTPClient client;
  
  std::string long_text = R"(
    Artificial intelligence (AI) is intelligence demonstrated by machines, 
    as opposed to natural intelligence displayed by animals including humans. 
    AI research has been defined as the field of study of intelligent agents, 
    which refers to any system that perceives its environment and takes actions 
    that maximize its chance of achieving its goals. The term "artificial intelligence" 
    had previously been used to describe machines that mimic and display "human" 
    cognitive skills that are associated with the human mind, such as "learning" 
    and "problem-solving". This definition has since been rejected by major AI 
    researchers who now describe AI in terms of rationality and acting rationally, 
    which does not limit how intelligence can be articulated.
  )";
  
  json request = {
    {"model", "qwen2-0.5b"},
    {"messages", {{{"role", "user"}, {"content", "Please summarize the following text in 2-3 sentences: " + long_text}}}},
    {"max_tokens", 150}
  };
  
  auto response = client.post("http://localhost:8080/v1/chat/completions", request);
  auto result = json::parse(response.body);
  std::string summary = result["choices"][0]["message"]["content"];
  
  LOG(INFO) << "Original length: " << long_text.length();
  LOG(INFO) << "Summary: " << summary;
  LOG(INFO) << "Summary length: " << summary.length();
  
  // éªŒè¯æ‘˜è¦æ›´çŸ­
  EXPECT_LT(summary.length(), long_text.length() * 0.5);
  
  // éªŒè¯æ‘˜è¦åŒ…å«å…³é”®è¯
  bool has_keywords = summary.find("AI") != std::string::npos ||
                      summary.find("artificial intelligence") != std::string::npos ||
                      summary.find("intelligence") != std::string::npos;
  
  EXPECT_TRUE(has_keywords) << "Summary doesn't contain key concepts";
  
  server.stop();
}
```

**ç¿»è¯‘**:
```cpp
TEST(E2EScenarios, Translation) {
  cLLMServer server;
  server.initialize("${CLLM_TEST_MODEL_PATH}");
  server.start();
  
  HTTPClient client;
  
  // è‹±è¯‘ä¸­
  json request_en_to_zh = {
    {"model", "qwen2-0.5b"},
    {"messages", {{{"role", "user"}, {"content", "Translate to Chinese: Artificial Intelligence is changing the world."}}}},
    {"max_tokens", 100}
  };
  
  auto response_en_to_zh = client.post("http://localhost:8080/v1/chat/completions", request_en_to_zh);
  auto result_en_to_zh = json::parse(response_en_to_zh.body);
  std::string translation_zh = result_en_to_zh["choices"][0]["message"]["content"];
  
  LOG(INFO) << "EN->ZH: " << translation_zh;
  
  // éªŒè¯ä¸­æ–‡ç¿»è¯‘åŒ…å«ä¸­æ–‡å­—ç¬¦
  bool has_chinese = std::any_of(translation_zh.begin(), translation_zh.end(), 
    [](char c) { return static_cast<unsigned char>(c) > 127; });
  
  EXPECT_TRUE(has_chinese) << "Translation doesn't contain Chinese characters";
  
  // ä¸­è¯‘è‹±
  json request_zh_to_en = {
    {"model", "qwen2-0.5b"},
    {"messages", {{{"role", "user"}, {"content", "ç¿»è¯‘æˆè‹±æ–‡ï¼šäººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œã€‚"}}}},
    {"max_tokens", 100}
  };
  
  auto response_zh_to_en = client.post("http://localhost:8080/v1/chat/completions", request_zh_to_en);
  auto result_zh_to_en = json::parse(response_zh_to_en.body);
  std::string translation_en = result_zh_to_en["choices"][0]["message"]["content"];
  
  LOG(INFO) << "ZH->EN: " << translation_en;
  
  // éªŒè¯è‹±æ–‡ç¿»è¯‘åŒ…å«å…³é”®è¯
  bool has_keywords = translation_en.find("AI") != std::string::npos ||
                      translation_en.find("artificial") != std::string::npos ||
                      translation_en.find("intelligence") != std::string::npos ||
                      translation_en.find("world") != std::string::npos;
  
  EXPECT_TRUE(has_keywords) << "Translation doesn't contain key words";
  
  server.stop();
}
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… ä»£ç ç”Ÿæˆæ ¼å¼æ­£ç¡®
- âœ… æ‘˜è¦é•¿åº¦åˆç†
- âœ… ç¿»è¯‘åŒ…å«æ­£ç¡®è¯­è¨€
- âœ… ä¸“ä¸šä»»åŠ¡å®Œæˆåº¦è‰¯å¥½

---

### P5.1.4: è´¨é‡è¯„ä¼° (90åˆ†é’Ÿ)

#### è¯„ä¼°ç»´åº¦
- å‡†ç¡®æ€§ï¼ˆAccuracyï¼‰
- æµç•…æ€§ï¼ˆFluencyï¼‰
- ç›¸å…³æ€§ï¼ˆRelevanceï¼‰
- å®Œæ•´æ€§ï¼ˆCompletenessï¼‰

#### è¯„ä¼°æ–¹æ³•

```cpp
TEST(E2EScenarios, QualityEvaluation) {
  cLLMServer server;
  server.initialize("${CLLM_TEST_MODEL_PATH}");
  server.start();
  
  HTTPClient client;
  
  // æµ‹è¯•é›†
  std::vector<std::tuple<std::string, std::string, std::vector<std::string>>> test_set = {
    {
      "Question", 
      "What is machine learning?",
      {"machine", "learn", "data", "algorithm", "model"}
    },
    {
      "Question",
      "Explain quantum computing.",
      {"quantum", "qubit", "superposition", "computing"}
    },
    // ... æ›´å¤šæµ‹è¯•ç”¨ä¾‹
  };
  
  struct QualityScore {
    double accuracy = 0.0;
    double fluency = 0.0;
    double relevance = 0.0;
    double completeness = 0.0;
  };
  
  std::vector<QualityScore> scores;
  
  for (const auto& [type, question, keywords] : test_set) {
    json request = {
      {"model", "qwen2-0.5b"},
      {"messages", {{{"role", "user"}, {"content", question}}}},
      {"max_tokens", 200}
    };
    
    auto response = client.post("http://localhost:8080/v1/chat/completions", request);
    auto result = json::parse(response.body);
    std::string answer = result["choices"][0]["message"]["content"];
    
    QualityScore score;
    
    // 1. å‡†ç¡®æ€§ï¼šæ£€æŸ¥å…³é”®è¯è¦†ç›–
    int keywords_found = 0;
    for (const auto& keyword : keywords) {
      if (answer.find(keyword) != std::string::npos) {
        keywords_found++;
      }
    }
    score.accuracy = static_cast<double>(keywords_found) / keywords.size();
    
    // 2. æµç•…æ€§ï¼šæ£€æŸ¥åŸºæœ¬è¯­æ³•ï¼ˆç®€å•å¯å‘å¼ï¼‰
    bool has_punctuation = answer.find(".") != std::string::npos || 
                           answer.find("ã€‚") != std::string::npos;
    bool reasonable_length = answer.length() > 20 && answer.length() < 1000;
    score.fluency = (has_punctuation && reasonable_length) ? 1.0 : 0.5;
    
    // 3. ç›¸å…³æ€§ï¼šç­”æ¡ˆé•¿åº¦åˆç†
    score.relevance = (answer.length() > 30) ? 1.0 : 0.5;
    
    // 4. å®Œæ•´æ€§ï¼šç­”æ¡ˆä¸æ˜¯å¤ªçŸ­
    score.completeness = (answer.length() > 50) ? 1.0 : 0.5;
    
    scores.push_back(score);
    
    LOG(INFO) << "Question: " << question;
    LOG(INFO) << "Answer: " << answer;
    LOG(INFO) << "Scores - Accuracy: " << score.accuracy 
              << ", Fluency: " << score.fluency
              << ", Relevance: " << score.relevance
              << ", Completeness: " << score.completeness;
    LOG(INFO) << "---";
  }
  
  // è®¡ç®—å¹³å‡åˆ†
  double avg_accuracy = 0.0, avg_fluency = 0.0, avg_relevance = 0.0, avg_completeness = 0.0;
  
  for (const auto& score : scores) {
    avg_accuracy += score.accuracy;
    avg_fluency += score.fluency;
    avg_relevance += score.relevance;
    avg_completeness += score.completeness;
  }
  
  int n = scores.size();
  avg_accuracy /= n;
  avg_fluency /= n;
  avg_relevance /= n;
  avg_completeness /= n;
  
  double overall_score = (avg_accuracy + avg_fluency + avg_relevance + avg_completeness) / 4.0;
  
  LOG(INFO) << "===== Quality Evaluation Results =====";
  LOG(INFO) << "Average Accuracy: " << avg_accuracy * 5 << " / 5.0";
  LOG(INFO) << "Average Fluency: " << avg_fluency * 5 << " / 5.0";
  LOG(INFO) << "Average Relevance: " << avg_relevance * 5 << " / 5.0";
  LOG(INFO) << "Average Completeness: " << avg_completeness * 5 << " / 5.0";
  LOG(INFO) << "Overall Score: " << overall_score * 5 << " / 5.0";
  
  // æœŸæœ›æ€»åˆ† > 4.0/5.0
  EXPECT_GT(overall_score * 5, 4.0);
  
  server.stop();
}
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… å¹³å‡å‡†ç¡®æ€§ > 0.7
- âœ… å¹³å‡æµç•…æ€§ > 0.8
- âœ… å¹³å‡ç›¸å…³æ€§ > 0.8
- âœ… å¹³å‡å®Œæ•´æ€§ > 0.7
- âœ… æ€»ä½“è¯„åˆ† > 4.0/5.0

---

## âœ… æ€»ä½“éªŒæ”¶æ ‡å‡†

### å¿…é¡»å®Œæˆ

- [ ] P5.1.1: å•è½®é—®ç­”åœºæ™¯é€šè¿‡
- [ ] P5.1.2: å¤šè½®å¯¹è¯åœºæ™¯é€šè¿‡
- [ ] P5.1.3: ä¸“ä¸šä»»åŠ¡åœºæ™¯é€šè¿‡
- [ ] P5.1.4: è´¨é‡è¯„ä¼°è¾¾æ ‡

### è´¨é‡æŒ‡æ ‡

- [ ] äº‹å®é—®ç­”æ­£ç¡®ç‡ > 60%
- [ ] æ¨ç†é—®ç­”æ­£ç¡®ç‡ > 40%
- [ ] æ•°å­¦é—®ç­”æ­£ç¡®ç‡ > 70%
- [ ] ä¸Šä¸‹æ–‡ä¿æŒæ­£ç¡®
- [ ] ä»£ç ç”Ÿæˆæ ¼å¼æ­£ç¡®
- [ ] ç¿»è¯‘è´¨é‡è‰¯å¥½
- [ ] æ€»ä½“è¯„åˆ† > 4.0/5.0

---

## ğŸ“Š æ‰§è¡ŒæŠ¥å‘Š

**æ‰§è¡Œæ—¶é—´**: ________

**å®Œæˆæƒ…å†µ**:
- P5.1.1: â˜ å®Œæˆ / â˜ å¤±è´¥
- P5.1.2: â˜ å®Œæˆ / â˜ å¤±è´¥
- P5.1.3: â˜ å®Œæˆ / â˜ å¤±è´¥
- P5.1.4: â˜ å®Œæˆ / â˜ å¤±è´¥

**è´¨é‡å¾—åˆ†**:
- äº‹å®é—®ç­”æ­£ç¡®ç‡: ________%
- æ¨ç†é—®ç­”æ­£ç¡®ç‡: ________%
- æ•°å­¦é—®ç­”æ­£ç¡®ç‡: ________%
- æ€»ä½“è´¨é‡è¯„åˆ†: ________ / 5.0

**æ€»ä½“çŠ¶æ€**: â˜ æˆåŠŸ / â˜ éƒ¨åˆ†æˆåŠŸ / â˜ å¤±è´¥

---

## ğŸ‰ æµ‹è¯•å®Œæˆ

Phase 5 æ˜¯æœ€åä¸€ä¸ªæµ‹è¯•é˜¶æ®µã€‚å®Œæˆåï¼š

```bash
touch /tmp/cllm_test_locks/phase5.done
touch /tmp/cllm_test_locks/all_phases.done

# ç”Ÿæˆæœ€ç»ˆæµ‹è¯•æŠ¥å‘Š
python3 scripts/generate_final_report.py

echo "========================================="
echo "ğŸ‰ æ‰€æœ‰æµ‹è¯•é˜¶æ®µå®Œæˆï¼"
echo "========================================="
echo "æ€»è€—æ—¶: 74å°æ—¶"
echo "æµ‹è¯•é˜¶æ®µ: 6ä¸ª"
echo "æµ‹è¯•ä»»åŠ¡: 72ä¸ª"
echo ""
echo "æœ€ç»ˆæŠ¥å‘Š: test_reports/final_report.md"
echo "========================================="
```

---

**âœ… cLLM åˆ†é˜¶æ®µé›†æˆæµ‹è¯•å…¨éƒ¨å®Œæˆï¼**
