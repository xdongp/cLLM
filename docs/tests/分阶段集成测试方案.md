# cLLM åˆ†é˜¶æ®µé›†æˆæµ‹è¯•æ–¹æ¡ˆ

**ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¥æœŸ**: 2026-01-11  
**ç›®æ ‡**: æ„å»ºå¯å¹¶è¡ŒåŒ–ã€è§£è€¦çš„åˆ†é˜¶æ®µé›†æˆæµ‹è¯•ä½“ç³»  

---

## ğŸ“‹ ç›®å½•

1. [æµ‹è¯•æ¶æ„æ€»è§ˆ](#æµ‹è¯•æ¶æ„æ€»è§ˆ)
2. [æµ‹è¯•é˜¶æ®µåˆ’åˆ†](#æµ‹è¯•é˜¶æ®µåˆ’åˆ†)
3. [æ¨¡å—è§£è€¦ç­–ç•¥](#æ¨¡å—è§£è€¦ç­–ç•¥)
4. [å¹¶è¡ŒåŒ–è®¾è®¡](#å¹¶è¡ŒåŒ–è®¾è®¡)
5. [ç»†ç²’åº¦ä»»åŠ¡æ‹†åˆ†](#ç»†ç²’åº¦ä»»åŠ¡æ‹†åˆ†)
6. [Agent åˆ†å·¥æ–¹æ¡ˆ](#agent-åˆ†å·¥æ–¹æ¡ˆ)
7. [æµ‹è¯•æ•°æ®å‡†å¤‡](#æµ‹è¯•æ•°æ®å‡†å¤‡)
8. [æ‰§è¡Œæµç¨‹](#æ‰§è¡Œæµç¨‹)

---

## ğŸ—ï¸ æµ‹è¯•æ¶æ„æ€»è§ˆ

### æ ¸å¿ƒæ¨¡å—

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     cLLM é›†æˆæµ‹è¯•æ¶æ„                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ HTTP Server â”‚â—„â”€â”€â”€â”€â–ºâ”‚  Tokenizer  â”‚â—„â”€â”€â”€â”€â–ºâ”‚   Model     â”‚ â”‚
â”‚  â”‚             â”‚      â”‚ (HFTokenizer)â”‚      â”‚  Executor   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                     â”‚                     â”‚        â”‚
â”‚         â”‚                     â”‚                     â”‚        â”‚
â”‚         â–¼                     â–¼                     â–¼        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚            LibTorch Backend (æ¨ç†åç«¯)              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                  â”‚
â”‚                           â–¼                                  â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚                  â”‚   Qwen3 Model   â”‚                        â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æµ‹è¯•å±‚æ¬¡ç»“æ„

```
Level 0: å•å…ƒæµ‹è¯• (Unit Tests)
    â”œâ”€â”€ HTTP Server å•å…ƒæµ‹è¯•
    â”œâ”€â”€ HFTokenizer å•å…ƒæµ‹è¯•
    â”œâ”€â”€ ModelExecutor å•å…ƒæµ‹è¯•
    â”œâ”€â”€ LibTorch Backend å•å…ƒæµ‹è¯•
    â””â”€â”€ Qwen3 åŠ è½½æµ‹è¯•

Level 1: æ¨¡å—é›†æˆæµ‹è¯• (Module Integration)
    â”œâ”€â”€ HTTP + Tokenizer é›†æˆ
    â”œâ”€â”€ Tokenizer + ModelExecutor é›†æˆ
    â”œâ”€â”€ ModelExecutor + LibTorch é›†æˆ
    â””â”€â”€ LibTorch + Qwen3 é›†æˆ

Level 2: å­ç³»ç»Ÿé›†æˆæµ‹è¯• (Subsystem Integration)
    â”œâ”€â”€ å‰ç«¯å­ç³»ç»Ÿ (HTTP + Tokenizer)
    â”œâ”€â”€ æ¨ç†å­ç³»ç»Ÿ (ModelExecutor + LibTorch)
    â””â”€â”€ ç«¯åˆ°ç«¯å­ç³»ç»Ÿ (Tokenizer â†’ Executor â†’ Backend)

Level 3: ç³»ç»Ÿé›†æˆæµ‹è¯• (System Integration)
    â””â”€â”€ å®Œæ•´æµç¨‹ (HTTP â†’ Tokenizer â†’ Executor â†’ LibTorch â†’ Qwen3)

Level 4: ç«¯åˆ°ç«¯æµ‹è¯• (E2E Tests)
    â”œâ”€â”€ åŠŸèƒ½åœºæ™¯æµ‹è¯•
    â”œâ”€â”€ æ€§èƒ½æµ‹è¯•
    â””â”€â”€ å‹åŠ›æµ‹è¯•
```

---

## ğŸ¯ æµ‹è¯•é˜¶æ®µåˆ’åˆ†

### é˜¶æ®µ 0: å‡†å¤‡é˜¶æ®µ (Phase 0 - Preparation)

**ç›®æ ‡**: å‡†å¤‡æµ‹è¯•ç¯å¢ƒå’Œæ•°æ®

**ä»»åŠ¡æ¸…å•**:
- [ ] **P0.1**: ä¸‹è½½ Qwen3 æ¨¡å‹
- [ ] **P0.2**: å‡†å¤‡æµ‹è¯•æ•°æ®é›†
- [ ] **P0.3**: é…ç½®æµ‹è¯•ç¯å¢ƒ
- [ ] **P0.4**: ç¼–è¯‘æ‰€æœ‰æµ‹è¯•ç›®æ ‡
- [ ] **P0.5**: åˆ›å»º Mock å¯¹è±¡å’Œ Stub

**å¯å¹¶è¡Œ**: âœ… æ‰€æœ‰ä»»åŠ¡

**ä¾èµ–**: æ— 

**è¾“å‡º**:
- Qwen3 æ¨¡å‹æ–‡ä»¶ï¼ˆtokenizer.json, model weightsï¼‰
- æµ‹è¯•æ•°æ®é›†ï¼ˆJSON æ–‡ä»¶ï¼‰
- ç¼–è¯‘äº§ç‰©ï¼ˆæ‰€æœ‰æµ‹è¯•äºŒè¿›åˆ¶ï¼‰
- Mock/Stub ä»£ç 

---

### é˜¶æ®µ 1: å•å…ƒæµ‹è¯•é˜¶æ®µ (Phase 1 - Unit Tests)

**ç›®æ ‡**: éªŒè¯æ¯ä¸ªæ¨¡å—çš„ç‹¬ç«‹åŠŸèƒ½

#### 1.1 HTTP Server å•å…ƒæµ‹è¯•

**èŒƒå›´**:
- è·¯ç”±æ³¨å†Œå’Œè§£æ
- è¯·æ±‚/å“åº”å¤„ç†
- JSON åºåˆ—åŒ–/ååºåˆ—åŒ–
- é”™è¯¯å¤„ç†

**æµ‹è¯•ä»»åŠ¡**:
```cpp
// Task 1.1.1: è·¯ç”±æµ‹è¯•
test_http_route_registration()
test_http_route_matching()
test_http_method_filtering()

// Task 1.1.2: è¯·æ±‚å¤„ç†æµ‹è¯•
test_http_request_parsing()
test_http_query_params()
test_http_body_parsing()

// Task 1.1.3: å“åº”æµ‹è¯•
test_http_response_building()
test_http_status_codes()
test_http_headers()

// Task 1.1.4: é”™è¯¯å¤„ç†æµ‹è¯•
test_http_404_not_found()
test_http_500_internal_error()
test_http_timeout_handling()
```

**Mock ä¾èµ–**:
- Mock Tokenizerï¼ˆè¿”å›å‡çš„ Token IDsï¼‰
- Mock ModelExecutorï¼ˆè¿”å›å‡çš„æ¨ç†ç»“æœï¼‰

**å¹¶è¡Œåº¦**: 4 ä¸ª Agent å¯å¹¶è¡Œ

**é¢„ä¼°æ—¶é—´**: 2 å°æ—¶

---

#### 1.2 HFTokenizer å•å…ƒæµ‹è¯•

**èŒƒå›´**:
- tokenizer.json åŠ è½½
- æ–‡æœ¬ç¼–ç /è§£ç 
- ç‰¹æ®Š Token å¤„ç†
- æ‰¹é‡å¤„ç†

**æµ‹è¯•ä»»åŠ¡**:
```cpp
// Task 1.2.1: åŠ è½½æµ‹è¯•
test_hf_load_tokenizer()
test_hf_invalid_path()
test_hf_missing_config()

// Task 1.2.2: ç¼–ç æµ‹è¯•
test_hf_encode_english()
test_hf_encode_chinese()
test_hf_encode_mixed()
test_hf_encode_special_chars()

// Task 1.2.3: è§£ç æµ‹è¯•
test_hf_decode_tokens()
test_hf_decode_empty()
test_hf_decode_special_tokens()

// Task 1.2.4: æ‰¹é‡å¤„ç†æµ‹è¯•
test_hf_batch_encode()
test_hf_batch_decode()
test_hf_batch_performance()
```

**Mock ä¾èµ–**:
- æ— ï¼ˆä½¿ç”¨çœŸå® tokenizers-cppï¼‰

**å¹¶è¡Œåº¦**: 4 ä¸ª Agent å¯å¹¶è¡Œ

**é¢„ä¼°æ—¶é—´**: 2 å°æ—¶

---

#### 1.3 ModelExecutor å•å…ƒæµ‹è¯•

**èŒƒå›´**:
- æ¨¡å‹åˆå§‹åŒ–
- æ¨ç†æ¥å£
- æ‰¹å¤„ç†ç®¡ç†
- çŠ¶æ€ç®¡ç†

**æµ‹è¯•ä»»åŠ¡**:
```cpp
// Task 1.3.1: åˆå§‹åŒ–æµ‹è¯•
test_executor_init()
test_executor_load_config()
test_executor_invalid_config()

// Task 1.3.2: æ¨ç†æ¥å£æµ‹è¯•
test_executor_forward()
test_executor_generate()
test_executor_batch_forward()

// Task 1.3.3: çŠ¶æ€ç®¡ç†æµ‹è¯•
test_executor_state_reset()
test_executor_state_save()
test_executor_state_restore()

// Task 1.3.4: é”™è¯¯å¤„ç†æµ‹è¯•
test_executor_invalid_input()
test_executor_memory_overflow()
test_executor_timeout()
```

**Mock ä¾èµ–**:
- Mock LibTorch Backendï¼ˆè¿”å›å‡çš„ Tensorï¼‰
- Mock Tokenizerï¼ˆæä¾› Token IDsï¼‰

**å¹¶è¡Œåº¦**: 4 ä¸ª Agent å¯å¹¶è¡Œ

**é¢„ä¼°æ—¶é—´**: 3 å°æ—¶

---

#### 1.4 LibTorch Backend å•å…ƒæµ‹è¯•

**èŒƒå›´**:
- Torch æ¨¡å‹åŠ è½½
- Tensor æ“ä½œ
- å‰å‘æ¨ç†
- å†…å­˜ç®¡ç†

**æµ‹è¯•ä»»åŠ¡**:
```cpp
// Task 1.4.1: æ¨¡å‹åŠ è½½æµ‹è¯•
test_libtorch_load_model()
test_libtorch_invalid_model()
test_libtorch_model_format()

// Task 1.4.2: Tensor æ“ä½œæµ‹è¯•
test_libtorch_tensor_creation()
test_libtorch_tensor_conversion()
test_libtorch_tensor_reshape()

// Task 1.4.3: æ¨ç†æµ‹è¯•
test_libtorch_forward()
test_libtorch_batch_forward()
test_libtorch_dynamic_shape()

// Task 1.4.4: å†…å­˜æµ‹è¯•
test_libtorch_memory_allocation()
test_libtorch_memory_release()
test_libtorch_cuda_memory()
```

**Mock ä¾èµ–**:
- ä½¿ç”¨å°å‹æµ‹è¯•æ¨¡å‹ï¼ˆé Qwen3ï¼‰

**å¹¶è¡Œåº¦**: 4 ä¸ª Agent å¯å¹¶è¡Œ

**é¢„ä¼°æ—¶é—´**: 3 å°æ—¶

---

#### 1.5 Qwen3 æ¨¡å‹å•å…ƒæµ‹è¯•

**èŒƒå›´**:
- æ¨¡å‹æ–‡ä»¶åŠ è½½
- Tokenizer å…¼å®¹æ€§
- åŸºæœ¬æ¨ç†éªŒè¯

**æµ‹è¯•ä»»åŠ¡**:
```cpp
// Task 1.5.1: æ¨¡å‹åŠ è½½æµ‹è¯•
test_qwen3_load_model()
test_qwen3_config_parsing()
test_qwen3_weights_validation()

// Task 1.5.2: Tokenizer æµ‹è¯•
test_qwen3_tokenizer_load()
test_qwen3_special_tokens()
test_qwen3_vocab_size()

// Task 1.5.3: åŸºæœ¬æ¨ç†æµ‹è¯•
test_qwen3_single_forward()
test_qwen3_input_validation()
test_qwen3_output_shape()
```

**Mock ä¾èµ–**:
- æ— ï¼ˆä½¿ç”¨çœŸå® Qwen3 æ¨¡å‹ï¼‰

**å¹¶è¡Œåº¦**: 3 ä¸ª Agent å¯å¹¶è¡Œ

**é¢„ä¼°æ—¶é—´**: 2 å°æ—¶

---

### é˜¶æ®µ 2: æ¨¡å—é›†æˆæµ‹è¯• (Phase 2 - Module Integration)

**ç›®æ ‡**: éªŒè¯ä¸¤ä¸ªæ¨¡å—é—´çš„æ¥å£å’Œæ•°æ®æµ

#### 2.1 HTTP Server + HFTokenizer é›†æˆ

**æµ‹è¯•åœºæ™¯**:
```
HTTP Request â†’ JSON è§£æ â†’ æ–‡æœ¬æå– â†’ Tokenizer ç¼–ç  â†’ è¿”å› Token IDs
```

**æµ‹è¯•ä»»åŠ¡**:
```cpp
// Task 2.1.1: ç«¯ç‚¹é›†æˆæµ‹è¯•
test_http_tokenize_endpoint()
test_http_detokenize_endpoint()

// Task 2.1.2: æ•°æ®æµæµ‹è¯•
test_http_to_tokenizer_data_flow()
test_tokenizer_to_http_response()

// Task 2.1.3: é”™è¯¯ä¼ æ’­æµ‹è¯•
test_http_tokenizer_error_handling()
test_http_tokenizer_timeout()

// Task 2.1.4: æ‰¹é‡è¯·æ±‚æµ‹è¯•
test_http_batch_tokenize()
test_http_concurrent_requests()
```

**Mock ä¾èµ–**:
- Mock ModelExecutorï¼ˆæ¨¡æ‹Ÿæ¨ç†ç»“æœï¼‰

**å¹¶è¡Œåº¦**: 2 ä¸ª Agentï¼ˆåŠŸèƒ½æµ‹è¯• + æ€§èƒ½æµ‹è¯•ï¼‰

**é¢„ä¼°æ—¶é—´**: 3 å°æ—¶

---

#### 2.2 HFTokenizer + ModelExecutor é›†æˆ

**æµ‹è¯•åœºæ™¯**:
```
Token IDs â†’ Tensor è½¬æ¢ â†’ ModelExecutor æ¨ç† â†’ è¾“å‡º Logits
```

**æµ‹è¯•ä»»åŠ¡**:
```cpp
// Task 2.2.1: æ•°æ®æ ¼å¼è½¬æ¢æµ‹è¯•
test_tokens_to_tensor()
test_tensor_shape_validation()

// Task 2.2.2: æ¨ç†æµç¨‹æµ‹è¯•
test_tokenizer_executor_forward()
test_tokenizer_executor_generate()

// Task 2.2.3: æ‰¹å¤„ç†æµ‹è¯•
test_tokenizer_executor_batch()
test_tokenizer_executor_dynamic_batch()

// Task 2.2.4: çŠ¶æ€åŒæ­¥æµ‹è¯•
test_tokenizer_executor_state_sync()
test_tokenizer_executor_reset()
```

**Mock ä¾èµ–**:
- Mock LibTorch Backendï¼ˆè¿”å›å‡çš„æ¨ç†ç»“æœï¼‰

**å¹¶è¡Œåº¦**: 2 ä¸ª Agent

**é¢„ä¼°æ—¶é—´**: 3 å°æ—¶

---

#### 2.3 ModelExecutor + LibTorch Backend é›†æˆ

**æµ‹è¯•åœºæ™¯**:
```
Input Tensor â†’ LibTorch å‰å‘æ¨ç† â†’ Output Tensor â†’ ç»“æœè§£æ
```

**æµ‹è¯•ä»»åŠ¡**:
```cpp
// Task 2.3.1: æ¨ç†æµç¨‹æµ‹è¯•
test_executor_backend_forward()
test_executor_backend_generate()

// Task 2.3.2: å†…å­˜ç®¡ç†æµ‹è¯•
test_executor_backend_memory_sync()
test_executor_backend_cuda_transfer()

// Task 2.3.3: æ€§èƒ½æµ‹è¯•
test_executor_backend_throughput()
test_executor_backend_latency()

// Task 2.3.4: é”™è¯¯æ¢å¤æµ‹è¯•
test_executor_backend_error_recovery()
test_executor_backend_timeout_handling()
```

**Mock ä¾èµ–**:
- ä½¿ç”¨å°å‹æµ‹è¯•æ¨¡å‹

**å¹¶è¡Œåº¦**: 2 ä¸ª Agent

**é¢„ä¼°æ—¶é—´**: 4 å°æ—¶

---

#### 2.4 LibTorch Backend + Qwen3 Model é›†æˆ

**æµ‹è¯•åœºæ™¯**:
```
åŠ è½½ Qwen3 â†’ æ¨ç†éªŒè¯ â†’ è¾“å‡ºè´¨é‡æ£€æŸ¥
```

**æµ‹è¯•ä»»åŠ¡**:
```cpp
// Task 2.4.1: æ¨¡å‹åŠ è½½é›†æˆæµ‹è¯•
test_backend_qwen3_load()
test_backend_qwen3_config_sync()

// Task 2.4.2: æ¨ç†æ­£ç¡®æ€§æµ‹è¯•
test_backend_qwen3_forward()
test_backend_qwen3_output_validation()

// Task 2.4.3: æ€§èƒ½æµ‹è¯•
test_backend_qwen3_throughput()
test_backend_qwen3_memory_usage()

// Task 2.4.4: ç¨³å®šæ€§æµ‹è¯•
test_backend_qwen3_long_sequence()
test_backend_qwen3_continuous_inference()
```

**Mock ä¾èµ–**:
- æ— ï¼ˆä½¿ç”¨çœŸå®æ¨¡å‹ï¼‰

**å¹¶è¡Œåº¦**: 2 ä¸ª Agent

**é¢„ä¼°æ—¶é—´**: 4 å°æ—¶

---

### é˜¶æ®µ 3: å­ç³»ç»Ÿé›†æˆæµ‹è¯• (Phase 3 - Subsystem Integration)

**ç›®æ ‡**: éªŒè¯å¤šä¸ªæ¨¡å—ç»„åˆçš„åŠŸèƒ½

#### 3.1 å‰ç«¯å­ç³»ç»Ÿæµ‹è¯• (HTTP + Tokenizer)

**æµ‹è¯•åœºæ™¯**:
```
å®Œæ•´çš„å‰ç«¯å¤„ç†æµç¨‹ï¼š
HTTP Request â†’ å‚æ•°éªŒè¯ â†’ æ–‡æœ¬é¢„å¤„ç† â†’ Tokenization â†’ è¿”å›ç»“æœ
```

**æµ‹è¯•ä»»åŠ¡**:
```cpp
// Task 3.1.1: å®Œæ•´æµç¨‹æµ‹è¯•
test_frontend_complete_flow()
test_frontend_chat_endpoint()
test_frontend_completion_endpoint()

// Task 3.1.2: å¹¶å‘æµ‹è¯•
test_frontend_concurrent_requests()
test_frontend_load_balancing()

// Task 3.1.3: æ€§èƒ½æµ‹è¯•
test_frontend_throughput()
test_frontend_latency()

// Task 3.1.4: å®¹é”™æµ‹è¯•
test_frontend_error_recovery()
test_frontend_timeout_handling()
```

**Mock ä¾èµ–**:
- Mock ModelExecutorï¼ˆæ¨¡æ‹Ÿæ¨ç†ï¼‰

**å¹¶è¡Œåº¦**: 2 ä¸ª Agent

**é¢„ä¼°æ—¶é—´**: 4 å°æ—¶

---

#### 3.2 æ¨ç†å­ç³»ç»Ÿæµ‹è¯• (ModelExecutor + LibTorch + Qwen3)

**æµ‹è¯•åœºæ™¯**:
```
å®Œæ•´çš„æ¨ç†æµç¨‹ï¼š
Token IDs â†’ ModelExecutor â†’ LibTorch â†’ Qwen3 â†’ è¾“å‡º Token IDs
```

**æµ‹è¯•ä»»åŠ¡**:
```cpp
// Task 3.2.1: å®Œæ•´æ¨ç†æµç¨‹æµ‹è¯•
test_inference_complete_flow()
test_inference_generation()

// Task 3.2.2: æ‰¹å¤„ç†æµ‹è¯•
test_inference_batch_processing()
test_inference_dynamic_batching()

// Task 3.2.3: æ€§èƒ½æµ‹è¯•
test_inference_throughput()
test_inference_memory_efficiency()

// Task 3.2.4: è´¨é‡æµ‹è¯•
test_inference_output_quality()
test_inference_consistency()
```

**Mock ä¾èµ–**:
- Mock HTTP Serverï¼ˆæä¾›è¾“å…¥ï¼‰

**å¹¶è¡Œåº¦**: 2 ä¸ª Agent

**é¢„ä¼°æ—¶é—´**: 5 å°æ—¶

---

#### 3.3 ç«¯åˆ°ç«¯å­ç³»ç»Ÿæµ‹è¯• (Tokenizer â†’ Executor â†’ Backend)

**æµ‹è¯•åœºæ™¯**:
```
å®Œæ•´çš„æ¨ç†é“¾è·¯ï¼ˆä¸å« HTTPï¼‰ï¼š
æ–‡æœ¬ â†’ Tokenizer â†’ Executor â†’ LibTorch â†’ æ–‡æœ¬è¾“å‡º
```

**æµ‹è¯•ä»»åŠ¡**:
```cpp
// Task 3.3.1: å®Œæ•´é“¾è·¯æµ‹è¯•
test_e2e_text_to_text()
test_e2e_chat_completion()

// Task 3.3.2: æµå¼è¾“å‡ºæµ‹è¯•
test_e2e_streaming()
test_e2e_streaming_latency()

// Task 3.3.3: å¤šè½®å¯¹è¯æµ‹è¯•
test_e2e_multi_turn_chat()
test_e2e_context_management()

// Task 3.3.4: è¾¹ç•Œæµ‹è¯•
test_e2e_long_input()
test_e2e_long_output()
test_e2e_special_characters()
```

**Mock ä¾èµ–**:
- æ— ï¼ˆçœŸå®æµç¨‹ï¼‰

**å¹¶è¡Œåº¦**: 2 ä¸ª Agent

**é¢„ä¼°æ—¶é—´**: 5 å°æ—¶

---

### é˜¶æ®µ 4: ç³»ç»Ÿé›†æˆæµ‹è¯• (Phase 4 - System Integration)

**ç›®æ ‡**: éªŒè¯å®Œæ•´ç³»ç»ŸåŠŸèƒ½

#### 4.1 å®Œæ•´ç³»ç»ŸåŠŸèƒ½æµ‹è¯•

**æµ‹è¯•åœºæ™¯**:
```
å®Œæ•´çš„ç«¯åˆ°ç«¯æµç¨‹ï¼š
HTTP Request â†’ Tokenizer â†’ Executor â†’ LibTorch â†’ Qwen3 â†’ HTTP Response
```

**æµ‹è¯•ä»»åŠ¡**:
```cpp
// Task 4.1.1: æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•
test_system_chat_completion()
test_system_text_completion()
test_system_streaming()

// Task 4.1.2: API å…¼å®¹æ€§æµ‹è¯•
test_system_openai_api()
test_system_custom_api()

// Task 4.1.3: å¤šåœºæ™¯æµ‹è¯•
test_system_short_text()
test_system_long_text()
test_system_multi_turn()
test_system_concurrent_users()

// Task 4.1.4: é”™è¯¯å¤„ç†æµ‹è¯•
test_system_error_propagation()
test_system_graceful_degradation()
```

**Mock ä¾èµ–**:
- æ— ï¼ˆçœŸå®ç³»ç»Ÿï¼‰

**å¹¶è¡Œåº¦**: 3 ä¸ª Agentï¼ˆåŠŸèƒ½ + æ€§èƒ½ + ç¨³å®šæ€§ï¼‰

**é¢„ä¼°æ—¶é—´**: 6 å°æ—¶

---

#### 4.2 æ€§èƒ½åŸºå‡†æµ‹è¯•

**æµ‹è¯•ç»´åº¦**:
- ååé‡ï¼ˆQPSï¼‰
- å»¶è¿Ÿï¼ˆP50, P95, P99ï¼‰
- èµ„æºä½¿ç”¨ï¼ˆCPU, å†…å­˜, GPUï¼‰

**æµ‹è¯•ä»»åŠ¡**:
```cpp
// Task 4.2.1: ååé‡æµ‹è¯•
test_benchmark_throughput_single()
test_benchmark_throughput_batch()
test_benchmark_throughput_concurrent()

// Task 4.2.2: å»¶è¿Ÿæµ‹è¯•
test_benchmark_latency_p50()
test_benchmark_latency_p95()
test_benchmark_latency_p99()

// Task 4.2.3: èµ„æºæµ‹è¯•
test_benchmark_cpu_usage()
test_benchmark_memory_usage()
test_benchmark_gpu_usage()

// Task 4.2.4: æ‰©å±•æ€§æµ‹è¯•
test_benchmark_scaling_users()
test_benchmark_scaling_batch_size()
```

**å¹¶è¡Œåº¦**: 3 ä¸ª Agentï¼ˆç‹¬ç«‹æµ‹è¯•ä¸åŒç»´åº¦ï¼‰

**é¢„ä¼°æ—¶é—´**: 6 å°æ—¶

---

#### 4.3 å‹åŠ›æµ‹è¯•å’Œç¨³å®šæ€§æµ‹è¯•

**æµ‹è¯•åœºæ™¯**:
- é•¿æ—¶é—´è¿è¡Œ
- é«˜å¹¶å‘
- å¼‚å¸¸æ³¨å…¥

**æµ‹è¯•ä»»åŠ¡**:
```cpp
// Task 4.3.1: å‹åŠ›æµ‹è¯•
test_stress_high_concurrency()
test_stress_long_running()
test_stress_memory_pressure()

// Task 4.3.2: ç¨³å®šæ€§æµ‹è¯•
test_stability_continuous_load()
test_stability_spike_load()
test_stability_resource_exhaustion()

// Task 4.3.3: å¼‚å¸¸æ³¨å…¥æµ‹è¯•
test_chaos_network_failure()
test_chaos_disk_failure()
test_chaos_oom_killer()

// Task 4.3.4: æ¢å¤æµ‹è¯•
test_recovery_crash_restart()
test_recovery_state_restore()
```

**å¹¶è¡Œåº¦**: 2 ä¸ª Agent

**é¢„ä¼°æ—¶é—´**: 8 å°æ—¶

---

### é˜¶æ®µ 5: ç«¯åˆ°ç«¯åœºæ™¯æµ‹è¯• (Phase 5 - E2E Scenarios)

**ç›®æ ‡**: éªŒè¯çœŸå®ä½¿ç”¨åœºæ™¯

#### 5.1 çœŸå®åœºæ™¯æµ‹è¯•

**æµ‹è¯•åœºæ™¯**:
```
1. å•è½®é—®ç­”
2. å¤šè½®å¯¹è¯
3. ä»£ç ç”Ÿæˆ
4. æ–‡æœ¬æ‘˜è¦
5. ç¿»è¯‘ä»»åŠ¡
```

**æµ‹è¯•ä»»åŠ¡**:
```cpp
// Task 5.1.1: å•è½®é—®ç­”æµ‹è¯•
test_scenario_qa_factual()
test_scenario_qa_reasoning()
test_scenario_qa_math()

// Task 5.1.2: å¤šè½®å¯¹è¯æµ‹è¯•
test_scenario_chat_casual()
test_scenario_chat_technical()
test_scenario_chat_long_context()

// Task 5.1.3: ä¸“ä¸šä»»åŠ¡æµ‹è¯•
test_scenario_code_generation()
test_scenario_text_summarization()
test_scenario_translation()

// Task 5.1.4: è´¨é‡è¯„ä¼°
test_scenario_output_quality()
test_scenario_response_coherence()
```

**å¹¶è¡Œåº¦**: 3 ä¸ª Agentï¼ˆä¸åŒåœºæ™¯ç±»å‹ï¼‰

**é¢„ä¼°æ—¶é—´**: 6 å°æ—¶

---

## ğŸ”§ æ¨¡å—è§£è€¦ç­–ç•¥

### 1. æ¥å£æŠ½è±¡

#### 1.1 Tokenizer æ¥å£æŠ½è±¡

```cpp
// include/cllm/tokenizer/interface.h
class ITokenizer {
public:
    virtual ~ITokenizer() = default;
    
    // æ ¸å¿ƒæ¥å£
    virtual std::vector<int> encode(const std::string& text) = 0;
    virtual std::string decode(const std::vector<int>& ids) = 0;
    
    // æ‰¹é‡æ¥å£
    virtual std::vector<std::vector<int>> batchEncode(
        const std::vector<std::string>& texts) = 0;
    
    // çŠ¶æ€æŸ¥è¯¢
    virtual bool isLoaded() const = 0;
    virtual int getVocabSize() const = 0;
};

// Mock å®ç°
class MockTokenizer : public ITokenizer {
private:
    std::unordered_map<std::string, std::vector<int>> mockResponses_;
    
public:
    void setMockResponse(const std::string& text, 
                         const std::vector<int>& ids) {
        mockResponses_[text] = ids;
    }
    
    std::vector<int> encode(const std::string& text) override {
        if (mockResponses_.count(text)) {
            return mockResponses_[text];
        }
        // é»˜è®¤è¿”å›ç®€å•çš„å‡æ•°æ®
        return {101, 102, 103};
    }
    
    std::string decode(const std::vector<int>& ids) override {
        return "mock_output";
    }
    
    bool isLoaded() const override { return true; }
    int getVocabSize() const override { return 10000; }
};
```

---

#### 1.2 ModelExecutor æ¥å£æŠ½è±¡

```cpp
// include/cllm/model/interface.h
class IModelExecutor {
public:
    virtual ~IModelExecutor() = default;
    
    // æ¨ç†æ¥å£
    virtual torch::Tensor forward(const torch::Tensor& input) = 0;
    virtual std::vector<int> generate(
        const std::vector<int>& prompt,
        int maxLength) = 0;
    
    // çŠ¶æ€ç®¡ç†
    virtual void reset() = 0;
    virtual bool isReady() const = 0;
};

// Mock å®ç°
class MockModelExecutor : public IModelExecutor {
private:
    std::vector<int> mockOutput_ = {1, 2, 3, 4, 5};
    
public:
    void setMockOutput(const std::vector<int>& output) {
        mockOutput_ = output;
    }
    
    torch::Tensor forward(const torch::Tensor& input) override {
        // è¿”å›å‡çš„ Tensor
        return torch::randn({1, 10, 32000});
    }
    
    std::vector<int> generate(
        const std::vector<int>& prompt,
        int maxLength) override {
        return mockOutput_;
    }
    
    void reset() override {}
    bool isReady() const override { return true; }
};
```

---

#### 1.3 Backend æ¥å£æŠ½è±¡

```cpp
// include/cllm/inference/interface.h
class IInferenceBackend {
public:
    virtual ~IInferenceBackend() = default;
    
    // æ¨¡å‹åŠ è½½
    virtual bool loadModel(const std::string& modelPath) = 0;
    
    // æ¨ç†æ¥å£
    virtual torch::Tensor forward(
        const torch::Tensor& input,
        const torch::Tensor& attention_mask) = 0;
    
    // çŠ¶æ€æŸ¥è¯¢
    virtual bool isModelLoaded() const = 0;
    virtual std::string getModelType() const = 0;
};

// Mock å®ç°
class MockBackend : public IInferenceBackend {
private:
    bool loaded_ = false;
    
public:
    bool loadModel(const std::string& modelPath) override {
        loaded_ = true;
        return true;
    }
    
    torch::Tensor forward(
        const torch::Tensor& input,
        const torch::Tensor& attention_mask) override {
        auto batch_size = input.size(0);
        auto seq_len = input.size(1);
        // è¿”å›å‡çš„ logits
        return torch::randn({batch_size, seq_len, 32000});
    }
    
    bool isModelLoaded() const override { return loaded_; }
    std::string getModelType() const override { return "mock"; }
};
```

---

#### 1.4 HTTP Server æ¥å£æŠ½è±¡

```cpp
// include/cllm/server/interface.h
class IHTTPHandler {
public:
    virtual ~IHTTPHandler() = default;
    
    // è¯·æ±‚å¤„ç†
    virtual nlohmann::json handleRequest(
        const nlohmann::json& request) = 0;
    
    // è·¯ç”±æ³¨å†Œ
    virtual void registerRoute(
        const std::string& path,
        std::function<nlohmann::json(const nlohmann::json&)> handler) = 0;
};

// Mock å®ç°
class MockHTTPHandler : public IHTTPHandler {
private:
    std::map<std::string, nlohmann::json> mockResponses_;
    
public:
    void setMockResponse(const std::string& path,
                         const nlohmann::json& response) {
        mockResponses_[path] = response;
    }
    
    nlohmann::json handleRequest(
        const nlohmann::json& request) override {
        std::string path = request["path"];
        if (mockResponses_.count(path)) {
            return mockResponses_[path];
        }
        return {{"status", "ok"}};
    }
    
    void registerRoute(
        const std::string& path,
        std::function<nlohmann::json(const nlohmann::json&)> handler) override {
        // Mock å®ç°
    }
};
```

---

### 2. ä¾èµ–æ³¨å…¥

#### 2.1 æ„é€ å‡½æ•°æ³¨å…¥

```cpp
// ä½¿ç”¨ä¾èµ–æ³¨å…¥å®ç°è§£è€¦
class HTTPServer {
private:
    std::shared_ptr<ITokenizer> tokenizer_;
    std::shared_ptr<IModelExecutor> executor_;
    
public:
    // æ„é€ å‡½æ•°æ³¨å…¥ä¾èµ–
    HTTPServer(std::shared_ptr<ITokenizer> tokenizer,
               std::shared_ptr<IModelExecutor> executor)
        : tokenizer_(tokenizer), executor_(executor) {}
    
    void handleChatRequest(const nlohmann::json& request) {
        // ä½¿ç”¨æ³¨å…¥çš„ä¾èµ–
        auto text = request["messages"][0]["content"].get<std::string>();
        auto tokens = tokenizer_->encode(text);
        auto output = executor_->generate(tokens, 100);
        auto response = tokenizer_->decode(output);
        // ...
    }
};

// æµ‹è¯•ä¸­ä½¿ç”¨ Mock å¯¹è±¡
TEST(HTTPServerTest, ChatRequestWithMock) {
    // åˆ›å»º Mock å¯¹è±¡
    auto mockTokenizer = std::make_shared<MockTokenizer>();
    auto mockExecutor = std::make_shared<MockModelExecutor>();
    
    // è®¾ç½® Mock è¡Œä¸º
    mockTokenizer->setMockResponse("Hello", {1, 2, 3});
    mockExecutor->setMockOutput({4, 5, 6});
    
    // æ³¨å…¥ Mock å¯¹è±¡
    HTTPServer server(mockTokenizer, mockExecutor);
    
    // æµ‹è¯•
    nlohmann::json request = {
        {"messages", {{{"role", "user"}, {"content", "Hello"}}}}
    };
    server.handleChatRequest(request);
    
    // éªŒè¯
    // ...
}
```

---

### 3. æµ‹è¯•éš”ç¦»ç­–ç•¥

#### 3.1 ä½¿ç”¨ Test Fixture

```cpp
// tests/fixtures/tokenizer_test_fixture.h
class TokenizerTestFixture : public ::testing::Test {
protected:
    std::shared_ptr<ITokenizer> tokenizer_;
    std::string testModelPath_;
    
    void SetUp() override {
        testModelPath_ = std::getenv("CLLM_TEST_MODEL_PATH");
        if (testModelPath_.empty()) {
            // ä½¿ç”¨ Mock
            tokenizer_ = std::make_shared<MockTokenizer>();
        } else {
            // ä½¿ç”¨çœŸå® Tokenizer
            tokenizer_ = std::make_shared<HFTokenizer>();
            tokenizer_->load(testModelPath_);
        }
    }
    
    void TearDown() override {
        tokenizer_.reset();
    }
};

// ä½¿ç”¨ Fixture
TEST_F(TokenizerTestFixture, EncodeTest) {
    auto ids = tokenizer_->encode("Hello, world!");
    EXPECT_FALSE(ids.empty());
}
```

---

#### 3.2 æµ‹è¯•æ•°æ®éš”ç¦»

```cpp
// tests/data/test_data_manager.h
class TestDataManager {
public:
    static TestDataManager& getInstance() {
        static TestDataManager instance;
        return instance;
    }
    
    // è·å–æµ‹è¯•æ•°æ®
    std::vector<std::string> getTestTexts(const std::string& category) {
        if (category == "short") {
            return {"Hello", "Hi", "Test"};
        } else if (category == "long") {
            return {/* long texts */};
        }
        return {};
    }
    
    // è·å–é¢„æœŸç»“æœ
    std::vector<int> getExpectedTokens(const std::string& text) {
        // ä»æ–‡ä»¶åŠ è½½æˆ–ç¡¬ç¼–ç 
        return expectedResults_[text];
    }
    
private:
    std::map<std::string, std::vector<int>> expectedResults_;
    
    TestDataManager() {
        loadTestData();
    }
    
    void loadTestData() {
        // ä» JSON æ–‡ä»¶åŠ è½½æµ‹è¯•æ•°æ®
        std::ifstream f("tests/data/test_cases.json");
        nlohmann::json data = nlohmann::json::parse(f);
        // ...
    }
};
```

---

## ğŸš€ å¹¶è¡ŒåŒ–è®¾è®¡

### 1. ä»»åŠ¡ä¾èµ–å›¾

```
Phase 0 (å‡†å¤‡)
    â”œâ”€â”€ P0.1 (ä¸‹è½½æ¨¡å‹)
    â”œâ”€â”€ P0.2 (å‡†å¤‡æ•°æ®)
    â”œâ”€â”€ P0.3 (é…ç½®ç¯å¢ƒ)
    â”œâ”€â”€ P0.4 (ç¼–è¯‘æµ‹è¯•)
    â””â”€â”€ P0.5 (åˆ›å»ºMock)
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚               â”‚           â”‚          â”‚
Phase 1.1       Phase 1.2   Phase 1.3  Phase 1.4
(HTTPå•å…ƒ)      (Tokenizer) (Executor) (Backend)
    â”‚               â”‚           â”‚          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚               â”‚
            â–¼               â–¼
        Phase 2.1       Phase 2.2
     (HTTP+Tokenizer) (Tokenizer+Executor)
            â”‚               â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
                Phase 3.1
             (å‰ç«¯å­ç³»ç»Ÿ)
                    â”‚
                    â–¼
                Phase 4.1
              (ç³»ç»Ÿé›†æˆ)
                    â”‚
                    â–¼
                Phase 5.1
             (E2Eåœºæ™¯)
```

---

### 2. å¹¶è¡Œæ‰§è¡Œç­–ç•¥

#### 2.1 é˜¶æ®µå†…å¹¶è¡Œ

**Phase 1 å•å…ƒæµ‹è¯• - å®Œå…¨å¹¶è¡Œ**:
```bash
# 5 ä¸ªæ¨¡å—å¯ä»¥å®Œå…¨å¹¶è¡Œæµ‹è¯•
Agent-1: test_http_server_unit
Agent-2: test_hf_tokenizer_unit
Agent-3: test_model_executor_unit
Agent-4: test_libtorch_backend_unit
Agent-5: test_qwen3_model_unit

# æ‰§è¡Œ
parallel ::: \
  "./build/bin/test_http_server_unit" \
  "./build/bin/test_hf_tokenizer_unit" \
  "./build/bin/test_model_executor_unit" \
  "./build/bin/test_libtorch_backend_unit" \
  "./build/bin/test_qwen3_model_unit"
```

**Phase 2 æ¨¡å—é›†æˆ - éƒ¨åˆ†å¹¶è¡Œ**:
```bash
# 2.1 å’Œ 2.3 å¯ä»¥å¹¶è¡Œï¼ˆæ— ä¾èµ–ï¼‰
Agent-1: test_http_tokenizer_integration
Agent-2: test_executor_backend_integration

# 2.2 å’Œ 2.4 å¯ä»¥å¹¶è¡Œ
Agent-3: test_tokenizer_executor_integration
Agent-4: test_backend_qwen3_integration
```

---

#### 2.2 è·¨é˜¶æ®µå¹¶è¡Œ

**åŒæ—¶è¿è¡Œä¸åŒé˜¶æ®µçš„ç‹¬ç«‹ä»»åŠ¡**:
```bash
# å½“ Phase 1.1 å’Œ 1.2 å®Œæˆå
# å¯ä»¥å¯åŠ¨ Phase 2.1 (HTTP + Tokenizer)
# åŒæ—¶ Phase 1.3, 1.4 è¿˜åœ¨è¿è¡Œ

# ä½¿ç”¨ä»»åŠ¡é˜Ÿåˆ—ç®¡ç†
TaskQueue:
  [Running] Phase 1.3 (Executor Unit)
  [Running] Phase 1.4 (Backend Unit)
  [Running] Phase 2.1 (HTTP + Tokenizer) â† æå‰å¯åŠ¨
  [Pending] Phase 2.2 (Tokenizer + Executor)
  [Pending] Phase 2.3 (Executor + Backend)
```

---

#### 2.3 æµ‹è¯•å¹¶è¡ŒåŒ–å·¥å…·

**ä½¿ç”¨ GoogleTest å¹¶è¡Œæ‰§è¡Œ**:
```cpp
// tests/parallel_runner.cpp
#include <gtest/gtest.h>
#include <thread>
#include <vector>

class ParallelTestRunner {
public:
    void runTestSuite(const std::string& filter) {
        std::vector<std::thread> threads;
        
        // åˆ†å‰²æµ‹è¯•ç”¨ä¾‹
        auto testCases = splitTestCases(filter);
        
        // å¹¶è¡Œæ‰§è¡Œ
        for (const auto& testCase : testCases) {
            threads.emplace_back([testCase]() {
                testing::GTEST_FLAG(filter) = testCase;
                testing::InitGoogleTest();
                RUN_ALL_TESTS();
            });
        }
        
        // ç­‰å¾…å®Œæˆ
        for (auto& t : threads) {
            t.join();
        }
    }
    
private:
    std::vector<std::string> splitTestCases(const std::string& filter) {
        // è§£ææµ‹è¯•è¿‡æ»¤å™¨ï¼Œåˆ†å‰²æˆç‹¬ç«‹çš„æµ‹è¯•ç”¨ä¾‹
        return {"Test1.*", "Test2.*", "Test3.*"};
    }
};
```

**ä½¿ç”¨è„šæœ¬å¹¶è¡Œæ‰§è¡Œ**:
```bash
#!/bin/bash
# scripts/parallel_test.sh

# å®šä¹‰æµ‹è¯•åˆ—è¡¨
tests=(
    "test_http_server_unit"
    "test_hf_tokenizer_unit"
    "test_model_executor_unit"
    "test_libtorch_backend_unit"
)

# å¹¶è¡Œæ‰§è¡Œå‡½æ•°
run_test() {
    local test_name=$1
    echo "Running $test_name..."
    ./build/bin/$test_name > logs/${test_name}.log 2>&1
    if [ $? -eq 0 ]; then
        echo "âœ… $test_name PASSED"
    else
        echo "âŒ $test_name FAILED"
    fi
}

# å¯¼å‡ºå‡½æ•°ä¾› parallel ä½¿ç”¨
export -f run_test

# ä½¿ç”¨ GNU parallel å¹¶è¡Œæ‰§è¡Œ
parallel -j 4 run_test ::: "${tests[@]}"

# æˆ–ä½¿ç”¨ xargs
printf '%s\n' "${tests[@]}" | xargs -P 4 -I {} bash -c 'run_test "$@"' _ {}
```

---

### 3. Agent åˆ†å·¥ç­–ç•¥

#### 3.1 æŒ‰æ¨¡å—åˆ†å·¥

```
Agent-1: HTTP Server ä¸“å®¶
  - è´Ÿè´£æ‰€æœ‰ HTTP Server ç›¸å…³æµ‹è¯•
  - Phase 1.1, 2.1, 3.1 çš„ HTTP éƒ¨åˆ†
  
Agent-2: Tokenizer ä¸“å®¶
  - è´Ÿè´£æ‰€æœ‰ Tokenizer ç›¸å…³æµ‹è¯•
  - Phase 1.2, 2.1, 2.2 çš„ Tokenizer éƒ¨åˆ†
  
Agent-3: ModelExecutor ä¸“å®¶
  - è´Ÿè´£æ‰€æœ‰ ModelExecutor ç›¸å…³æµ‹è¯•
  - Phase 1.3, 2.2, 2.3 çš„ Executor éƒ¨åˆ†
  
Agent-4: Backend ä¸“å®¶
  - è´Ÿè´£ LibTorch Backend å’Œ Qwen3 æµ‹è¯•
  - Phase 1.4, 1.5, 2.3, 2.4 çš„ Backend éƒ¨åˆ†
  
Agent-5: é›†æˆæµ‹è¯•ä¸“å®¶
  - è´Ÿè´£è·¨æ¨¡å—é›†æˆæµ‹è¯•
  - Phase 3, 4, 5 çš„åè°ƒå’Œæ‰§è¡Œ
```

---

#### 3.2 æŒ‰æµ‹è¯•ç±»å‹åˆ†å·¥

```
Agent-Functional: åŠŸèƒ½æµ‹è¯•
  - æ‰€æœ‰åŠŸèƒ½æ­£ç¡®æ€§æµ‹è¯•
  - è¦†ç›–æ‰€æœ‰é˜¶æ®µçš„åŠŸèƒ½ç”¨ä¾‹
  
Agent-Performance: æ€§èƒ½æµ‹è¯•
  - æ‰€æœ‰æ€§èƒ½åŸºå‡†æµ‹è¯•
  - Phase 4.2 çš„æ‰€æœ‰æ€§èƒ½æµ‹è¯•
  
Agent-Stability: ç¨³å®šæ€§æµ‹è¯•
  - å‹åŠ›æµ‹è¯•å’Œé•¿æ—¶é—´è¿è¡Œæµ‹è¯•
  - Phase 4.3 çš„æ‰€æœ‰ç¨³å®šæ€§æµ‹è¯•
  
Agent-E2E: ç«¯åˆ°ç«¯æµ‹è¯•
  - çœŸå®åœºæ™¯æµ‹è¯•
  - Phase 5 çš„æ‰€æœ‰åœºæ™¯æµ‹è¯•
```

---

#### 3.3 æ··åˆåˆ†å·¥æ¨¡å¼ï¼ˆæ¨èï¼‰

```
Phase 1 (å•å…ƒæµ‹è¯•):
  Agent-1: HTTP + Tokenizer å•å…ƒæµ‹è¯•
  Agent-2: ModelExecutor + Backend å•å…ƒæµ‹è¯•
  Agent-3: Qwen3 + Mock åˆ›å»º

Phase 2 (æ¨¡å—é›†æˆ):
  Agent-1: HTTP + Tokenizer é›†æˆ
  Agent-2: Tokenizer + Executor é›†æˆ
  Agent-3: Executor + Backend + Qwen3 é›†æˆ

Phase 3 (å­ç³»ç»Ÿ):
  Agent-1: å‰ç«¯å­ç³»ç»Ÿæµ‹è¯•
  Agent-2: æ¨ç†å­ç³»ç»Ÿæµ‹è¯•
  Agent-3: E2E å­ç³»ç»Ÿæµ‹è¯•

Phase 4 (ç³»ç»Ÿé›†æˆ):
  Agent-1: åŠŸèƒ½æµ‹è¯•
  Agent-2: æ€§èƒ½æµ‹è¯•
  Agent-3: ç¨³å®šæ€§æµ‹è¯•

Phase 5 (E2E åœºæ™¯):
  Agent-1: å¯¹è¯åœºæ™¯
  Agent-2: ä¸“ä¸šä»»åŠ¡åœºæ™¯
  Agent-3: è´¨é‡è¯„ä¼°
```

---

## ğŸ“¦ ç»†ç²’åº¦ä»»åŠ¡æ‹†åˆ†

### ä»»åŠ¡å…ƒæ•°æ®ç»“æ„

```json
{
  "task_id": "P1.1.1",
  "phase": 1,
  "module": "http_server",
  "type": "unit_test",
  "name": "HTTPè·¯ç”±æ³¨å†Œæµ‹è¯•",
  "description": "æµ‹è¯•HTTPè·¯ç”±æ³¨å†Œå’ŒåŒ¹é…åŠŸèƒ½",
  "dependencies": ["P0.4"],
  "estimated_time": "30min",
  "priority": "high",
  "assignee": "Agent-1",
  "parallel_with": ["P1.2.1", "P1.3.1"],
  "test_file": "tests/test_http_route.cpp",
  "mock_requirements": ["MockTokenizer", "MockExecutor"],
  "acceptance_criteria": [
    "æ‰€æœ‰è·¯ç”±æ­£ç¡®æ³¨å†Œ",
    "è·¯ç”±åŒ¹é…å‡†ç¡®",
    "é”™è¯¯è·¯ç”±è¿”å›404"
  ]
}
```

---

### å®Œæ•´ä»»åŠ¡æ¸…å•

#### Phase 0: å‡†å¤‡é˜¶æ®µ (5 ä¸ªä»»åŠ¡)

```json
[
  {
    "task_id": "P0.1",
    "name": "ä¸‹è½½Qwen3æ¨¡å‹",
    "agent": "Agent-Ops",
    "time": "60min",
    "parallel": true,
    "script": "scripts/download_qwen3.sh"
  },
  {
    "task_id": "P0.2",
    "name": "å‡†å¤‡æµ‹è¯•æ•°æ®é›†",
    "agent": "Agent-Data",
    "time": "30min",
    "parallel": true,
    "script": "scripts/prepare_test_data.py"
  },
  {
    "task_id": "P0.3",
    "name": "é…ç½®æµ‹è¯•ç¯å¢ƒ",
    "agent": "Agent-Ops",
    "time": "15min",
    "parallel": true,
    "script": "scripts/setup_test_env.sh"
  },
  {
    "task_id": "P0.4",
    "name": "ç¼–è¯‘æ‰€æœ‰æµ‹è¯•",
    "agent": "Agent-Build",
    "time": "20min",
    "parallel": false,
    "command": "make -j8 all_tests"
  },
  {
    "task_id": "P0.5",
    "name": "åˆ›å»ºMockå¯¹è±¡",
    "agent": "Agent-1",
    "time": "45min",
    "parallel": false,
    "files": [
      "tests/mocks/mock_tokenizer.h",
      "tests/mocks/mock_executor.h",
      "tests/mocks/mock_backend.h"
    ]
  }
]
```

---

#### Phase 1: å•å…ƒæµ‹è¯• (20 ä¸ªä»»åŠ¡)

**HTTP Server (4 ä¸ªä»»åŠ¡)**:
```json
[
  {"task_id": "P1.1.1", "name": "è·¯ç”±æµ‹è¯•", "time": "30min"},
  {"task_id": "P1.1.2", "name": "è¯·æ±‚å¤„ç†æµ‹è¯•", "time": "30min"},
  {"task_id": "P1.1.3", "name": "å“åº”æµ‹è¯•", "time": "30min"},
  {"task_id": "P1.1.4", "name": "é”™è¯¯å¤„ç†æµ‹è¯•", "time": "30min"}
]
```

**HFTokenizer (4 ä¸ªä»»åŠ¡)**:
```json
[
  {"task_id": "P1.2.1", "name": "åŠ è½½æµ‹è¯•", "time": "30min"},
  {"task_id": "P1.2.2", "name": "ç¼–ç æµ‹è¯•", "time": "30min"},
  {"task_id": "P1.2.3", "name": "è§£ç æµ‹è¯•", "time": "30min"},
  {"task_id": "P1.2.4", "name": "æ‰¹é‡å¤„ç†æµ‹è¯•", "time": "30min"}
]
```

**ModelExecutor (4 ä¸ªä»»åŠ¡)**:
```json
[
  {"task_id": "P1.3.1", "name": "åˆå§‹åŒ–æµ‹è¯•", "time": "45min"},
  {"task_id": "P1.3.2", "name": "æ¨ç†æ¥å£æµ‹è¯•", "time": "45min"},
  {"task_id": "P1.3.3", "name": "çŠ¶æ€ç®¡ç†æµ‹è¯•", "time": "45min"},
  {"task_id": "P1.3.4", "name": "é”™è¯¯å¤„ç†æµ‹è¯•", "time": "45min"}
]
```

**LibTorch Backend (4 ä¸ªä»»åŠ¡)**:
```json
[
  {"task_id": "P1.4.1", "name": "æ¨¡å‹åŠ è½½æµ‹è¯•", "time": "45min"},
  {"task_id": "P1.4.2", "name": "Tensoræ“ä½œæµ‹è¯•", "time": "45min"},
  {"task_id": "P1.4.3", "name": "æ¨ç†æµ‹è¯•", "time": "45min"},
  {"task_id": "P1.4.4", "name": "å†…å­˜æµ‹è¯•", "time": "45min"}
]
```

**Qwen3 Model (3 ä¸ªä»»åŠ¡)**:
```json
[
  {"task_id": "P1.5.1", "name": "æ¨¡å‹åŠ è½½æµ‹è¯•", "time": "30min"},
  {"task_id": "P1.5.2", "name": "Tokenizeræµ‹è¯•", "time": "30min"},
  {"task_id": "P1.5.3", "name": "åŸºæœ¬æ¨ç†æµ‹è¯•", "time": "60min"}
]
```

---

#### Phase 2: æ¨¡å—é›†æˆæµ‹è¯• (16 ä¸ªä»»åŠ¡)

```json
[
  {
    "task_id": "P2.1.1",
    "name": "HTTP+Tokenizerç«¯ç‚¹é›†æˆ",
    "time": "45min",
    "dependencies": ["P1.1.4", "P1.2.4"]
  },
  {
    "task_id": "P2.1.2",
    "name": "HTTP+Tokenizeræ•°æ®æµ",
    "time": "45min"
  },
  {
    "task_id": "P2.1.3",
    "name": "HTTP+Tokenizeré”™è¯¯ä¼ æ’­",
    "time": "45min"
  },
  {
    "task_id": "P2.1.4",
    "name": "HTTP+Tokenizeræ‰¹é‡è¯·æ±‚",
    "time": "45min"
  },
  
  {
    "task_id": "P2.2.1",
    "name": "Tokenizer+Executoræ•°æ®è½¬æ¢",
    "time": "45min",
    "dependencies": ["P1.2.4", "P1.3.4"]
  },
  {
    "task_id": "P2.2.2",
    "name": "Tokenizer+Executoræ¨ç†æµç¨‹",
    "time": "45min"
  },
  {
    "task_id": "P2.2.3",
    "name": "Tokenizer+Executoræ‰¹å¤„ç†",
    "time": "45min"
  },
  {
    "task_id": "P2.2.4",
    "name": "Tokenizer+ExecutorçŠ¶æ€åŒæ­¥",
    "time": "45min"
  },
  
  {
    "task_id": "P2.3.1",
    "name": "Executor+Backendæ¨ç†æµç¨‹",
    "time": "60min",
    "dependencies": ["P1.3.4", "P1.4.4"]
  },
  {
    "task_id": "P2.3.2",
    "name": "Executor+Backendå†…å­˜ç®¡ç†",
    "time": "60min"
  },
  {
    "task_id": "P2.3.3",
    "name": "Executor+Backendæ€§èƒ½æµ‹è¯•",
    "time": "60min"
  },
  {
    "task_id": "P2.3.4",
    "name": "Executor+Backendé”™è¯¯æ¢å¤",
    "time": "60min"
  },
  
  {
    "task_id": "P2.4.1",
    "name": "Backend+Qwen3æ¨¡å‹åŠ è½½",
    "time": "60min",
    "dependencies": ["P1.4.4", "P1.5.3"]
  },
  {
    "task_id": "P2.4.2",
    "name": "Backend+Qwen3æ¨ç†æ­£ç¡®æ€§",
    "time": "60min"
  },
  {
    "task_id": "P2.4.3",
    "name": "Backend+Qwen3æ€§èƒ½æµ‹è¯•",
    "time": "60min"
  },
  {
    "task_id": "P2.4.4",
    "name": "Backend+Qwen3ç¨³å®šæ€§æµ‹è¯•",
    "time": "60min"
  }
]
```

---

#### Phase 3: å­ç³»ç»Ÿæµ‹è¯• (12 ä¸ªä»»åŠ¡)

```json
[
  {
    "task_id": "P3.1.1",
    "name": "å‰ç«¯å­ç³»ç»Ÿå®Œæ•´æµç¨‹",
    "time": "60min",
    "dependencies": ["P2.1.4"]
  },
  {
    "task_id": "P3.1.2",
    "name": "å‰ç«¯å­ç³»ç»Ÿå¹¶å‘æµ‹è¯•",
    "time": "60min"
  },
  {
    "task_id": "P3.1.3",
    "name": "å‰ç«¯å­ç³»ç»Ÿæ€§èƒ½æµ‹è¯•",
    "time": "60min"
  },
  {
    "task_id": "P3.1.4",
    "name": "å‰ç«¯å­ç³»ç»Ÿå®¹é”™æµ‹è¯•",
    "time": "60min"
  },
  
  {
    "task_id": "P3.2.1",
    "name": "æ¨ç†å­ç³»ç»Ÿå®Œæ•´æµç¨‹",
    "time": "75min",
    "dependencies": ["P2.2.4", "P2.3.4", "P2.4.4"]
  },
  {
    "task_id": "P3.2.2",
    "name": "æ¨ç†å­ç³»ç»Ÿæ‰¹å¤„ç†",
    "time": "75min"
  },
  {
    "task_id": "P3.2.3",
    "name": "æ¨ç†å­ç³»ç»Ÿæ€§èƒ½æµ‹è¯•",
    "time": "75min"
  },
  {
    "task_id": "P3.2.4",
    "name": "æ¨ç†å­ç³»ç»Ÿè´¨é‡æµ‹è¯•",
    "time": "75min"
  },
  
  {
    "task_id": "P3.3.1",
    "name": "E2Eå­ç³»ç»Ÿå®Œæ•´é“¾è·¯",
    "time": "75min",
    "dependencies": ["P2.2.4", "P2.3.4"]
  },
  {
    "task_id": "P3.3.2",
    "name": "E2Eå­ç³»ç»Ÿæµå¼è¾“å‡º",
    "time": "75min"
  },
  {
    "task_id": "P3.3.3",
    "name": "E2Eå­ç³»ç»Ÿå¤šè½®å¯¹è¯",
    "time": "75min"
  },
  {
    "task_id": "P3.3.4",
    "name": "E2Eå­ç³»ç»Ÿè¾¹ç•Œæµ‹è¯•",
    "time": "75min"
  }
]
```

---

#### Phase 4: ç³»ç»Ÿé›†æˆæµ‹è¯• (16 ä¸ªä»»åŠ¡)

```json
[
  {
    "task_id": "P4.1.1",
    "name": "ç³»ç»Ÿæ ¸å¿ƒåŠŸèƒ½æµ‹è¯•",
    "time": "90min",
    "dependencies": ["P3.1.4", "P3.2.4", "P3.3.4"]
  },
  {
    "task_id": "P4.1.2",
    "name": "ç³»ç»ŸAPIå…¼å®¹æ€§æµ‹è¯•",
    "time": "90min"
  },
  {
    "task_id": "P4.1.3",
    "name": "ç³»ç»Ÿå¤šåœºæ™¯æµ‹è¯•",
    "time": "90min"
  },
  {
    "task_id": "P4.1.4",
    "name": "ç³»ç»Ÿé”™è¯¯å¤„ç†æµ‹è¯•",
    "time": "90min"
  },
  
  {
    "task_id": "P4.2.1",
    "name": "æ€§èƒ½åŸºå‡†-ååé‡",
    "time": "90min",
    "dependencies": ["P4.1.4"]
  },
  {
    "task_id": "P4.2.2",
    "name": "æ€§èƒ½åŸºå‡†-å»¶è¿Ÿ",
    "time": "90min"
  },
  {
    "task_id": "P4.2.3",
    "name": "æ€§èƒ½åŸºå‡†-èµ„æºä½¿ç”¨",
    "time": "90min"
  },
  {
    "task_id": "P4.2.4",
    "name": "æ€§èƒ½åŸºå‡†-æ‰©å±•æ€§",
    "time": "90min"
  },
  
  {
    "task_id": "P4.3.1",
    "name": "å‹åŠ›æµ‹è¯•",
    "time": "120min",
    "dependencies": ["P4.1.4"]
  },
  {
    "task_id": "P4.3.2",
    "name": "ç¨³å®šæ€§æµ‹è¯•",
    "time": "120min"
  },
  {
    "task_id": "P4.3.3",
    "name": "å¼‚å¸¸æ³¨å…¥æµ‹è¯•",
    "time": "120min"
  },
  {
    "task_id": "P4.3.4",
    "name": "æ¢å¤æµ‹è¯•",
    "time": "120min"
  }
]
```

---

#### Phase 5: E2E åœºæ™¯æµ‹è¯• (12 ä¸ªä»»åŠ¡)

```json
[
  {
    "task_id": "P5.1.1",
    "name": "å•è½®é—®ç­”åœºæ™¯",
    "time": "90min",
    "dependencies": ["P4.1.4"]
  },
  {
    "task_id": "P5.1.2",
    "name": "å¤šè½®å¯¹è¯åœºæ™¯",
    "time": "90min"
  },
  {
    "task_id": "P5.1.3",
    "name": "ä¸“ä¸šä»»åŠ¡åœºæ™¯",
    "time": "90min"
  },
  {
    "task_id": "P5.1.4",
    "name": "è´¨é‡è¯„ä¼°",
    "time": "90min"
  }
]
```

---

## ğŸ‘¥ Agent åˆ†å·¥æ–¹æ¡ˆ

### åˆ†å·¥è¡¨

| Agent ID | è§’è‰² | ä¸»è¦èŒè´£ | Phase åˆ†å·¥ | é¢„ä¼°æ€»å·¥æ—¶ |
|----------|------|---------|-----------|-----------|
| **Agent-1** | HTTP ä¸“å®¶ | HTTP Server ç›¸å…³ | P1.1, P2.1, P3.1 | 10h |
| **Agent-2** | Tokenizer ä¸“å®¶ | HFTokenizer ç›¸å…³ | P1.2, P2.1, P2.2 | 8h |
| **Agent-3** | Executor ä¸“å®¶ | ModelExecutor ç›¸å…³ | P1.3, P2.2, P2.3 | 10h |
| **Agent-4** | Backend ä¸“å®¶ | LibTorch + Qwen3 | P1.4, P1.5, P2.3, P2.4 | 12h |
| **Agent-5** | é›†æˆä¸“å®¶ | å­ç³»ç»Ÿé›†æˆ | P3.2, P3.3 | 10h |
| **Agent-6** | åŠŸèƒ½æµ‹è¯• | ç³»ç»ŸåŠŸèƒ½æµ‹è¯• | P4.1, P5.1 | 12h |
| **Agent-7** | æ€§èƒ½æµ‹è¯• | æ€§èƒ½å’Œå‹åŠ›æµ‹è¯• | P4.2, P4.3 | 14h |

---

### è¯¦ç»†åˆ†å·¥

#### Agent-1: HTTP Server ä¸“å®¶

**èŒè´£**:
- HTTP Server å•å…ƒæµ‹è¯•
- HTTP + Tokenizer é›†æˆæµ‹è¯•
- å‰ç«¯å­ç³»ç»Ÿæµ‹è¯•

**ä»»åŠ¡åˆ—è¡¨**:
```
Phase 1.1 - HTTP Server å•å…ƒæµ‹è¯• (2h)
  âœ“ P1.1.1: è·¯ç”±æµ‹è¯• (30min)
  âœ“ P1.1.2: è¯·æ±‚å¤„ç†æµ‹è¯• (30min)
  âœ“ P1.1.3: å“åº”æµ‹è¯• (30min)
  âœ“ P1.1.4: é”™è¯¯å¤„ç†æµ‹è¯• (30min)

Phase 2.1 - HTTP + Tokenizer é›†æˆ (3h)
  âœ“ P2.1.1: ç«¯ç‚¹é›†æˆæµ‹è¯• (45min)
  âœ“ P2.1.2: æ•°æ®æµæµ‹è¯• (45min)
  âœ“ P2.1.3: é”™è¯¯ä¼ æ’­æµ‹è¯• (45min)
  âœ“ P2.1.4: æ‰¹é‡è¯·æ±‚æµ‹è¯• (45min)

Phase 3.1 - å‰ç«¯å­ç³»ç»Ÿ (4h)
  âœ“ P3.1.1: å®Œæ•´æµç¨‹æµ‹è¯• (60min)
  âœ“ P3.1.2: å¹¶å‘æµ‹è¯• (60min)
  âœ“ P3.1.3: æ€§èƒ½æµ‹è¯• (60min)
  âœ“ P3.1.4: å®¹é”™æµ‹è¯• (60min)

åä½œä»»åŠ¡:
  - ä¸ Agent-2 åä½œå®Œæˆ Phase 2.1
  - æä¾› Mock HTTP Server ç»™å…¶ä»– Agent
```

**æŠ€èƒ½è¦æ±‚**:
- ç†Ÿæ‚‰ HTTP åè®®
- ç†Ÿæ‚‰ REST API è®¾è®¡
- äº†è§£å¹¶å‘å¤„ç†

---

#### Agent-2: Tokenizer ä¸“å®¶

**èŒè´£**:
- HFTokenizer å•å…ƒæµ‹è¯•
- Tokenizer ç›¸å…³é›†æˆæµ‹è¯•

**ä»»åŠ¡åˆ—è¡¨**:
```
Phase 1.2 - HFTokenizer å•å…ƒæµ‹è¯• (2h)
  âœ“ P1.2.1: åŠ è½½æµ‹è¯• (30min)
  âœ“ P1.2.2: ç¼–ç æµ‹è¯• (30min)
  âœ“ P1.2.3: è§£ç æµ‹è¯• (30min)
  âœ“ P1.2.4: æ‰¹é‡å¤„ç†æµ‹è¯• (30min)

Phase 2.1 - HTTP + Tokenizer é›†æˆ (3h)
  âœ“ ååŠ© Agent-1 å®Œæˆé›†æˆæµ‹è¯•

Phase 2.2 - Tokenizer + Executor é›†æˆ (3h)
  âœ“ P2.2.1: æ•°æ®è½¬æ¢æµ‹è¯• (45min)
  âœ“ P2.2.2: æ¨ç†æµç¨‹æµ‹è¯• (45min)
  âœ“ P2.2.3: æ‰¹å¤„ç†æµ‹è¯• (45min)
  âœ“ P2.2.4: çŠ¶æ€åŒæ­¥æµ‹è¯• (45min)

åä½œä»»åŠ¡:
  - ä¸ Agent-1 åä½œå®Œæˆ Phase 2.1
  - ä¸ Agent-3 åä½œå®Œæˆ Phase 2.2
  - æä¾› Mock Tokenizer ç»™å…¶ä»– Agent
```

**æŠ€èƒ½è¦æ±‚**:
- ç†Ÿæ‚‰ tokenizers-cpp åº“
- äº†è§£ NLP åŸºç¡€
- ç†Ÿæ‚‰å­—ç¬¦ç¼–ç 

---

#### Agent-3: ModelExecutor ä¸“å®¶

**èŒè´£**:
- ModelExecutor å•å…ƒæµ‹è¯•
- Executor ç›¸å…³é›†æˆæµ‹è¯•

**ä»»åŠ¡åˆ—è¡¨**:
```
Phase 1.3 - ModelExecutor å•å…ƒæµ‹è¯• (3h)
  âœ“ P1.3.1: åˆå§‹åŒ–æµ‹è¯• (45min)
  âœ“ P1.3.2: æ¨ç†æ¥å£æµ‹è¯• (45min)
  âœ“ P1.3.3: çŠ¶æ€ç®¡ç†æµ‹è¯• (45min)
  âœ“ P1.3.4: é”™è¯¯å¤„ç†æµ‹è¯• (45min)

Phase 2.2 - Tokenizer + Executor é›†æˆ (3h)
  âœ“ ååŠ© Agent-2 å®Œæˆé›†æˆæµ‹è¯•

Phase 2.3 - Executor + Backend é›†æˆ (4h)
  âœ“ P2.3.1: æ¨ç†æµç¨‹æµ‹è¯• (60min)
  âœ“ P2.3.2: å†…å­˜ç®¡ç†æµ‹è¯• (60min)
  âœ“ P2.3.3: æ€§èƒ½æµ‹è¯• (60min)
  âœ“ P2.3.4: é”™è¯¯æ¢å¤æµ‹è¯• (60min)

åä½œä»»åŠ¡:
  - ä¸ Agent-2 åä½œå®Œæˆ Phase 2.2
  - ä¸ Agent-4 åä½œå®Œæˆ Phase 2.3
  - æä¾› Mock Executor ç»™å…¶ä»– Agent
```

**æŠ€èƒ½è¦æ±‚**:
- ç†Ÿæ‚‰æ¨¡å‹æ¨ç†æµç¨‹
- äº†è§£æ‰¹å¤„ç†æœºåˆ¶
- ç†Ÿæ‚‰çŠ¶æ€ç®¡ç†

---

#### Agent-4: Backend & Model ä¸“å®¶

**èŒè´£**:
- LibTorch Backend å•å…ƒæµ‹è¯•
- Qwen3 æ¨¡å‹å•å…ƒæµ‹è¯•
- Backend ç›¸å…³é›†æˆæµ‹è¯•

**ä»»åŠ¡åˆ—è¡¨**:
```
Phase 1.4 - LibTorch Backend å•å…ƒæµ‹è¯• (3h)
  âœ“ P1.4.1: æ¨¡å‹åŠ è½½æµ‹è¯• (45min)
  âœ“ P1.4.2: Tensoræ“ä½œæµ‹è¯• (45min)
  âœ“ P1.4.3: æ¨ç†æµ‹è¯• (45min)
  âœ“ P1.4.4: å†…å­˜æµ‹è¯• (45min)

Phase 1.5 - Qwen3 Model å•å…ƒæµ‹è¯• (2h)
  âœ“ P1.5.1: æ¨¡å‹åŠ è½½æµ‹è¯• (30min)
  âœ“ P1.5.2: Tokenizeræµ‹è¯• (30min)
  âœ“ P1.5.3: åŸºæœ¬æ¨ç†æµ‹è¯• (60min)

Phase 2.3 - Executor + Backend é›†æˆ (4h)
  âœ“ ååŠ© Agent-3 å®Œæˆé›†æˆæµ‹è¯•

Phase 2.4 - Backend + Qwen3 é›†æˆ (4h)
  âœ“ P2.4.1: æ¨¡å‹åŠ è½½æµ‹è¯• (60min)
  âœ“ P2.4.2: æ¨ç†æ­£ç¡®æ€§æµ‹è¯• (60min)
  âœ“ P2.4.3: æ€§èƒ½æµ‹è¯• (60min)
  âœ“ P2.4.4: ç¨³å®šæ€§æµ‹è¯• (60min)

åä½œä»»åŠ¡:
  - ä¸ Agent-3 åä½œå®Œæˆ Phase 2.3
  - æä¾› Mock Backend ç»™å…¶ä»– Agent
```

**æŠ€èƒ½è¦æ±‚**:
- ç†Ÿæ‚‰ LibTorch/PyTorch
- äº†è§£æ·±åº¦å­¦ä¹ æ¨ç†
- ç†Ÿæ‚‰ GPU ç¼–ç¨‹ï¼ˆå¯é€‰ï¼‰

---

#### Agent-5: é›†æˆæµ‹è¯•ä¸“å®¶

**èŒè´£**:
- å­ç³»ç»Ÿé›†æˆæµ‹è¯•
- ç«¯åˆ°ç«¯æµ‹è¯•åè°ƒ

**ä»»åŠ¡åˆ—è¡¨**:
```
Phase 3.2 - æ¨ç†å­ç³»ç»Ÿæµ‹è¯• (5h)
  âœ“ P3.2.1: å®Œæ•´æµç¨‹æµ‹è¯• (75min)
  âœ“ P3.2.2: æ‰¹å¤„ç†æµ‹è¯• (75min)
  âœ“ P3.2.3: æ€§èƒ½æµ‹è¯• (75min)
  âœ“ P3.2.4: è´¨é‡æµ‹è¯• (75min)

Phase 3.3 - E2E å­ç³»ç»Ÿæµ‹è¯• (5h)
  âœ“ P3.3.1: å®Œæ•´é“¾è·¯æµ‹è¯• (75min)
  âœ“ P3.3.2: æµå¼è¾“å‡ºæµ‹è¯• (75min)
  âœ“ P3.3.3: å¤šè½®å¯¹è¯æµ‹è¯• (75min)
  âœ“ P3.3.4: è¾¹ç•Œæµ‹è¯• (75min)

åä½œä»»åŠ¡:
  - åè°ƒæ‰€æœ‰ Agent çš„é›†æˆå·¥ä½œ
  - ç®¡ç†æµ‹è¯•ä¾èµ–å…³ç³»
```

**æŠ€èƒ½è¦æ±‚**:
- å…¨æ ˆæµ‹è¯•ç»éªŒ
- é¡¹ç›®ç®¡ç†èƒ½åŠ›
- é—®é¢˜è¯Šæ–­èƒ½åŠ›

---

#### Agent-6: åŠŸèƒ½æµ‹è¯•ä¸“å®¶

**èŒè´£**:
- ç³»ç»ŸåŠŸèƒ½æµ‹è¯•
- E2E åœºæ™¯æµ‹è¯•

**ä»»åŠ¡åˆ—è¡¨**:
```
Phase 4.1 - ç³»ç»ŸåŠŸèƒ½æµ‹è¯• (6h)
  âœ“ P4.1.1: æ ¸å¿ƒåŠŸèƒ½æµ‹è¯• (90min)
  âœ“ P4.1.2: APIå…¼å®¹æ€§æµ‹è¯• (90min)
  âœ“ P4.1.3: å¤šåœºæ™¯æµ‹è¯• (90min)
  âœ“ P4.1.4: é”™è¯¯å¤„ç†æµ‹è¯• (90min)

Phase 5.1 - E2E åœºæ™¯æµ‹è¯• (6h)
  âœ“ P5.1.1: å•è½®é—®ç­”åœºæ™¯ (90min)
  âœ“ P5.1.2: å¤šè½®å¯¹è¯åœºæ™¯ (90min)
  âœ“ P5.1.3: ä¸“ä¸šä»»åŠ¡åœºæ™¯ (90min)
  âœ“ P5.1.4: è´¨é‡è¯„ä¼° (90min)

åä½œä»»åŠ¡:
  - ä¸ Agent-5 åè°ƒç³»ç»Ÿæµ‹è¯•
  - æ”¶é›†ç”¨æˆ·åé¦ˆ
```

**æŠ€èƒ½è¦æ±‚**:
- åŠŸèƒ½æµ‹è¯•ç»éªŒ
- åœºæ™¯è®¾è®¡èƒ½åŠ›
- è´¨é‡è¯„ä¼°èƒ½åŠ›

---

#### Agent-7: æ€§èƒ½æµ‹è¯•ä¸“å®¶

**èŒè´£**:
- æ€§èƒ½åŸºå‡†æµ‹è¯•
- å‹åŠ›æµ‹è¯•å’Œç¨³å®šæ€§æµ‹è¯•

**ä»»åŠ¡åˆ—è¡¨**:
```
Phase 4.2 - æ€§èƒ½åŸºå‡†æµ‹è¯• (6h)
  âœ“ P4.2.1: ååé‡æµ‹è¯• (90min)
  âœ“ P4.2.2: å»¶è¿Ÿæµ‹è¯• (90min)
  âœ“ P4.2.3: èµ„æºä½¿ç”¨æµ‹è¯• (90min)
  âœ“ P4.2.4: æ‰©å±•æ€§æµ‹è¯• (90min)

Phase 4.3 - å‹åŠ›å’Œç¨³å®šæ€§æµ‹è¯• (8h)
  âœ“ P4.3.1: å‹åŠ›æµ‹è¯• (120min)
  âœ“ P4.3.2: ç¨³å®šæ€§æµ‹è¯• (120min)
  âœ“ P4.3.3: å¼‚å¸¸æ³¨å…¥æµ‹è¯• (120min)
  âœ“ P4.3.4: æ¢å¤æµ‹è¯• (120min)

åä½œä»»åŠ¡:
  - æä¾›æ€§èƒ½åŸºå‡†æ•°æ®
  - åˆ†ææ€§èƒ½ç“¶é¢ˆ
```

**æŠ€èƒ½è¦æ±‚**:
- æ€§èƒ½æµ‹è¯•ç»éªŒ
- å‹åŠ›æµ‹è¯•å·¥å…·ä½¿ç”¨
- æ€§èƒ½åˆ†æèƒ½åŠ›

---

## ğŸ“Š æµ‹è¯•æ•°æ®å‡†å¤‡

### æµ‹è¯•æ•°æ®ç»“æ„

```json
{
  "test_data": {
    "tokenizer": {
      "short_texts": [
        "Hello, world!",
        "ä½ å¥½ï¼Œä¸–ç•Œï¼",
        "Test input"
      ],
      "long_texts": [
        "This is a very long text for testing...",
        "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„ä¸­æ–‡æµ‹è¯•æ–‡æœ¬..."
      ],
      "special_cases": [
        "ğŸ˜€ğŸ‰",
        "Text with\nnewlines",
        "Mixedä¸­è‹±æ–‡text"
      ],
      "expected_results": {
        "Hello, world!": {
          "token_ids": [101, 7592, 1010, 2088, 999, 102],
          "decoded": "Hello, world!"
        }
      }
    },
    "inference": {
      "prompts": [
        {
          "text": "What is artificial intelligence?",
          "max_length": 50,
          "expected_keywords": ["AI", "machine", "learning"]
        },
        {
          "text": "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
          "max_length": 100,
          "expected_keywords": ["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ "]
        }
      ]
    },
    "performance": {
      "batch_sizes": [1, 4, 8, 16, 32],
      "sequence_lengths": [10, 50, 100, 500, 1000],
      "concurrency_levels": [1, 5, 10, 20, 50]
    },
    "stress": {
      "duration_minutes": [5, 15, 30, 60],
      "request_rates": [10, 50, 100, 200],
      "payload_sizes": [100, 1000, 10000]
    }
  }
}
```

### æµ‹è¯•æ•°æ®ç”Ÿæˆè„šæœ¬

```python
# scripts/generate_test_data.py
import json
import random
import string

class TestDataGenerator:
    def __init__(self):
        self.data = {
            "tokenizer": {},
            "inference": {},
            "performance": {},
            "stress": {}
        }
    
    def generate_tokenizer_data(self):
        """ç”Ÿæˆ Tokenizer æµ‹è¯•æ•°æ®"""
        # çŸ­æ–‡æœ¬
        short_texts = [
            "Hello, world!",
            "ä½ å¥½ï¼Œä¸–ç•Œï¼",
            "Test input",
            "Simple test",
            "ã“ã‚“ã«ã¡ã¯"
        ]
        
        # é•¿æ–‡æœ¬
        long_texts = []
        for i in range(5):
            text = " ".join([
                "This is a long text for testing."
            ] * 50)
            long_texts.append(text)
        
        # ç‰¹æ®Šæƒ…å†µ
        special_cases = [
            "ğŸ˜€ğŸ‰ğŸš€",
            "Text with\nnewlines\nand\ttabs",
            "Mixedä¸­è‹±æ–‡æ—¥æœ¬èªtext",
            "Special chars: !@#$%^&*()",
            "Numbers: 0123456789"
        ]
        
        self.data["tokenizer"] = {
            "short_texts": short_texts,
            "long_texts": long_texts,
            "special_cases": special_cases
        }
    
    def generate_inference_data(self):
        """ç”Ÿæˆæ¨ç†æµ‹è¯•æ•°æ®"""
        prompts = [
            {
                "text": "What is artificial intelligence?",
                "max_length": 50,
                "temperature": 0.7,
                "expected_keywords": ["AI", "machine", "learning"]
            },
            {
                "text": "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
                "max_length": 100,
                "temperature": 0.7,
                "expected_keywords": ["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ "]
            },
            {
                "text": "Write a Python function to calculate fibonacci",
                "max_length": 200,
                "temperature": 0.3,
                "expected_keywords": ["def", "fibonacci", "return"]
            }
        ]
        
        self.data["inference"]["prompts"] = prompts
    
    def generate_performance_data(self):
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æ•°æ®"""
        self.data["performance"] = {
            "batch_sizes": [1, 2, 4, 8, 16, 32],
            "sequence_lengths": [10, 50, 100, 200, 500, 1000],
            "concurrency_levels": [1, 5, 10, 20, 50, 100]
        }
    
    def generate_stress_data(self):
        """ç”Ÿæˆå‹åŠ›æµ‹è¯•æ•°æ®"""
        self.data["stress"] = {
            "duration_minutes": [5, 15, 30, 60],
            "request_rates": [10, 50, 100, 200, 500],
            "payload_sizes": [100, 500, 1000, 5000, 10000]
        }
    
    def save(self, filepath):
        """ä¿å­˜æµ‹è¯•æ•°æ®"""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)
    
    def generate_all(self):
        """ç”Ÿæˆæ‰€æœ‰æµ‹è¯•æ•°æ®"""
        self.generate_tokenizer_data()
        self.generate_inference_data()
        self.generate_performance_data()
        self.generate_stress_data()

if __name__ == "__main__":
    generator = TestDataGenerator()
    generator.generate_all()
    generator.save("tests/data/test_cases.json")
    print("âœ… æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆ!")
```

---

## ğŸš€ æ‰§è¡Œæµç¨‹

### 1. å‡†å¤‡é˜¶æ®µæ‰§è¡Œ

```bash
#!/bin/bash
# scripts/prepare_tests.sh

echo "========================================="
echo "Phase 0: æµ‹è¯•å‡†å¤‡é˜¶æ®µ"
echo "========================================="

# P0.1: ä¸‹è½½ Qwen3 æ¨¡å‹
echo "ğŸ“¥ ä¸‹è½½ Qwen3 æ¨¡å‹..."
./scripts/download_qwen3.sh &
PID_DOWNLOAD=$!

# P0.2: å‡†å¤‡æµ‹è¯•æ•°æ®
echo "ğŸ“Š ç”Ÿæˆæµ‹è¯•æ•°æ®..."
python3 scripts/generate_test_data.py &
PID_DATA=$!

# P0.3: é…ç½®æµ‹è¯•ç¯å¢ƒ
echo "âš™ï¸  é…ç½®æµ‹è¯•ç¯å¢ƒ..."
export CLLM_TEST_MODEL_PATH="./test_models/qwen3"
export CLLM_TEST_DATA_PATH="./tests/data"
mkdir -p logs

# P0.4: ç¼–è¯‘æ‰€æœ‰æµ‹è¯•
echo "ğŸ”¨ ç¼–è¯‘æµ‹è¯•ç¨‹åº..."
cd build
cmake .. -DUSE_TOKENIZERS_CPP=ON -DBUILD_TESTS=ON
make -j8 all_tests
cd ..

# ç­‰å¾…å¹¶è¡Œä»»åŠ¡å®Œæˆ
wait $PID_DOWNLOAD
echo "âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ"

wait $PID_DATA
echo "âœ… æµ‹è¯•æ•°æ®å‡†å¤‡å®Œæˆ"

# P0.5: åˆ›å»º Mock å¯¹è±¡ï¼ˆå·²ç»åœ¨ä»£ç ä¸­ï¼‰
echo "âœ… Mock å¯¹è±¡å·²å°±ç»ª"

echo "========================================="
echo "âœ… Phase 0 å®Œæˆï¼"
echo "========================================="
```

---

### 2. å•å…ƒæµ‹è¯•æ‰§è¡Œ

```bash
#!/bin/bash
# scripts/run_unit_tests.sh

echo "========================================="
echo "Phase 1: å•å…ƒæµ‹è¯•é˜¶æ®µ"
echo "========================================="

# å®šä¹‰æµ‹è¯•åˆ—è¡¨
declare -A tests=(
    ["P1.1"]="test_http_server_unit"
    ["P1.2"]="test_hf_tokenizer_unit"
    ["P1.3"]="test_model_executor_unit"
    ["P1.4"]="test_libtorch_backend_unit"
    ["P1.5"]="test_qwen3_model_unit"
)

# å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•
run_test() {
    local phase=$1
    local test_name=$2
    
    echo "ğŸ§ª [$phase] è¿è¡Œ $test_name..."
    ./build/bin/$test_name --gtest_output=json:logs/${test_name}.json \
        > logs/${test_name}.log 2>&1
    
    if [ $? -eq 0 ]; then
        echo "âœ… [$phase] $test_name PASSED"
        return 0
    else
        echo "âŒ [$phase] $test_name FAILED"
        return 1
    fi
}

export -f run_test

# ä½¿ç”¨ parallel å¹¶è¡Œæ‰§è¡Œ
parallel -j 5 run_test ::: "${!tests[@]}" ::: "${tests[@]}"

# æ±‡æ€»ç»“æœ
echo "========================================="
echo "ğŸ“Š Phase 1 æµ‹è¯•ç»“æœæ±‡æ€»"
echo "========================================="

passed=0
failed=0

for phase in "${!tests[@]}"; do
    test_name="${tests[$phase]}"
    if grep -q '"failures": 0' logs/${test_name}.json 2>/dev/null; then
        echo "âœ… [$phase] ${test_name}"
        ((passed++))
    else
        echo "âŒ [$phase] ${test_name}"
        ((failed++))
    fi
done

echo "========================================="
echo "é€šè¿‡: $passed, å¤±è´¥: $failed"
echo "========================================="
```

---

### 3. é›†æˆæµ‹è¯•æ‰§è¡Œ

```bash
#!/bin/bash
# scripts/run_integration_tests.sh

echo "========================================="
echo "Phase 2: æ¨¡å—é›†æˆæµ‹è¯•é˜¶æ®µ"
echo "========================================="

# Phase 2.1 å’Œ 2.3 å¯ä»¥å¹¶è¡Œï¼ˆæ— ä¾èµ–ï¼‰
echo "ğŸ”„ å¹¶è¡Œæ‰§è¡Œ Phase 2.1 å’Œ 2.3..."
./build/bin/test_http_tokenizer_integration > logs/P2.1.log 2>&1 &
PID_21=$!

./build/bin/test_executor_backend_integration > logs/P2.3.log 2>&1 &
PID_23=$!

# ç­‰å¾…å®Œæˆ
wait $PID_21
echo "âœ… Phase 2.1 å®Œæˆ"

wait $PID_23
echo "âœ… Phase 2.3 å®Œæˆ"

# Phase 2.2 å’Œ 2.4 å¯ä»¥å¹¶è¡Œ
echo "ğŸ”„ å¹¶è¡Œæ‰§è¡Œ Phase 2.2 å’Œ 2.4..."
./build/bin/test_tokenizer_executor_integration > logs/P2.2.log 2>&1 &
PID_22=$!

./build/bin/test_backend_qwen3_integration > logs/P2.4.log 2>&1 &
PID_24=$!

# ç­‰å¾…å®Œæˆ
wait $PID_22
echo "âœ… Phase 2.2 å®Œæˆ"

wait $PID_24
echo "âœ… Phase 2.4 å®Œæˆ"

echo "========================================="
echo "âœ… Phase 2 å®Œæˆï¼"
echo "========================================="
```

---

### 4. å®Œæ•´æ‰§è¡Œè„šæœ¬

```bash
#!/bin/bash
# scripts/run_all_tests.sh

set -e

echo "ğŸš€ å¼€å§‹ cLLM åˆ†é˜¶æ®µé›†æˆæµ‹è¯•"
echo "========================================="

# è®°å½•å¼€å§‹æ—¶é—´
START_TIME=$(date +%s)

# Phase 0: å‡†å¤‡
echo "ğŸ“‹ Phase 0: å‡†å¤‡é˜¶æ®µ"
./scripts/prepare_tests.sh

# Phase 1: å•å…ƒæµ‹è¯•
echo "ğŸ“‹ Phase 1: å•å…ƒæµ‹è¯•é˜¶æ®µ"
./scripts/run_unit_tests.sh

# Phase 2: æ¨¡å—é›†æˆæµ‹è¯•
echo "ğŸ“‹ Phase 2: æ¨¡å—é›†æˆæµ‹è¯•é˜¶æ®µ"
./scripts/run_integration_tests.sh

# Phase 3: å­ç³»ç»Ÿæµ‹è¯•
echo "ğŸ“‹ Phase 3: å­ç³»ç»Ÿæµ‹è¯•é˜¶æ®µ"
./scripts/run_subsystem_tests.sh

# Phase 4: ç³»ç»Ÿé›†æˆæµ‹è¯•
echo "ğŸ“‹ Phase 4: ç³»ç»Ÿé›†æˆæµ‹è¯•é˜¶æ®µ"
./scripts/run_system_tests.sh

# Phase 5: E2E åœºæ™¯æµ‹è¯•
echo "ğŸ“‹ Phase 5: E2E åœºæ™¯æµ‹è¯•é˜¶æ®µ"
./scripts/run_e2e_tests.sh

# è®°å½•ç»“æŸæ—¶é—´
END_TIME=$(date +%s)
DURATION=$((END_TIME - START_TIME))

echo "========================================="
echo "âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼"
echo "â±ï¸  æ€»è€—æ—¶: ${DURATION}ç§’ ($((DURATION / 60))åˆ†é’Ÿ)"
echo "========================================="

# ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
python3 scripts/generate_test_report.py
```

---

## ğŸ“ˆ ç›‘æ§å’ŒæŠ¥å‘Š

### æµ‹è¯•è¿›åº¦ç›‘æ§

```python
# scripts/monitor_tests.py
import json
import os
from datetime import datetime

class TestMonitor:
    def __init__(self):
        self.results = {}
    
    def check_test_status(self, test_name):
        """æ£€æŸ¥æµ‹è¯•çŠ¶æ€"""
        log_file = f"logs/{test_name}.json"
        if not os.path.exists(log_file):
            return "pending"
        
        with open(log_file) as f:
            data = json.load(f)
            if data.get("failures", 0) == 0:
                return "passed"
            else:
                return "failed"
    
    def generate_progress_report(self):
        """ç”Ÿæˆè¿›åº¦æŠ¥å‘Š"""
        report = {
            "timestamp": datetime.now().isoformat(),
            "phases": {}
        }
        
        # æ£€æŸ¥æ¯ä¸ªé˜¶æ®µçš„è¿›åº¦
        for phase in ["P1", "P2", "P3", "P4", "P5"]:
            phase_tests = self.get_phase_tests(phase)
            total = len(phase_tests)
            passed = sum(1 for t in phase_tests 
                        if self.check_test_status(t) == "passed")
            failed = sum(1 for t in phase_tests 
                        if self.check_test_status(t) == "failed")
            pending = total - passed - failed
            
            report["phases"][phase] = {
                "total": total,
                "passed": passed,
                "failed": failed,
                "pending": pending,
                "progress": f"{passed}/{total}"
            }
        
        return report
    
    def save_report(self, filepath):
        """ä¿å­˜æŠ¥å‘Š"""
        report = self.generate_progress_report()
        with open(filepath, 'w') as f:
            json.dump(report, f, indent=2)

if __name__ == "__main__":
    monitor = TestMonitor()
    monitor.save_report("logs/test_progress.json")
```

---

## ğŸ“ æ€»ç»“

### å…³é”®ç‰¹æ€§

1. **åˆ†é˜¶æ®µæ‰©å±•**: 5 ä¸ªé˜¶æ®µï¼Œä»å•å…ƒæµ‹è¯•åˆ° E2E æµ‹è¯•
2. **æ¨¡å—è§£è€¦**: ä½¿ç”¨æ¥å£æŠ½è±¡å’Œä¾èµ–æ³¨å…¥
3. **å¹¶è¡ŒåŒ–**: æœ€å¤š 7 ä¸ª Agent å¹¶è¡Œæ‰§è¡Œ
4. **ç»†ç²’åº¦æ‹†åˆ†**: 81 ä¸ªç‹¬ç«‹ä»»åŠ¡
5. **å®Œæ•´è¦†ç›–**: HTTPã€Tokenizerã€Executorã€Backendã€Qwen3

### æ‰§è¡Œæ—¶é—´ä¼°ç®—

| é˜¶æ®µ | ä¸²è¡Œæ—¶é—´ | å¹¶è¡Œæ—¶é—´ï¼ˆ7 Agentï¼‰| åŠ é€Ÿæ¯” |
|------|---------|------------------|--------|
| Phase 0 | 3h | 1h | 3x |
| Phase 1 | 12h | 3h | 4x |
| Phase 2 | 14h | 4h | 3.5x |
| Phase 3 | 15h | 5h | 3x |
| Phase 4 | 24h | 8h | 3x |
| Phase 5 | 6h | 2h | 3x |
| **æ€»è®¡** | **74h** | **23h** | **3.2x** |

### èµ„æºéœ€æ±‚

- **è®¡ç®—èµ„æº**: 7 ä¸ªå¹¶è¡Œ Agentï¼ˆæ¨è 8 æ ¸ CPU + GPUï¼‰
- **å­˜å‚¨ç©ºé—´**: ~50GBï¼ˆæ¨¡å‹ + æ—¥å¿— + æ•°æ®ï¼‰
- **æµ‹è¯•æ—¶é—´**: çº¦ 1 ä¸ªå·¥ä½œæ—¥ï¼ˆ23 å°æ—¶ï¼‰
- **äººåŠ›æŠ•å…¥**: 7 äººå¹¶è¡Œæˆ– 1 äººä¸²è¡Œæ‰§è¡Œ

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¶é—´**: 2026-01-11  
**ä¸‹ä¸€æ¬¡æ›´æ–°**: æ‰§è¡Œåæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
