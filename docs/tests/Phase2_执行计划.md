# Phase 2: æ¨¡å—é›†æˆæµ‹è¯•é˜¶æ®µ æ‰§è¡Œè®¡åˆ’

**è´Ÿè´£Agent**: Agent-2  
**é¢„è®¡è€—æ—¶**: 14å°æ—¶  
**ä¾èµ–**: Phase 1 å®Œæˆ  
**æ‰§è¡Œæ—¶é—´**: T+15h ~ T+29h  

---

## ğŸ“‹ é˜¶æ®µç›®æ ‡

éªŒè¯ç›¸é‚»æ¨¡å—ä¹‹é—´çš„æ¥å£é›†æˆå’Œæ•°æ®æµï¼Œç¡®ä¿æ¨¡å—é—´å¯ä»¥æ­£ç¡®åä½œã€‚

---

## ğŸ“Š ä»»åŠ¡æ¸…å•

| å­é˜¶æ®µ | ä»»åŠ¡æ•° | è€—æ—¶ | ä¾èµ– | çŠ¶æ€ |
|--------|--------|------|------|------|
| P2.1: HTTP + Tokenizer é›†æˆ | 4 | 3h | P1.1, P1.2 | â³ å¾…æ‰§è¡Œ |
| P2.2: Tokenizer + Executor é›†æˆ | 4 | 4h | P1.2, P1.3 | â³ å¾…æ‰§è¡Œ |
| P2.3: Executor + Backend é›†æˆ | 4 | 4h | P1.3, P1.4 | â³ å¾…æ‰§è¡Œ |
| P2.4: Backend + Qwen3 é›†æˆ | 4 | 3h | P1.4, P1.5 | â³ å¾…æ‰§è¡Œ |

**æ€»è®¡**: 16ä¸ªä»»åŠ¡ï¼Œ14å°æ—¶

---

## ğŸ“ è¯¦ç»†ä»»åŠ¡è¯´æ˜

### P2.1: HTTP + Tokenizer é›†æˆ (3å°æ—¶)

#### æµ‹è¯•é‡ç‚¹
- `/v1/tokenize` ç«¯ç‚¹é›†æˆ
- `/v1/detokenize` ç«¯ç‚¹é›†æˆ
- HTTP â†’ Tokenizer æ•°æ®æµ
- é”™è¯¯ä¼ æ’­

#### å…³é”®æµ‹è¯•ç”¨ä¾‹

**P2.1.1: `/v1/tokenize` ç«¯ç‚¹æµ‹è¯•** (45åˆ†é’Ÿ)
```cpp
TEST(HTTPTokenizerIntegration, TokenizeEndpoint) {
  HTTPServer server;
  HFTokenizer tokenizer;
  tokenizer.load("${CLLM_TEST_MODEL_PATH}");
  
  server.registerTokenizer(&tokenizer);
  
  HTTPRequest request;
  request.path = "/v1/tokenize";
  request.method = "POST";
  request.body = R"({"text":"Hello, world!"})";
  
  auto response = server.handle(request);
  
  EXPECT_EQ(response.statusCode, 200);
  auto json = response.parseJSON();
  EXPECT_TRUE(json.contains("tokens"));
  EXPECT_GT(json["tokens"].size(), 0);
}
```

**P2.1.2: `/v1/detokenize` ç«¯ç‚¹æµ‹è¯•** (45åˆ†é’Ÿ)
```cpp
TEST(HTTPTokenizerIntegration, DetokenizeEndpoint) {
  HTTPServer server;
  HFTokenizer tokenizer;
  tokenizer.load("${CLLM_TEST_MODEL_PATH}");
  
  server.registerTokenizer(&tokenizer);
  
  HTTPRequest request;
  request.path = "/v1/detokenize";
  request.method = "POST";
  request.body = R"({"tokens":[100, 200, 300]})";
  
  auto response = server.handle(request);
  
  EXPECT_EQ(response.statusCode, 200);
  auto json = response.parseJSON();
  EXPECT_TRUE(json.contains("text"));
}
```

**P2.1.3: é”™è¯¯ä¼ æ’­æµ‹è¯•** (45åˆ†é’Ÿ)
```cpp
TEST(HTTPTokenizerIntegration, ErrorPropagation) {
  HTTPServer server;
  HFTokenizer tokenizer;
  // ä¸åŠ è½½tokenizerï¼Œæ¨¡æ‹Ÿé”™è¯¯
  
  server.registerTokenizer(&tokenizer);
  
  HTTPRequest request;
  request.path = "/v1/tokenize";
  request.body = R"({"text":"test"})";
  
  auto response = server.handle(request);
  
  EXPECT_NE(response.statusCode, 200);
  EXPECT_TRUE(response.body.find("error") != std::string::npos);
}
```

**P2.1.4: æ‰¹é‡è¯·æ±‚æµ‹è¯•** (45åˆ†é’Ÿ)
```cpp
TEST(HTTPTokenizerIntegration, BatchRequests) {
  HTTPServer server;
  HFTokenizer tokenizer;
  tokenizer.load("${CLLM_TEST_MODEL_PATH}");
  
  server.registerTokenizer(&tokenizer);
  
  HTTPRequest request;
  request.path = "/v1/tokenize/batch";
  request.body = R"({"texts":["First text", "Second text", "Third text"]})";
  
  auto response = server.handle(request);
  
  EXPECT_EQ(response.statusCode, 200);
  auto json = response.parseJSON();
  EXPECT_EQ(json["results"].size(), 3);
}
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… Tokenize ç«¯ç‚¹æ­£å¸¸å·¥ä½œ
- âœ… Detokenize ç«¯ç‚¹æ­£å¸¸å·¥ä½œ
- âœ… é”™è¯¯æ­£ç¡®ä¼ æ’­åˆ° HTTP å“åº”
- âœ… æ‰¹é‡è¯·æ±‚æ­£ç¡®å¤„ç†

---

### P2.2: Tokenizer + Executor é›†æˆ (4å°æ—¶)

#### æµ‹è¯•é‡ç‚¹
- Token IDs â†’ Tensor è½¬æ¢
- Tokenizer â†’ Executor æ•°æ®æµ
- æ‰¹å¤„ç†é›†æˆ
- çŠ¶æ€åŒæ­¥

#### å…³é”®æµ‹è¯•ç”¨ä¾‹

**P2.2.1: æ•°æ®æ ¼å¼è½¬æ¢æµ‹è¯•** (60åˆ†é’Ÿ)
```cpp
TEST(TokenizerExecutorIntegration, DataConversion) {
  HFTokenizer tokenizer;
  tokenizer.load("${CLLM_TEST_MODEL_PATH}");
  
  ModelExecutor executor;
  executor.initialize(defaultConfig());
  executor.setBackend(std::make_unique<MockBackend>());
  
  std::string text = "Hello, world!";
  auto token_ids = tokenizer.encode(text);
  
  auto output = executor.forward(token_ids);
  
  EXPECT_FALSE(output.empty());
  EXPECT_EQ(output.size(), token_ids.size());
}
```

**P2.2.2: æ¨ç†æµç¨‹æµ‹è¯•** (60åˆ†é’Ÿ)
```cpp
TEST(TokenizerExecutorIntegration, InferencePipeline) {
  HFTokenizer tokenizer;
  tokenizer.load("${CLLM_TEST_MODEL_PATH}");
  
  ModelExecutor executor;
  executor.initialize(defaultConfig());
  executor.setBackend(std::make_unique<MockBackend>());
  
  std::string prompt = "What is AI?";
  auto input_ids = tokenizer.encode(prompt);
  auto output_ids = executor.generate(input_ids, 20);
  auto output_text = tokenizer.decode(output_ids);
  
  EXPECT_FALSE(output_text.empty());
  EXPECT_GT(output_text.length(), prompt.length());
}
```

**P2.2.3: æ‰¹å¤„ç†æµ‹è¯•** (60åˆ†é’Ÿ)
**P2.2.4: çŠ¶æ€åŒæ­¥æµ‹è¯•** (60åˆ†é’Ÿ)

**éªŒæ”¶æ ‡å‡†**:
- âœ… Token IDs æ­£ç¡®è½¬æ¢ä¸º Executor è¾“å…¥
- âœ… æ¨ç†æµç¨‹å®Œæ•´
- âœ… æ‰¹é‡æ¨ç†æ­£ç¡®
- âœ… çŠ¶æ€ä¸€è‡´æ€§

---

### P2.3: Executor + Backend é›†æˆ (4å°æ—¶)

#### æµ‹è¯•é‡ç‚¹
- Executor â†’ LibTorch æ¨ç†æµç¨‹
- Tensor ä¼ é€’
- å†…å­˜ç®¡ç†
- é”™è¯¯æ¢å¤

#### å…³é”®æµ‹è¯•ç”¨ä¾‹

**P2.3.1: æ¨ç†æµç¨‹æµ‹è¯•** (60åˆ†é’Ÿ)
```cpp
TEST(ExecutorBackendIntegration, ForwardPass) {
  ModelExecutor executor;
  executor.initialize(defaultConfig());
  
  LibTorchBackend backend;
  backend.loadModel("${CLLM_TEST_MODEL_PATH}");
  
  executor.setBackend(&backend);
  
  std::vector<int> input_ids = {1, 100, 200, 300, 2};
  auto output = executor.forward(input_ids);
  
  EXPECT_FALSE(output.empty());
  EXPECT_EQ(output.size(), input_ids.size());
}
```

**P2.3.2: å†…å­˜ç®¡ç†æµ‹è¯•** (60åˆ†é’Ÿ)
```cpp
TEST(ExecutorBackendIntegration, MemoryManagement) {
  ModelExecutor executor;
  executor.initialize(defaultConfig());
  
  LibTorchBackend backend;
  backend.loadModel("${CLLM_TEST_MODEL_PATH}");
  executor.setBackend(&backend);
  
  size_t initial_memory = backend.getMemoryUsage();
  
  // æ‰§è¡Œå¤šæ¬¡æ¨ç†
  for (int i = 0; i < 100; ++i) {
    std::vector<int> input_ids = {1, 100 + i, 2};
    executor.forward(input_ids);
  }
  
  size_t final_memory = backend.getMemoryUsage();
  
  // å†…å­˜å¢é•¿åº”è¯¥æœ‰é™
  EXPECT_LT(final_memory - initial_memory, 100 * 1024 * 1024); // < 100MB
}
```

**P2.3.3: æ€§èƒ½æµ‹è¯•** (60åˆ†é’Ÿ)
**P2.3.4: é”™è¯¯æ¢å¤æµ‹è¯•** (60åˆ†é’Ÿ)

**éªŒæ”¶æ ‡å‡†**:
- âœ… æ¨ç†æµç¨‹æ­£ç¡®
- âœ… å†…å­˜ä½¿ç”¨åˆç†
- âœ… ååé‡è¾¾æ ‡ï¼ˆ> 10 tokens/sï¼‰
- âœ… é”™è¯¯æ­£ç¡®æ¢å¤

---

### P2.4: Backend + Qwen3 é›†æˆ (3å°æ—¶)

#### æµ‹è¯•é‡ç‚¹
- Qwen3 æ¨¡å‹åŠ è½½åˆ° LibTorch
- æ¨ç†æ­£ç¡®æ€§éªŒè¯
- æ€§èƒ½æµ‹è¯•
- é•¿æ—¶é—´ç¨³å®šæ€§

#### å…³é”®æµ‹è¯•ç”¨ä¾‹

**P2.4.1: æ¨¡å‹åŠ è½½é›†æˆ** (45åˆ†é’Ÿ)
```cpp
TEST(BackendQwen3Integration, ModelLoading) {
  LibTorchBackend backend;
  
  bool success = backend.loadModel("${CLLM_TEST_MODEL_PATH}/qwen3.pt");
  
  EXPECT_TRUE(success);
  EXPECT_TRUE(backend.isModelLoaded());
  EXPECT_EQ(backend.getModelName(), "Qwen3");
}
```

**P2.4.2: æ¨ç†æ­£ç¡®æ€§éªŒè¯** (45åˆ†é’Ÿ)
```cpp
TEST(BackendQwen3Integration, InferenceCorrectness) {
  LibTorchBackend backend;
  backend.loadModel("${CLLM_TEST_MODEL_PATH}/qwen3.pt");
  
  // ä½¿ç”¨å·²çŸ¥è¾“å…¥è¾“å‡ºå¯¹
  torch::Tensor input = torch::tensor({{1, 100, 200, 2}});
  auto output = backend.forward(input);
  
  EXPECT_EQ(output.sizes()[0], 1); // batch size
  EXPECT_EQ(output.sizes()[1], 4); // sequence length
  EXPECT_GT(output.sizes()[2], 0); // vocab size
}
```

**P2.4.3: æ€§èƒ½æµ‹è¯•** (45åˆ†é’Ÿ)
```cpp
TEST(BackendQwen3Integration, Performance) {
  LibTorchBackend backend;
  backend.loadModel("${CLLM_TEST_MODEL_PATH}/qwen3.pt");
  
  torch::Tensor input = torch::randint(0, 32000, {1, 100});
  
  auto start = std::chrono::high_resolution_clock::now();
  auto output = backend.forward(input);
  auto end = std::chrono::high_resolution_clock::now();
  
  auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
  
  // æ¨ç†å»¶è¿Ÿåº”è¯¥åˆç†
  EXPECT_LT(duration.count(), 1000); // < 1ç§’
}
```

**P2.4.4: ç¨³å®šæ€§æµ‹è¯•ï¼ˆé•¿æ—¶é—´è¿è¡Œï¼‰** (45åˆ†é’Ÿ)
```cpp
TEST(BackendQwen3Integration, LongRunningStability) {
  LibTorchBackend backend;
  backend.loadModel("${CLLM_TEST_MODEL_PATH}/qwen3.pt");
  
  // è¿è¡Œ1000æ¬¡æ¨ç†
  for (int i = 0; i < 1000; ++i) {
    torch::Tensor input = torch::randint(0, 32000, {1, 50});
    auto output = backend.forward(input);
    
    EXPECT_FALSE(output.numel() == 0);
    
    if (i % 100 == 0) {
      LOG(INFO) << "Progress: " << i << "/1000";
    }
  }
  
  // æ£€æŸ¥å†…å­˜æ³„æ¼
  size_t final_memory = backend.getMemoryUsage();
  EXPECT_LT(final_memory, 10 * 1024 * 1024 * 1024); // < 10GB
}
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… Qwen3 æ¨¡å‹æ­£ç¡®åŠ è½½
- âœ… æ¨ç†è¾“å‡ºæ­£ç¡®
- âœ… æ€§èƒ½è¾¾æ ‡
- âœ… é•¿æ—¶é—´è¿è¡Œç¨³å®šï¼ˆæ— å†…å­˜æ³„æ¼ï¼‰

---

## âœ… æ€»ä½“éªŒæ”¶æ ‡å‡†

### å¿…é¡»å®Œæˆ

- [ ] P2.1: HTTP + Tokenizer é›†æˆé€šè¿‡
- [ ] P2.2: Tokenizer + Executor é›†æˆé€šè¿‡
- [ ] P2.3: Executor + Backend é›†æˆé€šè¿‡
- [ ] P2.4: Backend + Qwen3 é›†æˆé€šè¿‡

### è´¨é‡æŒ‡æ ‡

- [ ] é›†æˆæµ‹è¯•è¦†ç›–ç‡ > 70%
- [ ] æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹é€šè¿‡ç‡ = 100%
- [ ] æ•°æ®æµæ— ä¸¢å¤±
- [ ] é”™è¯¯ä¼ æ’­æ­£ç¡®

---

## ğŸ“Š æ‰§è¡ŒæŠ¥å‘Š

**æ‰§è¡Œæ—¶é—´**: ________

**å®Œæˆæƒ…å†µ**:
- P2.1: â˜ å®Œæˆ / â˜ å¤±è´¥
- P2.2: â˜ å®Œæˆ / â˜ å¤±è´¥
- P2.3: â˜ å®Œæˆ / â˜ å¤±è´¥
- P2.4: â˜ å®Œæˆ / â˜ å¤±è´¥

**æ€»ä½“çŠ¶æ€**: â˜ æˆåŠŸ / â˜ éƒ¨åˆ†æˆåŠŸ / â˜ å¤±è´¥

---

## ğŸ”„ ä¸‹ä¸€æ­¥

Phase 2 å®Œæˆåï¼Œé€šçŸ¥ Agent-3 å¯åŠ¨ Phase 3:

```bash
touch /tmp/cllm_test_locks/phase2.done
echo "âœ… Phase 2 å®Œæˆï¼ŒAgent-3 å¯ä»¥å¯åŠ¨ Phase 3"
```
