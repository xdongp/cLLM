# Phase 3: å­ç³»ç»Ÿæµ‹è¯•é˜¶æ®µ æ‰§è¡Œè®¡åˆ’

**è´Ÿè´£Agent**: Agent-3  
**é¢„è®¡è€—æ—¶**: 15å°æ—¶  
**ä¾èµ–**: Phase 2 å®Œæˆ  
**æ‰§è¡Œæ—¶é—´**: T+29h ~ T+44h  

---

## ğŸ“‹ é˜¶æ®µç›®æ ‡

éªŒè¯å­ç³»ç»Ÿçº§åˆ«çš„åŠŸèƒ½é›†æˆï¼Œæµ‹è¯•å®Œæ•´çš„åŠŸèƒ½æµç¨‹å’Œå­ç³»ç»Ÿæ€§èƒ½ã€‚

---

## ğŸ“Š ä»»åŠ¡æ¸…å•

| å­é˜¶æ®µ | ä»»åŠ¡æ•° | è€—æ—¶ | ä¾èµ– | çŠ¶æ€ |
|--------|--------|------|------|------|
| P3.1: å‰ç«¯å­ç³»ç»Ÿï¼ˆHTTP + Tokenizerï¼‰ | 4 | 4h | P2.1 | â³ å¾…æ‰§è¡Œ |
| P3.2: æ¨ç†å­ç³»ç»Ÿï¼ˆExecutor + Backend + Qwen3ï¼‰ | 4 | 5h | P2.3, P2.4 | â³ å¾…æ‰§è¡Œ |
| P3.3: E2Eå­ç³»ç»Ÿï¼ˆTokenizer â†’ Executor â†’ Backendï¼‰ | 4 | 6h | P2.2, P3.2 | â³ å¾…æ‰§è¡Œ |

**æ€»è®¡**: 12ä¸ªä»»åŠ¡ï¼Œ15å°æ—¶

---

## ğŸ“ è¯¦ç»†ä»»åŠ¡è¯´æ˜

### P3.1: å‰ç«¯å­ç³»ç»Ÿæµ‹è¯•ï¼ˆHTTP + Tokenizerï¼‰ (4å°æ—¶)

#### æµ‹è¯•é‡ç‚¹
- å®Œæ•´çš„ HTTP è¯·æ±‚ â†’ å“åº”æµç¨‹
- å¹¶å‘å¤„ç†èƒ½åŠ›
- æ€§èƒ½æŒ‡æ ‡ï¼ˆå»¶è¿Ÿã€ååé‡ï¼‰
- å®¹é”™èƒ½åŠ›

#### ä»»åŠ¡åˆ—è¡¨

**P3.1.1: å®Œæ•´æµç¨‹æµ‹è¯•** (60åˆ†é’Ÿ)
```cpp
TEST(FrontendSubsystem, CompleteFlow) {
  // å¯åŠ¨ HTTP Server
  HTTPServer server("0.0.0.0", 8080);
  
  // åŠ è½½ Tokenizer
  HFTokenizer tokenizer;
  tokenizer.load("${CLLM_TEST_MODEL_PATH}");
  server.registerTokenizer(&tokenizer);
  
  server.start();
  
  // å‘é€HTTPè¯·æ±‚
  HTTPClient client;
  json request_body = {
    {"text", "Hello, world! This is a test."}
  };
  
  auto response = client.post("http://localhost:8080/v1/tokenize", request_body);
  
  EXPECT_EQ(response.status_code, 200);
  
  auto result = json::parse(response.body);
  EXPECT_TRUE(result.contains("tokens"));
  EXPECT_GT(result["tokens"].size(), 0);
  
  server.stop();
}
```

**P3.1.2: å¹¶å‘æµ‹è¯•ï¼ˆ50 å¹¶å‘ï¼‰** (60åˆ†é’Ÿ)
```cpp
TEST(FrontendSubsystem, ConcurrentRequests) {
  HTTPServer server("0.0.0.0", 8080);
  HFTokenizer tokenizer;
  tokenizer.load("${CLLM_TEST_MODEL_PATH}");
  server.registerTokenizer(&tokenizer);
  server.start();
  
  const int NUM_THREADS = 50;
  const int REQUESTS_PER_THREAD = 10;
  
  std::vector<std::thread> threads;
  std::atomic<int> success_count{0};
  std::atomic<int> error_count{0};
  
  for (int i = 0; i < NUM_THREADS; ++i) {
    threads.emplace_back([&, i]() {
      HTTPClient client;
      for (int j = 0; j < REQUESTS_PER_THREAD; ++j) {
        json request = {{"text", "Test " + std::to_string(i * 100 + j)}};
        auto response = client.post("http://localhost:8080/v1/tokenize", request);
        
        if (response.status_code == 200) {
          success_count++;
        } else {
          error_count++;
        }
      }
    });
  }
  
  for (auto& thread : threads) {
    thread.join();
  }
  
  EXPECT_EQ(success_count, NUM_THREADS * REQUESTS_PER_THREAD);
  EXPECT_EQ(error_count, 0);
  
  server.stop();
}
```

**P3.1.3: æ€§èƒ½æµ‹è¯•ï¼ˆå»¶è¿Ÿ/ååé‡ï¼‰** (60åˆ†é’Ÿ)
```cpp
TEST(FrontendSubsystem, PerformanceMetrics) {
  HTTPServer server("0.0.0.0", 8080);
  HFTokenizer tokenizer;
  tokenizer.load("${CLLM_TEST_MODEL_PATH}");
  server.registerTokenizer(&tokenizer);
  server.start();
  
  HTTPClient client;
  const int NUM_REQUESTS = 100;
  std::vector<double> latencies;
  
  auto start_time = std::chrono::high_resolution_clock::now();
  
  for (int i = 0; i < NUM_REQUESTS; ++i) {
    auto req_start = std::chrono::high_resolution_clock::now();
    
    json request = {{"text", "Performance test text"}};
    auto response = client.post("http://localhost:8080/v1/tokenize", request);
    
    auto req_end = std::chrono::high_resolution_clock::now();
    double latency = std::chrono::duration_cast<std::chrono::milliseconds>(
      req_end - req_start
    ).count();
    latencies.push_back(latency);
  }
  
  auto end_time = std::chrono::high_resolution_clock::now();
  double total_time = std::chrono::duration_cast<std::chrono::seconds>(
    end_time - start_time
  ).count();
  
  // è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
  std::sort(latencies.begin(), latencies.end());
  double p50 = latencies[NUM_REQUESTS * 50 / 100];
  double p95 = latencies[NUM_REQUESTS * 95 / 100];
  double p99 = latencies[NUM_REQUESTS * 99 / 100];
  double throughput = NUM_REQUESTS / total_time;
  
  LOG(INFO) << "Performance Metrics:";
  LOG(INFO) << "  P50 Latency: " << p50 << " ms";
  LOG(INFO) << "  P95 Latency: " << p95 << " ms";
  LOG(INFO) << "  P99 Latency: " << p99 << " ms";
  LOG(INFO) << "  Throughput: " << throughput << " req/s";
  
  // éªŒè¯æ€§èƒ½ç›®æ ‡
  EXPECT_LT(p99, 100); // P99 < 100ms
  EXPECT_GT(throughput, 10); // > 10 req/s
  
  server.stop();
}
```

**P3.1.4: å®¹é”™æµ‹è¯•** (60åˆ†é’Ÿ)
```cpp
TEST(FrontendSubsystem, FaultTolerance) {
  HTTPServer server("0.0.0.0", 8080);
  HFTokenizer tokenizer;
  tokenizer.load("${CLLM_TEST_MODEL_PATH}");
  server.registerTokenizer(&tokenizer);
  server.start();
  
  HTTPClient client;
  
  // æµ‹è¯•1: æ— æ•ˆè¾“å…¥
  json invalid_request = {{"invalid_field", "test"}};
  auto response1 = client.post("http://localhost:8080/v1/tokenize", invalid_request);
  EXPECT_EQ(response1.status_code, 400); // Bad Request
  
  // æµ‹è¯•2: ç©ºè¾“å…¥
  json empty_request = {{"text", ""}};
  auto response2 = client.post("http://localhost:8080/v1/tokenize", empty_request);
  EXPECT_TRUE(response2.status_code == 200 || response2.status_code == 400);
  
  // æµ‹è¯•3: è¶…é•¿è¾“å…¥
  std::string long_text(10000, 'a');
  json long_request = {{"text", long_text}};
  auto response3 = client.post("http://localhost:8080/v1/tokenize", long_request);
  EXPECT_TRUE(response3.status_code == 200 || response3.status_code == 413); // Payload Too Large
  
  // æµ‹è¯•4: æ— æ•ˆè·¯å¾„
  auto response4 = client.post("http://localhost:8080/invalid/path", {});
  EXPECT_EQ(response4.status_code, 404);
  
  // ç³»ç»Ÿåº”è¯¥ä»ç„¶æ­£å¸¸å·¥ä½œ
  json valid_request = {{"text", "Test after errors"}};
  auto response5 = client.post("http://localhost:8080/v1/tokenize", valid_request);
  EXPECT_EQ(response5.status_code, 200);
  
  server.stop();
}
```

**éªŒæ”¶æ ‡å‡†**:
- âœ… å®Œæ•´è¯·æ±‚â†’å“åº”æµç¨‹æ­£å¸¸
- âœ… 50 å¹¶å‘æ— é”™è¯¯
- âœ… P99 å»¶è¿Ÿ < 100ms
- âœ… ååé‡ > 10 req/s
- âœ… å¼‚å¸¸æƒ…å†µæ­£ç¡®å¤„ç†

---

### P3.2: æ¨ç†å­ç³»ç»Ÿæµ‹è¯•ï¼ˆExecutor + Backend + Qwen3ï¼‰ (5å°æ—¶)

#### æµ‹è¯•é‡ç‚¹
- å®Œæ•´æ¨ç†æµç¨‹
- æ‰¹å¤„ç†æ€§èƒ½
- æ¨ç†ååé‡
- è¾“å‡ºè´¨é‡

#### ä»»åŠ¡åˆ—è¡¨

**P3.2.1: å®Œæ•´æ¨ç†æµç¨‹** (75åˆ†é’Ÿ)
```cpp
TEST(InferenceSubsystem, CompletePipeline) {
  // åˆå§‹åŒ–ç»„ä»¶
  ModelExecutor executor;
  executor.initialize(defaultConfig());
  
  LibTorchBackend backend;
  backend.loadModel("${CLLM_TEST_MODEL_PATH}/qwen3.pt");
  
  executor.setBackend(&backend);
  
  // æ‰§è¡Œæ¨ç†
  std::vector<int> input_ids = {1, 15339, 11, 1917, 0, 2}; // "Hello, world!"
  auto output_ids = executor.generate(input_ids, 50);
  
  EXPECT_GT(output_ids.size(), input_ids.size());
  EXPECT_LE(output_ids.size(), input_ids.size() + 50);
  
  // éªŒè¯è¾“å‡ºçš„åˆç†æ€§
  for (auto id : output_ids) {
    EXPECT_GE(id, 0);
    EXPECT_LT(id, 32000); // Qwen3 vocab size
  }
}
```

**P3.2.2: æ‰¹å¤„ç†æµ‹è¯•** (75åˆ†é’Ÿ)
**P3.2.3: æ€§èƒ½æµ‹è¯•** (75åˆ†é’Ÿ)
**P3.2.4: è¾“å‡ºè´¨é‡æµ‹è¯•** (75åˆ†é’Ÿ)

**éªŒæ”¶æ ‡å‡†**:
- âœ… æ¨ç†æµç¨‹å®Œæ•´æ­£ç¡®
- âœ… æ‰¹å¤„ç†æ€§èƒ½è¾¾æ ‡
- âœ… ååé‡ > 100 tokens/s
- âœ… è¾“å‡ºè´¨é‡è‰¯å¥½

---

### P3.3: E2E å­ç³»ç»Ÿæµ‹è¯•ï¼ˆTokenizer â†’ Executor â†’ Backendï¼‰ (6å°æ—¶)

#### æµ‹è¯•é‡ç‚¹
- æ–‡æœ¬åˆ°æ–‡æœ¬å®Œæ•´é“¾è·¯
- æµå¼è¾“å‡º
- å¤šè½®å¯¹è¯
- è¾¹ç•Œæµ‹è¯•ï¼ˆé•¿è¾“å…¥/è¾“å‡ºï¼‰

#### ä»»åŠ¡åˆ—è¡¨

**P3.3.1: æ–‡æœ¬åˆ°æ–‡æœ¬å®Œæ•´é“¾è·¯** (90åˆ†é’Ÿ)
```cpp
TEST(E2ESubsystem, TextToText) {
  // åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
  HFTokenizer tokenizer;
  tokenizer.load("${CLLM_TEST_MODEL_PATH}");
  
  ModelExecutor executor;
  executor.initialize(defaultConfig());
  
  LibTorchBackend backend;
  backend.loadModel("${CLLM_TEST_MODEL_PATH}/qwen3.pt");
  executor.setBackend(&backend);
  
  // å®Œæ•´æµç¨‹ï¼šæ–‡æœ¬ â†’ Token IDs â†’ æ¨ç† â†’ Token IDs â†’ æ–‡æœ¬
  std::string input_text = "What is artificial intelligence?";
  
  auto input_ids = tokenizer.encode(input_text);
  auto output_ids = executor.generate(input_ids, 100);
  auto output_text = tokenizer.decode(output_ids);
  
  EXPECT_FALSE(output_text.empty());
  EXPECT_GT(output_text.length(), input_text.length());
  
  LOG(INFO) << "Input: " << input_text;
  LOG(INFO) << "Output: " << output_text;
}
```

**P3.3.2: æµå¼è¾“å‡ºæµ‹è¯•** (90åˆ†é’Ÿ)
```cpp
TEST(E2ESubsystem, StreamingOutput) {
  HFTokenizer tokenizer;
  tokenizer.load("${CLLM_TEST_MODEL_PATH}");
  
  ModelExecutor executor;
  executor.initialize(defaultConfig());
  
  LibTorchBackend backend;
  backend.loadModel("${CLLM_TEST_MODEL_PATH}/qwen3.pt");
  executor.setBackend(&backend);
  
  std::string prompt = "Write a short story:";
  auto input_ids = tokenizer.encode(prompt);
  
  std::vector<std::string> chunks;
  
  executor.generateStreaming(input_ids, 100, [&](const std::vector<int>& new_ids) {
    std::string chunk = tokenizer.decode(new_ids);
    chunks.push_back(chunk);
    LOG(INFO) << "Chunk: " << chunk;
  });
  
  EXPECT_GT(chunks.size(), 0);
  
  // æ‹¼æ¥æ‰€æœ‰chunk
  std::string full_output;
  for (const auto& chunk : chunks) {
    full_output += chunk;
  }
  
  EXPECT_FALSE(full_output.empty());
}
```

**P3.3.3: å¤šè½®å¯¹è¯æµ‹è¯•** (90åˆ†é’Ÿ)
**P3.3.4: è¾¹ç•Œæµ‹è¯•ï¼ˆé•¿è¾“å…¥/è¾“å‡ºï¼‰** (90åˆ†é’Ÿ)

**éªŒæ”¶æ ‡å‡†**:
- âœ… ç«¯åˆ°ç«¯æµç¨‹æ­£å¸¸
- âœ… æµå¼è¾“å‡ºæ­£ç¡®
- âœ… å¤šè½®ä¸Šä¸‹æ–‡æ­£ç¡®
- âœ… è¾¹ç•Œæƒ…å†µå¤„ç†æ­£ç¡®ï¼ˆé•¿è¾“å…¥ä¸å´©æºƒï¼‰

---

## âœ… æ€»ä½“éªŒæ”¶æ ‡å‡†

### å¿…é¡»å®Œæˆ

- [ ] P3.1: å‰ç«¯å­ç³»ç»Ÿæµ‹è¯•é€šè¿‡
- [ ] P3.2: æ¨ç†å­ç³»ç»Ÿæµ‹è¯•é€šè¿‡
- [ ] P3.3: E2Eå­ç³»ç»Ÿæµ‹è¯•é€šè¿‡

### è´¨é‡æŒ‡æ ‡

- [ ] å­ç³»ç»Ÿæµ‹è¯•è¦†ç›–ç‡ > 80%
- [ ] æ€§èƒ½æŒ‡æ ‡è¾¾æ ‡
- [ ] å¹¶å‘æµ‹è¯•é€šè¿‡
- [ ] å®¹é”™èƒ½åŠ›è‰¯å¥½

---

## ğŸ“Š æ‰§è¡ŒæŠ¥å‘Š

**æ‰§è¡Œæ—¶é—´**: ________

**å®Œæˆæƒ…å†µ**:
- P3.1: â˜ å®Œæˆ / â˜ å¤±è´¥
- P3.2: â˜ å®Œæˆ / â˜ å¤±è´¥
- P3.3: â˜ å®Œæˆ / â˜ å¤±è´¥

**æ€»ä½“çŠ¶æ€**: â˜ æˆåŠŸ / â˜ éƒ¨åˆ†æˆåŠŸ / â˜ å¤±è´¥

---

## ğŸ”„ ä¸‹ä¸€æ­¥

Phase 3 å®Œæˆåï¼Œé€šçŸ¥ Agent-4 å¯åŠ¨ Phase 4:

```bash
touch /tmp/cllm_test_locks/phase3.done
echo "âœ… Phase 3 å®Œæˆï¼ŒAgent-4 å¯ä»¥å¯åŠ¨ Phase 4"
```
