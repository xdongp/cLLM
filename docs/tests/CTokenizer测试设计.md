# CTokenizeræµ‹è¯•è®¾è®¡

## 1. æµ‹è¯•ç›®æ ‡

### 1.1 æ ¸å¿ƒåŠŸèƒ½éªŒè¯
- éªŒè¯æ–‡æœ¬ç¼–ç åŠŸèƒ½ï¼ˆtext â†’ token IDsï¼‰
- éªŒè¯æ–‡æœ¬è§£ç åŠŸèƒ½ï¼ˆtoken IDs â†’ textï¼‰
- éªŒè¯ç‰¹æ®ŠTokenå¤„ç†ï¼ˆBOS/EOS/PAD/UNKç­‰ï¼‰
- éªŒè¯æ¨¡å‹ç±»å‹è‡ªåŠ¨æ£€æµ‹åŠŸèƒ½
- éªŒè¯FIMï¼ˆFill-in-the-Middleï¼‰å¤„ç†ï¼ˆé’ˆå¯¹Qwenæ¨¡å‹ï¼‰

### 1.2 æ€§èƒ½æŒ‡æ ‡éªŒè¯
- ç¼–ç é€Ÿåº¦ â‰¥ 50MB/s
- å†…å­˜å ç”¨ â‰¤ 50MB
- æ¨¡å‹åŠ è½½æ—¶é—´ â‰¤ 100ms
- æ”¯æŒå¹¶å‘è®¿é—®

### 1.3 å…¼å®¹æ€§éªŒè¯
- æ”¯æŒQwenç³»åˆ—æ¨¡å‹ï¼ˆQwenã€Qwen2ç­‰ï¼‰
- æ”¯æŒDeepSeekç³»åˆ—æ¨¡å‹ï¼ˆDeepSeek-LLMã€DeepSeek-Coderã€DeepSeek3ç­‰ï¼‰
- æ”¯æŒLlamaç³»åˆ—æ¨¡å‹
- å‘åå…¼å®¹ç°æœ‰SentencePieceæ¨¡å‹

## 2. æµ‹è¯•ç­–ç•¥

### 2.1 æµ‹è¯•å±‚çº§
- **å•å…ƒæµ‹è¯•**ï¼šéªŒè¯å„ä¸ªç»„ä»¶çš„ç‹¬ç«‹åŠŸèƒ½
- **é›†æˆæµ‹è¯•**ï¼šéªŒè¯ç»„ä»¶é—´çš„åä½œ
- **æ€§èƒ½æµ‹è¯•**ï¼šéªŒè¯æ€§èƒ½æŒ‡æ ‡æ˜¯å¦è¾¾æ ‡
- **å‹åŠ›æµ‹è¯•**ï¼šéªŒè¯åœ¨æç«¯æ¡ä»¶ä¸‹çš„ç¨³å®šæ€§
- **å›å½’æµ‹è¯•**ï¼šé˜²æ­¢å¼•å…¥æ–°çš„bug

### 2.2 æµ‹è¯•æ–¹æ³•
- **é»‘ç›’æµ‹è¯•**ï¼šéªŒè¯æ¥å£åŠŸèƒ½
- **ç™½ç›’æµ‹è¯•**ï¼šéªŒè¯å†…éƒ¨é€»è¾‘å®ç°
- **è¾¹ç•Œæµ‹è¯•**ï¼šéªŒè¯è¾¹ç•Œæ¡ä»¶å¤„ç†
- **å¼‚å¸¸æµ‹è¯•**ï¼šéªŒè¯é”™è¯¯å¤„ç†æœºåˆ¶

## 3. å•å…ƒæµ‹è¯•è®¾è®¡

### 3.1 CTokenizeræ¥å£æµ‹è¯•

#### 3.1.1 åŸºç¡€åŠŸèƒ½æµ‹è¯•
```cpp
TEST(CTokenizerTest, EncodeDecodeBasic) {
    // æµ‹è¯•åŸºæœ¬çš„ç¼–ç è§£ç åŠŸèƒ½
    std::unique_ptr<CTokenizer> tokenizer = std::make_unique<SentencePieceTokenizer>(ModelType::QWEN);
    ASSERT_TRUE(tokenizer->load("test_models/qwen/tokenizer.model"));
    
    std::string text = "Hello, world!";
    auto tokens = tokenizer->encode(text);
    ASSERT_FALSE(tokens.empty());
    
    std::string decoded = tokenizer->decode(tokens);
    EXPECT_EQ(decoded, text);
}

TEST(CTokenizerTest, VocabOperations) {
    // æµ‹è¯•è¯æ±‡è¡¨æ“ä½œ
    std::unique_ptr<CTokenizer> tokenizer = std::make_unique<SentencePieceTokenizer>(ModelType::QWEN);
    ASSERT_TRUE(tokenizer->load("test_models/qwen/tokenizer.model"));
    
    int vocabSize = tokenizer->getVocabSize();
    EXPECT_GT(vocabSize, 0);
    
    // æµ‹è¯•IDåˆ°Tokençš„è½¬æ¢
    std::string token = tokenizer->idToToken(100); // å‡è®¾ID 100å­˜åœ¨
    EXPECT_FALSE(token.empty());
    
    // æµ‹è¯•Tokenåˆ°IDçš„è½¬æ¢
    llama_token id = tokenizer->tokenToId(token);
    EXPECT_EQ(token, tokenizer->idToToken(id));
}
```

#### 3.1.2 ç‰¹æ®ŠTokenå¤„ç†æµ‹è¯•
```cpp
TEST(CTokenizerTest, SpecialTokens) {
    std::unique_ptr<CTokenizer> tokenizer = std::make_unique<SentencePieceTokenizer>(ModelType::QWEN);
    ASSERT_TRUE(tokenizer->load("test_models/qwen/tokenizer.model"));
    
    // æµ‹è¯•ç‰¹æ®ŠToken
    llama_token bosId = tokenizer->getBosId();
    llama_token eosId = tokenizer->getEosId();
    llama_token padId = tokenizer->getPadId();
    
    EXPECT_GT(bosId, 0);
    EXPECT_GT(eosId, 0);
    EXPECT_GE(padId, 0); // padIdå¯èƒ½ä¸º-1ï¼ˆæœªè®¾ç½®ï¼‰
    
    // æµ‹è¯•å¸¦ç‰¹æ®ŠTokençš„ç¼–ç 
    std::string text = "Hello";
    auto idsWithoutSpecial = tokenizer->encode(text, false);
    auto idsWithSpecial = tokenizer->encode(text, true);
    
    // å¸¦ç‰¹æ®ŠTokençš„åºåˆ—åº”è¯¥æ›´é•¿
    EXPECT_GE(idsWithSpecial.size(), idsWithoutSpecial.size());
}
```

#### 3.1.3 è¾¹ç•Œæ¡ä»¶æµ‹è¯•
```cpp
TEST(CTokenizerTest, BoundaryConditions) {
    std::unique_ptr<CTokenizer> tokenizer = std::make_unique<SentencePieceTokenizer>(ModelType::QWEN);
    ASSERT_TRUE(tokenizer->load("test_models/qwen/tokenizer.model"));
    
    // ç©ºå­—ç¬¦ä¸²æµ‹è¯•
    auto emptyIds = tokenizer->encode("");
    EXPECT_TRUE(emptyIds.empty() || emptyIds.size() == 2); // å¯èƒ½åŒ…å«BOS/EOS
    
    // å•å­—ç¬¦æµ‹è¯•
    auto singleCharIds = tokenizer->encode("A");
    EXPECT_FALSE(singleCharIds.empty());
    
    std::string singleDecoded = tokenizer->decode(singleCharIds);
    EXPECT_EQ(singleDecoded, "A");
    
    // ç‰¹æ®Šå­—ç¬¦æµ‹è¯•
    std::string specialText = "Hello, ä¸–ç•Œ! ğŸŒ";
    auto specialIds = tokenizer->encode(specialText);
    ASSERT_FALSE(specialIds.empty());
    
    std::string specialDecoded = tokenizer->decode(specialIds);
    EXPECT_EQ(specialDecoded, specialText);
}
```

### 3.2 QwenTokenizeræµ‹è¯•

#### 3.2.1 FIMå¤„ç†æµ‹è¯•
```cpp
TEST(QwenTokenizerTest, FimProcessing) {
    QwenTokenizer tokenizer;
    ASSERT_TRUE(tokenizer.load("test_models/qwen/tokenizer.model"));
    
    // æµ‹è¯•FIMå¤„ç†
    std::string text = "<|fim_pre|>def hello():<|fim_suf|>    return 'world'<|fim_end|>";
    auto ids = tokenizer.encode(text);
    ASSERT_FALSE(ids.empty());
    
    std::string decoded = tokenizer.decode(ids);
    EXPECT_EQ(decoded, text);
}

TEST(QwenTokenizerTest, FimDetection) {
    QwenTokenizer tokenizer;
    
    // æµ‹è¯•FIMæ ‡è®°æ£€æµ‹
    EXPECT_TRUE(tokenizer.needsFimProcessing("<|fim_begin|>test<|fim_end|>"));
    EXPECT_TRUE(tokenizer.needsFimProcessing("test `` code ``"));
    EXPECT_FALSE(tokenizer.needsFimProcessing("regular text"));
}
```

#### 3.2.2 Qwenç‰¹å®šåŠŸèƒ½æµ‹è¯•
```cpp
TEST(QwenTokenizerTest, QwenSpecificFeatures) {
    QwenTokenizer tokenizer;
    ASSERT_TRUE(tokenizer.load("test_models/qwen/tokenizer.model"));
    
    // æµ‹è¯•Qwenç‰¹æœ‰çš„é¢„å¤„ç†
    std::string code = "def function():\n    pass";
    auto tokens = tokenizer.encode(code);
    EXPECT_FALSE(tokens.empty());
    
    std::string decoded = tokenizer.decode(tokens);
    EXPECT_EQ(decoded, code);
}
```

### 3.3 DeepSeekTokenizeræµ‹è¯•

#### 3.3.1 é¢„å¤„ç†æµ‹è¯•
```cpp
TEST(DeepSeekTokenizerTest, Preprocessing) {
    DeepSeekTokenizer tokenizer(ModelType::DEEPSEEK_CODER);
    ASSERT_TRUE(tokenizer.load("test_models/deepseek-coder/tokenizer.model"));
    
    std::string code = "class MyClass:\n    def method(self):\n        return True";
    auto tokens = tokenizer.encode(code);
    ASSERT_FALSE(tokens.empty());
    
    std::string decoded = tokenizer.decode(tokens);
    EXPECT_EQ(decoded, code);
}

TEST(DeepSeekTokenizerTest, ModelTypeSpecific) {
    // æµ‹è¯•ä¸åŒDeepSeekæ¨¡å‹ç±»å‹çš„å¤„ç†
    {
        DeepSeekTokenizer llmTokenizer(ModelType::DEEPSEEK_LLM);
        ASSERT_TRUE(llmTokenizer.load("test_models/deepseek-llm/tokenizer.model"));
        std::string text = "Hello world";
        auto tokens = llmTokenizer.encode(text);
        EXPECT_FALSE(tokens.empty());
    }
    
    {
        DeepSeekTokenizer coderTokenizer(ModelType::DEEPSEEK_CODER);
        ASSERT_TRUE(coderTokenizer.load("test_models/deepseek-coder/tokenizer.model"));
        std::string code = "def hello(): pass";
        auto tokens = coderTokenizer.encode(code);
        EXPECT_FALSE(tokens.empty());
    }
}
```

### 3.4 ModelDetectoræµ‹è¯•

#### 3.4.1 æ¨¡å‹ç±»å‹æ£€æµ‹æµ‹è¯•
```cpp
TEST(ModelDetectorTest, AutoDetection) {
    // æµ‹è¯•æ¨¡å‹è‡ªåŠ¨æ£€æµ‹åŠŸèƒ½
    ModelType type = ModelDetector::detectModelType("test_models/qwen/config.json");
    EXPECT_EQ(type, ModelType::QWEN);
    
    type = ModelDetector::detectModelType("test_models/deepseek-coder/config.json");
    EXPECT_EQ(type, ModelType::DEEPSEEK_CODER);
    
    type = ModelDetector::detectModelType("test_models/deepseek-llm/config.json");
    EXPECT_EQ(type, ModelType::DEEPSEEK_LLM);
}

TEST(ModelDetectorTest, InvalidConfig) {
    // æµ‹è¯•æ— æ•ˆé…ç½®æ–‡ä»¶çš„å¤„ç†
    ModelType type = ModelDetector::detectModelType("nonexistent/config.json");
    EXPECT_EQ(type, ModelType::SPM); // åº”è¯¥è¿”å›é»˜è®¤ç±»å‹
}
```

### 3.5 TokenizerManageræµ‹è¯•

#### 3.5.1 åˆ†è¯å™¨è·å–æµ‹è¯•
```cpp
TEST(TokenizerManagerTest, GetTokenizer) {
    TokenizerManager manager;
    
    auto qwenTokenizer = manager.getTokenizer("qwen");
    ASSERT_NE(qwenTokenizer, nullptr);
    EXPECT_EQ(qwenTokenizer->getModelType(), ModelType::QWEN);
    
    auto deepseekTokenizer = manager.getTokenizer("deepseek-coder");
    ASSERT_NE(deepseekTokenizer, nullptr);
    EXPECT_EQ(deepseekTokenizer->getModelType(), ModelType::DEEPSEEK_CODER);
    
    auto llamaTokenizer = manager.getTokenizer("llama");
    ASSERT_NE(llamaTokenizer, nullptr);
    EXPECT_EQ(llamaTokenizer->getModelType(), ModelType::LLAMA);
}

TEST(TokenizerManagerTest, CacheBehavior) {
    // æµ‹è¯•åˆ†è¯å™¨ç¼“å­˜è¡Œä¸º
    TokenizerManager manager;
    
    auto tokenizer1 = manager.getTokenizer("qwen");
    auto tokenizer2 = manager.getTokenizer("qwen");
    
    // åº”è¯¥è¿”å›åŒä¸€ä¸ªå®ä¾‹ï¼ˆå¦‚æœæ˜¯å•ä¾‹å®ç°ï¼‰
    EXPECT_EQ(tokenizer1, tokenizer2);
}
```

## 4. é›†æˆæµ‹è¯•è®¾è®¡

### 4.1 ç«¯åˆ°ç«¯æµ‹è¯•
```cpp
TEST(IntegrationTest, EndToEnd) {
    // æ¨¡æ‹Ÿå®Œæ•´çš„å·¥ä½œæµ
    TokenizerManager manager;
    auto tokenizer = manager.getTokenizer("qwen");
    ASSERT_NE(tokenizer, nullptr);
    
    std::string input = "This is a test sentence for end-to-end validation.";
    auto tokens = tokenizer->encode(input);
    ASSERT_FALSE(tokens.empty());
    
    std::string output = tokenizer->decode(tokens);
    EXPECT_EQ(input, output);
    
    // éªŒè¯è¯æ±‡è¡¨å¤§å°çš„ä¸€è‡´æ€§
    int vocabSize = tokenizer->getVocabSize();
    EXPECT_GT(vocabSize, 1000); // åˆç†çš„è¯æ±‡è¡¨å¤§å°
}

TEST(IntegrationTest, MultiModelSupport) {
    TokenizerManager manager;
    
    // æµ‹è¯•ä¸åŒæ¨¡å‹ç±»å‹çš„åˆ†è¯å™¨
    std::vector<std::string> modelTypes = {"qwen", "deepseek-llm", "deepseek-coder", "llama"};
    
    for (const auto& modelType : modelTypes) {
        auto tokenizer = manager.getTokenizer(modelType);
        ASSERT_NE(tokenizer, nullptr) << "Failed to get tokenizer for " << modelType;
        
        std::string testText = "Test text for " + modelType;
        auto tokens = tokenizer->encode(testText);
        ASSERT_FALSE(tokens.empty()) << "Encoding failed for " << modelType;
        
        std::string decoded = tokenizer->decode(tokens);
        EXPECT_EQ(decoded, testText) << "Decoding mismatch for " << modelType;
    }
}
```

### 4.2 æ‰¹å¤„ç†æµ‹è¯•
```cpp
TEST(BatchTokenizerTest, BatchEncodeDecode) {
    TokenizerManager manager;
    auto tokenizer = manager.getTokenizer("qwen");
    ASSERT_NE(tokenizer, nullptr);
    
    std::vector<std::string> texts = {
        "Hello, world!",
        "This is a test sentence.",
        "Another test with numbers: 12345",
        "Mixed content: Hello ä¸–ç•Œ ğŸŒ"
    };
    
    // æ‰¹é‡ç¼–ç æµ‹è¯•
    for (const auto& text : texts) {
        auto tokens = tokenizer->encode(text);
        ASSERT_FALSE(tokens.empty());
        
        std::string decoded = tokenizer->decode(tokens);
        EXPECT_EQ(decoded, text);
    }
}
```

## 5. æ€§èƒ½æµ‹è¯•è®¾è®¡

### 5.1 ç¼–ç é€Ÿåº¦æµ‹è¯•
```cpp
TEST(PerformanceTest, EncodeSpeed) {
    TokenizerManager manager;
    auto tokenizer = manager.getTokenizer("qwen");
    ASSERT_NE(tokenizer, nullptr);
    ASSERT_TRUE(tokenizer->load("test_models/qwen/tokenizer.model"));
    
    std::string longText;
    for (int i = 0; i < 1000; ++i) {
        longText += "This is a test sentence for performance evaluation. ";
    }
    
    auto start = std::chrono::high_resolution_clock::now();
    auto tokens = tokenizer->encode(longText);
    auto end = std::chrono::high_resolution_clock::now();
    
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
    EXPECT_LT(duration.count(), 1000); // åº”è¯¥åœ¨1ç§’å†…å®Œæˆ
    
    EXPECT_FALSE(tokens.empty());
    
    // è®¡ç®—ç¼–ç é€Ÿåº¦ (å­—ç¬¦/ç§’)
    double speed = (double)longText.length() / (duration.count() / 1000.0);
    EXPECT_GT(speed, 50000); // è‡³å°‘50KB/s
}

TEST(PerformanceTest, DecodeSpeed) {
    TokenizerManager manager;
    auto tokenizer = manager.getTokenizer("qwen");
    ASSERT_NE(tokenizer, nullptr);
    ASSERT_TRUE(tokenizer->load("test_models/qwen/tokenizer.model"));
    
    std::string text = "Performance test text. ";
    auto tokens = tokenizer->encode(text);
    
    // é‡å¤å¤šæ¬¡ä»¥è·å¾—æ›´å¥½çš„æµ‹é‡ç»“æœ
    std::vector<std::vector<llama_token>> batchTokens;
    for (int i = 0; i < 1000; ++i) {
        batchTokens.push_back(tokens);
    }
    
    auto start = std::chrono::high_resolution_clock::now();
    for (const auto& tokenList : batchTokens) {
        std::string decoded = tokenizer->decode(tokenList);
    }
    auto end = std::chrono::high_resolution_clock::now();
    
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
    EXPECT_LT(duration.count(), 1000); // åº”è¯¥åœ¨1ç§’å†…å®Œæˆ
}

TEST(PerformanceTest, MemoryUsage) {
    TokenizerManager manager;
    auto tokenizer = manager.getTokenizer("qwen");
    ASSERT_NE(tokenizer, nullptr);
    ASSERT_TRUE(tokenizer->load("test_models/qwen/tokenizer.model"));
    
    // æ£€æŸ¥åˆå§‹å†…å­˜ä½¿ç”¨
    size_t initialMemory = getCurrentMemoryUsage();
    
    // æ‰§è¡Œå¤šæ¬¡ç¼–ç /è§£ç æ“ä½œ
    for (int i = 0; i < 10000; ++i) {
        std::string text = "Test " + std::to_string(i);
        auto tokens = tokenizer->encode(text);
        std::string decoded = tokenizer->decode(tokens);
    }
    
    size_t finalMemory = getCurrentMemoryUsage();
    
    // å†…å­˜å¢é•¿ä¸åº”è¶…è¿‡é˜ˆå€¼ï¼ˆä¾‹å¦‚10MBï¼‰
    EXPECT_LT(finalMemory - initialMemory, 10 * 1024 * 1024);
}
```

## 6. éªŒè¯æµ‹è¯•è®¾è®¡

### 6.1 ç²¾åº¦éªŒè¯
```cpp
TEST(ValidationTest, CrossPlatformConsistency) {
    // éªŒè¯åœ¨ä¸åŒå¹³å°ä¸Šäº§ç”Ÿçš„ç»“æœä¸€è‡´æ€§
    TokenizerManager manager;
    auto tokenizer = manager.getTokenizer("qwen");
    ASSERT_NE(tokenizer, nullptr);
    ASSERT_TRUE(tokenizer->load("test_models/qwen/tokenizer.model"));
    
    std::vector<std::string> testCases = {
        "Hello, world!",
        "æµ‹è¯•ä¸­æ–‡åˆ†è¯",
        "Test with numbers: 123456789",
        "Mixed: Hello ä¸–ç•Œ ğŸŒ emoji",
        "Special chars: !@#$%^&*()",
        "Long text with multiple sentences. This is sentence two. And this is three."
    };
    
    for (const auto& testCase : testCases) {
        auto tokens = tokenizer->encode(testCase);
        std::string decoded = tokenizer->decode(tokens);
        
        EXPECT_EQ(testCase, decoded) << "Mismatch for test case: " << testCase;
        
        // éªŒè¯è¯æ±‡è¡¨å¤§å°çš„ä¸€è‡´æ€§
        int vocabSize = tokenizer->getVocabSize();
        EXPECT_GT(vocabSize, 0);
    }
}

TEST(ValidationTest, ModelSpecificFeatures) {
    // éªŒè¯ç‰¹å®šæ¨¡å‹çš„ç‰¹å¾
    {
        // Qwenæ¨¡å‹ç‰¹æœ‰åŠŸèƒ½æµ‹è¯•
        QwenTokenizer qwenTokenizer;
        ASSERT_TRUE(qwenTokenizer.load("test_models/qwen/tokenizer.model"));
        
        // æµ‹è¯•Qwenç‰¹æœ‰çš„FIMå¤„ç†
        std::string code = "def function():\n    pass";
        auto tokens = qwenTokenizer.encode(code);
        EXPECT_FALSE(tokens.empty());
        
        std::string decoded = qwenTokenizer.decode(tokens);
        EXPECT_EQ(decoded, code);
    }
    
    {
        // DeepSeekæ¨¡å‹ç‰¹æœ‰åŠŸèƒ½æµ‹è¯•
        DeepSeekTokenizer deepseekTokenizer(ModelType::DEEPSEEK_CODER);
        ASSERT_TRUE(deepseekTokenizer.load("test_models/deepseek-coder/tokenizer.model"));
        
        // æµ‹è¯•DeepSeekç‰¹å®šçš„é¢„å¤„ç†
        std::string code = "class MyClass:\n    def method(self):\n        return True";
        auto tokens = deepseekTokenizer.encode(code);
        EXPECT_FALSE(tokens.empty());
        
        std::string decoded = deepseekTokenizer.decode(tokens);
        EXPECT_EQ(decoded, code);
    }
}
```

### 6.2 å›å½’æµ‹è¯•
```cpp
TEST(RegressionTest, KnownIssues) {
    // é’ˆå¯¹å·²çŸ¥é—®é¢˜çš„å›å½’æµ‹è¯•
    TokenizerManager manager;
    auto tokenizer = manager.getTokenizer("qwen");
    ASSERT_NE(tokenizer, nullptr);
    ASSERT_TRUE(tokenizer->load("test_models/qwen/tokenizer.model"));
    
    // æµ‹è¯•å¯èƒ½å¯¼è‡´é—®é¢˜çš„ç‰¹å®šè¾“å…¥
    std::vector<std::string> problematicInputs = {
        "", // ç©ºå­—ç¬¦ä¸²
        " ", // å•ç©ºæ ¼
        "\n", // å•æ¢è¡Œ
        "\t", // å•åˆ¶è¡¨ç¬¦
        "\r\n", // Windowsæ¢è¡Œ
        std::string(1000, 'A'), // é•¿é‡å¤å­—ç¬¦ä¸²
        "A" + std::string(1000, 'B') + "C", // é•¿ä¸­é—´å­—ç¬¦ä¸²
        "!@#$%^&*()_+-=[]{}|;:,.<>?", // æ‰€æœ‰ç‰¹æ®Šå­—ç¬¦
        "Î±Î²Î³Î´ÎµÎ¶Î·Î¸Î¹ÎºÎ»Î¼Î½Î¾Î¿Ï€ÏÏƒÏ„Ï…Ï†Ï‡ÏˆÏ‰", // å¸Œè…Šå­—æ¯
        "ã‚ã„ã†ãˆãŠã‹ããã‘ã“", // æ—¥æ–‡å¹³å‡å
        "í•œêµ­ì–´ í…ŒìŠ¤íŠ¸", // éŸ©æ–‡
        "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©", // é˜¿æ‹‰ä¼¯æ–‡
        "Ğ ÑƒÑÑĞºĞ¸Ğ¹", // ä¿„æ–‡
        " ğŸŒ âœ¨ ğŸš€ " // è¡¨æƒ…ç¬¦å·
    };
    
    for (const auto& input : problematicInputs) {
        try {
            auto tokens = tokenizer->encode(input);
            std::string decoded = tokenizer->decode(tokens);
            
            // å¯¹äºå¤§å¤šæ•°è¾“å…¥ï¼Œç¼–ç åå†è§£ç åº”è¯¥å¾—åˆ°ç›¸åŒçš„ç»“æœ
            EXPECT_EQ(decoded, input) << "Regression detected for input: " << input;
        } catch (const std::exception& e) {
            ADD_FAILURE() << "Exception thrown for input '" << input << "': " << e.what();
        }
    }
}
```

## 7. æµ‹è¯•æ•°æ®å‡†å¤‡

### 7.1 æµ‹è¯•æ¨¡å‹æ–‡ä»¶
- `test_models/qwen/` - Qwenæ¨¡å‹æµ‹è¯•æ–‡ä»¶
- `test_models/deepseek-coder/` - DeepSeek Coderæ¨¡å‹æµ‹è¯•æ–‡ä»¶
- `test_models/deepseek-llm/` - DeepSeek LLMæ¨¡å‹æµ‹è¯•æ–‡ä»¶
- `test_models/llama/` - Llamaæ¨¡å‹æµ‹è¯•æ–‡ä»¶

### 7.2 æµ‹è¯•é…ç½®æ–‡ä»¶
- `tokenizer.model` - SentencePieceæ¨¡å‹æ–‡ä»¶
- `config.json` - æ¨¡å‹é…ç½®æ–‡ä»¶
- `tokenizer.json` - åˆ†è¯å™¨é…ç½®æ–‡ä»¶

## 8. æµ‹è¯•æ‰§è¡Œç­–ç•¥

### 8.1 æµ‹è¯•è¦†ç›–ç‡
- åŠŸèƒ½è¦†ç›–ç‡: ç¡®ä¿æ‰€æœ‰åŠŸèƒ½éƒ½ç»è¿‡æµ‹è¯•
- ä»£ç è¦†ç›–ç‡: ç›®æ ‡è¾¾åˆ°85%ä»¥ä¸Š
- æ•°æ®è¦†ç›–ç‡: æ¶µç›–å„ç§è¾“å…¥ç±»å‹å’Œè¾¹ç•Œæ¡ä»¶

### 8.2 è‡ªåŠ¨åŒ–æµ‹è¯•
```bash
# å®Œæ•´æµ‹è¯•å¥—ä»¶æ‰§è¡Œ
./bin/ctokenizer_tests --gtest_filter=* --verbose

# æ€§èƒ½æµ‹è¯•
./bin/ctokenizer_benchmark --model=qwen --text=performance_test.txt --iterations=1000

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
gcovr --html --html-details -o coverage.html
```

### 8.3 æµ‹è¯•ç¯å¢ƒ
- Linux/Windows/macOS å¤šå¹³å°æ”¯æŒ
- ä¸åŒæ¨¡å‹æ ¼å¼æµ‹è¯•
- å¤šçº¿ç¨‹å¹¶å‘æµ‹è¯•

## 9. é¢„æœŸç»“æœ

### 9.1 åŠŸèƒ½éªŒè¯
- æ‰€æœ‰ç¼–ç è§£ç åŠŸèƒ½æ­£å¸¸å·¥ä½œ
- ç‰¹æ®ŠTokenå¤„ç†æ­£ç¡®
- æ¨¡å‹ç±»å‹æ£€æµ‹å‡†ç¡®
- FIMå¤„ç†åŠŸèƒ½æ­£å¸¸

### 9.2 æ€§èƒ½éªŒè¯
- ç¼–ç é€Ÿåº¦æ»¡è¶³ â‰¥ 50MB/s è¦æ±‚
- å†…å­˜å ç”¨æ»¡è¶³ â‰¤ 50MB è¦æ±‚
- æ¨¡å‹åŠ è½½æ—¶é—´æ»¡è¶³ â‰¤ 100ms è¦æ±‚

### 9.3 å…¼å®¹æ€§éªŒè¯
- æ”¯æŒæ‰€æœ‰ç›®æ ‡æ¨¡å‹ç±»å‹
- ä¸ç°æœ‰ç³»ç»Ÿå…¼å®¹
- å‘åå…¼å®¹æ€§ä¿æŒ

## 10. é£é™©ä¸ç¼“è§£

### 10.1 æ½œåœ¨é£é™©
- æ¨¡å‹æ–‡ä»¶åŠ è½½å¤±è´¥
- å†…å­˜æ³„æ¼
- å¹¶å‘è®¿é—®é—®é¢˜
- æ€§èƒ½ä¸è¾¾æ ‡

### 10.2 ç¼“è§£æªæ–½
- å®Œå–„çš„é”™è¯¯å¤„ç†æœºåˆ¶
- ä¸¥æ ¼çš„å†…å­˜ç®¡ç†
- çº¿ç¨‹å®‰å…¨æµ‹è¯•
- æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–