# cLLM Linux ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»å¦‚ä½•åœ¨ Linux ç”Ÿäº§ç¯å¢ƒï¼ˆUbuntu/CentOSï¼‰ä¸Šéƒ¨ç½² cLLM æœåŠ¡ï¼Œæ”¯æŒ **GPU æ¨¡å¼** å’Œ **çº¯ CPU æ¨¡å¼**ã€‚

> **ç”Ÿäº§ç¯å¢ƒè¯´æ˜**ï¼š
> - æœ¬æŒ‡å—ä»…æ¶µç›– **GGUF æ¨¡å‹ + llama.cpp åç«¯** çš„éƒ¨ç½²ï¼Œè¿™æ˜¯å”¯ä¸€æ¨èçš„ç”Ÿäº§é…ç½®
> - ä½¿ç”¨ **GGUF å†…ç½® Tokenizer**ï¼Œæ— éœ€é¢å¤–çš„ tokenizer æ–‡ä»¶
> - ç¦ç”¨ tokenizers-cpp å’Œ LibTorchï¼Œç®€åŒ–ä¾èµ–å’Œéƒ¨ç½²æµç¨‹
> - å…¶ä»–åç«¯ï¼ˆKylinã€LibTorchï¼‰ä¸ºå®éªŒæ€§åŠŸèƒ½ï¼Œä»…ä¾›å¼€å‘æµ‹è¯•

## éƒ¨ç½²æ¨¡å¼é€‰æ‹©

| æ¨¡å¼ | é€‚ç”¨åœºæ™¯ | æ€§èƒ½ | ä¾èµ– |
|------|----------|------|------|
| **GPU æ¨¡å¼** | æœ‰ NVIDIA æ˜¾å¡ | é«˜ååã€ä½å»¶è¿Ÿ | CUDA + é©±åŠ¨ |
| **CPU æ¨¡å¼** | æ—  GPU æˆ–äº‘æœåŠ¡å™¨ | ä¸­ç­‰ï¼Œé€‚åˆå°æ¨¡å‹ | æ— ç‰¹æ®Šä¾èµ– |

## ç›®å½•

1. [ç³»ç»Ÿè¦æ±‚](#1-ç³»ç»Ÿè¦æ±‚)
2. [ç¯å¢ƒå‡†å¤‡](#2-ç¯å¢ƒå‡†å¤‡)
3. [ä¾èµ–å®‰è£…](#3-ä¾èµ–å®‰è£…)
4. [é¡¹ç›®ç¼–è¯‘](#4-é¡¹ç›®ç¼–è¯‘)
5. [æ¨¡å‹å‡†å¤‡](#5-æ¨¡å‹å‡†å¤‡)
6. [é…ç½®æ–‡ä»¶](#6-é…ç½®æ–‡ä»¶)
7. [æœåŠ¡éƒ¨ç½²](#7-æœåŠ¡éƒ¨ç½²)
8. [ç›‘æ§ä¸æ—¥å¿—](#8-ç›‘æ§ä¸æ—¥å¿—)
9. [æ€§èƒ½è°ƒä¼˜](#9-æ€§èƒ½è°ƒä¼˜)
10. [æ•…éšœæ’æŸ¥](#10-æ•…éšœæ’æŸ¥)

---

## 1. ç³»ç»Ÿè¦æ±‚

### 1.1 ç¡¬ä»¶è¦æ±‚

**GPU æ¨¡å¼**ï¼š
| ç»„ä»¶ | æœ€ä½é…ç½® | æ¨èé…ç½® |
|------|----------|----------|
| CPU | 4 æ ¸ | 8+ æ ¸ |
| å†…å­˜ | 16 GB | 32+ GB |
| GPU | NVIDIA GTX 1080 (8GB) | NVIDIA RTX 3090/4090 (24GB) |
| ç£ç›˜ | 50 GB SSD | 100+ GB NVMe SSD |

**CPU æ¨¡å¼**ï¼š
| ç»„ä»¶ | æœ€ä½é…ç½® | æ¨èé…ç½® |
|------|----------|----------|
| CPU | 8 æ ¸ | 16+ æ ¸ï¼ˆæ”¯æŒ AVX2ï¼‰ |
| å†…å­˜ | 16 GB | 32+ GB |
| GPU | ä¸éœ€è¦ | - |
| ç£ç›˜ | 50 GB SSD | 100+ GB NVMe SSD |

> **CPU æ¨¡å¼è¯´æ˜**ï¼šæ¨èä½¿ç”¨æ”¯æŒ AVX2/AVX-512 æŒ‡ä»¤é›†çš„ç°ä»£ CPUï¼ˆIntel Haswell+/AMD Zen+ï¼‰ä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚

### 1.2 è½¯ä»¶è¦æ±‚

**GPU æ¨¡å¼**ï¼š
| ç»„ä»¶ | ç‰ˆæœ¬è¦æ±‚ | è¯´æ˜ |
|------|----------|------|
| æ“ä½œç³»ç»Ÿ | Ubuntu 20.04/22.04 LTS æˆ– CentOS 7/8/Stream | |
| NVIDIA é©±åŠ¨ | >= 525.x | å¿…éœ€ |
| CUDA | >= 11.8ï¼ˆæ¨è 12.xï¼‰ | å¿…éœ€ |
| GCC | >= 9.0ï¼ˆæ¨è 11.xï¼‰ | å¿…éœ€ |
| CMake | >= 3.18 | å¿…éœ€ |

**CPU æ¨¡å¼**ï¼š
| ç»„ä»¶ | ç‰ˆæœ¬è¦æ±‚ | è¯´æ˜ |
|------|----------|------|
| æ“ä½œç³»ç»Ÿ | Ubuntu 20.04/22.04 LTS æˆ– CentOS 7/8/Stream | |
| GCC | >= 9.0ï¼ˆæ¨è 11.xï¼‰ | å¿…éœ€ |
| CMake | >= 3.18 | å¿…éœ€ |
| OpenBLAS | æœ€æ–°ç‰ˆ | æ¨èï¼ŒåŠ é€ŸçŸ©é˜µè¿ç®— |

### 1.3 GPU æ˜¾å­˜è¦æ±‚ï¼ˆGPU æ¨¡å¼ï¼‰

| æ¨¡å‹å¤§å° | æœ€ä½æ˜¾å­˜ | æ¨èæ˜¾å­˜ |
|----------|----------|----------|
| 0.5B-1B  | 4 GB     | 8 GB     |
| 3B-7B    | 8 GB     | 16 GB    |
| 13B-14B  | 16 GB    | 24 GB    |
| 32B+     | 24 GB    | 48+ GB   |

### 1.4 å†…å­˜è¦æ±‚ï¼ˆCPU æ¨¡å¼ï¼‰

| æ¨¡å‹å¤§å° | é‡åŒ–æ ¼å¼ | æœ€ä½å†…å­˜ | æ¨èå†…å­˜ |
|----------|----------|----------|----------|
| 0.5B-1B  | Q4_K_M   | 4 GB     | 8 GB     |
| 3B-7B    | Q4_K_M   | 8 GB     | 16 GB    |
| 7B-14B   | Q4_K_M   | 16 GB    | 32 GB    |
| 32B+     | Q4_K_M   | 32 GB    | 64+ GB   |

> **æç¤º**ï¼šCPU æ¨¡å¼ä¸‹æ¨èä½¿ç”¨ Q4_K_M æˆ– Q4_K_S é‡åŒ–æ¨¡å‹ä»¥å‡å°‘å†…å­˜å ç”¨ã€‚

---

## 2. ç¯å¢ƒå‡†å¤‡

### 2.1 Ubuntu 22.04

```bash
# æ›´æ–°ç³»ç»Ÿ
sudo apt update && sudo apt upgrade -y

# å®‰è£…åŸºç¡€å·¥å…·
sudo apt install -y build-essential git wget curl vim htop

# å®‰è£…ç¼–è¯‘ä¾èµ–
sudo apt install -y cmake pkg-config libssl-dev libcurl4-openssl-dev
```

### 2.2 CentOS 7/8

```bash
# CentOS 7 - å¯ç”¨ SCL è·å–æ–°ç‰ˆ GCC
sudo yum install -y centos-release-scl
sudo yum install -y devtoolset-11-gcc devtoolset-11-gcc-c++
scl enable devtoolset-11 bash

# CentOS 8/Stream
sudo dnf install -y gcc-toolset-11
scl enable gcc-toolset-11 bash

# é€šç”¨ä¾èµ–
sudo yum install -y git wget curl vim htop cmake3 openssl-devel
```

---

## 3. ä¾èµ–å®‰è£…

> **CPU æ¨¡å¼**ï¼šå¯è·³è¿‡ 3.1-3.3 èŠ‚ï¼ˆNVIDIA é©±åŠ¨ã€CUDAã€cuDNNï¼‰ï¼Œç›´æ¥åˆ° 3.4 èŠ‚ã€‚

### 3.1 NVIDIA é©±åŠ¨å®‰è£…ï¼ˆGPU æ¨¡å¼ï¼‰

#### Ubuntu

```bash
# æ–¹æ³• 1: ä½¿ç”¨ ubuntu-driversï¼ˆæ¨èï¼‰
sudo ubuntu-drivers autoinstall

# æ–¹æ³• 2: æ‰‹åŠ¨å®‰è£…æŒ‡å®šç‰ˆæœ¬
sudo apt install -y nvidia-driver-535

# é‡å¯
sudo reboot

# éªŒè¯å®‰è£…
nvidia-smi
```

#### CentOS

```bash
# ç¦ç”¨ nouveau é©±åŠ¨
sudo bash -c "echo 'blacklist nouveau' >> /etc/modprobe.d/blacklist.conf"
sudo bash -c "echo 'options nouveau modeset=0' >> /etc/modprobe.d/blacklist.conf"
sudo dracut --force
sudo reboot

# å®‰è£…é©±åŠ¨ï¼ˆä» NVIDIA å®˜ç½‘ä¸‹è½½ .run æ–‡ä»¶ï¼‰
sudo chmod +x NVIDIA-Linux-x86_64-535.xxx.run
sudo ./NVIDIA-Linux-x86_64-535.xxx.run

# éªŒè¯
nvidia-smi
```

### 3.2 CUDA å®‰è£…ï¼ˆGPU æ¨¡å¼ï¼‰

```bash
# ä¸‹è½½ CUDA 12.xï¼ˆä»¥ Ubuntu 22.04 ä¸ºä¾‹ï¼‰
wget https://developer.download.nvidia.com/compute/cuda/12.4.0/local_installers/cuda_12.4.0_550.54.14_linux.run

# å®‰è£…ï¼ˆè·³è¿‡é©±åŠ¨ï¼Œå› ä¸ºå·²å®‰è£…ï¼‰
sudo sh cuda_12.4.0_550.54.14_linux.run --toolkit --silent

# é…ç½®ç¯å¢ƒå˜é‡
cat >> ~/.bashrc << 'EOF'
export CUDA_HOME=/usr/local/cuda
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
EOF
source ~/.bashrc

# éªŒè¯
nvcc --version
```

### 3.3 cuDNN å®‰è£…ï¼ˆå¯é€‰ï¼‰

> **æ³¨æ„**ï¼šllama.cpp åç«¯ä½¿ç”¨ cuBLASï¼Œ**ä¸ä¾èµ– cuDNN**ã€‚å¦‚æœåªéƒ¨ç½² llama.cpp åç«¯ï¼Œå¯ä»¥è·³è¿‡æ­¤æ­¥éª¤ã€‚

```bash
# ä» NVIDIA å¼€å‘è€…ç½‘ç«™ä¸‹è½½ cuDNNï¼ˆéœ€è¦æ³¨å†Œï¼‰
# https://developer.nvidia.com/cudnn

# è§£å‹å¹¶å®‰è£…
tar -xvf cudnn-linux-x86_64-8.9.x.xx_cuda12-archive.tar.xz
sudo cp cudnn-linux-x86_64-8.9.x.xx_cuda12-archive/include/* /usr/local/cuda/include/
sudo cp cudnn-linux-x86_64-8.9.x.xx_cuda12-archive/lib/* /usr/local/cuda/lib64/
sudo ldconfig

# éªŒè¯
cat /usr/local/cuda/include/cudnn_version.h | grep CUDNN_MAJOR -A 2
```

### 3.4 ç³»ç»Ÿä¾èµ–å®‰è£…

#### Ubuntu

```bash
sudo apt install -y \
    libyaml-cpp-dev \
    libspdlog-dev \
    nlohmann-json3-dev \
    libomp-dev \
    libopenblas-dev \
    python3-pip
```

#### CentOS

```bash
# EPEL ä»“åº“
sudo yum install -y epel-release

# ä¾èµ–åŒ…
sudo yum install -y \
    yaml-cpp-devel \
    spdlog-devel \
    openblas-devel \
    python3-pip

# nlohmann-json éœ€è¦æ‰‹åŠ¨å®‰è£…
git clone https://github.com/nlohmann/json.git
cd json && mkdir build && cd build
cmake .. && sudo make install
```

### 3.5 vcpkg å®‰è£…ï¼ˆæ¨èçš„åŒ…ç®¡ç†æ–¹å¼ï¼‰

```bash
# å®‰è£… vcpkg
git clone https://github.com/microsoft/vcpkg.git ~/vcpkg
cd ~/vcpkg && ./bootstrap-vcpkg.sh

# å®‰è£…ä¾èµ–
~/vcpkg/vcpkg install \
    nlohmann-json \
    yaml-cpp \
    spdlog \
    asio

# é…ç½®ç¯å¢ƒå˜é‡
export VCPKG_ROOT=~/vcpkg
export CMAKE_TOOLCHAIN_FILE=$VCPKG_ROOT/scripts/buildsystems/vcpkg.cmake
```

---

## 4. é¡¹ç›®ç¼–è¯‘

### 4.1 è·å–æºç 

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/xdongp/cLLM.git
cd cLLM

# åˆå§‹åŒ–å­æ¨¡å—
git submodule update --init --recursive
```

### 4.2 ç¼–è¯‘ llama.cpp

æ ¹æ®éƒ¨ç½²æ¨¡å¼é€‰æ‹©ç¼–è¯‘é€‰é¡¹ï¼š

#### GPU æ¨¡å¼ï¼ˆCUDAï¼‰

```bash
cd third_party/llama.cpp
mkdir -p build && cd build

# é…ç½®ï¼ˆå¯ç”¨ CUDAï¼‰
cmake .. \
    -DGGML_CUDA=ON \
    -DCMAKE_CUDA_ARCHITECTURES="75;80;86;89" \
    -DGGML_CUDA_F16=ON \
    -DCMAKE_BUILD_TYPE=Release

make -j$(nproc)
cd ../../..
```

#### CPU æ¨¡å¼ï¼ˆçº¯ CPUï¼‰

```bash
cd third_party/llama.cpp
mkdir -p build && cd build

# é…ç½®ï¼ˆçº¯ CPUï¼Œå¯ç”¨ä¼˜åŒ–ï¼‰
cmake .. \
    -DGGML_CUDA=OFF \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DCMAKE_BUILD_TYPE=Release

make -j$(nproc)
cd ../../..
```

> **CPU ä¼˜åŒ–æç¤º**ï¼š
> - `GGML_BLAS=ON` å¯ç”¨ BLAS åŠ é€ŸçŸ©é˜µè¿ç®—
> - ç¡®ä¿å·²å®‰è£… OpenBLASï¼š`sudo apt install libopenblas-dev`ï¼ˆUbuntuï¼‰
> - llama.cpp ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨ AVX2/AVX-512 æŒ‡ä»¤é›†

**CUDA æ¶æ„ä»£å·è¯´æ˜**ï¼ˆGPU æ¨¡å¼ï¼‰ï¼š

`CMAKE_CUDA_ARCHITECTURES` æŒ‡å®šç¼–è¯‘å™¨ä¸ºå“ªäº› GPU æ¶æ„ç”Ÿæˆä¼˜åŒ–ä»£ç ã€‚æ•°å­—ä»£è¡¨ NVIDIA GPU çš„**è®¡ç®—èƒ½åŠ›ï¼ˆCompute Capabilityï¼‰**ã€‚

| ä»£å· | æ¶æ„åç§° | GPU ç³»åˆ— | è¯´æ˜ |
|------|----------|----------|------|
| 61 | Pascal | GTX 10xx (1060, 1070, 1080) | è¾ƒè€ï¼Œå¯ä¸æ”¯æŒ |
| 75 | Turing | GTX 16xx, RTX 20xx | æ¶ˆè´¹çº§å…¥é—¨ |
| 80 | Ampere | A100, RTX 30xx | æ•°æ®ä¸­å¿ƒ/æ¶ˆè´¹çº§ |
| 86 | Ampere | RTX 30xx Ti | æ¶ˆè´¹çº§ä¸»æµ |
| 89 | Ada Lovelace | RTX 40xx (4060-4090) | æœ€æ–°æ¶ˆè´¹çº§ |
| 90 | Hopper | H100, H200 | æ•°æ®ä¸­å¿ƒ |

**æŸ¥çœ‹ä½ çš„ GPU æ¶æ„ä»£å·**ï¼š
```bash
nvidia-smi --query-gpu=compute_cap --format=csv
# è¾“å‡ºç¤ºä¾‹: 8.6 è¡¨ç¤ºæ¶æ„ä»£å· 86ï¼ˆRTX 3080ï¼‰
```

**é…ç½®å»ºè®®**ï¼š
```bash
# åªç¼–è¯‘ä½ å®é™…ä½¿ç”¨çš„æ¶æ„ï¼ˆå‡å°‘ç¼–è¯‘æ—¶é—´ï¼‰
-DCMAKE_CUDA_ARCHITECTURES="86"           # åªæœ‰ RTX 3080
-DCMAKE_CUDA_ARCHITECTURES="89"           # åªæœ‰ RTX 4090
-DCMAKE_CUDA_ARCHITECTURES="80;90"        # æ•°æ®ä¸­å¿ƒ A100 + H100
-DCMAKE_CUDA_ARCHITECTURES="75;80;86;89"  # å…¼å®¹å¤šç§æ¶ˆè´¹çº§ GPU
```

### 4.3 ç¼–è¯‘ cLLM

#### GPU æ¨¡å¼

```bash
mkdir -p build && cd build

cmake .. \
    -DCMAKE_BUILD_TYPE=Release \
    -DUSE_TOKENIZERS_CPP=OFF \
    -DUSE_LIBTORCH=OFF

make -j$(nproc)
./bin/cllm_server --help
```

#### CPU æ¨¡å¼

```bash
mkdir -p build && cd build

cmake .. \
    -DCMAKE_BUILD_TYPE=Release \
    -DUSE_TOKENIZERS_CPP=OFF \
    -DUSE_LIBTORCH=OFF

make -j$(nproc)
./bin/cllm_server --help
```

**ç¼–è¯‘é€‰é¡¹è¯´æ˜**ï¼š

| é€‰é¡¹ | å€¼ | è¯´æ˜ |
|------|-----|------|
| `USE_TOKENIZERS_CPP` | OFF | ç¦ç”¨ tokenizers-cppï¼Œä½¿ç”¨ GGUF å†…ç½® tokenizer |
| `USE_LIBTORCH` | OFF | ç¦ç”¨ LibTorch åç«¯ï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰ |

> **æ³¨æ„**ï¼š
> - cLLM æœ¬èº«ä¸éœ€è¦ CUDAï¼ŒCUDA ä¾èµ–åœ¨ llama.cpp ç¼–è¯‘æ—¶å¤„ç†
> - ç”Ÿäº§ç¯å¢ƒåªä½¿ç”¨ llama.cpp åç«¯ï¼ŒKylin å’Œ LibTorch åç«¯ä»…ä¾›å¼€å‘æµ‹è¯•

---

## 5. æ¨¡å‹å‡†å¤‡

### 5.1 ä¸‹è½½ GGUF æ¨¡å‹

```bash
# åˆ›å»ºæ¨¡å‹ç›®å½•
sudo mkdir -p /opt/models
sudo chown $USER:$USER /opt/models

# ä½¿ç”¨ huggingface-cli ä¸‹è½½
pip3 install huggingface_hub
huggingface-cli download \
    Qwen/Qwen2.5-7B-Instruct-GGUF \
    qwen2.5-7b-instruct-q4_k_m.gguf \
    --local-dir /opt/models/qwen2.5-7b

# æˆ–ä½¿ç”¨ wget ç›´æ¥ä¸‹è½½
wget -P /opt/models/qwen2.5-7b \
    https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF/resolve/main/qwen2.5-7b-instruct-q4_k_m.gguf
```

### 5.2 æ¨¡å‹æ–‡ä»¶ç»“æ„

ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ GGUF å†…ç½® Tokenizerï¼Œæ¨¡å‹ç›®å½•åªéœ€ GGUF æ–‡ä»¶ï¼š

```
/opt/models/qwen2.5-7b/
â””â”€â”€ qwen2.5-7b-instruct-q4_k_m.gguf    # GGUF æ¨¡å‹æ–‡ä»¶ï¼ˆä»…éœ€æ­¤æ–‡ä»¶ï¼‰
```

> **è¯´æ˜**ï¼šGGUF æ¨¡å‹å·²å†…ç½®å®Œæ•´çš„ tokenizer ä¿¡æ¯ï¼Œæ— éœ€é¢å¤–ä¸‹è½½ `tokenizer.json`ã€‚

---

## 6. é…ç½®æ–‡ä»¶

> **é‡è¦**ï¼šç”Ÿäº§ç¯å¢ƒå¿…é¡»ä½¿ç”¨ `backend.type: "llama_cpp"`ï¼Œè¿™æ˜¯å”¯ä¸€æ”¯æŒç”Ÿäº§éƒ¨ç½²çš„åç«¯ã€‚

### 6.1 GPU æ¨¡å¼é…ç½®

åˆ›å»º `/opt/cllm/config/production_gpu.yaml`ï¼š

```yaml
# cLLM ç”Ÿäº§ç¯å¢ƒé…ç½® - GPU æ¨¡å¼
# åç«¯: llama.cpp (GGUF)
# Tokenizer: GGUF å†…ç½®

server:
  host: "0.0.0.0"
  port: 8080
  num_threads: 16

model:
  path: "/opt/models/qwen2.5-7b/qwen2.5-7b-instruct-q4_k_m.gguf"
  vocab_size: 152064
  max_context_length: 32768
  default_max_tokens: 2048

backend:
  type: "llama_cpp"         # ğŸ”¥ ç”Ÿäº§ç¯å¢ƒå”¯ä¸€æ¨èåç«¯
  
  llama_cpp:
    n_batch: 2048          # GPU å¯ä»¥è®¾ç½®æ›´å¤§
    n_threads: 8           # CPU çº¿ç¨‹ï¼ˆç”¨äºé GPU æ“ä½œï¼‰
    n_gpu_layers: 99       # ğŸ”¥ å…³é”®ï¼š99 = æ‰€æœ‰å±‚æ”¾ GPU
    n_ctx: 32768
    n_seq_max: 8
    use_mmap: true
    use_mlock: true
    flash_attn: true       # Flash Attentionï¼ˆGPUï¼‰

# Tokenizer ä½¿ç”¨ GGUF å†…ç½®ï¼Œæ— éœ€é¢å¤–é…ç½®
# tokenizer:
#   type: "gguf"           # è‡ªåŠ¨ä» GGUF æ¨¡å‹è¯»å–

scheduler:
  max_batch_size: 8
  request_timeout: 600.0
  default_max_tokens: 2048

resources:
  max_context_length: 32768
  kv_cache_max_size: 32

logging:
  level: "info"
  file: "/var/log/cllm/cllm.log"
```

### 6.2 CPU æ¨¡å¼é…ç½®

åˆ›å»º `/opt/cllm/config/production_cpu.yaml`ï¼š

```yaml
# cLLM ç”Ÿäº§ç¯å¢ƒé…ç½® - CPU æ¨¡å¼
# åç«¯: llama.cpp (GGUF)
# Tokenizer: GGUF å†…ç½®

server:
  host: "0.0.0.0"
  port: 8080
  num_threads: 16          # å»ºè®® = CPU æ ¸å¿ƒæ•°

model:
  path: "/opt/models/qwen2.5-3b/qwen2.5-3b-instruct-q4_k_m.gguf"  # æ¨èå°æ¨¡å‹
  vocab_size: 152064
  max_context_length: 8192   # CPU æ¨¡å¼å»ºè®®å‡å°
  default_max_tokens: 1024

backend:
  type: "llama_cpp"         # ğŸ”¥ ç”Ÿäº§ç¯å¢ƒå”¯ä¸€æ¨èåç«¯
  
  llama_cpp:
    n_batch: 512           # CPU æ¨¡å¼å»ºè®®è¾ƒå°å€¼
    n_threads: 16          # ğŸ”¥ å…³é”®ï¼šè®¾ç½®ä¸º CPU æ ¸å¿ƒæ•°
    n_gpu_layers: 0        # ğŸ”¥ å…³é”®ï¼š0 = çº¯ CPU æ¨¡å¼
    n_ctx: 8192            # CPU æ¨¡å¼å»ºè®®å‡å°
    n_seq_max: 2           # CPU å¹¶å‘èƒ½åŠ›æœ‰é™
    use_mmap: true
    use_mlock: false       # CPU æ¨¡å¼å¯å…³é—­

# Tokenizer ä½¿ç”¨ GGUF å†…ç½®ï¼Œæ— éœ€é¢å¤–é…ç½®
# tokenizer:
#   type: "gguf"           # è‡ªåŠ¨ä» GGUF æ¨¡å‹è¯»å–

scheduler:
  max_batch_size: 2        # CPU æ¨¡å¼å»ºè®®å‡å°
  request_timeout: 600.0
  default_max_tokens: 1024

resources:
  max_context_length: 8192
  kv_cache_max_size: 8     # CPU æ¨¡å¼å‡å°

logging:
  level: "info"
  file: "/var/log/cllm/cllm.log"
```

**CPU æ¨¡å¼é…ç½®è¦ç‚¹**ï¼š
| å‚æ•° | GPU æ¨¡å¼ | CPU æ¨¡å¼ | è¯´æ˜ |
|------|----------|----------|------|
| `n_gpu_layers` | 99 | **0** | CPU æ¨¡å¼å¿…é¡»ä¸º 0 |
| `n_threads` | 8 | **CPU æ ¸å¿ƒæ•°** | å½±å“æ¨ç†é€Ÿåº¦ |
| `n_batch` | 2048 | 512 | CPU å¤„ç†èƒ½åŠ›æœ‰é™ |
| `n_ctx` | 32768 | 8192 | å‡å°‘å†…å­˜å ç”¨ |
| `n_seq_max` | 8 | 2 | å‡å°‘å¹¶å‘å‹åŠ› |
| `max_batch_size` | 8 | 2 | å‡å°‘è°ƒåº¦å‹åŠ› |

### 6.3 ç¯å¢ƒå˜é‡é…ç½®

**GPU æ¨¡å¼** (`/opt/cllm/env_gpu.sh`)ï¼š
```bash
#!/bin/bash
export CUDA_HOME=/usr/local/cuda
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:/opt/cllm/lib:$LD_LIBRARY_PATH
export CUDA_VISIBLE_DEVICES=0
export LLAMA_LOG_LEVEL=warn
export OMP_NUM_THREADS=8
```

**CPU æ¨¡å¼** (`/opt/cllm/env_cpu.sh`)ï¼š
```bash
#!/bin/bash
export LD_LIBRARY_PATH=/opt/cllm/lib:$LD_LIBRARY_PATH
export LLAMA_LOG_LEVEL=warn

# ğŸ”¥ CPU çº¿ç¨‹é…ç½®ï¼ˆè®¾ç½®ä¸º CPU æ ¸å¿ƒæ•°ï¼‰
export OMP_NUM_THREADS=16
export MKL_NUM_THREADS=16
export OPENBLAS_NUM_THREADS=16

# ç¦ç”¨ NUMA äº¤é”™ï¼ˆå• NUMA èŠ‚ç‚¹ä¼˜åŒ–ï¼‰
export GOMP_CPU_AFFINITY="0-15"
```

---

## 7. æœåŠ¡éƒ¨ç½²

### 7.1 ç›®å½•ç»“æ„

```bash
# åˆ›å»ºéƒ¨ç½²ç›®å½•
sudo mkdir -p /opt/cllm/{bin,config,lib,logs}
sudo chown -R $USER:$USER /opt/cllm

# å¤åˆ¶æ–‡ä»¶
cp build/bin/cllm_server /opt/cllm/bin/
cp -r config/* /opt/cllm/config/
cp build/lib/*.so /opt/cllm/lib/ 2>/dev/null || true

# åˆ›å»ºæ—¥å¿—ç›®å½•
sudo mkdir -p /var/log/cllm
sudo chown $USER:$USER /var/log/cllm
```

### 7.2 Systemd æœåŠ¡é…ç½®

**GPU æ¨¡å¼** - åˆ›å»º `/etc/systemd/system/cllm.service`ï¼š

```ini
[Unit]
Description=cLLM Large Language Model Server (GPU)
After=network.target

[Service]
Type=simple
User=cllm
Group=cllm
WorkingDirectory=/opt/cllm

# GPU æ¨¡å¼ç¯å¢ƒå˜é‡
Environment="CUDA_HOME=/usr/local/cuda"
Environment="LD_LIBRARY_PATH=/usr/local/cuda/lib64:/opt/cllm/lib"
Environment="CUDA_VISIBLE_DEVICES=0"
Environment="OMP_NUM_THREADS=8"

ExecStart=/opt/cllm/bin/cllm_server --config /opt/cllm/config/production_gpu.yaml

Restart=always
RestartSec=10
LimitNOFILE=65535
LimitNPROC=65535

StandardOutput=append:/var/log/cllm/cllm.log
StandardError=append:/var/log/cllm/cllm.error.log

[Install]
WantedBy=multi-user.target
```

**CPU æ¨¡å¼** - åˆ›å»º `/etc/systemd/system/cllm.service`ï¼š

```ini
[Unit]
Description=cLLM Large Language Model Server (CPU)
After=network.target

[Service]
Type=simple
User=cllm
Group=cllm
WorkingDirectory=/opt/cllm

# CPU æ¨¡å¼ç¯å¢ƒå˜é‡
Environment="LD_LIBRARY_PATH=/opt/cllm/lib"
Environment="OMP_NUM_THREADS=16"
Environment="OPENBLAS_NUM_THREADS=16"

ExecStart=/opt/cllm/bin/cllm_server --config /opt/cllm/config/production_cpu.yaml

Restart=always
RestartSec=10
LimitNOFILE=65535
LimitNPROC=65535

StandardOutput=append:/var/log/cllm/cllm.log
StandardError=append:/var/log/cllm/cllm.error.log

[Install]
WantedBy=multi-user.target
```

### 7.3 åˆ›å»ºæœåŠ¡ç”¨æˆ·

```bash
# åˆ›å»ºä¸“ç”¨ç”¨æˆ·
sudo useradd -r -s /bin/false -d /opt/cllm cllm

# è®¾ç½®æƒé™
sudo chown -R cllm:cllm /opt/cllm /var/log/cllm

# æ·»åŠ ç”¨æˆ·åˆ° video ç»„ï¼ˆGPU è®¿é—®æƒé™ï¼‰
sudo usermod -aG video cllm
```

### 7.4 å¯åŠ¨æœåŠ¡

```bash
# é‡æ–°åŠ è½½ systemd
sudo systemctl daemon-reload

# å¯åŠ¨æœåŠ¡
sudo systemctl start cllm

# å¼€æœºè‡ªå¯
sudo systemctl enable cllm

# æŸ¥çœ‹çŠ¶æ€
sudo systemctl status cllm

# æŸ¥çœ‹æ—¥å¿—
sudo journalctl -u cllm -f
```

### 7.5 å¥åº·æ£€æŸ¥

```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:8080/health

# æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
curl http://localhost:8080/model/info

# æµ‹è¯•ç”Ÿæˆ
curl -X POST http://localhost:8080/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Hello", "max_tokens": 50}'
```

---

## 8. ç›‘æ§ä¸æ—¥å¿—

### 8.1 GPU ç›‘æ§

```bash
# å®æ—¶ç›‘æ§ GPU
watch -n 1 nvidia-smi

# GPU ç›‘æ§è„šæœ¬
cat > /opt/cllm/monitor_gpu.sh << 'EOF'
#!/bin/bash
while true; do
    nvidia-smi --query-gpu=timestamp,name,utilization.gpu,utilization.memory,memory.used,memory.total,temperature.gpu \
        --format=csv >> /var/log/cllm/gpu_stats.csv
    sleep 60
done
EOF
chmod +x /opt/cllm/monitor_gpu.sh
```

### 8.2 æ—¥å¿—è½®è½¬

åˆ›å»º `/etc/logrotate.d/cllm`ï¼š

```
/var/log/cllm/*.log {
    daily
    rotate 14
    compress
    delaycompress
    missingok
    notifempty
    create 0644 cllm cllm
    postrotate
        systemctl reload cllm > /dev/null 2>&1 || true
    endscript
}
```

### 8.3 Prometheus æŒ‡æ ‡ï¼ˆå¯é€‰ï¼‰

å¦‚æœéœ€è¦é›†æˆ Prometheus ç›‘æ§ï¼š

```bash
# ä½¿ç”¨ nvidia_gpu_exporter
docker run -d \
  --name nvidia_exporter \
  --gpus all \
  -p 9835:9835 \
  utkuozdemir/nvidia_gpu_exporter:1.2.0
```

---

## 9. æ€§èƒ½è°ƒä¼˜

### 9.1 GPU ä¼˜åŒ–

```bash
# è®¾ç½® GPU æŒä¹…æ¨¡å¼ï¼ˆå‡å°‘å¯åŠ¨å»¶è¿Ÿï¼‰
sudo nvidia-smi -pm 1

# è®¾ç½® GPU æ—¶é’Ÿï¼ˆå¯é€‰ï¼Œæå‡æ€§èƒ½ï¼‰
sudo nvidia-smi -lgc 1500,1500  # é”å®š GPU æ—¶é’Ÿ

# è®¾ç½® GPU åŠŸè€—é™åˆ¶ï¼ˆå¯é€‰ï¼‰
sudo nvidia-smi -pl 350  # è®¾ç½®åŠŸè€—ä¸Šé™
```

### 9.2 CPU ä¼˜åŒ–

```bash
# æ£€æŸ¥ CPU æ”¯æŒçš„æŒ‡ä»¤é›†
cat /proc/cpuinfo | grep -E "avx|avx2|avx512" | head -1

# è®¾ç½® CPU æ€§èƒ½æ¨¡å¼ï¼ˆå…³é—­èŠ‚èƒ½ï¼‰
echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

# ç¦ç”¨é€æ˜å¤§é¡µï¼ˆå¯èƒ½å½±å“å»¶è¿Ÿï¼‰
echo never | sudo tee /sys/kernel/mm/transparent_hugepage/enabled

# NUMA ä¼˜åŒ–ï¼ˆå¤š CPU æœåŠ¡å™¨ï¼‰
# ç»‘å®šåˆ°å•ä¸ª NUMA èŠ‚ç‚¹
numactl --cpunodebind=0 --membind=0 /opt/cllm/bin/cllm_server --config ...
```

**CPU çº¿ç¨‹é…ç½®å»ºè®®**ï¼š
| CPU æ ¸å¿ƒæ•° | `n_threads` | `OMP_NUM_THREADS` | è¯´æ˜ |
|------------|-------------|-------------------|------|
| 4 æ ¸ | 4 | 4 | å°å‹æœåŠ¡å™¨ |
| 8 æ ¸ | 8 | 8 | æ ‡å‡†é…ç½® |
| 16 æ ¸ | 16 | 16 | æ¨èé…ç½® |
| 32+ æ ¸ | 16-24 | 16-24 | è¿‡å¤šçº¿ç¨‹å¯èƒ½é™ä½æ•ˆç‡ |

### 9.3 ç³»ç»Ÿä¼˜åŒ–

```bash
# å¢åŠ æ–‡ä»¶æè¿°ç¬¦é™åˆ¶
cat >> /etc/security/limits.conf << EOF
cllm soft nofile 65535
cllm hard nofile 65535
cllm soft nproc 65535
cllm hard nproc 65535
EOF

# ä¼˜åŒ–ç½‘ç»œå‚æ•°
cat >> /etc/sysctl.conf << EOF
net.core.somaxconn = 65535
net.ipv4.tcp_max_syn_backlog = 65535
net.core.netdev_max_backlog = 65535
EOF
sudo sysctl -p
```

### 9.4 é…ç½®è°ƒä¼˜å»ºè®®

**GPU æ¨¡å¼**ï¼š
| å‚æ•° | å°æ¨¡å‹ (< 3B) | ä¸­ç­‰æ¨¡å‹ (3-14B) | å¤§æ¨¡å‹ (> 14B) |
|------|---------------|------------------|----------------|
| `n_batch` | 512 | 1024-2048 | 512-1024 |
| `n_ctx` | 8192 | 16384-32768 | 8192-16384 |
| `n_seq_max` | 8-16 | 4-8 | 2-4 |
| `max_batch_size` | 16 | 8 | 4 |

**CPU æ¨¡å¼**ï¼š
| å‚æ•° | å°æ¨¡å‹ (< 3B) | ä¸­ç­‰æ¨¡å‹ (3-7B) | è¯´æ˜ |
|------|---------------|-----------------|------|
| `n_batch` | 256-512 | 128-256 | CPU å¤„ç†èƒ½åŠ›æœ‰é™ |
| `n_ctx` | 4096-8192 | 2048-4096 | å‡å°‘å†…å­˜å ç”¨ |
| `n_seq_max` | 2-4 | 1-2 | å‡å°‘å¹¶å‘ |
| `max_batch_size` | 4 | 2 | å‡å°‘è°ƒåº¦å‹åŠ› |

> **CPU æ¨¡å¼å»ºè®®**ï¼šæ¨èä½¿ç”¨ 3B ä»¥ä¸‹çš„å°æ¨¡å‹ï¼ˆå¦‚ Qwen2.5-3Bï¼‰ï¼Œé…åˆ Q4_K_M é‡åŒ–è·å¾—æœ€ä½³æ€§ä»·æ¯”ã€‚

---

## 10. æ•…éšœæ’æŸ¥

### 10.1 å¸¸è§é—®é¢˜

#### CUDA å†…å­˜ä¸è¶³

```
Error: CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. å‡å°‘ `n_ctx` ä¸Šä¸‹æ–‡é•¿åº¦
2. å‡å°‘ `n_batch` æ‰¹å¤„ç†å¤§å°
3. ä½¿ç”¨æ›´å°çš„é‡åŒ–ç‰ˆæœ¬ï¼ˆå¦‚ Q4_K_M â†’ Q4_K_Sï¼‰
4. å‡å°‘ `n_gpu_layers`ï¼ˆéƒ¨åˆ†å±‚æ”¾ CPUï¼‰

#### GPU é©±åŠ¨é—®é¢˜

```
Error: CUDA driver version is insufficient
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æ›´æ–°é©±åŠ¨
sudo apt install nvidia-driver-535
sudo reboot
```

#### æ¨¡å‹åŠ è½½å¤±è´¥

```
Error: Failed to load model
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®
2. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§ï¼ˆMD5/SHA256ï¼‰
3. æ£€æŸ¥ç£ç›˜ç©ºé—´å’Œå†…å­˜

### 10.2 è¯Šæ–­å‘½ä»¤

```bash
# æ£€æŸ¥ GPU çŠ¶æ€
nvidia-smi -q

# æ£€æŸ¥ CUDA ç‰ˆæœ¬
nvcc --version
cat /usr/local/cuda/version.txt

# æ£€æŸ¥ llama.cpp CUDA æ”¯æŒ
ldd /opt/cllm/bin/cllm_server | grep -i cuda

# æ£€æŸ¥æœåŠ¡æ—¥å¿—
sudo journalctl -u cllm -n 100 --no-pager

# æ£€æŸ¥ç«¯å£å ç”¨
sudo netstat -tlnp | grep 8080

# æ£€æŸ¥å†…å­˜ä½¿ç”¨
free -h
cat /proc/meminfo | grep -E "MemTotal|MemFree|MemAvailable"

# æ£€æŸ¥ GPU æ˜¾å­˜ä½¿ç”¨
nvidia-smi --query-gpu=memory.used,memory.total --format=csv
```

### 10.3 æ€§èƒ½è¯Šæ–­

```bash
# ä½¿ç”¨å†…ç½® benchmark
curl -X POST http://localhost:8080/benchmark \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Hello", "max_tokens": 100, "iterations": 10}'

# ä½¿ç”¨ unified_benchmark.py
python3 tools/unified_benchmark.py \
  --server-type cllm \
  --server-url http://localhost:8080 \
  --requests 50 \
  --concurrency 4 \
  --max-tokens 100
```

---

## é™„å½•

### A. å¿«é€Ÿéƒ¨ç½²è„šæœ¬

```bash
#!/bin/bash
# deploy.sh - cLLM å¿«é€Ÿéƒ¨ç½²è„šæœ¬

set -e

INSTALL_DIR="/opt/cllm"
MODEL_DIR="/opt/models"
LOG_DIR="/var/log/cllm"

echo "=== cLLM éƒ¨ç½²è„šæœ¬ ==="

# æ£€æŸ¥ root æƒé™
if [[ $EUID -ne 0 ]]; then
   echo "è¯·ä½¿ç”¨ root æƒé™è¿è¡Œæ­¤è„šæœ¬"
   exit 1
fi

# åˆ›å»ºç›®å½•
mkdir -p $INSTALL_DIR/{bin,config,lib}
mkdir -p $MODEL_DIR
mkdir -p $LOG_DIR

# åˆ›å»ºç”¨æˆ·
useradd -r -s /bin/false -d $INSTALL_DIR cllm 2>/dev/null || true
usermod -aG video cllm

# è®¾ç½®æƒé™
chown -R cllm:cllm $INSTALL_DIR $LOG_DIR

echo "=== éƒ¨ç½²å®Œæˆ ==="
echo "è¯·æ‰‹åŠ¨å®Œæˆä»¥ä¸‹æ­¥éª¤ï¼š"
echo "1. å¤åˆ¶ç¼–è¯‘å¥½çš„ cllm_server åˆ° $INSTALL_DIR/bin/"
echo "2. é…ç½® $INSTALL_DIR/config/production.yaml"
echo "3. ä¸‹è½½æ¨¡å‹åˆ° $MODEL_DIR/"
echo "4. å¯åŠ¨æœåŠ¡: systemctl start cllm"
```

### B. å‚è€ƒé“¾æ¥

- [NVIDIA CUDA å®‰è£…æŒ‡å—](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/)
- [cuDNN å®‰è£…æŒ‡å—](https://docs.nvidia.com/deeplearning/cudnn/install-guide/)
- [llama.cpp æ–‡æ¡£](https://github.com/ggerganov/llama.cpp)
- [HuggingFace æ¨¡å‹ä¸‹è½½](https://huggingface.co/models)

---

## é™„å½• C: ç”Ÿäº§ç¯å¢ƒæ¶æ„è¯´æ˜

### æ¨èé…ç½®

| ç»„ä»¶ | ç”Ÿäº§ç¯å¢ƒé…ç½® | è¯´æ˜ |
|------|-------------|------|
| **Backend** | llama.cpp | å”¯ä¸€æ¨èçš„ç”Ÿäº§åç«¯ |
| **Tokenizer** | GGUF å†…ç½® | æ— éœ€é¢å¤–ä¾èµ– |
| **æ¨¡å‹æ ¼å¼** | GGUF (é‡åŒ–) | æ¨è Q4_K_M æˆ– Q4_K_S |

### ç¦ç”¨çš„ç»„ä»¶

| ç»„ä»¶ | CMake é€‰é¡¹ | è¯´æ˜ |
|------|-----------|------|
| tokenizers-cpp | `-DUSE_TOKENIZERS_CPP=OFF` | ä½¿ç”¨ GGUF å†…ç½® tokenizer |
| LibTorch | `-DUSE_LIBTORCH=OFF` | å®éªŒæ€§åŠŸèƒ½ï¼Œç”Ÿäº§ä¸ä½¿ç”¨ |
| Kylin Backend | é»˜è®¤ä¸å¯ç”¨ | å®éªŒæ€§è‡ªç ”åç«¯ |

### æœ€å°ä¾èµ–æ¸…å•

```bash
# å¿…éœ€ä¾èµ–
- GCC >= 9.0
- CMake >= 3.18
- nlohmann-json
- yaml-cpp
- spdlog
- SentencePiece (llama.cpp ä¾èµ–)

# GPU æ¨¡å¼é¢å¤–ä¾èµ–
- NVIDIA é©±åŠ¨ >= 525.x
- CUDA >= 11.8

# å¯é€‰ä¾èµ–ï¼ˆåŠ é€Ÿï¼‰
- OpenBLAS (CPU BLAS åŠ é€Ÿ)
- OpenMP (å¹¶è¡Œè®¡ç®—)
```

---

*æ–‡æ¡£ç‰ˆæœ¬: 2.0*  
*æœ€åæ›´æ–°: 2026-02-04*  
*æ”¯æŒåç«¯: llama.cpp (GGUF)*  
*Tokenizer: GGUF å†…ç½®*
