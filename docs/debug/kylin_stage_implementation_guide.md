# Kylin åˆ†é˜¶æ®µæµ‹è¯•å®æ–½æŒ‡å—

## æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›æ¯ä¸ªé˜¶æ®µçš„å…·ä½“å®æ–½æ­¥éª¤ã€æµ‹è¯•ç”¨ä¾‹å’ŒéªŒè¯æ–¹æ³•ã€‚

---

## Stage 0: åŸºç¡€ç¯å¢ƒéªŒè¯ âœ…

### å®æ–½çŠ¶æ€
- âœ… å·²å®Œæˆ

### æµ‹è¯•ç”¨ä¾‹
```cpp
TEST(Stage0, BasicEnvironment) {
    ModelConfig config;
    config.vocabSize = 151936;
    // ... å…¶ä»–é…ç½®
    
    KylinBackend backend(config, modelPath);
    ASSERT_TRUE(backend.initialize());
}
```

### éªŒè¯ç‚¹
- [x] KylinBackend æ„é€ æˆåŠŸ
- [x] æ¨¡å‹è·¯å¾„æ£€æµ‹æ­£ç¡®
- [x] è®¾å¤‡åç«¯é…ç½®æ­£ç¡®
- [x] åˆå§‹åŒ–æˆåŠŸ

---

## Stage 1: æ¨¡å‹åŠ è½½éªŒè¯ âœ…

### å®æ–½çŠ¶æ€
- âœ… å·²å®Œæˆ

### æµ‹è¯•ç”¨ä¾‹
```cpp
TEST(Stage1, ModelLoading) {
    KylinBackend backend(config, modelPath);
    backend.initialize();
    
    const auto& loadedConfig = backend.getConfig();
    ASSERT_EQ(loadedConfig.vocabSize, 151936);
    ASSERT_EQ(loadedConfig.hiddenSize, 1024);
    ASSERT_EQ(loadedConfig.numLayers, 28);
    // ...
}
```

### éªŒè¯ç‚¹
- [x] é…ç½®åŠ è½½æ­£ç¡®
- [x] æƒé‡æ˜ å°„å®Œæˆ
- [x] KV Cache åˆ†é…æˆåŠŸ

---

## Stage 2: Token Embedding éªŒè¯ ğŸ”„

### å®æ–½çŠ¶æ€
- ğŸ”„ å·²æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼Œéœ€è¦å®Œå–„éªŒè¯

### æµ‹è¯•ç”¨ä¾‹

#### ç”¨ä¾‹ 2.1: å•ä¸ª Token
```cpp
TEST(Stage2, SingleTokenEmbedding) {
    KylinBackend backend(config, modelPath);
    backend.initialize();
    
    std::vector<int> inputIds = {9707};  // "Hello"
    auto output = backend.forward(inputIds);
    
    // éªŒè¯è¾“å‡ºå½¢çŠ¶
    ASSERT_EQ(output.shape()[0], 1);
    ASSERT_EQ(output.shape()[1], 151936);
    
    // éªŒè¯ embedding ç»Ÿè®¡ï¼ˆä»æ—¥å¿—æå–ï¼‰
    // min, max, mean, nan, inf
}
```

#### ç”¨ä¾‹ 2.2: å¤šä¸ª Tokens
```cpp
TEST(Stage2, MultiTokenEmbedding) {
    std::vector<int> inputIds = {9707, 11, 1234};
    // éªŒè¯ embedding è¾“å‡ºå½¢çŠ¶ [1024, 3]
}
```

#### ç”¨ä¾‹ 2.3: ä¸ llama_cpp å¯¹æ¯”
```cpp
TEST(Stage2, CompareWithLlamaCpp) {
    // è¿è¡Œ Kylin
    auto kylinOutput = kylinBackend.forward({9707});
    
    // è¿è¡Œ llama_cpp
    auto llamaOutput = llamaCppBackend.forward({9707});
    
    // å¯¹æ¯” embedding è¾“å‡ºï¼ˆéœ€è¦ä»ä¸­é—´ç»“æœæå–ï¼‰
    // å¯¹æ¯”å‰10ä¸ªå€¼
}
```

### éªŒè¯æ–¹æ³•

**æ–¹æ³• 1: ä»æ—¥å¿—æå–**
```bash
# è¿è¡Œæµ‹è¯•
./build/bin/cllm_server --config config/config.yaml > /tmp/test.log 2>&1 &
curl -X POST http://localhost:8080/generate \
  -d '{"prompt": "Hi", "max_tokens": 1, "temperature": 0.0}'

# æå– embedding ç»Ÿè®¡
grep "\[Kylin Debug\] Embedding stats" /tmp/test.log
```

**æ–¹æ³• 2: æ·»åŠ å›è°ƒæ¥å£**
```cpp
// åœ¨ GGMLTransformerModel ä¸­æ·»åŠ å›è°ƒ
class EmbeddingCallback {
public:
    virtual void onEmbedding(const float* data, size_t size) = 0;
};

// åœ¨ forward() ä¸­è°ƒç”¨
if (embeddingCallback_) {
    embeddingCallback_->onEmbedding(
        static_cast<const float*>(debugEmbedding_->data),
        debugEmbedding_->ne[0] * debugEmbedding_->ne[1]
    );
}
```

### é¢„æœŸç»“æœ
- Shape: `[1024, seq_len]`
- Min: é€šå¸¸åœ¨ [-1, 1]
- Max: é€šå¸¸åœ¨ [-1, 1]
- Mean: æ¥è¿‘ 0
- NaN/Inf: 0
- å‰10ä¸ªå€¼ä¸ llama_cpp ä¸€è‡´ï¼ˆè¯¯å·® < 1e-5ï¼‰

---

## Stage 3: ç¬¬ä¸€å±‚ Transformer Block éªŒè¯ ğŸ”„

### å®æ–½çŠ¶æ€
- ğŸ”„ å·²æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼Œéœ€è¦å®Œå–„éªŒè¯

### æµ‹è¯•ç”¨ä¾‹

#### ç”¨ä¾‹ 3.1: å• Token é¦–æ¬¡æ¨ç†
```cpp
TEST(Stage3, Layer0SingleToken) {
    std::vector<int> inputIds = {9707};
    
    // éªŒè¯ç‚¹:
    // 1. Attention å½’ä¸€åŒ–è¾“å‡º
    // 2. QKV æŠ•å½±è¾“å‡º
    // 3. Q/K å½’ä¸€åŒ–è¾“å‡º
    // 4. RoPE åè¾“å‡º
    // 5. æ³¨æ„åŠ›è¾“å‡º
    // 6. FFN è¾“å‡º
    // 7. Layer 0 æœ€ç»ˆè¾“å‡º
}
```

#### ç”¨ä¾‹ 3.2: å¤š Token é¦–æ¬¡æ¨ç†
```cpp
TEST(Stage3, Layer0MultiToken) {
    std::vector<int> inputIds = {9707, 11, 1234};
    // éªŒè¯æ¯ä¸ªæ­¥éª¤çš„è¾“å‡ºå½¢çŠ¶å’Œæ•°å€¼
}
```

### éªŒè¯æ–¹æ³•

**æ·»åŠ ä¸­é—´èŠ‚ç‚¹ä¿å­˜**
```cpp
// åœ¨ buildLayerGraph ä¸­ä¿å­˜ä¸­é—´èŠ‚ç‚¹
class LayerDebugNodes {
public:
    ggml_tensor* attnNormOutput;
    ggml_tensor* qkvOutput;
    ggml_tensor* qNormOutput;
    ggml_tensor* kNormOutput;
    ggml_tensor* ropeQOutput;
    ggml_tensor* ropeKOutput;
    ggml_tensor* attentionOutput;
    ggml_tensor* ffnNormOutput;
    ggml_tensor* ffnOutput;
    ggml_tensor* layerOutput;
};

// åœ¨ forward() ä¸­æ‰“å°ç»Ÿè®¡
void printLayerStats(const LayerDebugNodes& nodes, size_t layerIdx) {
    if (nodes.attnNormOutput && nodes.attnNormOutput->data) {
        printTensorStats("AttnNorm", nodes.attnNormOutput, layerIdx);
    }
    // ... å…¶ä»–èŠ‚ç‚¹
}
```

### é¢„æœŸç»“æœ
- Layer 0 è¾“å‡º shape: `[1024, seq_len]`
- æ‰€æœ‰ä¸­é—´æ­¥éª¤æ—  NaN/Inf
- æ•°å€¼èŒƒå›´åˆç†
- ä¸ llama_cpp çš„ Layer 0 è¾“å‡ºå¯¹æ¯”

---

## Stage 4: æ³¨æ„åŠ›è®¡ç®—è¯¦ç»†éªŒè¯ â³

### å®æ–½çŠ¶æ€
- â³ å¾…å®ç°

### å­é˜¶æ®µæµ‹è¯•

#### Stage 4.1: QKV æŠ•å½±éªŒè¯

**æµ‹è¯•ç”¨ä¾‹**:
```cpp
TEST(Stage4_1, QKVProjection) {
    // è¾“å…¥: [1024, 1]
    // éªŒè¯:
    // - Q: [2048, 1] (16 heads * 128 head_dim)
    // - K: [1024, 1] (8 KV heads * 128 head_dim)
    // - V: [1024, 1]
    
    // éªŒè¯æ•°å€¼èŒƒå›´
    // éªŒè¯ä¸ llama_cpp çš„ Q/K/V è¾“å‡ºå¯¹æ¯”
}
```

**å®æ–½æ­¥éª¤**:
1. åœ¨ `buildAttentionGraph` ä¸­ä¿å­˜ Q/K/V æŠ•å½±åçš„èŠ‚ç‚¹
2. åœ¨ `forward()` ä¸­æ‰“å°ç»Ÿè®¡ä¿¡æ¯
3. å¯¹æ¯” llama_cpp çš„è¾“å‡º

#### Stage 4.2: Q/K å½’ä¸€åŒ–éªŒè¯

**æµ‹è¯•ç”¨ä¾‹**:
```cpp
TEST(Stage4_2, QKNormalization) {
    // éªŒè¯ Q/K norm æ­£ç¡®åº”ç”¨
    // éªŒè¯å¹¿æ’­æ­£ç¡®
    // éªŒè¯æ•°å€¼èŒƒå›´
}
```

**éªŒè¯ç‚¹**:
- Q norm æƒé‡å½¢çŠ¶: `[128]`
- K norm æƒé‡å½¢çŠ¶: `[128]`
- å½’ä¸€åŒ–åæ•°å€¼èŒƒå›´åˆç†
- ä¸ llama_cpp å¯¹æ¯”

#### Stage 4.3: RoPE éªŒè¯

**æµ‹è¯•ç”¨ä¾‹**:
```cpp
TEST(Stage4_3, RoPE) {
    // éªŒè¯ RoPE å‚æ•°
    // - freq_base = 1000000
    // - n_rot = 128
    // - n_ctx_orig = 40960
    
    // éªŒè¯ä½ç½®ç¼–ç æ­£ç¡®åº”ç”¨
    // å¯¹æ¯”ä¸åŒä½ç½®çš„ Q/K å€¼
}
```

**éªŒè¯æ–¹æ³•**:
```cpp
// åœ¨ buildAttentionGraph ä¸­
CLLM_DEBUG("[Attention L%zu] RoPE Q before: first 5 values: %.6f %.6f %.6f %.6f %.6f",
           layerIdx, qData[0], qData[1], qData[2], qData[3], qData[4]);

// åº”ç”¨ RoPE
q = ggml_rope_ext(...);

// éªŒè¯ RoPE å
CLLM_DEBUG("[Attention L%zu] RoPE Q after: first 5 values: %.6f %.6f %.6f %.6f %.6f",
           layerIdx, qData[0], qData[1], qData[2], qData[3], qData[4]);
```

#### Stage 4.4: KV Cache éªŒè¯

**æµ‹è¯•ç”¨ä¾‹**:
```cpp
TEST(Stage4_4, KVCache) {
    // é¦–æ¬¡æ¨ç†
    forward({9707});  // startPos=0
    
    // éªŒè¯ KV Cache å†™å…¥
    // - K cache å½¢çŠ¶: [128, 2048, 8]
    // - V cache å½¢çŠ¶: [128, 2048, 8]
    // - æ•°æ®æ­£ç¡®å†™å…¥ä½ç½® 0
    
    // å¢é‡æ¨ç†
    forward({11});  // startPos=1
    
    // éªŒè¯ KV Cache è¯»å–
    // - ä» cache è¯»å–ä½ç½® 0 çš„æ•°æ®
    // - æ–°æ•°æ®å†™å…¥ä½ç½® 1
    // - totalLen = 2
}
```

**éªŒè¯æ–¹æ³•**:
```cpp
// åœ¨ flushKVCache() åéªŒè¯
bool verifyKVCache(size_t layerIdx, size_t expectedLen) {
    auto kCache = kCaches_[layerIdx];
    auto vCache = vCaches_[layerIdx];
    
    // éªŒè¯å½¢çŠ¶
    // éªŒè¯æ•°æ®å®Œæ•´æ€§ï¼ˆæ—  NaN/Infï¼‰
    // éªŒè¯æ•°æ®èŒƒå›´
    return true;
}
```

#### Stage 4.5: GQA æ‰©å±•éªŒè¯

**æµ‹è¯•ç”¨ä¾‹**:
```cpp
TEST(Stage4_5, GQAExpansion) {
    // éªŒè¯ GQA æ‰©å±•
    // - è¾“å…¥: K[128, total_len, 8], V[128, total_len, 8]
    // - è¾“å‡º: K[128, total_len, 16], V[128, total_len, 16]
    // - éªŒè¯ head æ˜ å°„: Q head i -> KV head i/2
    
    // éªŒè¯æ‰©å±•åçš„æ•°æ®æ­£ç¡®
    // éªŒè¯ head é¡ºåºæ­£ç¡®
}
```

**éªŒè¯æ–¹æ³•**:
```cpp
// åœ¨ buildAttentionGraph ä¸­
if (nKVHeads < nHeads) {
    CLLM_DEBUG("[Attention L%zu] GQA: Before expansion - K shape: [%lld, %lld, %lld]",
               layerIdx, kFull->ne[0], kFull->ne[1], kFull->ne[2]);
    
    // GQA æ‰©å±•
    kExpanded = ...;
    
    CLLM_DEBUG("[Attention L%zu] GQA: After expansion - K shape: [%lld, %lld, %lld]",
               layerIdx, kExpanded->ne[0], kExpanded->ne[1], kExpanded->ne[2]);
    
    // éªŒè¯ head æ˜ å°„
    // head 0,1 -> KV head 0
    // head 2,3 -> KV head 1
    // ...
}
```

#### Stage 4.6: æ³¨æ„åŠ›åˆ†æ•°è®¡ç®—éªŒè¯

**æµ‹è¯•ç”¨ä¾‹**:
```cpp
TEST(Stage4_6, AttentionScores) {
    // éªŒè¯ Q@K^T è®¡ç®—
    // - Q: [128, seq_len, 16]
    // - K: [128, total_len, 16]
    // - Scores: [total_len, seq_len, 16]
    
    // éªŒè¯ç¼©æ”¾
    // - scale = 1/sqrt(128) â‰ˆ 0.0884
    
    // éªŒè¯å› æœ mask
    // - ä½ç½® i ä¸èƒ½çœ‹åˆ°ä½ç½® j (j > i + startPos)
    
    // éªŒè¯ softmax
    // - æ¯è¡Œçš„å’Œ = 1
    // - æ‰€æœ‰å€¼ >= 0
}
```

**éªŒè¯æ–¹æ³•**:
```cpp
// åœ¨ buildAttentionGraph ä¸­
ggml_tensor* scores = ggml_mul_mat(ctx, kExpanded, q);
CLLM_DEBUG("[Attention L%zu] Scores shape: [%lld, %lld, %lld]",
           layerIdx, scores->ne[0], scores->ne[1], scores->ne[2]);

scores = ggml_scale(ctx, scores, scale);
// æ‰“å°ç¼©æ”¾åçš„ç»Ÿè®¡

scores = ggml_diag_mask_inf(ctx, scores, startPos);
// éªŒè¯ mask æ­£ç¡®åº”ç”¨

ggml_tensor* attnWeights = ggml_soft_max(ctx, scores);
// éªŒè¯ softmax åæ¯è¡Œå’Œ = 1
```

#### Stage 4.7: æ³¨æ„åŠ›è¾“å‡ºéªŒè¯

**æµ‹è¯•ç”¨ä¾‹**:
```cpp
TEST(Stage4_7, AttentionOutput) {
    // éªŒè¯ Attention@V
    // - attnWeights: [total_len, seq_len, 16]
    // - V: [128, total_len, 16]
    // - Output: [128, seq_len, 16]
    
    // éªŒè¯è¾“å‡ºæŠ•å½±
    // - Output: [2048, seq_len]
}
```

---

## Stage 5: FFN è®¡ç®—éªŒè¯ â³

### å®æ–½çŠ¶æ€
- â³ å¾…å®ç°

### æµ‹è¯•ç”¨ä¾‹

#### ç”¨ä¾‹ 5.1: FFN å®Œæ•´æµç¨‹
```cpp
TEST(Stage5, FFNComputation) {
    // éªŒè¯ FFN å½’ä¸€åŒ–
    // éªŒè¯ Gate/Up æŠ•å½±
    // éªŒè¯ SiLU æ¿€æ´»
    // éªŒè¯ Down æŠ•å½±
    // éªŒè¯ SwiGLU ç»„åˆ
}
```

### éªŒè¯æ–¹æ³•

**æ·»åŠ  FFN è°ƒè¯•èŠ‚ç‚¹**
```cpp
// åœ¨ buildFFNGraph ä¸­
ggml_tensor* gate = ggml_mul_mat(ctx, layer.wGate, input);
ggml_tensor* up = ggml_mul_mat(ctx, layer.wUp, input);

// ä¿å­˜ä¸­é—´èŠ‚ç‚¹
debugFFNGate_ = gate;
debugFFNUp_ = up;

gate = ggml_silu(ctx, gate);
ggml_tensor* hidden = ggml_mul(ctx, gate, up);
debugFFNHidden_ = hidden;

ggml_tensor* output = ggml_mul_mat(ctx, layer.wDown, hidden);
debugFFNOutput_ = output;
```

---

## Stage 6: å¤šå±‚ç´¯ç§¯éªŒè¯ â³

### å®æ–½çŠ¶æ€
- â³ å¾…å®ç°

### æµ‹è¯•ç”¨ä¾‹

#### ç”¨ä¾‹ 6.1: é€å±‚è¾“å‡ºéªŒè¯
```cpp
TEST(Stage6, MultiLayerOutput) {
    // éªŒè¯æ¯ä¸€å±‚çš„è¾“å‡º
    // æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
    // æ£€æŸ¥æ®‹å·®è¿æ¥
}
```

### éªŒè¯æ–¹æ³•

**ä¿å­˜æ¯å±‚è¾“å‡º**
```cpp
// åœ¨ buildForwardGraph ä¸­
std::vector<ggml_tensor*> layerOutputs;
for (size_t i = 0; i < config_.blockCount; ++i) {
    hidden_states = buildLayerGraph(...);
    layerOutputs.push_back(hidden_states);
    
    // æ‰“å°æ¯å±‚ç»Ÿè®¡
    if (layerOutputs[i] && layerOutputs[i]->data) {
        printTensorStats("Layer " + std::to_string(i), 
                        layerOutputs[i], i);
    }
}
```

---

## Stage 7: æœ€ç»ˆè¾“å‡ºéªŒè¯ ğŸ”„

### å®æ–½çŠ¶æ€
- ğŸ”„ éƒ¨åˆ†å®Œæˆï¼ˆå·²æœ‰ logits ç»Ÿè®¡ï¼‰

### æµ‹è¯•ç”¨ä¾‹

#### ç”¨ä¾‹ 7.1: æœ€ç»ˆå½’ä¸€åŒ–
```cpp
TEST(Stage7, FinalNormalization) {
    // éªŒè¯æœ€ç»ˆ RMSNorm
    // éªŒè¯ outputNorm æƒé‡åº”ç”¨
}
```

#### ç”¨ä¾‹ 7.2: LM Head
```cpp
TEST(Stage7, LMHead) {
    // éªŒè¯ LM Head æŠ•å½±
    // éªŒè¯ logits å½¢çŠ¶ [seq_len, vocab]
    // éªŒè¯ logits æ•°å€¼èŒƒå›´
    // éªŒè¯ top-k tokens
}
```

---

## Stage 8: å¢é‡æ¨ç†éªŒè¯ âœ…

### å®æ–½çŠ¶æ€
- âœ… å·²å®Œå–„ï¼ˆ2026-01-23ï¼‰

### æ–°å¢æ¥å£

#### KV Cache éªŒè¯æ¥å£
```cpp
// åœ¨ GGMLTransformerModel ä¸­æ–°å¢

struct KVCacheStats {
    size_t layerIdx;          // å±‚ç´¢å¼•
    size_t headDim;           // head ç»´åº¦
    size_t maxSeq;            // æœ€å¤§åºåˆ—é•¿åº¦
    size_t nKVHeads;          // KV head æ•°é‡
    size_t currentLen;        // å½“å‰æœ‰æ•ˆé•¿åº¦
    TensorStats kStats;       // K cache ç»Ÿè®¡
    TensorStats vStats;       // V cache ç»Ÿè®¡
    bool isValid;             // æ˜¯å¦æœ‰æ•ˆï¼ˆæ—  NaN/Infï¼‰
};

// è·å–æŒ‡å®šå±‚çš„ KV Cache ç»Ÿè®¡
KVCacheStats getKVCacheStats(size_t layerIdx) const;

// è·å–æ‰€æœ‰å±‚çš„ KV Cache ç»Ÿè®¡
std::vector<KVCacheStats> getAllKVCacheStats() const;

// éªŒè¯ KV Cache æ•°æ®å®Œæ•´æ€§
bool validateKVCacheIntegrity(size_t expectedLen) const;

// è·å–æŒ‡å®šä½ç½®çš„ KV æ•°æ®
bool getKVAtPosition(size_t layerIdx, size_t position, 
                     std::vector<float>& kData, std::vector<float>& vData) const;
```

### æµ‹è¯•ç”¨ä¾‹

#### ç”¨ä¾‹ 8.1: é¦–æ¬¡æ¨ç†
```cpp
TEST(Stage8, FirstInference) {
    model.clearKVCache();
    std::vector<int32_t> firstToken = {9707};  // "Hello"
    
    auto firstLogits = model.forward(firstToken);
    
    // éªŒè¯ logits å½¢çŠ¶
    ASSERT_EQ(firstLogits.size(), vocabSize);
    
    // éªŒè¯ KV Cache é•¿åº¦
    size_t kvCacheLen = model.getKVCacheLength();
    ASSERT_EQ(kvCacheLen, 1);
    
    // éªŒè¯ KV Cache æ•°æ®å®Œæ•´æ€§
    ASSERT_TRUE(model.validateKVCacheIntegrity(kvCacheLen));
    
    // éªŒè¯ Layer 0 KV Cache ç»Ÿè®¡
    auto layer0Stats = model.getKVCacheStats(0);
    ASSERT_TRUE(layer0Stats.isValid);
    ASSERT_EQ(layer0Stats.kStats.nanCount, 0);
    ASSERT_EQ(layer0Stats.kStats.infCount, 0);
}
```

#### ç”¨ä¾‹ 8.2: å¢é‡æ¨ç†
```cpp
TEST(Stage8, IncrementalInference) {
    model.clearKVCache();
    std::vector<int32_t> tokens = {9707, 11, 1234};
    
    for (size_t i = 0; i < tokens.size(); ++i) {
        auto tokenLogits = model.forwardOneToken(tokens[i], i);
        
        // éªŒè¯ logits å½¢çŠ¶
        ASSERT_EQ(tokenLogits.size(), vocabSize);
        
        // éªŒè¯ KV Cache é•¿åº¦é€’å¢
        ASSERT_EQ(model.getKVCacheLength(), i + 1);
        
        // éªŒè¯ KV Cache æ•°æ®å®Œæ•´æ€§
        ASSERT_TRUE(model.validateKVCacheIntegrity(i + 1));
    }
}
```

#### ç”¨ä¾‹ 8.3: æ‰¹é‡æ¨ç† vs å¢é‡æ¨ç†ä¸€è‡´æ€§
```cpp
TEST(Stage8, BatchVsIncrementalConsistency) {
    std::vector<int32_t> tokens = {9707, 11, 1234};
    
    // å¢é‡æ¨ç†
    model.clearKVCache();
    std::vector<std::vector<float>> incrementalLogits;
    for (size_t i = 0; i < tokens.size(); ++i) {
        incrementalLogits.push_back(model.forwardOneToken(tokens[i], i));
    }
    
    // æ‰¹é‡æ¨ç†
    model.clearKVCache();
    auto batchLogits = model.forward(tokens);
    
    // å¯¹æ¯”æœ€åä¸€ä¸ªä½ç½®çš„ logits
    size_t lastPos = tokens.size() - 1;
    std::vector<float> batchLastLogits(
        batchLogits.begin() + lastPos * vocabSize,
        batchLogits.begin() + (lastPos + 1) * vocabSize
    );
    
    // éªŒè¯ä¸€è‡´æ€§ï¼ˆå®¹å·® 1e-2ï¼‰
    float maxDiff = 0.0f;
    for (size_t i = 0; i < vocabSize; ++i) {
        float diff = std::abs(batchLastLogits[i] - incrementalLogits.back()[i]);
        maxDiff = std::max(maxDiff, diff);
    }
    ASSERT_LT(maxDiff, 1e-2f);
}
```

#### ç”¨ä¾‹ 8.4: ä¸­é—´æ­¥éª¤ä¸€è‡´æ€§
```cpp
TEST(Stage8, IntermediateStepConsistency) {
    std::vector<int32_t> tokens = {9707, 11, 1234};
    
    // å…ˆåšå¢é‡æ¨ç†ä¿å­˜ç»“æœ
    model.clearKVCache();
    std::vector<std::vector<float>> incrementalLogits;
    for (size_t i = 0; i < tokens.size(); ++i) {
        incrementalLogits.push_back(model.forwardOneToken(tokens[i], i));
    }
    
    // å¯¹æ¯ä¸ªä¸­é—´æ­¥éª¤éªŒè¯
    for (size_t step = 1; step < tokens.size(); ++step) {
        model.clearKVCache();
        std::vector<int32_t> partialTokens(tokens.begin(), tokens.begin() + step + 1);
        auto partialBatchLogits = model.forward(partialTokens);
        
        // æå–æœ€åä¸€ä¸ªä½ç½®
        std::vector<float> partialBatchLast(
            partialBatchLogits.begin() + step * vocabSize,
            partialBatchLogits.begin() + (step + 1) * vocabSize
        );
        
        // éªŒè¯ä¸€è‡´æ€§
        float maxDiff = 0.0f;
        for (size_t i = 0; i < vocabSize; ++i) {
            maxDiff = std::max(maxDiff, std::abs(partialBatchLast[i] - incrementalLogits[step][i]));
        }
        ASSERT_LT(maxDiff, 1e-2f);
    }
}
```

#### ç”¨ä¾‹ 8.5: KV Cache ä½ç½®æ•°æ®éªŒè¯
```cpp
TEST(Stage8, KVCachePositionDataConsistency) {
    // é¦–æ¬¡æ¨ç†å•ä¸ªtoken
    model.clearKVCache();
    model.forward({9707});
    
    std::vector<float> firstKData, firstVData;
    model.getKVAtPosition(0, 0, firstKData, firstVData);
    
    // å¢é‡æ¨ç†å¤šä¸ªtoken
    model.clearKVCache();
    model.forwardOneToken(9707, 0);
    model.forwardOneToken(11, 1);
    model.forwardOneToken(1234, 2);
    
    std::vector<float> incrKData, incrVData;
    model.getKVAtPosition(0, 0, incrKData, incrVData);
    
    // éªŒè¯ä½ç½®0çš„KVæ•°æ®ä¸€è‡´
    float maxDiff = 0.0f;
    for (size_t i = 0; i < firstKData.size(); ++i) {
        maxDiff = std::max(maxDiff, std::abs(firstKData[i] - incrKData[i]));
        maxDiff = std::max(maxDiff, std::abs(firstVData[i] - incrVData[i]));
    }
    ASSERT_LT(maxDiff, 1e-5f);
}
```

#### ç”¨ä¾‹ 8.6: æ‰€æœ‰å±‚ KV Cache éªŒè¯
```cpp
TEST(Stage8, AllLayersKVCacheValidation) {
    model.clearKVCache();
    model.forward({9707, 11, 1234});
    
    auto allLayerStats = model.getAllKVCacheStats();
    
    for (const auto& stats : allLayerStats) {
        ASSERT_TRUE(stats.isValid) << "Layer " << stats.layerIdx << " KV cache is invalid";
        ASSERT_EQ(stats.kStats.nanCount, 0);
        ASSERT_EQ(stats.kStats.infCount, 0);
        ASSERT_EQ(stats.vStats.nanCount, 0);
        ASSERT_EQ(stats.vStats.infCount, 0);
    }
}
```

### éªŒè¯ç‚¹
- [x] é¦–æ¬¡æ¨ç† KV Cache å†™å…¥ä½ç½® 0
- [x] å¢é‡æ¨ç† KV Cache é•¿åº¦é€’å¢
- [x] KV Cache æ•°æ®å®Œæ•´æ€§ï¼ˆæ—  NaN/Infï¼‰
- [x] æ‰¹é‡æ¨ç†ä¸å¢é‡æ¨ç†è¾“å‡ºä¸€è‡´ï¼ˆå®¹å·® < 1e-2ï¼‰
- [x] ä¸­é—´æ­¥éª¤è¾“å‡ºä¸€è‡´æ€§
- [x] KV Cache ä½ç½®æ•°æ®ä¸€è‡´æ€§
- [x] æ‰€æœ‰å±‚ KV Cache æœ‰æ•ˆæ€§

### é¢„æœŸç»“æœ
- æ¯æ¬¡æ¨ç†å KV Cache é•¿åº¦æ­£ç¡®é€’å¢
- æ‰€æœ‰ KV Cache æ•°æ®æ—  NaN/Inf
- æ‰¹é‡æ¨ç†å’Œå¢é‡æ¨ç†çš„ logits å·®å¼‚ < 1e-2
- ç›¸åŒ token åœ¨ç›¸åŒä½ç½®çš„ KV æ•°æ®å·®å¼‚ < 1e-5

---

## Stage 9: ç«¯åˆ°ç«¯å¯¹æ¯” ğŸ”„

### å®æ–½çŠ¶æ€
- ğŸ”„ è¿›è¡Œä¸­

### æµ‹è¯•ç”¨ä¾‹

#### ç”¨ä¾‹ 9.1: è¾“å‡ºæ–‡æœ¬å¯¹æ¯”
```cpp
TEST(Stage9, OutputComparison) {
    // è¿è¡Œ Kylin
    auto kylinText = generate("Hi", maxTokens=5, temp=0.0);
    
    // è¿è¡Œ llama_cpp
    auto llamaText = generateLlamaCpp("Hi", maxTokens=5, temp=0.0);
    
    // å¯¹æ¯”è¾“å‡º
    ASSERT_EQ(kylinText, llamaText);
}
```

#### ç”¨ä¾‹ 9.2: Logits å¯¹æ¯”
```cpp
TEST(Stage9, LogitsComparison) {
    // å¯¹æ¯” logits åˆ†å¸ƒ
    // å¯¹æ¯” top-k tokens
    // å¯¹æ¯”æ•°å€¼å·®å¼‚
}
```

---

## å®æ–½ä¼˜å…ˆçº§

### é«˜ä¼˜å…ˆçº§ï¼ˆç«‹å³å®æ–½ï¼‰

1. **å®Œå–„ Stage 2-3 çš„éªŒè¯**
   - æ·»åŠ  embedding å’Œ Layer 0 çš„è¯¦ç»†å¯¹æ¯”
   - å®ç°ä¸ llama_cpp çš„è‡ªåŠ¨å¯¹æ¯”

2. **å®ç° Stage 4 çš„è¯¦ç»†éªŒè¯**
   - æ·»åŠ æ³¨æ„åŠ›è®¡ç®—çš„æ¯ä¸ªæ­¥éª¤çš„æ—¥å¿—
   - å®ç°ä¸­é—´èŠ‚ç‚¹çš„ç»Ÿè®¡æ‰“å°

3. **åˆ›å»ºæµ‹è¯•æ¡†æ¶**
   - å®Œå–„ `kylin_stage_test.cpp`
   - å®ç°è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬

### ä¸­ä¼˜å…ˆçº§ï¼ˆ1-2å¤©å†…ï¼‰

4. **å®ç° Stage 5-6 çš„éªŒè¯**
   - æ·»åŠ  FFN çš„è¯¦ç»†æ—¥å¿—
   - å®ç°å¤šå±‚è¾“å‡ºçš„éªŒè¯

5. ~~**å®ç° Stage 8 çš„éªŒè¯**~~ï¼ˆå·²å®Œæˆ 2026-01-23ï¼‰
   - âœ… æ·»åŠ å¢é‡æ¨ç†çš„æµ‹è¯•ç”¨ä¾‹
   - âœ… éªŒè¯ KV Cache çš„æ­£ç¡®æ€§
   - âœ… æ·»åŠ  KV Cache ç»Ÿè®¡æ¥å£
   - âœ… å®ç°æ‰¹é‡æ¨ç† vs å¢é‡æ¨ç†ä¸€è‡´æ€§éªŒè¯

### ä½ä¼˜å…ˆçº§ï¼ˆ3-5å¤©å†…ï¼‰

6. **æ€§èƒ½åŸºå‡†æµ‹è¯•**
   - è®°å½•æ¯ä¸ªé˜¶æ®µçš„æ€§èƒ½
   - å¯¹æ¯” llama_cpp çš„æ€§èƒ½

7. **è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆ**
   - ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š
   - å¯è§†åŒ–æ•°å€¼åˆ†å¸ƒ

---

## å·¥å…·å’Œè„šæœ¬

### 1. åˆ†é˜¶æ®µæµ‹è¯•ç¨‹åº

**æ–‡ä»¶**: `tools/kylin_stage_test.cpp`

**åŠŸèƒ½**:
- æŒ‰é˜¶æ®µæ‰§è¡Œæµ‹è¯•
- è‡ªåŠ¨ç”ŸæˆæŠ¥å‘Š
- æ”¯æŒä¸ llama_cpp å¯¹æ¯”

**ä½¿ç”¨æ–¹æ³•**:
```bash
./build/tools/kylin_stage_test <model_path> [prompt] [max_tokens] [temperature]
```

### 2. è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬

**æ–‡ä»¶**: `tools/run_kylin_stages.sh`

**åŠŸèƒ½**:
- è‡ªåŠ¨è¿è¡Œæ‰€æœ‰é˜¶æ®µ
- ç”Ÿæˆé˜¶æ®µæŠ¥å‘Š
- å¤±è´¥æ—¶åœæ­¢å¹¶æŠ¥å‘Š

**ä½¿ç”¨æ–¹æ³•**:
```bash
./tools/run_kylin_stages.sh [model_path] [prompt] [max_tokens] [temperature]
```

### 3. æ—¥å¿—åˆ†æå·¥å…·

**æ–‡ä»¶**: `tools/analyze_kylin_stages.py`

**åŠŸèƒ½**:
- ä»æ—¥å¿—ä¸­æå–å„é˜¶æ®µçš„ç»Ÿè®¡ä¿¡æ¯
- ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
- å¯è§†åŒ–æ•°å€¼åˆ†å¸ƒ

**ä½¿ç”¨æ–¹æ³•**:
```bash
python3 tools/analyze_kylin_stages.py /tmp/kylin_test.log
```

---

## æˆåŠŸæ ‡å‡†

### æ¯ä¸ªé˜¶æ®µçš„æˆåŠŸæ ‡å‡†

1. **æ— é”™è¯¯**: æ— å´©æºƒã€æ— å¼‚å¸¸ã€æ— æ–­è¨€å¤±è´¥
2. **æ•°å€¼åˆç†**: æ—  NaN/Infï¼Œæ•°å€¼èŒƒå›´åˆç†
3. **å½¢çŠ¶æ­£ç¡®**: æ‰€æœ‰å¼ é‡å½¢çŠ¶ç¬¦åˆé¢„æœŸ
4. **å¯¹æ¯”ä¸€è‡´**: ä¸ llama_cpp çš„è¾“å‡ºä¸€è‡´ï¼ˆè¯¯å·® < 1e-3ï¼‰

### æ•´ä½“æˆåŠŸæ ‡å‡†

1. **æ‰€æœ‰é˜¶æ®µé€šè¿‡**: Stage 0-9 å…¨éƒ¨é€šè¿‡
2. **è¾“å‡ºæ­£ç¡®**: ç”Ÿæˆçš„æ–‡æœ¬ä¸ llama_cpp ä¸€è‡´
3. **æ€§èƒ½å¯æ¥å—**: æ¨ç†é€Ÿåº¦åœ¨å¯æ¥å—èŒƒå›´å†…ï¼ˆ> 50 tokens/secï¼‰

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.1  
**åˆ›å»ºæ—¶é—´**: 2026-01-23  
**æœ€åæ›´æ–°**: 2026-01-23

### æ›´æ–°å†å²
- v1.1 (2026-01-23): å®Œå–„ Stage 8 å¢é‡æ¨ç†éªŒè¯ï¼Œæ·»åŠ  KV Cache è¯¦ç»†éªŒè¯æ¥å£
- v1.0 (2026-01-23): åˆå§‹ç‰ˆæœ¬
