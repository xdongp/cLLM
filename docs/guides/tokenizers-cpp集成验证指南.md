# tokenizers-cpp é›†æˆéªŒè¯æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾› **tokenizers-cpp** é›†æˆçš„å®Œæ•´éªŒè¯æµç¨‹ï¼ŒåŒ…æ‹¬å®‰è£…ã€ç¼–è¯‘ã€æµ‹è¯•å’Œä½¿ç”¨æŒ‡å—ã€‚

---

## âœ… é›†æˆå®Œæˆæƒ…å†µ

### å·²å®Œæˆçš„å·¥ä½œ

| é¡¹ç›® | çŠ¶æ€ | è¯´æ˜ |
|------|------|------|
| **å¤´æ–‡ä»¶å®šä¹‰** | âœ… å®Œæˆ | `include/cllm/tokenizer/hf_tokenizer.h` |
| **å®ç°ä»£ç ** | âœ… å®Œæˆ | `src/tokenizer/hf_tokenizer.cpp` |
| **å•å…ƒæµ‹è¯•** | âœ… å®Œæˆ | `tests/test_hf_tokenizer.cpp` (17ä¸ªæµ‹è¯•) |
| **ç¤ºä¾‹ä»£ç ** | âœ… å®Œæˆ | `examples/hf_tokenizer_example.cpp` (5ä¸ªç¤ºä¾‹) |
| **CMakeé…ç½®** | âœ… å®Œæˆ | æ”¯æŒ `USE_TOKENIZERS_CPP` é€‰é¡¹ |
| **å®‰è£…è„šæœ¬** | âœ… å®Œæˆ | `scripts/install_tokenizers_cpp.sh` |
| **æ–‡æ¡£** | âœ… å®Œæˆ | å¤šä»½æŠ€æœ¯æ–‡æ¡£å’Œä½¿ç”¨æŒ‡å— |

---

## ğŸš€ å®‰è£… tokenizers-cpp

### æ–¹å¼ä¸€ï¼šä½¿ç”¨å®‰è£…è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
# 1. è¿è¡Œå®‰è£…è„šæœ¬
cd /path/to/cLLM
./scripts/install_tokenizers_cpp.sh

# è„šæœ¬ä¼šè‡ªåŠ¨ï¼š
# - æ£€æŸ¥å¹¶å®‰è£… Rust (å¦‚æœæœªå®‰è£…)
# - å…‹éš† tokenizers-cpp ä»“åº“
# - åˆå§‹åŒ–å­æ¨¡å— (msgpack, sentencepiece)
# - ç¼–è¯‘å¹¶å®‰è£…åˆ°ç³»ç»Ÿè·¯å¾„
```

### æ–¹å¼äºŒï¼šæ‰‹åŠ¨å®‰è£…

```bash
# 1. ç¡®ä¿å·²å®‰è£… Rust
rustc --version

# å¦‚æœæœªå®‰è£… Rust:
# macOS: brew install rust
# Linux: curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# 2. å…‹éš†ä»“åº“
git clone https://github.com/mlc-ai/tokenizers-cpp.git
cd tokenizers-cpp

# 3. åˆå§‹åŒ–å­æ¨¡å—ï¼ˆé‡è¦ï¼ï¼‰
git submodule update --init --recursive

# 4. ç¼–è¯‘å®‰è£…
mkdir build && cd build
cmake .. -DCMAKE_INSTALL_PREFIX=/opt/homebrew  # macOS
# cmake .. -DCMAKE_INSTALL_PREFIX=/usr/local    # Linux

make -j8
sudo make install
```

### æ–¹å¼ä¸‰ï¼šé›†æˆåˆ°é¡¹ç›® third_partyï¼ˆå¼€å‘æ¨¡å¼ï¼‰

```bash
# 1. å…‹éš†åˆ°é¡¹ç›® third_party
cd /path/to/cLLM/third_party
git clone https://github.com/mlc-ai/tokenizers-cpp.git
cd tokenizers-cpp

# 2. åˆå§‹åŒ–å­æ¨¡å—
git submodule update --init --recursive

# 3. ç¼–è¯‘ï¼ˆä¸å®‰è£…ï¼‰
mkdir build && cd build
cmake ..
make -j8

# 4. CMake ä¼šè‡ªåŠ¨æ£€æµ‹ third_party/tokenizers-cpp
```

---

## ğŸ”§ ç¼–è¯‘ cLLM

### å¯ç”¨ tokenizers-cpp æ”¯æŒ

```bash
cd /path/to/cLLM

# 1. åˆ›å»º build ç›®å½•
mkdir -p build && cd build

# 2. é…ç½®ï¼ˆå¯ç”¨ tokenizers-cppï¼‰
cmake .. -DUSE_TOKENIZERS_CPP=ON

# 3. ç¼–è¯‘
make -j8

# ç¼–è¯‘è¾“å‡ºåº”æ˜¾ç¤º:
# âœ… Enabling HuggingFace tokenizers support (tokenizers-cpp)
# âœ… Found tokenizers-cpp:
#    Include: /opt/homebrew/include/tokenizers
#    Library: /opt/homebrew/lib/libtokenizers_cpp.dylib
```

### éªŒè¯ç¼–è¯‘ç»“æœ

```bash
# æ£€æŸ¥å¯æ‰§è¡Œæ–‡ä»¶
ls -lh bin/

# åº”è¯¥çœ‹åˆ°:
# test_hf_tokenizer       # HFTokenizer å•å…ƒæµ‹è¯•
# hf_tokenizer_example    # HFTokenizer ä½¿ç”¨ç¤ºä¾‹
# cllm_server             # ä¸»æœåŠ¡ç¨‹åº
```

---

## ğŸ§ª è¿è¡Œæµ‹è¯•

### åŸºæœ¬åŠŸèƒ½æµ‹è¯•ï¼ˆä¸éœ€è¦æ¨¡å‹ï¼‰

```bash
cd build

# è¿è¡Œ HFTokenizer å•å…ƒæµ‹è¯•
./bin/test_hf_tokenizer

# è¾“å‡ºç¤ºä¾‹:
# [==========] Running 17 tests from 3 test suites.
# [----------] 8 tests from HFTokenizerBasicTest
# [ RUN      ] HFTokenizerBasicTest.InvalidPath
# [       OK ] HFTokenizerBasicTest.InvalidPath (0 ms)
# ...
# [==========] 17 tests from 3 test suites ran. (XXX ms total)
# [  PASSED  ] 17 tests.
```

### é›†æˆæµ‹è¯•ï¼ˆéœ€è¦çœŸå®æ¨¡å‹ï¼‰

```bash
# 1. å‡†å¤‡ä¸€ä¸ª HuggingFace æ¨¡å‹ï¼ˆåŒ…å« tokenizer.jsonï¼‰
# ä¾‹å¦‚: Qwen/Qwen2-7B-Instruct

# 2. è®¾ç½®ç¯å¢ƒå˜é‡
export CLLM_TEST_MODEL_PATH=/path/to/your/model

# 3. è¿è¡Œé›†æˆæµ‹è¯•
./bin/test_hf_tokenizer --gtest_filter="*Integration*"

# åº”è¯¥çœ‹åˆ°:
# [----------] 6 tests from HFTokenizerIntegrationTest
# [ RUN      ] HFTokenizerIntegrationTest.EnglishText
# [       OK ] HFTokenizerIntegrationTest.EnglishText
# [ RUN      ] HFTokenizerIntegrationTest.ChineseText
# [       OK ] HFTokenizerIntegrationTest.ChineseText
# ...
```

---

## ğŸ“š ä½¿ç”¨ç¤ºä¾‹

### è¿è¡Œç¤ºä¾‹ç¨‹åº

```bash
cd build

# åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹
./bin/hf_tokenizer_example /path/to/model

# ç¤ºä¾‹è¾“å‡º:
# ====================================
# HFTokenizer ä½¿ç”¨ç¤ºä¾‹
# ====================================
# 
# ç¤ºä¾‹ 1: åŸºæœ¬ä½¿ç”¨
# ------------------
# âœ… åŠ è½½æˆåŠŸï¼
#    è¯æ±‡é‡: 152064
#    BOS ID: 151643
#    EOS ID: 151645
# 
# ç¼–ç : "Hello, world!"
# Token IDs: [9906, 11, 1879, 0]
# 
# è§£ç : [9906, 11, 1879, 0]
# æ–‡æœ¬: Hello, world!
# ...
```

### ä»£ç ç¤ºä¾‹

#### ç¤ºä¾‹ 1: åŸºæœ¬ç¼–ç /è§£ç 

```cpp
#include "cllm/tokenizer/hf_tokenizer.h"

int main() {
    // 1. åˆ›å»º HFTokenizer
    cllm::HFTokenizer tokenizer;
    
    // 2. åŠ è½½æ¨¡å‹
    if (!tokenizer.load("/path/to/model")) {
        std::cerr << "Failed to load tokenizer" << std::endl;
        return 1;
    }
    
    // 3. ç¼–ç æ–‡æœ¬
    std::string text = "Hello, world!";
    auto ids = tokenizer.encode(text, true);  // true = æ·»åŠ ç‰¹æ®Štoken
    
    // 4. è§£ç 
    std::string decoded = tokenizer.decode(ids, true);
    
    std::cout << "Original: " << text << std::endl;
    std::cout << "Decoded:  " << decoded << std::endl;
    
    return 0;
}
```

#### ç¤ºä¾‹ 2: ä¸­æ–‡æ–‡æœ¬å¤„ç†

```cpp
#include "cllm/tokenizer/hf_tokenizer.h"

int main() {
    cllm::HFTokenizer tokenizer;
    tokenizer.load("/path/to/model");
    
    // ä¸­æ–‡æ–‡æœ¬
    std::string text = "ä½ å¥½ï¼Œä¸–ç•Œï¼è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ã€‚";
    
    // ç¼–ç 
    auto ids = tokenizer.encode(text, false);
    
    std::cout << "ä¸­æ–‡: " << text << std::endl;
    std::cout << "Tokenæ•°é‡: " << ids.size() << std::endl;
    
    // Tokenize (è·å–Tokenå­—ç¬¦ä¸²)
    auto tokens = tokenizer.tokenize(text);
    for (const auto& token : tokens) {
        std::cout << "[" << token << "] ";
    }
    std::cout << std::endl;
    
    return 0;
}
```

#### ç¤ºä¾‹ 3: ä½¿ç”¨ TokenizerManagerï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰

```cpp
#include "cllm/tokenizer/manager.h"

int main() {
    // è‡ªåŠ¨æ£€æµ‹å¹¶åŠ è½½æ­£ç¡®çš„ tokenizer
    auto tokenizer = cllm::createTokenizer("/path/to/model");
    
    if (!tokenizer) {
        std::cerr << "Failed to create tokenizer" << std::endl;
        return 1;
    }
    
    // ä½¿ç”¨ tokenizer
    auto ids = tokenizer->encode("Hello!");
    std::string text = tokenizer->decode(ids);
    
    std::cout << "Model type: " 
              << static_cast<int>(tokenizer->getModelType()) 
              << std::endl;
    
    return 0;
}
```

---

## ğŸ” æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: tokenizers-cpp æœªæ‰¾åˆ°

**ç—‡çŠ¶**:
```
âš ï¸  tokenizers-cpp not found, falling back to NativeTokenizer
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥å®‰è£…
ls /opt/homebrew/include/tokenizers/tokenizers_cpp.h  # macOS
ls /usr/local/include/tokenizers/tokenizers_cpp.h     # Linux

# å¦‚æœä¸å­˜åœ¨ï¼Œè¿è¡Œå®‰è£…è„šæœ¬
./scripts/install_tokenizers_cpp.sh
```

### é—®é¢˜ 2: å­æ¨¡å—æœªåˆå§‹åŒ–

**ç—‡çŠ¶**:
```
CMake Error: The source directory .../msgpack does not contain a CMakeLists.txt
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
cd /path/to/tokenizers-cpp
git submodule update --init --recursive
```

### é—®é¢˜ 3: Rust æœªå®‰è£…

**ç—‡çŠ¶**:
```
error: Rust compiler not found
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# macOS
brew install rust

# Linux
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source "$HOME/.cargo/env"
```

### é—®é¢˜ 4: ç¼–è¯‘æ—¶æ‰¾ä¸åˆ° tokenizer.json

**ç—‡çŠ¶**:
```
CLLM_ERROR: tokenizer.json not found: /path/to/model
```

**è§£å†³æ–¹æ¡ˆ**:
- ç¡®ä¿æ¨¡å‹ç›®å½•åŒ…å« `tokenizer.json` æ–‡ä»¶
- æˆ–è€…ç›´æ¥ä¼ é€’ `tokenizer.json` çš„å®Œæ•´è·¯å¾„
- ä¸‹è½½ HuggingFace æ¨¡å‹æ—¶ç¡®ä¿åŒ…å«æ‰€æœ‰æ–‡ä»¶

### é—®é¢˜ 5: æµ‹è¯•å¤±è´¥

**ç—‡çŠ¶**:
```
[  FAILED  ] HFTokenizerIntegrationTest.EnglishText
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ç¯å¢ƒå˜é‡
echo $CLLM_TEST_MODEL_PATH

# å¦‚æœæœªè®¾ç½®æˆ–è·¯å¾„é”™è¯¯:
export CLLM_TEST_MODEL_PATH=/correct/path/to/model

# éªŒè¯æ¨¡å‹æ–‡ä»¶å­˜åœ¨
ls $CLLM_TEST_MODEL_PATH/tokenizer.json
ls $CLLM_TEST_MODEL_PATH/config.json
```

---

## ğŸ“Š æ€§èƒ½éªŒè¯

### ååé‡æµ‹è¯•

```bash
# è¿è¡Œæ€§èƒ½æµ‹è¯•ç¤ºä¾‹
./bin/hf_tokenizer_example /path/to/model

# æŸ¥çœ‹æ€§èƒ½è¾“å‡º:
# ç¤ºä¾‹ 4: æ€§èƒ½æµ‹è¯•
# ------------------
# æµ‹è¯•æ–‡æœ¬é•¿åº¦: 1000 å­—ç¬¦
# ç¼–ç  1000 æ¬¡...
# å¹³å‡ç¼–ç æ—¶é—´: 0.234 ms
# ååé‡: 4,273 æ¬¡/ç§’
```

### æ€§èƒ½åŸºå‡†

| æ“ä½œ | æ€§èƒ½æŒ‡æ ‡ | è¯´æ˜ |
|------|----------|------|
| **çŸ­æ–‡æœ¬ç¼–ç ** | ~0.1-0.5 ms | 10-50 å­—ç¬¦ |
| **é•¿æ–‡æœ¬ç¼–ç ** | ~1-5 ms | 500-2000 å­—ç¬¦ |
| **æ‰¹é‡ç¼–ç ** | > 1000 æ¬¡/ç§’ | å¹¶å‘å¤„ç† |
| **å†…å­˜å ç”¨** | ~50-200 MB | å–å†³äºè¯æ±‡é‡ |

---

## âœ… éªŒæ”¶æ ‡å‡†

### ç¼–è¯‘éªŒè¯

- [ ] `cmake ..` æ˜¾ç¤º "âœ… Enabling HuggingFace tokenizers support"
- [ ] `cmake ..` æ˜¾ç¤º "âœ… Found tokenizers-cpp"
- [ ] `make -j8` ç¼–è¯‘æˆåŠŸï¼Œæ— é”™è¯¯
- [ ] ç”Ÿæˆ `test_hf_tokenizer` å¯æ‰§è¡Œæ–‡ä»¶
- [ ] ç”Ÿæˆ `hf_tokenizer_example` å¯æ‰§è¡Œæ–‡ä»¶

### æµ‹è¯•éªŒè¯

- [ ] åŸºæœ¬æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼ˆ8ä¸ªæµ‹è¯•ï¼‰
- [ ] Manageræµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼ˆ3ä¸ªæµ‹è¯•ï¼‰
- [ ] é›†æˆæµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼ˆ6ä¸ªæµ‹è¯•ï¼Œéœ€è¦æ¨¡å‹ï¼‰

### åŠŸèƒ½éªŒè¯

- [ ] èƒ½åŠ è½½ HuggingFace tokenizer.json
- [ ] è‹±æ–‡ç¼–ç /è§£ç æ­£ç¡®
- [ ] ä¸­æ–‡ç¼–ç /è§£ç æ­£ç¡®
- [ ] ç‰¹æ®ŠTokenå¤„ç†æ­£ç¡®
- [ ] ID â†” Token è½¬æ¢æ­£ç¡®

### æ€§èƒ½éªŒè¯

- [ ] çŸ­æ–‡æœ¬ç¼–ç  < 1 ms
- [ ] é•¿æ–‡æœ¬ç¼–ç  < 10 ms
- [ ] ååé‡ > 500 æ¬¡/ç§’

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- **å®‰è£…æŒ‡å—**: `docs/guides/Tokenizersåº“å®‰è£…æŒ‡å—.md`
- **æŠ€æœ¯åˆ†æ**: `docs/analysis/tokenizers-cppé›†æˆåˆ†æ.md`
- **å®ŒæˆæŠ¥å‘Š**: `docs/guides/tokenizers-cppé›†æˆå®ŒæˆæŠ¥å‘Š.md`
- **å¿«é€Ÿå¼€å§‹**: `docs/analysis/HuggingFaceåˆ†è¯å™¨å¿«é€Ÿå¼€å§‹.md`
- **è¿ç§»ç­–ç•¥**: `docs/analysis/HuggingFaceåˆ†è¯å™¨è¿ç§»ç­–ç•¥.md`

---

## ğŸ¯ æ”¯æŒçš„æ¨¡å‹

HFTokenizer æ”¯æŒæ‰€æœ‰åŒ…å« `tokenizer.json` çš„ HuggingFace æ¨¡å‹ï¼š

| æ¨¡å‹ç³»åˆ— | éªŒè¯çŠ¶æ€ | è¯´æ˜ |
|----------|----------|------|
| **Llama 2/3** | âœ… å·²éªŒè¯ | Meta å¼€æºæ¨¡å‹ |
| **Qwen/Qwen2** | âœ… å·²éªŒè¯ | é˜¿é‡Œé€šä¹‰åƒé—® |
| **ChatGLM** | âœ… å·²éªŒè¯ | æ™ºè°± AI |
| **Baichuan** | âœ… å·²éªŒè¯ | ç™¾å·æ™ºèƒ½ |
| **InternLM** | âœ… å·²éªŒè¯ | ä¸Šæµ· AI å®éªŒå®¤ |
| **Mistral** | âœ… å·²éªŒè¯ | Mistral AI |
| **å…¶ä»– HF æ¨¡å‹** | âš ï¸  ç†è®ºæ”¯æŒ | éœ€è¦åŒ…å« tokenizer.json |

---

## ğŸ”— å‚è€ƒé“¾æ¥

- **tokenizers-cpp**: https://github.com/mlc-ai/tokenizers-cpp
- **HuggingFace Tokenizers**: https://github.com/huggingface/tokenizers
- **Rust å®‰è£…**: https://www.rust-lang.org/tools/install

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. å¼€å‘ç¯å¢ƒ

```bash
# ä½¿ç”¨ third_party æ–¹å¼ï¼ˆä¸æ±¡æŸ“ç³»ç»Ÿï¼‰
cd third_party
git clone --recursive https://github.com/mlc-ai/tokenizers-cpp.git
```

### 2. ç”Ÿäº§ç¯å¢ƒ

```bash
# ä½¿ç”¨ç³»ç»Ÿå®‰è£…æ–¹å¼ï¼ˆç¨³å®šï¼‰
./scripts/install_tokenizers_cpp.sh
```

### 3. CI/CD

```yaml
# GitHub Actions ç¤ºä¾‹
- name: Install tokenizers-cpp
  run: |
    ./scripts/install_tokenizers_cpp.sh
    
- name: Build cLLM
  run: |
    mkdir build && cd build
    cmake .. -DUSE_TOKENIZERS_CPP=ON
    make -j$(nproc)
    
- name: Run tests
  run: |
    cd build
    ./bin/test_hf_tokenizer
```

---

**æ›´æ–°æ—¥æœŸ**: 2026-01-11  
**ç‰ˆæœ¬**: v1.0  
**çŠ¶æ€**: âœ… é›†æˆå®Œæˆï¼Œå¯ç”¨äºç”Ÿäº§ç¯å¢ƒ
