# cLLM 服务器使用文档

## 概述

cLLM Server 是一个高性能的 LLM 推理服务器，支持 RESTful API 接口，提供文本生成和编码功能。

## 快速开始

### 1. 编译项目

```bash
cd /path/to/cLLM
make build
```

### 2. 启动服务器

```bash
# 基本用法
./build/bin/cllm_server --model-path /path/to/model

# 指定端口
./build/bin/cllm_server --model-path /path/to/model --port 9000

# 使用 LibTorch 后端
./build/bin/cllm_server --model-path /path/to/model --use-libtorch

# 设置日志级别和文件
./build/bin/cllm_server --model-path /path/to/model \
    --log-level debug \
    --log-file /path/to/logs/cllm.log
```

## 命令行参数

| 参数 | 说明 | 默认值 | 必需 |
|------|------|--------|------|
| `--model-path PATH` | 模型目录路径 | - | 是 |
| `--port PORT` | 服务器端口 | 8080 | 否 |
| `--host HOST` | 服务器主机地址 | 0.0.0.0 | 否 |
| `--quantization TYPE` | 量化类型 (fp16/int8/int4) | fp16 | 否 |
| `--max-batch-size SIZE` | 最大批处理大小 | 8 | 否 |
| `--max-context-length LEN` | 最大上下文长度 | 2048 | 否 |
| `--use-libtorch` | 使用 LibTorch 后端 | false (使用 Kylin) | 否 |
| `--config PATH` | 配置文件路径 | - | 否 |
| `--log-level LEVEL` | 日志级别 | info | 否 |
| `--log-file PATH` | 日志文件路径 | - | 否 |
| `--help` | 显示帮助信息 | - | 否 |

### 日志级别

- `trace`: 最详细的调试信息
- `debug`: 调试信息
- `info`: 一般信息（默认）
- `warn`: 警告信息
- `error`: 错误信息

## API 端点

### 1. 健康检查

检查服务器状态。

**请求:**
```bash
curl -X GET http://localhost:8080/health
```

**响应:**
```json
{
    "status": "healthy",
    "model_loaded": true
}
```

### 2. 文本编码

将文本编码为 token IDs。

**请求:**
```bash
curl -X POST http://localhost:8080/encode \
    -H "Content-Type: application/json" \
    -d '{
        "text": "Hello, world!"
    }'
```

**响应:**
```json
{
    "tokens": [15339, 11, 1917, 0],
    "length": 4
}
```

### 3. 文本生成

生成文本（非流式）。

**请求:**
```bash
curl -X POST http://localhost:8080/generate \
    -H "Content-Type: application/json" \
    -d '{
        "prompt": "Once upon a time",
        "max_tokens": 50,
        "temperature": 0.7,
        "top_p": 0.9
    }'
```

**响应:**
```json
{
    "id": "550e8400-e29b-41d4-a716-446655440000",
    "text": "Generated text goes here...",
    "response_time": 1.234,
    "tokens_per_second": 40.52
}
```

### 4. 流式文本生成

生成文本（流式）。

**请求:**
```bash
curl -X POST http://localhost:8080/generate_stream \
    -H "Content-Type: application/json" \
    -d '{
        "prompt": "Once upon a time",
        "max_tokens": 50,
        "temperature": 0.7,
        "stream": true
    }'
```

**响应 (Server-Sent Events):**
```
data: {"id":"550e8400...","token":"Once","done":false}

data: {"id":"550e8400...","token":" upon","done":false}

data: {"id":"550e8400...","token":" a","done":false}

data: {"id":"550e8400...","token":"","done":true}
```

## 测试

### 运行单元测试

```bash
# 运行所有测试
make test

# 运行端点测试（不需要启动服务器）
./build/bin/test_endpoints

# 运行集成测试（需要启动服务器）
./build/bin/test_server_integration
```

### 使用测试脚本

```bash
# 确保服务器已启动
./build/bin/cllm_server --model-path /path/to/model &

# 运行测试脚本
./scripts/test_server.sh

# 指定主机和端口
./scripts/test_server.sh 127.0.0.1 9000
```

## 性能优化建议

### 1. 批处理优化

增加批处理大小可以提高吞吐量：

```bash
./build/bin/cllm_server \
    --model-path /path/to/model \
    --max-batch-size 16
```

### 2. 使用 LibTorch 后端（GPU）

如果有 GPU，使用 LibTorch 后端可以显著提高性能：

```bash
./build/bin/cllm_server \
    --model-path /path/to/model \
    --use-libtorch
```

### 3. 量化

使用 INT8 或 INT4 量化可以减少内存使用和提高推理速度：

```bash
./build/bin/cllm_server \
    --model-path /path/to/model \
    --quantization int8
```

### 4. 增加上下文长度

对于长文本处理，可以增加上下文长度：

```bash
./build/bin/cllm_server \
    --model-path /path/to/model \
    --max-context-length 4096
```

## 故障排查

### 服务器无法启动

1. 检查模型路径是否正确
2. 检查端口是否被占用
3. 查看日志文件获取详细错误信息

### 推理速度慢

1. 尝试使用 LibTorch 后端（如果有 GPU）
2. 启用量化
3. 增加批处理大小
4. 减少最大上下文长度

### 内存不足

1. 使用量化（INT8 或 INT4）
2. 减少批处理大小
3. 减少最大上下文长度
4. 限制 KV 缓存大小

## 配置文件

可以使用 YAML 配置文件替代命令行参数：

```yaml
# config.yaml
server:
  port: 8080
  host: "0.0.0.0"

model:
  path: "/path/to/model"
  quantization: "fp16"

resources:
  max_batch_size: 8
  max_context_length: 2048
  kv_cache_max_size: 100
  kv_cache_max_memory_mb: 4096
```

启动服务器：

```bash
./build/bin/cllm_server --config config.yaml
```

## 生产部署

### 使用 systemd

创建 systemd 服务文件 `/etc/systemd/system/cllm.service`:

```ini
[Unit]
Description=cLLM Inference Server
After=network.target

[Service]
Type=simple
User=cllm
WorkingDirectory=/opt/cllm
ExecStart=/opt/cllm/bin/cllm_server \
    --model-path /opt/models/your-model \
    --port 8080 \
    --log-file /var/log/cllm/server.log \
    --log-level info
Restart=on-failure
RestartSec=10

[Install]
WantedBy=multi-user.target
```

启动服务：

```bash
sudo systemctl enable cllm
sudo systemctl start cllm
sudo systemctl status cllm
```

### 使用 Docker

创建 Dockerfile：

```dockerfile
FROM ubuntu:22.04
RUN apt-get update && apt-get install -y ...
COPY build/bin/cllm_server /usr/local/bin/
COPY models /opt/models
EXPOSE 8080
CMD ["cllm_server", "--model-path", "/opt/models", "--host", "0.0.0.0"]
```

构建和运行：

```bash
docker build -t cllm:latest .
docker run -p 8080:8080 cllm:latest
```

## 监控和日志

### 查看实时日志

```bash
tail -f /path/to/logs/cllm.log
```

### 监控性能指标

服务器会输出性能指标到日志：

- 请求处理时间
- Tokens per second
- 批处理大小
- 内存使用量

## 安全建议

1. **不要暴露到公网**: 使用反向代理（如 Nginx）
2. **添加认证**: 使用 API 密钥或 OAuth
3. **限制请求大小**: 防止 DoS 攻击
4. **使用 HTTPS**: 加密传输
5. **定期更新**: 保持依赖库最新

## 支持和反馈

如有问题或建议，请提交 Issue 或联系开发团队。
