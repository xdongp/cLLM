# è‡ªç ”æ¨ç†å¼•æ“è®¾è®¡æ–‡æ¡£

## ç¼–ç¨‹è§„èŒƒ

æœ¬æ¨¡å—çš„ç¼–ç å®ç°éµå¾ªä»¥ä¸‹è§„èŒƒå’Œçº¦å®šï¼š
- [C++ç¼–ç¨‹è§„èŒƒ.md](C++ç¼–ç¨‹è§„èŒƒ.md)ï¼šå®šä¹‰ç¼–ç é£æ ¼ã€å‘½åè§„èŒƒç­‰

## 0. æ–‡æ¡£æ¦‚è¿°

### 0.1 è®¾è®¡ç›®æ ‡

æœ¬æ–‡æ¡£æè¿°äº†ä¸€ä¸ªä»é›¶å¼€å§‹æ„å»ºçš„C++æ¨ç†å¼•æ“ï¼Œç±»ä¼¼llama.cppï¼Œç”¨äºé«˜æ•ˆæ‰§è¡ŒTransformeræ¨¡å‹æ¨ç†ã€‚

**æ ¸å¿ƒç›®æ ‡**ï¼š
- çº¯C++å®ç°ï¼Œæ— Pythonä¾èµ–
- æ”¯æŒQwen3ç­‰ä¸»æµTransformeræ¶æ„
- CPUä¼˜åŒ–ï¼Œå……åˆ†åˆ©ç”¨SIMDæŒ‡ä»¤
- æ”¯æŒå¤šç§é‡åŒ–æ ¼å¼ï¼ˆFP32/FP16/INT8/INT4ï¼‰
- é«˜æ€§èƒ½ï¼Œä½å»¶è¿Ÿ
- æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•

### 0.2 æŠ€æœ¯æŒ‘æˆ˜è¯„ä¼°

| æŠ€æœ¯é¢†åŸŸ | éš¾åº¦ | å·¥ä½œé‡ä¼°ç®— | å…³é”®æŒ‘æˆ˜ |
|---------|------|----------|---------|
| Transformeræ¶æ„å®ç° | â­â­â­â­â­ | 4-6å‘¨ | Multi-head attentionã€RoPEã€RMSNorm |
| æ¨¡å‹åŠ è½½å™¨ | â­â­â­â­ | 2-3å‘¨ | safetensorsè§£æã€æƒé‡æ˜ å°„ |
| SIMDä¼˜åŒ– | â­â­â­â­â­ | 3-4å‘¨ | AVX2/AVX-512çŸ©é˜µè¿ç®— |
| é‡åŒ–æ”¯æŒ | â­â­â­â­ | 2-3å‘¨ | INT8/INT4é‡åŒ–æ¨ç† |
| å†…å­˜ç®¡ç† | â­â­â­ | 1-2å‘¨ | é«˜æ•ˆå†…å­˜åˆ†é…å’ŒKV Cache |
| **æ€»è®¡** | - | **12-18å‘¨** | - |

### 0.3 å¼€å‘è·¯çº¿å›¾

```
é˜¶æ®µ1: åŸºç¡€æ¶æ„ (3å‘¨)
  â”œâ”€ å¼ é‡æŠ½è±¡å±‚
  â”œâ”€ å†…å­˜ç®¡ç†
  â””â”€ æ¨¡å‹åŠ è½½å™¨æ¡†æ¶

é˜¶æ®µ2: Transformeræ ¸å¿ƒ (6å‘¨)
  â”œâ”€ Attentionæœºåˆ¶
  â”œâ”€ Feed-Forwardç½‘ç»œ
  â”œâ”€ Normalizationå±‚
  â””â”€ Position Encoding (RoPE)

é˜¶æ®µ3: ä¼˜åŒ–ä¸é‡åŒ– (5å‘¨)
  â”œâ”€ SIMDä¼˜åŒ–
  â”œâ”€ é‡åŒ–æ”¯æŒ
  â””â”€ KV Cacheä¼˜åŒ–

é˜¶æ®µ4: é›†æˆä¸æµ‹è¯• (4å‘¨)
  â”œâ”€ é›†æˆåˆ°cLLMæ¡†æ¶
  â”œâ”€ å•å…ƒæµ‹è¯•
  â”œâ”€ æ€§èƒ½æµ‹è¯•
  â””â”€ ä¸ç°æœ‰ç³»ç»Ÿå¯¹æ¥
```

## 1. ç³»ç»Ÿæ¶æ„

### 1.1 æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   cLLM Server                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Scheduler  â”‚â”€â†’â”‚BatchManagerâ”‚â”€â†’â”‚ModelExecutorâ”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      è‡ªç ”æ¨ç†å¼•æ“ (InferenceEngine)        â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚  Layer 1: æ¨¡å‹æŠ½è±¡å±‚                       â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚  â”‚ ModelLoader  â”‚  ModelWeights         â”‚ â”‚
                    â”‚  â”‚ ModelConfig  â”‚  Tokenizer (å¤ç”¨)     â”‚ â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚  Layer 2: Transformeræ ¸å¿ƒå±‚                â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚  â”‚ TransformerModel                     â”‚ â”‚
                    â”‚  â”‚  â”œâ”€ TransformerBlock (x N layers)    â”‚ â”‚
                    â”‚  â”‚  â”‚   â”œâ”€ MultiHeadAttention           â”‚ â”‚
                    â”‚  â”‚  â”‚   â”‚   â”œâ”€ QKVProjection            â”‚ â”‚
                    â”‚  â”‚  â”‚   â”‚   â”œâ”€ AttentionCompute         â”‚ â”‚
                    â”‚  â”‚  â”‚   â”‚   â””â”€ OutputProjection         â”‚ â”‚
                    â”‚  â”‚  â”‚   â”œâ”€ FeedForwardNetwork           â”‚ â”‚
                    â”‚  â”‚  â”‚   â”‚   â”œâ”€ SwiGLU Activation        â”‚ â”‚
                    â”‚  â”‚  â”‚   â”‚   â””â”€ Linear Layers            â”‚ â”‚
                    â”‚  â”‚  â”‚   â””â”€ LayerNormalization (RMSNorm) â”‚ â”‚
                    â”‚  â”‚  â”œâ”€ Embedding Layer                   â”‚ â”‚
                    â”‚  â”‚  â””â”€ LM Head (output projection)       â”‚ â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚  Layer 3: ç®—å­å±‚ (Operators)               â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚  â”‚ MatMul    â”‚ Softmax  â”‚ LayerNorm    â”‚ â”‚
                    â”‚  â”‚ Embedding â”‚ RoPE     â”‚ SwiGLU       â”‚ â”‚
                    â”‚  â”‚ Add/Mul   â”‚ Reshape  â”‚ Transpose    â”‚ â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚  Layer 4: å¼ é‡ä¸å†…å­˜å±‚                     â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚  â”‚ Tensor        â”‚ TensorView           â”‚ â”‚
                    â”‚  â”‚ MemoryPool    â”‚ Allocator            â”‚ â”‚
                    â”‚  â”‚ KVCacheBuffer â”‚ MemoryMonitor        â”‚ â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚  Layer 5: ä¼˜åŒ–å±‚                           â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚  â”‚ SIMD Kernels (AVX2/AVX-512)          â”‚ â”‚
                    â”‚  â”‚ Quantization (INT8/INT4)             â”‚ â”‚
                    â”‚  â”‚ Kernel Fusion                        â”‚ â”‚
                    â”‚  â”‚ Memory Optimization                  â”‚ â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ¨¡å—ä¾èµ–å…³ç³»

```
ModelExecutor (ç°æœ‰)
    â”‚
    â”œâ”€â”€> InferenceEngine (æ–°å¢)
    â”‚       â”‚
    â”‚       â”œâ”€â”€> ModelLoader
    â”‚       â”œâ”€â”€> TransformerModel
    â”‚       â”‚       â”œâ”€â”€> TransformerBlock
    â”‚       â”‚       â”‚       â”œâ”€â”€> MultiHeadAttention
    â”‚       â”‚       â”‚       â”œâ”€â”€> FeedForwardNetwork
    â”‚       â”‚       â”‚       â””â”€â”€> RMSNorm
    â”‚       â”‚       â”œâ”€â”€> Embedding
    â”‚       â”‚       â””â”€â”€> LMHead
    â”‚       â”‚
    â”‚       â”œâ”€â”€> Tensor / TensorView
    â”‚       â”œâ”€â”€> MemoryPool
    â”‚       â””â”€â”€> SIMDKernels
    â”‚
    â”œâ”€â”€> KVCache (å¤ç”¨ç°æœ‰)
    â”œâ”€â”€> Sampler (å¤ç”¨ç°æœ‰)
    â””â”€â”€> Tokenizer (å¤ç”¨ç°æœ‰)
```

## 2. æ ¸å¿ƒç»„ä»¶è®¾è®¡

### 2.1 å¼ é‡æŠ½è±¡å±‚

#### 2.1.1 Tensorç±»

**æ–‡ä»¶**: `include/cllm/inference/tensor.h`

**å®ç°çŠ¶æ€**: âœ… å·²å®Œæˆï¼ˆMVP ç®€åŒ–ç‰ˆï¼‰

```cpp
namespace cllm {
namespace inference {

/**
 * @brief æ•°æ®ç±»å‹æšä¸¾ï¼ˆMVPé˜¶æ®µä»…æ”¯æŒFP32ï¼‰
 */
enum class DataType {
    FP32
};

/**
 * @brief è®¾å¤‡ç±»å‹æšä¸¾ï¼ˆMVPé˜¶æ®µä»…æ”¯æŒCPUï¼‰
 */
enum class Device {
    CPU
};

/**
 * @brief ç®€åŒ–ç‰ˆå¼ é‡ç±»
 *
 * MVPé˜¶æ®µçš„ç›®æ ‡æ˜¯æä¾›ä¸€ä¸ªè¶³å¤Ÿæ‰¿è½½ Transformer å‰å‘è®¡ç®—çš„æœ€å°å®ç°ï¼š
 * - ä»…æ”¯æŒ float æ•°æ®ç±»å‹
 * - ä»…æ”¯æŒ CPU è®¾å¤‡
 * - ä»¥ row-major æ–¹å¼å­˜å‚¨
 * - å½¢çŠ¶ä¿¡æ¯é€šè¿‡ std::vector<size_t> ç»´æŠ¤
 */
class Tensor {
public:
    /// é»˜è®¤æ„é€ ï¼Œå¾—åˆ°ä¸€ä¸ªç©ºå¼ é‡
    Tensor() = default;

    /// é€šè¿‡å½¢çŠ¶æ„é€ å¼ é‡
    explicit Tensor(const std::vector<size_t>& shape);

    /// é€šè¿‡åˆå§‹åŒ–åˆ—è¡¨æ„é€ å¼ é‡ï¼Œä¾‹å¦‚ Tensor({batch, seq, hidden})
    Tensor(std::initializer_list<size_t> shape);

    /// è·å–å¼ é‡å½¢çŠ¶
    const std::vector<size_t>& shape() const;

    /// è·å–ç»´åº¦ä¸ªæ•°
    size_t ndim() const;

    /// è·å–å…ƒç´ æ€»æ•°
    size_t size() const;

    /// è·å–åº•å±‚æ•°æ®æŒ‡é’ˆ
    float* data();

    /// è·å–å¸¸é‡æ•°æ®æŒ‡é’ˆ
    const float* data() const;

    /// æŒ‰ä¸€ç»´ç´¢å¼•è®¿é—®å…ƒç´ ï¼ˆè°ƒç”¨æ–¹éœ€ä¿è¯ç´¢å¼•åˆæ³•ï¼‰
    float& operator[](size_t index);

    /// æŒ‰ä¸€ç»´ç´¢å¼•è®¿é—®å…ƒç´ ï¼ˆå¸¸é‡ç‰ˆæœ¬ï¼‰
    const float& operator[](size_t index) const;

    /// é‡æ–°è®¾ç½®å½¢çŠ¶å¹¶é‡æ–°åˆ†é…å†…å­˜
    void resize(const std::vector<size_t>& newShape);

    /// å°†æ‰€æœ‰å…ƒç´ å¡«å……ä¸ºæŒ‡å®šå€¼
    void fill(float value);

private:
    std::vector<size_t> shape_;
    std::vector<float> data_;

    void allocate();
};

}} // namespace cllm::inference
```

**å®ç°è¯´æ˜**:
- MVP ç‰ˆæœ¬ä½¿ç”¨ `std::vector<float>` å†…éƒ¨å­˜å‚¨ï¼Œè‡ªåŠ¨ç®¡ç†å†…å­˜
- æš‚ä¸æ”¯æŒ FP16/INT8/INT4ï¼Œå¾…åç»­é˜¶æ®µæ·»åŠ 
- æš‚ä¸æ”¯æŒ GPUï¼Œä»… CPU
- æš‚ä¸å®ç°å¤æ‚çš„å½¢çŠ¶æ“ä½œï¼ˆtranspose, slice ç­‰ï¼‰ï¼Œå¾…éœ€è¦æ—¶æ·»åŠ 

#### 2.1.2 å†…å­˜ç®¡ç†

**å®ç°çŠ¶æ€**: âš ï¸ MVP é˜¶æ®µæš‚æœªå®ç°ï¼ˆä½¿ç”¨ std::vector è‡ªåŠ¨ç®¡ç†ï¼‰

**åç»­è®¡åˆ’**: åœ¨æ€§èƒ½ä¼˜åŒ–é˜¶æ®µï¼ˆé˜¶æ®µ3ï¼‰å®ç°ä¸“ç”¨å†…å­˜æ± 

**è®¾è®¡æ¥å£** (é¢„ç•™):
```cpp
namespace cllm {
namespace inference {

class MemoryPool {
public:
    explicit MemoryPool(size_t initialSize = 1024 * 1024 * 1024);  // 1GB
    ~MemoryPool();
    
    void* allocate(size_t bytes, size_t alignment = 64);
    void deallocate(void* ptr);
    void clear();
    
    size_t getTotalMemory() const;
    size_t getUsedMemory() const;
    size_t getAvailableMemory() const;
    
private:
    struct Block {
        void* ptr;
        size_t size;
        bool inUse;
    };
    
    std::vector<Block> blocks_;
    void* basePtr_;
    size_t totalMemory_;
    size_t usedMemory_;
    std::mutex mutex_;
    
    void* allocateAligned(size_t bytes, size_t alignment);
    void coalesceBlocks();
};

}} // namespace cllm::inference
```

**MVP æ›¿ä»£æ–¹æ¡ˆ**:
- å½“å‰ä½¿ç”¨ `std::vector<float>` ç®¡ç† Tensor å†…å­˜
- æƒé‡å¼ é‡ç”± `InferenceEngine` ç»Ÿä¸€æŒæœ‰å’Œç®¡ç†ç”Ÿå‘½å‘¨æœŸ
- ä¸´æ—¶è®¡ç®—ç¼“å†²åŒºä½¿ç”¨æ ˆæˆ–å †åˆ†é…ï¼Œç”± RAII è‡ªåŠ¨æ¸…ç†

### 2.2 æ¨¡å‹åŠ è½½å™¨

#### 2.2.1 ModelLoader å®ç°

**æ–‡ä»¶**: `include/cllm/inference/model_loader.h`

**å®ç°çŠ¶æ€**: âœ… å·²å®Œæˆï¼ˆMVP ç‰ˆæœ¬ï¼Œæ”¯æŒæ‰«å¹³ .bin æ ¼å¼ï¼‰

```cpp
namespace cllm {
namespace inference {

enum class WeightDType {
    FP32,
    FP16,
    INT8
};

class ModelLoader {
public:
    /// æ„é€ å‡½æ•°
    /// @param modelPath é¢„å¤„ç†åçš„äºŒè¿›åˆ¶æƒé‡æ–‡ä»¶è·¯å¾„ï¼ˆfp32/fp16/int8 æ‰«å¹³å­˜å‚¨ï¼‰
    /// @param config    æ¨¡å‹ç»“æ„é…ç½®ï¼Œç”¨äºæ¨å¯¼å„å¼ é‡å½¢çŠ¶
    ModelLoader(const std::string &modelPath, const ModelConfig &config);

    /// åŠ è½½æ•´ä¸ªäºŒè¿›åˆ¶æ–‡ä»¶åˆ°å†…å­˜ï¼›æˆåŠŸè¿”å› true
    /// ä¼šè‡ªåŠ¨æ£€æµ‹æ–‡ä»¶æ‰©å±•åæ¥æ¨å¯¼ dtype (fp32/fp16/int8)ï¼Œå¹¶åŠ è½½å¯¹åº”çš„å…ƒæ•°æ®
    bool load();

    const ModelConfig &getConfig() const;
    const std::string &getModelPath() const;
    WeightDType getDType() const;

    /// å°†æƒé‡å¡«å……è¿›è°ƒç”¨æ–¹æä¾›çš„å¼ é‡ä¸­ï¼Œå¼ é‡å½¢çŠ¶ç”±è°ƒç”¨æ–¹æ ¹æ® ModelConfig é¢„å…ˆåˆ†é…
    ///
    /// ç›®å‰çº¦å®šçš„æƒé‡é¡ºåºä¸ºï¼š
    /// 1. embedding: [vocabSize, hiddenSize]
    /// 2. å¯¹æ¯ä¸€å±‚ (0..numLayers-1)ï¼š
    ///    - wq:    [hiddenSize, qDim]  // qDim = numAttentionHeads * headDim
    ///    - wk:    [hiddenSize, kvDim] // kvDim = numKeyValueHeads * headDim
    ///    - wv:    [hiddenSize, kvDim]
    ///    - wo:    [qDim, hiddenSize]
    ///    - wGate: [hiddenSize, intermediateSize]
    ///    - wUp:   [hiddenSize, intermediateSize]
    ///    - wDown: [intermediateSize, hiddenSize]
    ///    - norm1: [hiddenSize]
    ///    - norm2: [hiddenSize]
    /// 3. finalNorm: [hiddenSize]
    /// 4. lmHead:    [hiddenSize, vocabSize]
    ///
    /// æ”¯æŒ GQA (Grouped Query Attention)ï¼šnumKeyValueHeads å¯ä»¥å°äº numAttentionHeads
    /// å¦‚æ–‡ä»¶å¤§å°ä¸ä¸Šè¿°æ¨å¯¼ä¸åŒ¹é…ï¼Œè¿”å› falseã€‚
    bool loadInto(
        Tensor &embedding,
        std::vector<Tensor> &wq,
        std::vector<Tensor> &wk,
        std::vector<Tensor> &wv,
        std::vector<Tensor> &wo,
        std::vector<Tensor> &wGate,
        std::vector<Tensor> &wUp,
        std::vector<Tensor> &wDown,
        std::vector<Tensor> &norm1,
        std::vector<Tensor> &norm2,
        Tensor &finalNorm,
        Tensor &lmHead
    ) const;

private:
    std::string modelPath_;
    ModelConfig config_;
    std::vector<float> weights_;
    WeightDType dtype_;
    float int8Scale_;  // ç”¨äº int8 åé‡åŒ–
    
    // å®é™…æŠ•å½±ç»´åº¦ï¼ˆä» .json å…ƒæ•°æ®åŠ è½½ï¼Œå¦‚æœä¸º 0 åˆ™ä½¿ç”¨ config_ æ¨å¯¼ï¼‰
    size_t actualQProjDim_;
    size_t actualKVProjDim_;

    bool loadBinaryFile();
    bool loadMetadata();  // åŠ è½½ .json å…ƒæ•°æ®ï¼ˆå¦‚ int8 scale å’Œå®é™…æŠ•å½±ç»´åº¦ï¼‰
    WeightDType detectDType() const;  // æ ¹æ®æ–‡ä»¶æ‰©å±•åæ¨å¯¼ dtype
};

struct ModelConfig {
    std::string modelType;
    size_t vocabSize;
    size_t hiddenSize;
    size_t numLayers;
    size_t numAttentionHeads;  // æ³¨æ„ï¼šå®é™…ä»£ç ä¸­ä¸º numHeads
    size_t numKeyValueHeads;   // ç”¨äº GQA
    size_t maxSeqLen;
    size_t intermediateSize;
    float rmsNormEps;
    float ropeTheta;
    
    void loadFromJson(const std::string& jsonPath);
    std::string toString() const;
};

}} // namespace cllm::inference
```

**å®ç°è¯´æ˜**:
- MVP ç‰ˆæœ¬æ”¯æŒæ‰«å¹³ .bin äºŒè¿›åˆ¶æ–‡ä»¶æ ¼å¼
- è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶æ‰©å±•åï¼š`.fp32.bin`, `.fp16.bin`, `.int8.bin`
- æ”¯æŒä» .json å…ƒæ•°æ®åŠ è½½ int8 scale å’Œ GQA é…ç½®
- æ­£åœ¨å¼€å‘ä¸­ï¼šå®Œæ•´çš„ GQA æ”¯æŒå’Œæƒé‡å¸ƒå±€æ£€æµ‹

**ä½¿ç”¨æµç¨‹**:
1. Python è„šæœ¬å°† HuggingFace safetensors å¯¼å‡ºä¸ºæ‰«å¹³ .bin æ–‡ä»¶
2. C++ ModelLoader æ ¹æ® ModelConfig æ¨å¯¼å½¢çŠ¶å¹¶æŒ‰é¡ºåºåŠ è½½
3. æ”¯æŒ fp32/fp16/int8 ä¸‰ç§æ•°æ®ç±»å‹

**åç»­è®¡åˆ’**:
- é˜¶æ®µ2ï¼šç›´æ¥æ”¯æŒ HuggingFace safetensors æ ¼å¼
- é˜¶æ®µ3ï¼šæ”¯æŒ GGUF/ggml æ ¼å¼ï¼Œä¸ llama.cpp ç”Ÿæ€å…¼å®¹

### 2.3 Transformeræ ¸å¿ƒç»„ä»¶

#### 2.3.1 RMSNorm (Layer Normalization)

**æ–‡ä»¶**: `include/cllm/inference/layers/rms_norm.h`

```cpp
namespace cllm {
namespace inference {

class RMSNorm {
public:
    RMSNorm(size_t hiddenSize, float eps = 1e-6);
    ~RMSNorm();
    
    void loadWeights(const Tensor& weight);
    Tensor forward(const Tensor& input);
    
private:
    size_t hiddenSize_;
    float eps_;
    Tensor weight_;  // [hidden_size]
    
    void computeRMSNorm(const float* input, float* output, size_t batchSize, size_t seqLen);
};

}} // namespace cllm::inference
```

**å®ç°åŸç†**:
```
RMSNorm(x) = x / sqrt(mean(x^2) + eps) * weight

æ­¥éª¤:
1. è®¡ç®— x^2
2. è®¡ç®—å‡å€¼ mean(x^2)
3. è®¡ç®— rms = sqrt(mean(x^2) + eps)
4. å½’ä¸€åŒ–: x / rms
5. ç¼©æ”¾: result * weight
```

#### 2.3.2 RoPE (Rotary Position Embedding)

**æ–‡ä»¶**: `include/cllm/inference/layers/rope.h`

```cpp
namespace cllm {
namespace inference {

class RoPE {
public:
    RoPE(size_t dimPerHead, size_t maxSeqLen, float theta = 10000.0f);
    ~RoPE();
    
    void apply(Tensor& q, Tensor& k, size_t seqLen, size_t posOffset = 0);
    
private:
    size_t dimPerHead_;
    size_t maxSeqLen_;
    float theta_;
    
    // é¢„è®¡ç®—çš„cos/sinè¡¨
    Tensor cosCache_;  // [maxSeqLen, dimPerHead/2]
    Tensor sinCache_;  // [maxSeqLen, dimPerHead/2]
    
    void precomputeFreqs();
    void applyRotary(float* data, size_t seqLen, size_t posOffset);
};

}} // namespace cllm::inference
```

**å®ç°åŸç†**:
```
RoPE: æ—‹è½¬ä½ç½®ç¼–ç 

å¯¹äºæ¯ä¸ªä½ç½®poså’Œç»´åº¦å¯¹(2i, 2i+1):
  freq = pos / (theta ^ (2i / dim))
  x[2i]'   = x[2i] * cos(freq) - x[2i+1] * sin(freq)
  x[2i+1]' = x[2i] * sin(freq) + x[2i+1] * cos(freq)

ä¼˜åŒ–: é¢„è®¡ç®—cos/sinè¡¨ï¼Œé¿å…é‡å¤è®¡ç®—
```

#### 2.3.3 Multi-Head Attention

**æ–‡ä»¶**: `include/cllm/inference/layers/attention.h`

```cpp
namespace cllm {
namespace inference {

class MultiHeadAttention {
public:
    MultiHeadAttention(
        size_t hiddenSize,
        size_t numHeads,
        size_t numKVHeads,  // ç”¨äºGQA
        size_t maxSeqLen,
        float ropeTheta = 10000.0f
    );
    ~MultiHeadAttention();
    
    void loadWeights(
        const Tensor& wq,
        const Tensor& wk,
        const Tensor& wv,
        const Tensor& wo
    );
    
    // forward with KV cache
    Tensor forward(
        const Tensor& input,
        Tensor* kCache,    // [numLayers, batchSize, numKVHeads, maxSeqLen, headDim]
        Tensor* vCache,
        size_t layerIdx,
        size_t seqLen,
        size_t posOffset
    );
    
private:
    size_t hiddenSize_;
    size_t numHeads_;
    size_t numKVHeads_;
    size_t headDim_;
    size_t maxSeqLen_;
    
    Tensor wq_;  // [hiddenSize, numHeads * headDim]
    Tensor wk_;  // [hiddenSize, numKVHeads * headDim]
    Tensor wv_;  // [hiddenSize, numKVHeads * headDim]
    Tensor wo_;  // [numHeads * headDim, hiddenSize]
    
    std::unique_ptr<RoPE> rope_;
    
    // è®¡ç®—æ­¥éª¤
    Tensor projectQKV(const Tensor& input);
    Tensor computeAttention(
        const Tensor& q,
        const Tensor& k,
        const Tensor& v,
        size_t seqLen
    );
    Tensor projectOutput(const Tensor& attnOut);
};

}} // namespace cllm::inference
```

**Attentionè®¡ç®—æµç¨‹**:
```
è¾“å…¥: x [batchSize, seqLen, hiddenSize]

1. QKVæŠ•å½±:
   Q = x @ Wq  -> [batchSize, seqLen, numHeads * headDim]
   K = x @ Wk  -> [batchSize, seqLen, numKVHeads * headDim]
   V = x @ Wv  -> [batchSize, seqLen, numKVHeads * headDim]

2. é‡å¡‘ä¸ºå¤šå¤´:
   Q -> [batchSize, numHeads, seqLen, headDim]
   K -> [batchSize, numKVHeads, seqLen, headDim]
   V -> [batchSize, numKVHeads, seqLen, headDim]

3. åº”ç”¨RoPE:
   Q' = RoPE(Q, position)
   K' = RoPE(K, position)

4. æ›´æ–°KV Cache:
   K_cache[layer, :, :, posOffset:posOffset+seqLen, :] = K'
   V_cache[layer, :, :, posOffset:posOffset+seqLen, :] = V'

5. è®¡ç®—Attentionåˆ†æ•°:
   scores = (Q' @ K_cache.T) / sqrt(headDim)
   scores = scores + causal_mask  # å› æœmask
   attn_weights = softmax(scores)

6. è®¡ç®—è¾“å‡º:
   attn_out = attn_weights @ V_cache
   attn_out -> [batchSize, seqLen, numHeads * headDim]

7. è¾“å‡ºæŠ•å½±:
   output = attn_out @ Wo
   output -> [batchSize, seqLen, hiddenSize]
```

#### 2.3.4 Feed-Forward Network (SwiGLU)

**æ–‡ä»¶**: `include/cllm/inference/layers/feed_forward.h`

```cpp
namespace cllm {
namespace inference {

class FeedForwardNetwork {
public:
    FeedForwardNetwork(size_t hiddenSize, size_t intermediateSize);
    ~FeedForwardNetwork();
    
    void loadWeights(
        const Tensor& wGate,
        const Tensor& wUp,
        const Tensor& wDown
    );
    
    Tensor forward(const Tensor& input);
    
private:
    size_t hiddenSize_;
    size_t intermediateSize_;
    
    Tensor wGate_;  // [hiddenSize, intermediateSize]
    Tensor wUp_;    // [hiddenSize, intermediateSize]
    Tensor wDown_;  // [intermediateSize, hiddenSize]
    
    Tensor swiGLU(const Tensor& gate, const Tensor& up);
};

}} // namespace cllm::inference
```

**SwiGLUæ¿€æ´»å‡½æ•°**:
```
SwiGLU(x) = (x @ W_gate * SiLU(x @ W_up)) @ W_down

å…¶ä¸­:
  SiLU(x) = x * sigmoid(x) = x / (1 + exp(-x))

æ­¥éª¤:
1. gate = x @ W_gate
2. up = x @ W_up
3. activated = gate * SiLU(up)
4. output = activated @ W_down
```

#### 2.3.5 Transformer Block

**æ–‡ä»¶**: `include/cllm/inference/layers/transformer_block.h`

```cpp
namespace cllm {
namespace inference {

class TransformerBlock {
public:
    TransformerBlock(
        size_t hiddenSize,
        size_t numHeads,
        size_t numKVHeads,
        size_t intermediateSize,
        size_t maxSeqLen,
        float rmsNormEps,
        float ropeTheta
    );
    ~TransformerBlock();
    
    void loadWeights(const std::map<std::string, Tensor>& weights);
    
    Tensor forward(
        const Tensor& input,
        Tensor* kCache,
        Tensor* vCache,
        size_t layerIdx,
        size_t seqLen,
        size_t posOffset
    );
    
private:
    std::unique_ptr<MultiHeadAttention> attention_;
    std::unique_ptr<FeedForwardNetwork> ffn_;
    std::unique_ptr<RMSNorm> inputNorm_;
    std::unique_ptr<RMSNorm> postAttnNorm_;
};

}} // namespace cllm::inference
```

**Transformer Blockç»“æ„**:
```
è¾“å…¥: x

1. Pre-Norm Attention:
   residual = x
   x = RMSNorm(x)
   x = MultiHeadAttention(x, kCache, vCache)
   x = x + residual

2. Pre-Norm FFN:
   residual = x
   x = RMSNorm(x)
   x = FeedForward(x)
   x = x + residual

è¾“å‡º: x
```

#### 2.3.6 å®Œæ•´Transformeræ¨¡å‹

**æ–‡ä»¶**: `include/cllm/inference/transformer_model.h`

```cpp
namespace cllm {
namespace inference {

class TransformerModel {
public:
    explicit TransformerModel(const ModelConfig& config);
    ~TransformerModel();
    
    void loadWeights(const std::map<std::string, Tensor>& weights);
    
    // å‰å‘ä¼ æ’­
    Tensor forward(
        const std::vector<int>& inputIds,
        Tensor* kCache,
        Tensor* vCache,
        size_t posOffset = 0
    );
    
    ModelConfig getConfig() const { return config_; }
    
private:
    ModelConfig config_;
    
    // ç»„ä»¶
    Tensor embedding_;  // [vocabSize, hiddenSize]
    std::vector<std::unique_ptr<TransformerBlock>> layers_;
    std::unique_ptr<RMSNorm> finalNorm_;
    Tensor lmHead_;  // [hiddenSize, vocabSize]
    
    // è¾…åŠ©æ–¹æ³•
    Tensor embed(const std::vector<int>& inputIds);
    Tensor generateLogits(const Tensor& hiddenStates);
};

}} // namespace cllm::inference
```

### 2.4 é«˜æ€§èƒ½ç®—å­

#### 2.4.1 çŸ©é˜µä¹˜æ³• (GEMM)

**æ–‡ä»¶**: `include/cllm/inference/kernels/matmul.h`

```cpp
namespace cllm {
namespace inference {
namespace kernels {

// é€šç”¨çŸ©é˜µä¹˜æ³•æ¥å£
void matmul(
    const float* A,      // [M, K]
    const float* B,      // [K, N]
    float* C,            // [M, N]
    size_t M,
    size_t N,
    size_t K,
    bool transposeA = false,
    bool transposeB = false
);

// SIMDä¼˜åŒ–ç‰ˆæœ¬
void matmul_avx2(
    const float* A,
    const float* B,
    float* C,
    size_t M,
    size_t N,
    size_t K
);

void matmul_avx512(
    const float* A,
    const float* B,
    float* C,
    size_t M,
    size_t N,
    size_t K
);

// é‡åŒ–çŸ©é˜µä¹˜æ³•
void matmul_int8(
    const int8_t* A,
    const int8_t* B,
    int32_t* C,
    size_t M,
    size_t N,
    size_t K,
    const float* scaleA,
    const float* scaleB
);

}}} // namespace cllm::inference::kernels
```

**SIMDä¼˜åŒ–ç­–ç•¥**:
```
1. å‘é‡åŒ–å†…å¾ªç¯ (ä½¿ç”¨AVX2/AVX-512)
2. åˆ†å—ç­–ç•¥ (Tiling) æé«˜ç¼“å­˜å‘½ä¸­ç‡
3. æ•°æ®é¢„å– (Prefetching)
4. å¾ªç¯å±•å¼€ (Loop Unrolling)

ç¤ºä¾‹ (AVX2):
for (size_t i = 0; i < M; i++) {
    for (size_t j = 0; j < N; j += 8) {  // 8ä¸ªfloat = 256ä½
        __m256 sum = _mm256_setzero_ps();
        for (size_t k = 0; k < K; k++) {
            __m256 a = _mm256_broadcast_ss(&A[i*K + k]);
            __m256 b = _mm256_loadu_ps(&B[k*N + j]);
            sum = _mm256_fmadd_ps(a, b, sum);
        }
        _mm256_storeu_ps(&C[i*N + j], sum);
    }
}
```

#### 2.4.2 Softmax

**æ–‡ä»¶**: `include/cllm/inference/kernels/softmax.h`

```cpp
namespace cllm {
namespace inference {
namespace kernels {

void softmax(
    const float* input,
    float* output,
    size_t outerDim,
    size_t innerDim
);

// æ•°å€¼ç¨³å®šç‰ˆæœ¬
void softmax_stable(
    const float* input,
    float* output,
    size_t outerDim,
    size_t innerDim
);

}}} // namespace cllm::inference::kernels
```

**æ•°å€¼ç¨³å®šSoftmax**:
```
æ ‡å‡†å…¬å¼: softmax(x_i) = exp(x_i) / sum(exp(x_j))
é—®é¢˜: exp(x) å®¹æ˜“æº¢å‡º

ç¨³å®šç‰ˆæœ¬:
1. æ‰¾åˆ°æœ€å¤§å€¼: x_max = max(x)
2. å‡å»æœ€å¤§å€¼: x' = x - x_max
3. è®¡ç®—softmax: softmax(x'_i) = exp(x'_i) / sum(exp(x'_j))

è¿™æ ·ä¿è¯ exp(x') ä¸ä¼šæº¢å‡º
```

### 2.5 é‡åŒ–æ”¯æŒ

#### 2.5.1 é‡åŒ–æ–¹æ¡ˆ

**æ–‡ä»¶**: `include/cllm/inference/quantization.h`

```cpp
namespace cllm {
namespace inference {

enum class QuantizationType {
    NONE,
    INT8,
    INT4,
    FP16
};

class Quantizer {
public:
    explicit Quantizer(QuantizationType type);
    ~Quantizer();
    
    // å¯¹ç§°é‡åŒ–
    void quantize_symmetric(
        const float* input,
        int8_t* output,
        float* scale,
        size_t size
    );
    
    // éå¯¹ç§°é‡åŒ–
    void quantize_asymmetric(
        const float* input,
        int8_t* output,
        float* scale,
        int8_t* zeroPoint,
        size_t size
    );
    
    // åé‡åŒ–
    void dequantize(
        const int8_t* input,
        float* output,
        const float* scale,
        size_t size
    );
    
    // é‡åŒ–æƒé‡
    Tensor quantizeWeights(const Tensor& weights);
    
private:
    QuantizationType type_;
    
    float computeScale(const float* data, size_t size);
    int8_t computeZeroPoint(const float* data, size_t size, float scale);
};

}} // namespace cllm::inference
```

**é‡åŒ–å…¬å¼**:
```
å¯¹ç§°é‡åŒ–:
  scale = max(abs(x)) / 127
  x_quant = round(x / scale)
  x_dequant = x_quant * scale

éå¯¹ç§°é‡åŒ–:
  scale = (max(x) - min(x)) / 255
  zero_point = round(-min(x) / scale)
  x_quant = round(x / scale) + zero_point
  x_dequant = (x_quant - zero_point) * scale
```

## 3. æ¨ç†å¼•æ“ä¸»ç±»

### 3.1 InferenceEngine

**æ–‡ä»¶**: `include/cllm/inference/inference_engine.h`

**å®ç°çŠ¶æ€**: âœ… å·²å®Œæˆ (MVP ç‰ˆæœ¬ï¼Œæ”¯æŒè‡ªç ”å¼•æ“ + LibTorch åç«¯)

```cpp
namespace cllm {
namespace inference {

/**
 * @brief è‡ªç ”æ¨ç†å¼•æ“ï¼ˆMVPç‰ˆæœ¬ï¼‰
 *
 * è´Ÿè´£ï¼š
 * - æŒæœ‰å†…éƒ¨çš„ TransformerModel æˆ– LibTorchBackend
 * - åœ¨åˆå§‹åŒ–é˜¶æ®µæ„é€ å¹¶ç»‘å®šå„å±‚æƒé‡å¼ é‡
 * - æä¾›å•åºåˆ— forward ä¸ä¸ BatchInput è¯­ä¹‰å¯¹é½çš„ forwardBatch æ¥å£
 */
class InferenceEngine {
public:
    /// ä½¿ç”¨ç»™å®šçš„æ¨¡å‹é…ç½®æ„é€ æ¨ç†å¼•æ“
    /// @param config æ¨¡å‹é…ç½®
    /// @param modelPath æ¨¡å‹è·¯å¾„ï¼ˆ.bin ç”¨äºè‡ªç ”å¼•æ“ï¼Œ.pt ç”¨äº LibTorchï¼‰
    /// @param useLibTorch æ˜¯å¦ä½¿ç”¨ LibTorch åç«¯ï¼ˆé»˜è®¤ falseï¼Œä½¿ç”¨è‡ªç ”å¼•æ“ï¼‰
    explicit InferenceEngine(
        const ModelConfig &config, 
        const std::string &modelPath = std::string(), 
        bool useLibTorch = false
    );

    /// åˆå§‹åŒ–å†…éƒ¨æ¨¡å‹å’Œæƒé‡
    /// - è‡ªç ”å¼•æ“ï¼šä½¿ç”¨ ModelLoader åŠ è½½æƒé‡æˆ–ä½¿ç”¨å ä½æƒé‡
    /// - LibTorch åç«¯ï¼šåŠ è½½ TorchScript æ¨¡å‹
    bool initialize();

    /// å•åºåˆ—å‰å‘æ¨ç†ï¼Œè¾“å…¥ token id åºåˆ—ï¼Œè¾“å‡º [seq_len, vocab_size] logits
    Tensor forward(const std::vector<int> &inputIds) const;

    /// æ‰¹å¤„ç†å‰å‘æ¨ç†ï¼Œä¸ BatchInput è¯­ä¹‰å¯¹é½
    ///
    /// @param flatInputIds å±•å¹³åçš„æ‰€æœ‰ token idï¼Œç­‰åŒäº BatchInput::inputIds
    /// @param requestPositions æ¯ä¸ªè¯·æ±‚åœ¨ flatInputIds ä¸­çš„èµ·æ­¢ä½ç½® [start, end)
    /// @param batchSize æ‰¹å¤§å°ï¼ˆè¯·æ±‚æ•°ï¼‰
    /// @return å½¢çŠ¶ä¸º [total_tokens, vocab_size] çš„ logits Tensor
    Tensor forwardBatch(
        const std::vector<int> &flatInputIds,
        const std::vector<std::pair<size_t, size_t>> &requestPositions,
        size_t batchSize
    ) const;

    const ModelConfig &getConfig() const { return externalConfig_; }

private:
    // å¤–éƒ¨ä¼ å…¥çš„æ¨¡å‹é…ç½®ï¼ˆæ¥è‡ª ModelExecutorï¼‰ï¼Œä¸»è¦ç”¨äº vocabSize ç­‰å¯¹é½
    ModelConfig externalConfig_;

    // å†…éƒ¨ç”¨äº TransformerModel çš„ç²¾ç®€é…ç½®ï¼Œé¿å…ä¸€æ¬¡æ€§åˆ†é…è¿‡å¤§çš„æƒé‡çŸ©é˜µ
    ModelConfig internalConfig_;

    // å†…éƒ¨ Transformer æ¨¡å‹ï¼ˆè‡ªç ”å¼•æ“ï¼‰
    TransformerModel model_;

    // æ¨¡å‹è·¯å¾„ä¸åŠ è½½å™¨ï¼ˆå­˜åœ¨è·¯å¾„æ—¶ä¼˜å…ˆä½¿ç”¨çœŸå®æƒé‡ï¼‰
    std::string modelPath_;
    std::unique_ptr<ModelLoader> loader_;
    
    // LibTorch åç«¯ï¼ˆå¯é€‰ï¼‰
    bool useLibTorch_;
    std::unique_ptr<LibTorchBackend> libtorchBackend_;

    // æƒé‡å¼ é‡çš„å®é™…å­˜å‚¨ï¼Œç”Ÿå‘½å‘¨æœŸç”± InferenceEngine ç®¡ç†
    Tensor embedding_;                 // [vocabSize, hiddenSize]
    Tensor lmHead_;                    // [hiddenSize, vocabSize]
    std::vector<Tensor> wq_;           // æ¯å±‚ [hiddenSize, hiddenSize]
    std::vector<Tensor> wk_;
    std::vector<Tensor> wv_;
    std::vector<Tensor> wo_;
    std::vector<Tensor> wGate_;        // æ¯å±‚ [hiddenSize, intermediateSize]
    std::vector<Tensor> wUp_;
    std::vector<Tensor> wDown_;        // æ¯å±‚ [intermediateSize, hiddenSize]
    std::vector<Tensor> norm1_;        // æ¯å±‚ [hiddenSize]
    std::vector<Tensor> norm2_;        // æ¯å±‚ [hiddenSize]
    Tensor finalNormWeight_;           // [hiddenSize]

    bool initialized_;

    /// ä¸ºå†…éƒ¨æ¨¡å‹é…ç½®é€‰æ‹©ä¸€ä¸ªè¾ƒå°ä½†ç»“æ„åˆç†çš„éšè—ç»´åº¦ç­‰å‚æ•°
    void _prepareInternalConfig();

    /// ä½¿ç”¨ç®€å•çš„å¯é‡å¤æ¨¡å¼å¡«å……æƒé‡å¼ é‡ï¼Œé¿å…å…¨é›¶è¾“å‡º
    void _initializeWeights();
};

} // namespace inference
} // namespace cllm
```

**å®ç°è¯´æ˜**:

1. **åŒåç«¯æ”¯æŒ**:
   - **è‡ªç ”å¼•æ“**: ä½¿ç”¨ `TransformerModel` + è‡ªå®ç°çš„ç®—å­
   - **LibTorch åç«¯**: ä½¿ç”¨ `LibTorchBackend` + TorchScript æ¨¡å‹
   - é€šè¿‡ `useLibTorch` å‚æ•°åœ¨æ„é€ æ—¶é€‰æ‹©

2. **æƒé‡åŠ è½½ç­–ç•¥**:
   - å¦‚æœæä¾› `modelPath`ï¼šä½¿ç”¨ `ModelLoader` åŠ è½½çœŸå®æƒé‡
   - å¦‚æœæœªæä¾›ï¼šä½¿ç”¨å ä½æƒé‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰

3. **æ‰¹å¤„ç†æ¥å£**:
   - `forwardBatch` ä¸ `BatchInput`/`BatchOutput` è¯­ä¹‰å®Œå…¨å¯¹é½
   - è¾“å…¥ï¼šå±•å¹³çš„ token æ•°ç»„ + æ¯ä¸ªè¯·æ±‚çš„è¾¹ç•Œ
   - è¾“å‡ºï¼š`[total_tokens, vocab_size]` çš„ logits

4. **é…ç½®ç®¡ç†**:
   - `externalConfig_`: æ¥è‡ª `ModelExecutor`ï¼Œä¿æŒå®Œæ•´é…ç½®
   - `internalConfig_`: ç”¨äºå ä½æƒé‡ï¼Œä½¿ç”¨ç²¾ç®€é…ç½®å‡å°‘å†…å­˜

### 3.2 LibTorch åç«¯

**æ–‡ä»¶**: `include/cllm/inference/libtorch_backend.h`

**å®ç°çŠ¶æ€**: âœ… å·²å®Œæˆï¼ˆé›†æˆ PyTorch C++ APIï¼‰

```cpp
namespace cllm {
namespace inference {

/**
 * @brief LibTorch æ¨ç†åç«¯
 *
 * ä½¿ç”¨ PyTorch C++ API (LibTorch) è¿›è¡Œæ¨ç†ï¼š
 * - åŠ è½½ TorchScript æ¨¡å‹ï¼ˆ.pt æ–‡ä»¶ï¼‰
 * - æ”¯æŒ LibTorch å†…ç½®é‡åŒ–ï¼ˆint8/fp16ï¼‰
 * - åˆ©ç”¨ MKL-DNN/oneDNN ä¼˜åŒ–
 */
class LibTorchBackend {
public:
    /**
     * @brief æ„é€ å‡½æ•°
     * @param modelPath TorchScript æ¨¡å‹è·¯å¾„ï¼ˆ.pt æ–‡ä»¶ï¼‰
     * @param config æ¨¡å‹é…ç½®
     */
    explicit LibTorchBackend(const std::string &modelPath, const ModelConfig &config);

    /**
     * @brief åˆå§‹åŒ–åŠ è½½æ¨¡å‹
     * @return true æˆåŠŸï¼Œfalse å¤±è´¥
     */
    bool initialize();

    /**
     * @brief å•åºåˆ—å‰å‘æ¨ç†
     * @param inputIds è¾“å…¥ token id åºåˆ—
     * @return [seq_len, vocab_size] logits å¼ é‡
     */
    Tensor forward(const std::vector<int> &inputIds);

    /**
     * @brief æ‰¹å¤„ç†å‰å‘æ¨ç†
     * @param flatInputIds å±•å¹³åçš„æ‰€æœ‰ token id
     * @param requestPositions æ¯ä¸ªè¯·æ±‚çš„èµ·æ­¢ä½ç½®
     * @param batchSize æ‰¹å¤§å°
     * @return [total_tokens, vocab_size] logits å¼ é‡
     */
    Tensor forwardBatch(
        const std::vector<int> &flatInputIds,
        const std::vector<std::pair<size_t, size_t>> &requestPositions,
        size_t batchSize
    );

    bool isInitialized() const { return initialized_; }
    const ModelConfig &getConfig() const { return config_; }

private:
    std::string modelPath_;           ///< TorchScript æ¨¡å‹è·¯å¾„
    ModelConfig config_;              ///< æ¨¡å‹é…ç½®
    torch::jit::script::Module model_; ///< LibTorch æ¨¡å‹
    torch::Device device_;            ///< æ¨ç†è®¾å¤‡ï¼ˆCPU/GPUï¼‰
    bool initialized_;                ///< æ˜¯å¦å·²åˆå§‹åŒ–

    torch::Tensor vecToTensor(const std::vector<int> &vec);
    Tensor torchTensorToTensor(const torch::Tensor &torchTensor);
};

} // namespace inference
} // namespace cllm
```

**å®ç°ç‰¹ç‚¹**:

1. **TorchScript æ¨¡å‹åŠ è½½**:
   - ä½¿ç”¨ `torch::jit::load()` åŠ è½½ .pt æ–‡ä»¶
   - æ”¯æŒ CPU å’Œ GPU è®¾å¤‡
   - è®¾ç½® `eval()` æ¨¡å¼ç¦ç”¨ dropout ç­‰

2. **è¾“å…¥å½¢çŠ¶å¤„ç†**:
   - TorchScript trace ä¼šå›ºåŒ–è¾“å…¥å½¢çŠ¶ï¼ˆå½“å‰ä¸º 8 tokensï¼‰
   - è‡ªåŠ¨å¡«å……ï¼ˆpaddingï¼‰æˆ–æˆªæ–­ï¼ˆtruncationï¼‰è¾“å…¥
   - æå–åŸå§‹é•¿åº¦çš„è¾“å‡º

3. **é‡åŒ–æ”¯æŒ**:
   - æ”¯æŒåŠ è½½ LibTorch å†…ç½®é‡åŒ–æ¨¡å‹
   - æ”¯æŒ int8/fp16 é‡åŒ–
   - åˆ©ç”¨ MKL-DNN/oneDNN åŠ é€Ÿ

4. **æ‰¹å¤„ç†å®ç°**:
   - é€è¯·æ±‚è°ƒç”¨ `forward()`
   - å°†ç»“æœæ‹¼æ¥ä¸º `[total_tokens, vocab_size]`
   - ä¸ `InferenceEngine::forwardBatch` æ¥å£ä¿æŒä¸€è‡´

### 3.3 é›†æˆåˆ° ModelExecutor

**ä¿®æ”¹æ–‡ä»¶**: `src/model/executor.cpp`

**å®ç°çŠ¶æ€**: âœ… å·²å®Œæˆ

```cpp
#include "cllm/inference/inference_engine.h"

namespace cllm {

ModelExecutor::ModelExecutor(
    const std::string& modelPath,
    const std::string& quantization,
    bool enableSIMD,
    bool useLibTorch
)
    : modelPath_(modelPath)
    , quantization_(quantization)
    , enableSIMD_(enableSIMD)
    , useLibTorch_(useLibTorch)
    , inferenceEngine_(nullptr) {
    
    std::cout << "[ModelExecutor] Initializing with "
              << (useLibTorch_ ? "LibTorch" : "Custom") 
              << " backend" << std::endl;
    
    // åˆå§‹åŒ– InferenceEngine
    inferenceEngine_ = std::make_unique<inference::InferenceEngine>(
        config_, 
        modelPath_, 
        useLibTorch_
    );
    
    if (!inferenceEngine_->initialize()) {
        throw std::runtime_error("ModelExecutor: failed to initialize inference engine");
    }
}

FloatArray ModelExecutor::_executeModelInference(const BatchInput& input) {
    if (!isModelLoaded_) {
        throw std::runtime_error("Model is not loaded");
    }
    
    std::cerr << "[DEBUG] ModelExecutor::_executeModelInference()" << std::endl;
    
    // è°ƒç”¨è‡ªç ”æ¨ç†å¼•æ“ï¼Œè·å¾— [total_tokens, vocab_size] logits
    inference::Tensor logitsTensor = inferenceEngine_->forwardBatch(
        input.inputIds,
        input.requestPositions,
        input.batchSize
    );
    
    // è½¬æ¢ä¸º FloatArray
    const auto& logitsShape = logitsTensor.shape();
    if (logitsShape.size() != 2 || 
        logitsShape[0] != input.getTotalTokens() || 
        logitsShape[1] != config_.vocabSize) {
        throw std::runtime_error("ModelExecutor::_executeModelInference: logits shape mismatch");
    }
    
    const float* src = logitsTensor.data();
    size_t outputSize = logitsShape[0] * logitsShape[1];
    FloatArray outputTensor(outputSize);
    
    for (size_t i = 0; i < outputSize; ++i) {
        outputTensor[i] = src[i];
    }
    
    std::cerr << "[DEBUG]   Generated logits for " << logitsShape[0] << " token positions" << std::endl;
    return outputTensor;
}

} // namespace cllm
```

**é›†æˆè¯´æ˜**:

1. **æ„é€ æ—¶åˆå§‹åŒ–**:
   - æ ¹æ® `useLibTorch` å‚æ•°é€‰æ‹©åç«¯
   - ä¼ å…¥ `ModelConfig` å’Œ `modelPath`
   - è°ƒç”¨ `initialize()` åŠ è½½æ¨¡å‹

2. **æ¨ç†æ‰§è¡Œ**:
   - `_executeModelInference` è°ƒç”¨ `inferenceEngine_->forwardBatch`
   - è¾“å…¥ï¼š`BatchInput` çš„ `inputIds` + `requestPositions` + `batchSize`
   - è¾“å‡ºï¼š`[total_tokens, vocab_size]` çš„ `FloatArray`

3. **ä¸ç°æœ‰ç³»ç»Ÿå¯¹é½**:
   - ä¿æŒ `BatchInput` / `BatchOutput` æ¥å£ä¸å˜
   - ä¿æŒ logits å¸ƒå±€ä¸º `[total_tokens, vocab_size]`
   - ä¸ Sampler æ¨¡å—å®Œå…¨å…¼å®¹

4. **é…ç½®åŠ è½½**:
   - åœ¨ `main.cpp` ä¸­ä» JSON åŠ è½½ ModelConfig
   - é€šè¿‡ `setConfig()` ä¼ é€’ç»™ InferenceEngine
   - æ”¯æŒåŠ¨æ€é…ç½® vocab_size ç­‰å‚æ•°

## 3.4 å½“å‰å®ç°çŠ¶æ€æ€»ç»“

### 3.4.1 MVP å·²å®ŒæˆåŠŸèƒ½ (âœ…)

| ç»„ä»¶ | çŠ¶æ€ | å®ç°èŒƒå›´ | æ–‡ä»¶ |
|------|------|---------|------|
| **Tensor ç±»** | âœ… | FP32ã€CPUã€åŸºç¡€æ“ä½œ | `tensor.h` |
| **ModelLoader** | âœ… | æ‰«å¹³ .bin æ ¼å¼ã€fp32/fp16/int8 | `model_loader.h/.cpp` |
| **LibTorchBackend** | âœ… | TorchScript åŠ è½½ã€æ‰¹å¤„ç† | `libtorch_backend.h/.cpp` |
| **InferenceEngine** | âœ… | åŒåç«¯æ”¯æŒã€æ‰¹å¤„ç†æ¥å£ | `inference_engine.h/.cpp` |
| **TransformerModel** | âœ… | åŸºç¡€ Transformer å‰å‘ | `transformer_model.h/.cpp` |
| **MultiHeadAttention** | âœ… | æ—  KV Cache ç‰ˆæœ¬ | `attention.h/.cpp` |
| **FeedForward** | âœ… | SwiGLU æ¿€æ´» | `feed_forward.h/.cpp` |
| **RoPE** | âœ… | æ—‹è½¬ä½ç½®ç¼–ç  | `rope.h/.cpp` |
| **Kernels** | âœ… | åŸºç¡€ç®—å­ï¼ˆmatmulã€softmaxç­‰ï¼‰| `kernels.h/.cpp` |
| **ModelExecutor é›†æˆ** | âœ… | ä¸ç°æœ‰ç³»ç»Ÿå¯¹é½ | `executor.cpp` |

### 3.4.2 æ­£åœ¨å¼€å‘ä¸­ (ğŸš§)

| åŠŸèƒ½ | çŠ¶æ€ | è®¡åˆ’ | ä¼˜å…ˆçº§ |
|------|------|------|--------|
| **KV Cache æ”¯æŒ** | ğŸš§ | å¢é‡ç”Ÿæˆä¼˜åŒ– | ä¸­ |
| **GQA å®Œæ•´æ”¯æŒ** | ğŸš§ | KV heads repeat æ‰©å±• | ä¸­ |
| **SIMD ä¼˜åŒ–** | âš ï¸ | AVX2/NEON ç®—å­ | ä½ |
| **ç›´æ¥ SafeTensors æ”¯æŒ** | âš ï¸ | æ— éœ€ Python è½¬æ¢ | ä½ |

### 3.4.3 å·²éªŒè¯åŠŸèƒ½ (âœ…)

1. **å®Œæ•´çš„ /generate ç«¯ç‚¹æµ‹è¯•**:
   - âœ… æœåŠ¡å™¨å¯åŠ¨æˆåŠŸ
   - âœ… æ¨¡å‹é…ç½®ä» JSON æ­£ç¡®åŠ è½½ (vocab_size=151936)
   - âœ… çŸ­è¾“å…¥æµ‹è¯•é€šè¿‡ ("Hi", 1ç§’å†…å®Œæˆ)
   - âœ… ä¸­ç­‰è¾“å…¥æµ‹è¯•é€šè¿‡ ("Hello", 2ç§’å†…å®Œæˆ)
   - âœ… æ‰¹å¤„ç†æ¥å£å¯¹é½ BatchInput/BatchOutput

2. **LibTorch åç«¯éªŒè¯**:
   - âœ… TorchScript æ¨¡å‹åŠ è½½æˆåŠŸ
   - âœ… è‡ªåŠ¨è¾“å…¥å¡«å……/æˆªæ–­ (8 tokens trace é™åˆ¶)
   - âœ… æ‰¹å¤„ç†é€è¯·æ±‚å¤„ç†
   - âœ… logits å½¢çŠ¶æ­£ç¡® `[total_tokens, vocab_size]`

3. **ä¸ç°æœ‰ç³»ç»Ÿé›†æˆ**:
   - âœ… Scheduler æ‰¹å¤„ç†æµç¨‹
   - âœ… Sampler é‡‡æ ·æµç¨‹
   - âœ… Tokenizer ç¼–è§£ç 
   - âœ… HTTP Server /generate ç«¯ç‚¹

### 3.4.4 å·²çŸ¥é™åˆ¶

1. **TorchScript Trace é™åˆ¶**:
   - è¾“å…¥å½¢çŠ¶å›ºåŒ–ä¸º 8 tokens
   - ä¸´æ—¶è§£å†³ï¼šè¾“å…¥é™åˆ¶åœ¨ 7 tokens ä»¥å†…
   - é•¿æœŸæ–¹æ¡ˆï¼šé‡æ–°å¯¼å‡ºæ”¯æŒåŠ¨æ€è¾“å…¥çš„æ¨¡å‹

2. **æ€§èƒ½ç“¶é¢ˆ**:
   - CPU FP32 æ¨ç†é€Ÿåº¦è¾ƒæ…¢ (~200ms/token)
   - è§£å†³æ–¹æ¡ˆï¼šå¯¼å‡º FP16/INT8 é‡åŒ–æ¨¡å‹ï¼ˆå·²åˆ—å…¥è®¡åˆ’ï¼‰

3. **KV Cache**:
   - å½“å‰ MVP æ—  KV Cacheï¼Œæ¯æ­¥é‡ç®—æ•´ä¸ªåºåˆ—
   - å½±å“ï¼šç”Ÿæˆé€Ÿåº¦éšåºåˆ—é•¿åº¦çº¿æ€§å¢åŠ 
   - è§£å†³æ–¹æ¡ˆï¼šå®ç°å¢é‡ç”Ÿæˆï¼ˆæ­£åœ¨å¼€å‘ï¼‰

### 3.4.5 ä¸‹ä¸€æ­¥è®¡åˆ’

**çŸ­æœŸï¼ˆ1-2å‘¨ï¼‰**:
1. âœ… ä¿®å¤ /generate ç«¯ç‚¹è¶…æ—¶é—®é¢˜ (å·²å®Œæˆ)
2. ğŸ”´ å¯¼å‡º FP16 é‡åŒ–æ¨¡å‹ä»¥æå‡é€Ÿåº¦
3. ğŸ”´ å®ç°åŸºç¡€ KV Cache æ”¯æŒ

**ä¸­æœŸï¼ˆ2-4å‘¨ï¼‰**:
1. å®Œæ•´çš„ GQA æ”¯æŒ
2. SIMD ä¼˜åŒ–å…³é”®ç®—å­
3. ç›´æ¥æ”¯æŒ SafeTensors æ ¼å¼

**é•¿æœŸï¼ˆ4+å‘¨ï¼‰**:
1. è€ƒè™‘ ONNX Runtime åç«¯
2. è¯„ä¼°é›†æˆ llama.cpp æˆ– vLLM
3. å¤šè¯­è¨€æ¨¡å‹æ”¯æŒ

## 3.5 ä¸ç°æœ‰å®ç°å¯¹é½ä¸é€»è¾‘æ¾„æ¸…

æœ¬èŠ‚åŸºäºå½“å‰é¡¹ç›®çš„ `python/model_executor.py` å®ç°ä»¥åŠ `llama.cpp` çš„å…¬å¼€è®¾è®¡ï¼Œå¯¹æœ¬è®¾è®¡ä¸­å°šä¸å¤Ÿæ¸…æ™°çš„å…³é”®é€»è¾‘è¿›è¡Œæ¾„æ¸…å’Œè¡¥å……ã€‚

### 3.3.1 æ‰¹å¤„ç†æ¥å£ä¸BatchInput/BatchOutputå¯¹é½

- **ç°çŠ¶**ï¼š
  - æœ¬è®¾è®¡ä¸­ `InferenceEngine::forward(const std::vector<int>& inputIds)` æ˜¯**å•åºåˆ—**æ¥å£ï¼Œåªé€‚ç”¨äºç®€å•åœºæ™¯ã€‚
  - ç°æœ‰C++ `ModelExecutor` ä»¥åŠ Python ç‰ˆ `ModelExecutor.forward` éƒ½å›´ç»• **BatchInput/BatchOutput** ç»„ç»‡æ•°æ®ï¼š
    - è¾“å…¥æ˜¯**å±•å¹³åçš„tokenæ•°ç»„** + æ¯ä¸ªè¯·æ±‚çš„è¾¹ç•Œä¿¡æ¯ (`request_positions` / `requestPositions`)ï¼›
    - è¾“å‡ºæ˜¯ `[total_active_tokens, vocab_size]` å¸ƒå±€çš„logitsï¼Œä»¥åŠä¸è¾“å…¥ä¸€ä¸€å¯¹åº”çš„ä½ç½®ä¿¡æ¯ã€‚

- **æ¾„æ¸…åçš„è®¾è®¡**ï¼š
  - åœ¨å®ç°å±‚é¢ï¼Œ`InferenceEngine` éœ€è¦æä¾›ä¸€ä¸ª**æ‰¹å¤„ç†å‰å‘æ¥å£**ï¼Œä¸ cLLM ç°æœ‰æ‰¹å¤„ç†è¯­ä¹‰å¯¹é½ï¼š
    - æ¥å£å½¢å¼ç¤ºæ„ï¼ˆä¼ªä»£ç ï¼‰ï¼š
      ```cpp
      // ä¸ BatchInput è¯­ä¹‰å¯¹é½çš„æ‰¹å¤„ç†å‰å‘æ¥å£
      Tensor forwardBatch(
          const std::vector<int>& flatInputIds,          // å±•å¹³åçš„æ‰€æœ‰token
          const std::vector<size_t>& requestPositions,   // æ¯ä¸ªè¯·æ±‚åœ¨flatInputIdsä¸­çš„ç»“æŸä½ç½®
          const std::vector<std::string>& sequenceIds,   // ä¸è¯·æ±‚ä¸€ä¸€å¯¹åº”çš„sequence id
          size_t batchSize                               // è¯·æ±‚æ•°
      );
      ```
    - è¿”å›çš„ `Tensor` å½¢çŠ¶ä¸º `[total_active_tokens, vocab_size]`ï¼Œè¡Œé¡ºåºä¸ `flatInputIds` å®Œå…¨ä¸€è‡´ã€‚
    - `ModelExecutor::forward(const BatchInput&)` ä»…ä½œä¸ºè–„å°è£…ï¼š
      - è´Ÿè´£æŠŠ `BatchInput` å±•å¹³ä¸º `flatInputIds` / `requestPositions` / `sequenceIds`ï¼›
      - è°ƒç”¨ `InferenceEngine::forwardBatch`ï¼›
      - æŠŠè¿”å›çš„ `Tensor` æ‹†å› `BatchOutput` æ‰€æœŸæœ›çš„ `FloatArray logits`ã€`requestPositions` ç­‰ã€‚
  - `InferenceEngine::forward(const std::vector<int>& inputIds)` å¯ä»¥å®ç°ä¸ºå¯¹ `batchSize = 1` çš„ `forwardBatch` çš„ç®€å•åŒ…è£…ï¼Œæ–¹ä¾¿å•åºåˆ—è°ƒè¯•ã€‚

- **ä¸ Python å®ç°çš„å¯¹åº”å…³ç³»**ï¼š
  - Python ä¸­ `_forward_sync` çš„ `batch_inputs` å­—å…¸åŒ…å« `input_ids`ã€`request_positions`ã€`batch_size`ã€`sequence_ids` ç­‰å­—æ®µï¼›
  - æœ¬è®¾è®¡ä¸­çš„ `forwardBatch` ç›´æ¥å¯¹åº”è¿™ç»„å­—æ®µï¼Œåªæ˜¯ç”¨å¼ºç±»å‹çš„C++æ¥å£è¡¨è¾¾å‡ºæ¥ã€‚

### 3.3.2 logits å¸ƒå±€ä¸é‡‡æ ·æ¶ˆè´¹æ–¹å¼

- **logits å¸ƒå±€**ï¼š
  - ä¸ºå…¼å®¹å½“å‰ cLLM `BatchOutput` ä»¥åŠ Sampler çš„å®ç°ï¼Œ`InferenceEngine::forwardBatch` è¿”å›çš„logitsåº”é‡‡ç”¨ï¼š
    - å½¢çŠ¶ï¼š`[total_active_tokens, vocab_size]`
    - å†…å­˜å¸ƒå±€ï¼šè¡Œä¼˜å…ˆï¼ˆrow-majorï¼‰ï¼Œç¬¬ `i` è¡Œå¯¹åº”ç¬¬ `i` ä¸ªè¾“å…¥tokenã€‚
  - è¿™ä¸ç°æœ‰ `FloatArray` çš„ä½¿ç”¨æ–¹å¼ä¿æŒä¸€è‡´ï¼ŒSampler å¯ä»¥æŒ‰â€œæ¯ä¸ªè¯·æ±‚æœ€åä¸€ä¸ªtokenæ‰€åœ¨è¡Œâ€çš„æ–¹å¼å–å‡ºlogitsè¿›è¡Œé‡‡æ ·ã€‚

- **è§£ç é˜¶æ®µæ¶ˆè´¹é€»è¾‘ï¼ˆå‚è€ƒ python/model_executor.py å’Œ llama.cppï¼‰**ï¼š
  - **Prefill é˜¶æ®µ**ï¼š
    - ä¸€æ¬¡æ€§å–‚å…¥è¾ƒé•¿çš„promptï¼Œæ¯ä¸ªä½ç½®éƒ½å‚ä¸attentionè®¡ç®—ï¼›
    - logitçŸ©é˜µä¼šåŒ…å«æ‰€æœ‰ä½ç½®çš„è¾“å‡ºï¼Œä½†é€šå¸¸åªä½¿ç”¨æœ€åä¸€ä¸ªä½ç½®çš„logitsåšä¸‹ä¸€æ­¥é‡‡æ ·ï¼›
    - å…¶ä½™ä½ç½®çš„logitså¯é€‰åœ°ç”¨äºè°ƒè¯•æˆ–åˆ†æã€‚
  - **Decode é˜¶æ®µï¼ˆå¢é‡ç”Ÿæˆï¼‰**ï¼š
    - æ¯æ­¥ä»…è¿½åŠ 1ä¸ªtokenï¼Œä½†å¯èƒ½æœ‰å¤šä¸ªå¹¶å‘è¯·æ±‚ï¼ˆbatchï¼‰ï¼›
    - å¯¹æ¯ä¸ªè¯·æ±‚ï¼Œåªå…³æ³¨**è¯¥è¯·æ±‚æœ€åä¸€ä¸ªtoken**å¯¹åº”çš„é‚£ä¸€è¡Œlogitsï¼›
    - è¿™ä¸€æ¶ˆè´¹æ¨¡å¼ä¸ `llama.cpp` ä¸­â€œæ¯æ­¥åªå…³å¿ƒæœ€åä¸€ä¸ªä½ç½®çš„logitsâ€çš„åšæ³•ä¿æŒä¸€è‡´ã€‚

### 3.3.3 KV Cache å½¢çŠ¶ä¸ç”Ÿå‘½å‘¨æœŸ

- **è®¾è®¡åˆç¨¿ä¸­çš„å½¢çŠ¶**ï¼š
  - æ–‡æ¡£ä¸­ç»™å‡ºçš„å½¢çŠ¶ `Tensor kCache_  // [numLayers, batchSize, numKVHeads, maxSeqLen, headDim]` æ˜¯**é€»è¾‘è§†å›¾**ï¼Œæ–¹ä¾¿ç†è§£å±‚æ¬¡ç»“æ„ï¼›
  - å®é™…å®ç°ä¸­ï¼Œå‡ºäºå†…å­˜ä¸cacheå‹å¥½æ€§è€ƒè™‘ï¼Œæ›´å¯èƒ½é‡‡ç”¨**æ‰å¹³åŒ–çš„ä¸€ç»´buffer + æ˜¾å¼ç´¢å¼•è®¡ç®—**ï¼Œç±»ä¼¼ `llama.cpp` çš„åšæ³•ã€‚

- **å‚è€ƒ llama.cpp çš„å®è·µ**ï¼š
  - `llama.cpp` ä½¿ç”¨ `llama_kv_cache` ç»´æŠ¤æ¯å±‚çš„K/Vç¼“å­˜ï¼Œæ¯ä¸ªtokenåœ¨batchä¸­éƒ½æºå¸¦ï¼š(token id, pos, seq_id)ï¼›
  - KVç¼“å­˜æŒ‰(layer, head, seq_pos, head_dim)ç»„ç»‡ï¼Œå¹¶é€šè¿‡ `seq_id` åŒºåˆ†ä¸åŒè¯·æ±‚ï¼›
  - é€šè¿‡ `llama_batch` ç»“æ„åˆ†åˆ«ä¼ å…¥ `token[]`, `pos[]`, `seq_id[]`ï¼Œå®ç°**çµæ´»çš„è¿ç»­/ä¸è¿ç»­batch**ä¸**prefill/decodeæ··åˆ**åœºæ™¯ã€‚

- **æ¾„æ¸…åçš„ KV Cache è®¾è®¡**ï¼š
  - ä¿æŒæ–‡æ¡£ä¸­ç»™å‡ºçš„é€»è¾‘ç»´åº¦ä¸å˜ï¼Œä½†åœ¨å®ç°ä¸Šï¼š
    - æŒ‰å±‚ï¼ˆlayerï¼‰åˆ†å—å­˜å‚¨ï¼Œæ¯å±‚çš„K/Vç¼“å­˜å„è‡ªæ˜¯ä¸€å—è¿ç»­å†…å­˜ï¼›
    - `batchSize` ç»´åº¦æ›´å¤šæ˜¯â€œæœ¬æ­¥å‚ä¸è®¡ç®—çš„sequenceé›†åˆâ€ï¼Œè€Œä¸æ˜¯å›ºå®šå¤§å°çš„ç‰©ç†ç»´åº¦ï¼›
    - `seqLen` / `posOffset` å‚æ•°ç”¨äºä»å¤–éƒ¨çš„ `BatchInput` / `RequestState` ä¸­æ¨å¯¼å‡ºæ¯ä¸ªtokenåœ¨å…¨å±€åºåˆ—ä¸­çš„ä½ç½®ï¼›
    - KV Cache çš„å®é™…ç”Ÿå‘½å‘¨æœŸç®¡ç†åº”ä¸ç°æœ‰ `KVCache` æ¨¡å—å¯¹é½ï¼š
      - `InferenceEngine` ä¸ç›´æ¥â€œæ‹¥æœ‰â€æ‰€æœ‰KVå†…å­˜ï¼Œè€Œæ˜¯é€šè¿‡è½»é‡å¥æŸ„æˆ–è§†å›¾è®¿é—® `KVCache` æ¨¡å—åˆ†é…çš„bufferï¼›
      - å½“è¯·æ±‚å®Œæˆæˆ–è¢«å–æ¶ˆæ—¶ï¼Œç”±ä¸Šå±‚è°ƒåº¦å™¨é€šçŸ¥ `KVCache` å›æ”¶å¯¹åº”çš„sequenceæ§½ä½ã€‚

### 3.3.4 æ¨¡å‹æƒé‡æ ¼å¼ä¸åç§°æ˜ å°„

- **ç°çŠ¶**ï¼š
  - æ–‡æ¡£ä¸­ä»…æŠ½è±¡å‡º `ModelLoader::parseSafeTensors` å’Œ `mapWeightName`ï¼Œä½†å¯¹**å…·ä½“çš„æƒé‡å‘½åè§„åˆ™å’Œæ ¼å¼**æ²¡æœ‰å±•å¼€ã€‚
  - å½“å‰é¡¹ç›® Python ä¾§ä½¿ç”¨çš„æ˜¯ HuggingFace `AutoModelForCausalLM` + safetensorsï¼›`llama.cpp` åˆ™ä½¿ç”¨ GGUF/ggml è‡ªå·±çš„ä¸€å¥—æ ¼å¼ã€‚

- **æ¾„æ¸…ä¸çº¦æŸ**ï¼š
  - **ç¬¬ä¸€é˜¶æ®µç›®æ ‡**ï¼šç›´æ¥æ”¯æŒ HuggingFace safetensors æ ¼å¼çš„ Qwen3-0.6Bï¼ˆä¸ `python/model_executor.py` ä½¿ç”¨çš„æ¨¡å‹è·¯å¾„ä¿æŒä¸€è‡´ï¼‰ï¼›
    - `ModelLoader` éœ€è¦æ¸…æ™°å®šä¹‰ä» HF æƒé‡ååˆ°å†…éƒ¨æƒé‡åï¼ˆå¦‚ `layers.N.attention.wq` ç­‰ï¼‰çš„æ˜ å°„è¡¨ï¼›
    - Qwen3 çš„æƒé‡å‘½åè§„åˆ™ï¼ˆä¾‹å¦‚ `model.layers.0.self_attn.q_proj.weight`ï¼‰éœ€è¦åœ¨è®¾è®¡æ–‡æ¡£ä¸­è¡¥å……ä¸€ä»½å¯¹ç…§è¡¨ï¼ˆåç»­å¯ä»¥å•ç‹¬æ‰©å±•ä¸€ä¸ªâ€œæƒé‡æ˜ å°„â€å°èŠ‚ï¼‰ã€‚
  - **ä¸­é•¿æœŸç›®æ ‡**ï¼šå¯ä»¥æ–°å¢ GGUF backendï¼Œä¸ `llama.cpp` ç”Ÿæ€å…¼å®¹ï¼š
    - åœ¨ `ModelLoader` å†…éƒ¨æŠ½è±¡å‡º `IWeightBackend` æ¥å£ï¼Œæ”¯æŒ `SafeTensorsBackend` ä¸ `GgufBackend` ä¸¤ç§å®ç°ï¼›
    - è¿™ä¸ä¼šæ”¹å˜ä¸Šå±‚ `TransformerModel` / `InferenceEngine` çš„æ¥å£ï¼Œåªæ˜¯æƒé‡æ¥æºä¸åŒã€‚

### 3.3.5 çº¿ç¨‹æ¨¡å‹ä¸å¹¶è¡Œç­–ç•¥

- **Python å®ç°å‚è€ƒ**ï¼š
  - `python/model_executor.py` å†…éƒ¨åˆ›å»ºäº†ä¸€ä¸ª `ThreadPoolExecutor`ï¼Œé€šè¿‡ `loop.run_in_executor` æŠŠæ¨ç†ä»»åŠ¡ offload åˆ°çº¿ç¨‹æ± ï¼›
  - è¿™æ ·å¯ä»¥é¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼ŒåŒæ—¶åˆ©ç”¨å¤šæ ¸ CPUã€‚

- **ä¸ cLLM çº¿ç¨‹æ± çš„ååŒ**ï¼š
  - cLLM å·²æœ‰ç‹¬ç«‹çš„ `thread_pool` æ¨¡å—å’Œè°ƒåº¦å™¨ï¼Œè´Ÿè´£æ•´ä¸ªç³»ç»ŸèŒƒå›´çš„å¹¶è¡Œä»»åŠ¡è°ƒåº¦ï¼›
  - ä¸ºé¿å…çº¿ç¨‹è¿‡åº¦è†¨èƒ€ï¼Œè‡ªç ”æ¨ç†å¼•æ“ä¸å†åœ¨å†…éƒ¨åˆ›å»ºæ–°çš„é€šç”¨çº¿ç¨‹æ± ï¼Œè€Œæ˜¯éµå¾ªï¼š
    - **æ•°å€¼ç®—å­å†…éƒ¨**ï¼ˆå¦‚SIMD matmulï¼‰ä½¿ç”¨æ•°æ®çº§å¹¶è¡Œï¼Œä½†ä¸ä¸»åŠ¨å†åˆ‡åˆ†ä¸ºå¤šä¸ªç³»ç»Ÿçº¿ç¨‹ï¼›
    - **ä»»åŠ¡çº§å¹¶è¡Œ**ç”±ä¸Šå±‚è°ƒåº¦å™¨+çº¿ç¨‹æ± ç»Ÿä¸€ç®¡ç†ï¼Œä¾‹å¦‚å¤šä¸ªBatchå¹¶è¡Œã€å¤šä¸ªè¯·æ±‚å¹¶è¡Œï¼›
    - å¦‚æœåç»­éœ€è¦åœ¨æ¨ç†å¼•æ“å†…éƒ¨åšå¤šçº¿ç¨‹ï¼ˆä¾‹å¦‚åˆ†å±‚å¹¶è¡Œï¼‰ï¼Œä¹Ÿåº”é€šè¿‡ä¸ç°æœ‰çº¿ç¨‹æ± æ¥å£åå•†çš„æ–¹å¼å®ç°ã€‚

### 3.3.6 ä»éœ€åœ¨å®ç°ä¸­éªŒè¯çš„å¼€æ”¾é—®é¢˜

åœ¨å½“å‰è®¾è®¡å±‚é¢ï¼Œä»æœ‰è‹¥å¹²ç»†èŠ‚éœ€è¦åœ¨ç¼–ç ä¸å®éªŒé˜¶æ®µè¿›ä¸€æ­¥éªŒè¯ï¼š

- **KV Cache å®¹é‡ä¸å›æ”¶ç­–ç•¥**ï¼š
  - å¦‚ä½•åœ¨é•¿å¯¹è¯åœºæ™¯ä¸‹æ§åˆ¶æ¯ä¸ªsequenceæœ€å¤§é•¿åº¦ã€ä½•æ—¶è£å‰ªæˆ–ä¸¢å¼ƒå†å²tokenï¼›
  - ä¸ç°æœ‰ `KVCache` æ¨¡å—çš„æŒ‡æ ‡ï¼ˆå‘½ä¸­ç‡ã€å†…å­˜å ç”¨ï¼‰å¯¹é½ã€‚
- **é‡åŒ–ç²’åº¦çš„æœ€ç»ˆé€‰æ‹©**ï¼š
  - ç›®å‰æ–‡æ¡£ä»…å®šä¹‰äº†å¼ é‡çº§scaleçš„INT8/INT4æ–¹æ¡ˆï¼Œå®é™…å¯èƒ½éœ€è¦ per-channel æˆ– block-wise é‡åŒ–ä»¥è·å¾—æ›´å¥½ç²¾åº¦/æ€§èƒ½æŠ˜ä¸­ï¼›
  - æ˜¯å¦ä¸ `llama.cpp` çš„ Q4_0/Q5_0 ç­‰æ ¼å¼å…¼å®¹ï¼Œéœ€è¦åœ¨åç»­é˜¶æ®µæ˜ç¡®ã€‚
- **æ›´é«˜é˜¶ä¼˜åŒ–ï¼ˆå¦‚Flash Attentionã€MLAç­‰ï¼‰**ï¼š
  - ç›®å‰åªè§„åˆ’äº†ä¼ ç»ŸAttention + KV Cacheä¼˜åŒ–ï¼Œæœªæ¥å¯ä»¥å‚è€ƒç¤¾åŒºè¿›å±•ï¼ˆå¦‚FlashAttentionã€Multi-Head Latent Attentionï¼‰è¿›è¡Œè¿­ä»£ï¼›
  - è¿™äº›ä¼˜åŒ–ä¼šå½±å“ç®—å­æ¥å£ä¸å†…å­˜å¸ƒå±€ï¼Œéœ€è¦å•ç‹¬ä¸€è½®è®¾è®¡è¯„å®¡ã€‚

ä¸Šè¿°å¼€æ”¾é—®é¢˜å¹¶ä¸é˜»ç¢ç¬¬ä¸€ç‰ˆå¼•æ“çš„å®ç°ï¼Œä½†éœ€è¦åœ¨è½åœ°è¿‡ç¨‹ä¸­æŒç»­å›é¡¾å¹¶æ›´æ–°æœ¬è®¾è®¡æ–‡æ¡£ï¼Œä»¥ä¿æŒâ€œè®¾è®¡ â†” å®ç°â€ä¸€è‡´æ€§ã€‚

## 4. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 4.1 SIMDä¼˜åŒ–æ£€æŸ¥æ¸…å•

- [ ] AVX2çŸ©é˜µä¹˜æ³•æ ¸å¿ƒ
- [ ] AVX-512çŸ©é˜µä¹˜æ³•æ ¸å¿ƒ  
- [ ] SIMD Softmax
- [ ] SIMD RMSNorm
- [ ] SIMD RoPE
- [ ] å‘é‡åŒ–Add/Mulæ“ä½œ

### 4.2 å†…å­˜ä¼˜åŒ–

- [ ] å†…å­˜æ± ç®¡ç†
- [ ] KV Cacheé¢„åˆ†é…
- [ ] æƒé‡å…±äº«
- [ ] ä¸´æ—¶ç¼“å†²åŒºå¤ç”¨
- [ ] å†…å­˜å¯¹é½ (64å­—èŠ‚)

### 4.3 è®¡ç®—ä¼˜åŒ–

- [ ] çŸ©é˜µåˆ†å— (Tiling)
- [ ] Kernel Fusion
- [ ] é¢„è®¡ç®—cos/sinè¡¨
- [ ] å¾ªç¯å±•å¼€
- [ ] æ•°æ®é¢„å–

## 5. æµ‹è¯•ç­–ç•¥

### 5.1 å•å…ƒæµ‹è¯•

**æ–‡ä»¶**: `tests/test_inference_engine.cpp`

```cpp
// æµ‹è¯•å„ä¸ªç»„ä»¶
TEST(TensorTest, BasicOperations)
TEST(RMSNormTest, Forward)
TEST(RoPETest, RotaryEmbedding)
TEST(AttentionTest, MultiHeadAttention)
TEST(FFNTest, SwiGLU)
TEST(TransformerBlockTest, Forward)
TEST(ModelLoaderTest, LoadSafeTensors)
TEST(InferenceEngineTest, EndToEnd)
```

### 5.2 ç²¾åº¦éªŒè¯

```python
# å¯¹æ¯”Pythonå‚è€ƒå®ç°
import torch
from transformers import AutoModelForCausalLM

# åŠ è½½PyTorchæ¨¡å‹
model_pt = AutoModelForCausalLM.from_pretrained(model_path)

# åŠ è½½è‡ªç ”å¼•æ“
engine = InferenceEngine(model_path)

# ç›¸åŒè¾“å…¥
input_ids = [1, 2, 3, 4, 5]

# æ¯”è¾ƒè¾“å‡º
output_pt = model_pt(torch.tensor([input_ids]))
output_cpp = engine.forward(input_ids)

# è®¡ç®—å·®å¼‚
max_diff = max_absolute_difference(output_pt, output_cpp)
assert max_diff < 1e-3  # FP32ç²¾åº¦é˜ˆå€¼
```

### 5.3 æ€§èƒ½åŸºå‡†æµ‹è¯•

```cpp
// Benchmarkæµ‹è¯•
void benchmark_inference() {
    InferenceEngine engine(model_path);
    
    // ä¸åŒåºåˆ—é•¿åº¦
    std::vector<size_t> seqLengths = {1, 10, 50, 100, 200, 500};
    
    for (size_t len : seqLengths) {
        std::vector<int> input(len, 1);
        
        auto start = std::chrono::high_resolution_clock::now();
        for (int i = 0; i < 100; i++) {
            engine.forward(input);
        }
        auto end = std::chrono::high_resolution_clock::now();
        
        auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
        std::cout << "SeqLen=" << len 
                  << " AvgTime=" << duration.count() / 100.0 << "ms\n";
    }
}
```

## 6. å¼€å‘è®¡åˆ’

### 6.0 ç¬¬ä¸€é˜¶æ®µæœ€å°å¯è¡Œå®ç°ï¼ˆMVPï¼‰

æœ¬èŠ‚ä»æ•´ä½“è®¾è®¡ä¸­æŠ½å–å‡º**ç¬¬ä¸€é˜¶æ®µæœ€å°å¯è¡Œå®ç°ï¼ˆMVPï¼‰**ï¼Œç”¨äºæŒ‡å¯¼å®é™…ç¼–ç ã€‚MVP çš„ç›®æ ‡æ˜¯ï¼š

- å»æ‰å½“å‰ C++ `ModelExecutor` ä¸­çš„ mock logits å®ç°ï¼Œ
- ç”¨çœŸå®çš„ Transformer è®¡ç®—é“¾è·¯ï¼ˆåŸºäº Qwen3-0.6B æƒé‡ï¼‰ï¼Œ
- åœ¨ **å•æœº CPU** ä¸Šè·‘é€š `/generate` çš„å®Œæ•´æµç¨‹ï¼Œ
- å…ˆä¿è¯ **åŠŸèƒ½æ­£ç¡® + ä¸ Python ç‰ˆæœ¬å¯¹é½**ï¼Œå†é€æ­¥æ¼”è¿›åˆ°é«˜æ€§èƒ½/é‡åŒ–ç‰ˆæœ¬ã€‚

#### 6.0.1 MVP èŒƒå›´ä¸çº¦æŸ

- **å¿…é¡»åŒ…å«**ï¼š
  - çœŸå®æ¨¡å‹å‰å‘è®¡ç®—ï¼ˆEmbedding â†’ N å±‚ TransformerBlock â†’ FinalNorm â†’ LM Headï¼‰ï¼Œ
  - æ”¯æŒå• batchï¼ˆ`batchSize = 1`ï¼‰å’ŒåŸºç¡€çš„ Prefill + Decodeï¼Œ
  - è¾“å‡º logits å½¢çŠ¶ä¸º `[total_tokens, vocab_size]`ï¼Œä¸ç°æœ‰ `BatchOutput` / `Sampler` å¯¹é½ï¼Œ
  - æ¥å…¥ç°æœ‰ C++ `ModelExecutor`ï¼Œè®© `/generate` èƒ½ç”ŸæˆçœŸå®æ–‡æœ¬ã€‚
- **æš‚ä¸åŒ…å«**ï¼š
  - SIMD/AVX ä¼˜åŒ–ï¼ˆå…¨éƒ¨ç”¨ naÃ¯ve for å¾ªç¯å®ç°ï¼‰ï¼Œ
  - é‡åŒ–ï¼ˆä»…æ”¯æŒ FP32ï¼‰ï¼Œ
  - é«˜çº§ KV Cache ä¼˜åŒ–ï¼ˆåªåšæœ´ç´ ç‰ˆï¼Œç”šè‡³å¯ä»¥å…ˆæ—  KV å†è¡¥ï¼‰ï¼Œ
  - é€šç”¨å¤šåç«¯/å¤šæ¨¡å‹æ”¯æŒï¼ˆå…ˆåš Qwen3 ä¸“ç”¨ï¼‰ã€‚

#### 6.0.2 MVP å®æ–½æ­¥éª¤ï¼ˆå»ºè®®ç¼–ç é¡ºåºï¼‰

**æ­¥éª¤ Aï¼šæœ€å°è®¡ç®—éª¨æ¶ï¼ˆå•åºåˆ—ã€æ—  KVï¼‰**

1. **Tensor + åŸºç¡€ç®—å­**  
   - åœ¨ `include/cllm/inference/` ä¸‹å®ç°æœ€ç®€ç‰ˆ `Tensor`ï¼š
     - ä»…æ”¯æŒ `float`ã€CPUï¼›
     - å†…éƒ¨å¯ä»¥ç”¨ `std::vector<float>` æ‰¿è½½æ•°æ®ï¼›
     - æä¾› `shape() / size() / data()` å’Œç®€å• `reshape()`ã€‚  
   - åœ¨ `kernels` ä¸­å®ç°åŸºç¡€ç®—å­ï¼š
     - `matmul(A[M,K], B[K,N]) -> C[M,N]`ï¼ˆnaÃ¯ve å®ç°ï¼‰ï¼Œ
     - æ•°å€¼ç¨³å®šç‰ˆæœ¬ `softmax_stable`ï¼ˆæŒ‰æœ€åä¸€ç»´å½’ä¸€åŒ–ï¼‰ï¼Œ
     - `rmsnorm`ï¼ˆæŒ‰æœ€åä¸€ç»´å½’ä¸€åŒ–ï¼‰ï¼Œ
     - `silu` æ¿€æ´»å‡½æ•°ã€‚  
   - ç¼–å†™å°å‹å•å…ƒæµ‹è¯•ï¼ŒéªŒè¯ç®—å­åœ¨å°çŸ©é˜µä¸Šçš„æ­£ç¡®æ€§ã€‚

2. **RoPE + æ—  KV çš„ Multi-Head Attention**  
   - å®ç° RoPEï¼š
     - è¾“å…¥ Q/K å½¢çŠ¶ä¸º `[batch, num_heads, seq_len, head_dim]`ï¼›
     - å…ˆä¸åšé¢„è®¡ç®—è¡¨ä¼˜åŒ–ï¼Œå¯ç›´æ¥åœ¨æ¯æ¬¡è°ƒç”¨ä¸­è®¡ç®— `cos/sin`ï¼›
     - æ”¯æŒ Qwen3 æ‰€éœ€çš„ç»´åº¦å’Œ `theta`ã€‚  
   - å®ç° `MultiHeadAttention::forward_no_kv`ï¼š
     - ä»è¾“å…¥ `[batch, seq_len, hidden_size]` æŠ•å½±å‡º Q/K/Vï¼›
     - reshape + transpose åˆ° `[batch, heads, seq, head_dim]`ï¼›
     - åº”ç”¨ RoPEï¼›
     - è®¡ç®—æ ‡å‡† scaled dot-product attention + softmaxï¼›
     - åˆå¹¶ headså¹¶é€šè¿‡ `Wo` åšè¾“å‡ºæŠ•å½±ã€‚  
   - åˆç‰ˆå¯ä»¥åªæ”¯æŒ `numKVHeads == numHeads`ï¼Œä¸åš GQAï¼Œç¡®ä¿é€»è¾‘æ¸…æ™°æ˜“æµ‹ã€‚

3. **FeedForward + TransformerBlock + TransformerModel**  
   - å®ç° `FeedForwardNetwork`ï¼ˆSwiGLUï¼‰ï¼š
     - `gate = x @ W_gate`ï¼Œ`up = x @ W_up`ï¼Œ
     - `activated = gate * SiLU(up)`ï¼Œ
     - `output = activated @ W_down`ã€‚  
   - å®ç° `TransformerBlock`ï¼š
     - Pre-Norm Attention + æ®‹å·®ï¼Œ
     - Pre-Norm FFN + æ®‹å·®ã€‚  
   - å®ç° `TransformerModel`ï¼š
     - Embedding lookupï¼ˆ`[seq] -> [seq, hidden]`ï¼‰ï¼Œ
     - ä¾æ¬¡é€šè¿‡ N å±‚ `TransformerBlock`ï¼Œ
     - æœ€ååš RMSNorm + `lm_head` æŠ•å½±åˆ° vocabã€‚  
   - MVP é˜¶æ®µï¼š
     - ä»…æ”¯æŒ `batch=1`ï¼Œ
     - ä» `ModelConfig` ä¸­è¯»å– Qwen3 çš„åŸºæœ¬ç»“æ„å‚æ•°ï¼ˆå±‚æ•°ã€hidden sizeã€heads ç­‰ï¼‰ã€‚

**æ­¥éª¤ Bï¼šQwen3 æƒé‡åŠ è½½ + å•åºåˆ—å‰å‘æ‰“é€š**

4. **Qwen3 å®šåˆ¶ç‰ˆ ModelLoaderï¼ˆç¬¬ä¸€é˜¶æ®µå¯ä»¥å€ŸåŠ©ç¦»çº¿è½¬æ¢ï¼‰**  
   - ä¸ºé™ä½å¤æ‚åº¦ï¼ŒMVP å¯ä»¥é‡‡ç”¨â€œPython å¯¼å‡º + C++ ç®€å•è¯»å–â€çš„ç­–ç•¥ï¼š
     - ä½¿ç”¨ Python è„šæœ¬ä» HF safetensors ä¸­å¯¼å‡ºä¸€ä¸ªç»Ÿä¸€çš„äºŒè¿›åˆ¶æƒé‡æ–‡ä»¶ï¼ˆæˆ–è‹¥å¹² `.bin`ï¼‰ï¼Œ
       å¹¶åœ¨å¯¼å‡ºæ—¶å®Œæˆæƒé‡å‘½åæ˜ å°„ï¼ˆHF åç§° â†’ å†…éƒ¨åç§°ï¼‰ï¼›
     - C++ çš„ `ModelLoader` åªè´Ÿè´£æ‰“å¼€ `.bin` æ–‡ä»¶ï¼ŒæŒ‰å›ºå®šé¡ºåº/offset åŠ è½½åˆ° `Tensor`ã€‚  
   - ç¬¬ä¸€é˜¶æ®µè‡³å°‘è¦åŠ è½½ï¼š
     - Embedding,
     - æ¯å±‚ Attention çš„ Wq/Wk/Wv/Woï¼Œ
     - æ¯å±‚ FFN çš„ Wgate/Wup/Wdownï¼Œ
     - æ¯å±‚ RMSNorm çš„æƒé‡ï¼Œ
     - Final RMSNorm æƒé‡ï¼Œ
     - lm_head æƒé‡ã€‚

5. **å®ç° InferenceEngine::initialize + forwardï¼ˆå•åºåˆ—ï¼‰**  
   - åœ¨ `InferenceEngine` ä¸­ï¼š
     - æ„é€ æ—¶åˆ›å»º `ModelLoader` å’Œ `TransformerModel`ï¼›
     - `initialize()` è°ƒç”¨ `loader.load()` + `model.loadWeights()`ï¼›
     - ä¿å­˜ `ModelConfig`ã€‚  
   - `Tensor forward(const std::vector<int>& inputIds)`ï¼š
     - è°ƒç”¨ `TransformerModel::forward`ï¼Œå¾—åˆ° logits `[1, seq_len, vocab_size]`ï¼›
     - å‹æˆ `[seq_len, vocab_size]` ä»¥å…¼å®¹ç°æœ‰ `FloatArray` ä½¿ç”¨ä¹ æƒ¯ã€‚  
   - å…ˆä¸åš KV Cacheï¼šæ¯æ¬¡ decode é‡æ–°è·‘æ•´æ®µå‰å‘ï¼Œè™½æ…¢ä½†é€»è¾‘ç®€å•ï¼Œä¾¿äºéªŒè¯ã€‚

6. **æ¥å…¥ ModelExecutor::_executeModelInferenceï¼ˆå•è¯·æ±‚ç‰ˆï¼‰**  
   - åœ¨ `ModelExecutor` æ„é€ å‡½æ•°ä¸­ï¼š
     - åˆ›å»º `InferenceEngine`ï¼Œå¹¶è°ƒç”¨ `initialize()`ï¼›
     - åˆå§‹åŒ–å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸ã€‚  
   - åœ¨ `_executeModelInference` ä¸­ï¼š
     - æš‚æ—¶å‡è®¾ä¼ å…¥çš„ `FloatArray` ä¸­å­˜çš„æ˜¯ token idï¼ˆåç»­å†ä¸ tokenizer/encode é€»è¾‘å¯¹é½ï¼‰ï¼›
     - è½¬ä¸º `std::vector<int> inputIds`ï¼›
     - è°ƒç”¨ `inferenceEngine_->forward(inputIds)`ï¼›
     - å°† logits æ‹·è´å› `FloatArray`ï¼Œç¡®ä¿å¸ƒå±€ä¸º `[total_tokens, vocab_size]`ã€‚  
   - ç›®æ ‡ï¼šåœ¨ `batchSize = 1` çš„æƒ…å†µä¸‹ï¼Œ/generate èƒ½çœŸæ­£èµ°åˆ°çœŸå® logitsï¼Œè€Œé mock å®ç°ã€‚

7. **ä¸ Python model_executor.py åšå¯¹é½æµ‹è¯•**  
   - é€‰å®šåŒä¸€ä¸ª Qwen3 æ¨¡å‹è·¯å¾„ï¼›
   - å›ºå®šä¸€ä¸ªçŸ­ promptï¼ˆä¾‹å¦‚ "ä½ å¥½" æˆ–å›ºå®šçš„ token id åˆ—è¡¨ï¼‰ï¼›
   - åˆ†åˆ«ç”¨ï¼š
     - Python `ModelExecutor._forward_sync`ï¼Œ
     - C++ `InferenceEngine::forward`ï¼›
   - å¯¹æ¯”ï¼š
     - æœ€åä¸€ token çš„ logitsï¼Œ
     - top-kï¼ˆä¾‹å¦‚ top-5ï¼‰ çš„ token id å’Œæ¦‚ç‡ï¼›
   - åœ¨ FP32 ä¸‹ï¼Œ`max_diff` æ§åˆ¶åœ¨ä¸€ä¸ªåˆç†é˜ˆå€¼ï¼ˆå¦‚ 1e-3ï¼‰å†…å³å¯è®¤ä¸ºå®ç°æ­£ç¡®ã€‚

**æ­¥éª¤ Cï¼šBatch + ç®€å• KV Cacheï¼Œä½¿å…¶å¯è¢« cLLM è°ƒåº¦å™¨é©±åŠ¨**

8. **å®ç° InferenceEngine::forwardBatchï¼ˆé€»è¾‘ä¸Šå¯¹é½ BatchInput/BatchOutputï¼‰**  
   - æ–°å¢æ‰¹å¤„ç†æ¥å£ï¼š
     ```cpp
     Tensor forwardBatch(
         const std::vector<int>& flatInputIds,
         const std::vector<size_t>& requestPositions,
         const std::vector<std::string>& sequenceIds,
         size_t batchSize
     );
     ```
   - MVP å®ç°å¯ä»¥å…ˆç”¨â€œé€è¯·æ±‚å¾ªç¯â€çš„æ–¹å¼ï¼š
     - æŒ‰ `requestPositions` åˆ‡åˆ† `flatInputIds`ï¼Œè¿˜åŸæ¯ä¸ªè¯·æ±‚çš„ `inputIds`ï¼›
     - é€è¯·æ±‚è°ƒç”¨å•åºåˆ— `forward`ï¼›
     - å°†æ¯ä¸ªè¯·æ±‚çš„ logits æŒ‰é¡ºåºæ‹¼æ¥ï¼Œå½¢æˆ `[total_tokens, vocab_size]`ï¼›
   - ç„¶ååœ¨ `ModelExecutor::forward(const BatchInput&)` ä¸­ï¼š
     - ä» `BatchInput` ç»„è£… `flatInputIds` / `requestPositions` / `sequenceIds`ï¼›
     - è°ƒç”¨ `forwardBatch`ï¼›
     - æŠŠè¿”å›çš„ logits å†™å…¥ `BatchOutput` ä¸­çš„ `FloatArray`ï¼Œä¸ç°æœ‰è°ƒåº¦/é‡‡æ ·é€»è¾‘å¯¹é½ã€‚

9. **åŠ å…¥ç®€å•ç‰ˆ KV Cache æ”¯æŒ Decode**  
   - åœ¨ Multi-Head Attention ä¸­å¢åŠ ä¸€ä¸ªæ”¯æŒ KV Cache çš„ `forward` é‡è½½ï¼š
     - è¾“å…¥æ–°å¢ `kCache` / `vCache` å¥æŸ„å’Œ `posOffset`ï¼›
     - å¯¹äº decode é˜¶æ®µï¼Œä»…å¯¹æ–°å¢ token è®¡ç®— Q/K/Vï¼Œä½† attention çš„ K/V åŒ…æ‹¬å†å²ç¼“å­˜ï¼›
   - åœ¨ `InferenceEngine` ä¸­ç»´æŠ¤ä¸€ä¸ªç®€æ˜“çš„ per-sequence KV ç¼“å­˜ç»“æ„ï¼š
     - `std::unordered_map<sequence_id, KVBuffer>`ï¼›
     - `KVBuffer` å†…éƒ¨ç”¨æ‰å¹³ `std::vector<float>` è¡¨ç¤º `[layers, heads, pos, dim]` çš„é€»è¾‘å¸ƒå±€ï¼›
   - è°ƒåº¦ç­–ç•¥ï¼š
     - Prefill é˜¶æ®µï¼šä¸ä½¿ç”¨ KVï¼Œç›´æ¥è·‘å…¨åºåˆ—ï¼Œå¹¶åœ¨ç»“æŸæ—¶æŠŠæ‰€æœ‰ K/V å­˜å…¥ç¼“å­˜ï¼›
     - Decode é˜¶æ®µï¼šæ¯æ­¥ä»…è¿½åŠ 1ä¸ª tokenï¼Œä½¿ç”¨ç¼“å­˜çš„ K/V + æ–° token çš„ K/V åš attentionã€‚  
   - ç›®æ ‡ï¼šé…åˆç°æœ‰ Scheduler/BatchProcessorï¼Œèƒ½æŒç»­è°ƒç”¨ decode æ­¥ï¼Œè®© `/generate` ç”Ÿæˆå¤š token æ–‡æœ¬ã€‚

ä»¥ä¸Š 9 ä¸ªæ­¥éª¤å®Œæˆåï¼Œå³å¯è®¤ä¸º**ç¬¬ä¸€é˜¶æ®µ MVP å°±ç»ª**ï¼š
- C++ ä¾§æ‹¥æœ‰ä¸€ä¸ªçœŸå®çš„ã€è‡ªç ”çš„ Transformer æ¨ç†é“¾è·¯ï¼›
- `/generate` èƒ½åœ¨ CPU ä¸Šç”¨çœŸå®æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ï¼›
- è¡Œä¸ºä¸ Python `model_executor.py` åœ¨æ•°å€¼ä¸ŠåŸºæœ¬å¯¹é½ï¼Œä¸ºåç»­ SIMD ä¼˜åŒ–ã€é‡åŒ–æ”¯æŒã€æ‰¹é‡é«˜æ€§èƒ½æ¨ç†æ‰“ä¸‹åŸºç¡€ã€‚

### 6.1 é˜¶æ®µ1: åŸºç¡€æ¶æ„ (3å‘¨)

**Week 1**: å¼ é‡æŠ½è±¡å’Œå†…å­˜ç®¡ç†
- [ ] Tensorç±»å®ç°
- [ ] MemoryPoolå®ç°
- [ ] åŸºç¡€æµ‹è¯•

**Week 2**: æ¨¡å‹åŠ è½½å™¨
- [ ] SafeTensorsè§£æ
- [ ] Configè§£æ
- [ ] æƒé‡åŠ è½½æµ‹è¯•

**Week 3**: åŸºç¡€ç®—å­
- [ ] MatMul (CPUç‰ˆæœ¬)
- [ ] Softmax
- [ ] Add/Mul/Reshape

### 6.2 é˜¶æ®µ2: Transformeræ ¸å¿ƒ (6å‘¨)

**Week 4-5**: Normalizationå’ŒPosition Encoding
- [ ] RMSNormå®ç°
- [ ] RoPEå®ç°
- [ ] å•å…ƒæµ‹è¯•

**Week 6-7**: Attentionæœºåˆ¶
- [ ] QKVæŠ•å½±
- [ ] Attentionè®¡ç®—
- [ ] KV Cacheé›†æˆ
- [ ] ç²¾åº¦æµ‹è¯•

**Week 8-9**: Feed-Forward Network
- [ ] Linearå±‚
- [ ] SwiGLUæ¿€æ´»
- [ ] Transformer Blockç»„è£…
- [ ] ç«¯åˆ°ç«¯æµ‹è¯•

### 6.3 é˜¶æ®µ3: ä¼˜åŒ–ä¸é‡åŒ– (5å‘¨)

**Week 10-11**: SIMDä¼˜åŒ–
- [ ] AVX2çŸ©é˜µä¹˜æ³•
- [ ] AVX-512çŸ©é˜µä¹˜æ³•
- [ ] å‘é‡åŒ–Softmax/RMSNorm
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•

**Week 12-13**: é‡åŒ–æ”¯æŒ
- [ ] INT8é‡åŒ–
- [ ] INT4é‡åŒ–
- [ ] é‡åŒ–æ¨ç†
- [ ] ç²¾åº¦éªŒè¯

**Week 14**: KV Cacheä¼˜åŒ–
- [ ] åŠ¨æ€KV Cache
- [ ] å†…å­˜ä¼˜åŒ–
- [ ] æ€§èƒ½è°ƒä¼˜

### 6.4 é˜¶æ®µ4: é›†æˆä¸æµ‹è¯• (4å‘¨)

**Week 15**: é›†æˆåˆ°cLLM
- [ ] ModelExecutoré›†æˆ
- [ ] æ¥å£é€‚é…
- [ ] åŠŸèƒ½æµ‹è¯•

**Week 16**: ç«¯åˆ°ç«¯æµ‹è¯•
- [ ] /generateç«¯ç‚¹æµ‹è¯•
- [ ] /encodeç«¯ç‚¹æµ‹è¯•
- [ ] å¹¶å‘æµ‹è¯•

**Week 17**: æ€§èƒ½ä¼˜åŒ–
- [ ] Profilingåˆ†æ
- [ ] ç“¶é¢ˆä¼˜åŒ–
- [ ] å†…å­˜ä¼˜åŒ–

**Week 18**: æ–‡æ¡£å’Œå‘å¸ƒ
- [ ] APIæ–‡æ¡£
- [ ] ä½¿ç”¨æŒ‡å—
- [ ] æ€§èƒ½æŠ¥å‘Š

## 7. é£é™©è¯„ä¼°

| é£é™©é¡¹ | ä¸¥é‡ç¨‹åº¦ | ç¼“è§£æªæ–½ |
|-------|---------|---------|
| SIMDä¼˜åŒ–éš¾åº¦é«˜ | é«˜ | å‚è€ƒllama.cppå®ç°ï¼Œåˆ†é˜¶æ®µä¼˜åŒ– |
| ç²¾åº¦æŸå¤± | ä¸­ | ä¸¥æ ¼çš„å•å…ƒæµ‹è¯•å’Œç²¾åº¦éªŒè¯ |
| å†…å­˜æ³„æ¼ | ä¸­ | ä½¿ç”¨RAIIï¼Œvalgrindæ£€æµ‹ |
| æ€§èƒ½ä¸è¾¾æ ‡ | é«˜ | å……åˆ†Profilingï¼ŒæŒç»­ä¼˜åŒ– |
| å¼€å‘å‘¨æœŸè¿‡é•¿ | é«˜ | é‡‡ç”¨æ•æ·è¿­ä»£ï¼Œä¼˜å…ˆæ ¸å¿ƒåŠŸèƒ½ |

## 8. æˆåŠŸæ ‡å‡†

### 8.1 åŠŸèƒ½æ ‡å‡†
- âœ… æ”¯æŒQwen3-0.6Bæ¨¡å‹å®Œæ•´æ¨ç†
- âœ… è¾“å‡ºä¸PyTorchç‰ˆæœ¬ç²¾åº¦å·®å¼‚ < 1e-3
- âœ… æ”¯æŒFP32/FP16/INT8é‡åŒ–
- âœ… æ­£ç¡®å¤„ç†KV Cache
- âœ… é›†æˆåˆ°cLLMæœåŠ¡å™¨

### 8.2 æ€§èƒ½æ ‡å‡†
- âœ… Prefillé˜¶æ®µ (seq_len=512): < 500ms
- âœ… Decodeé˜¶æ®µ (seq_len=1): < 50ms
- âœ… å†…å­˜å ç”¨: < 2GB (FP32)
- âœ… ååé‡: > 10 tokens/sec

### 8.3 ä»£ç è´¨é‡æ ‡å‡†
- âœ… ä»£ç è¦†ç›–ç‡ > 80%
- âœ… æ— å†…å­˜æ³„æ¼ (valgrindéªŒè¯)
- âœ… éµå¾ªC++ç¼–ç¨‹è§„èŒƒ
- âœ… å®Œæ•´çš„æ–‡æ¡£å’Œæ³¨é‡Š

## 9. å‚è€ƒèµ„æ–™

### 9.1 å­¦æœ¯è®ºæ–‡
- Attention Is All You Need (TransformeråŸè®ºæ–‡)
- RoFormer: Enhanced Transformer with Rotary Position Embedding (RoPE)
- GLU Variants Improve Transformer (SwiGLU)
- Root Mean Square Layer Normalization (RMSNorm)

### 9.2 å¼€æºé¡¹ç›®
- llama.cpp: https://github.com/ggerganov/llama.cpp
- ggml: https://github.com/ggerganov/ggml
- llama2.c: https://github.com/karpathy/llama2.c

### 9.3 ä¼˜åŒ–èµ„æº
- Intel Intrinsics Guide: https://www.intel.com/content/www/us/en/docs/intrinsics-guide/
- Agner Fog's Optimization Manuals: https://www.agner.org/optimize/

## 10. æ€»ç»“

æœ¬è®¾è®¡æ–‡æ¡£æè¿°äº†ä¸€ä¸ªä»é›¶æ„å»ºçš„C++æ¨ç†å¼•æ“ï¼Œæ—¨åœ¨æä¾›é«˜æ€§èƒ½ã€ä½å»¶è¿Ÿçš„Transformeræ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚

**å…³é”®ç‰¹æ€§**ï¼š
- çº¯C++å®ç°ï¼Œæ— Pythonä¾èµ–
- å®Œæ•´çš„Transformeræ¶æ„æ”¯æŒ
- SIMDä¼˜åŒ–çš„é«˜æ€§èƒ½ç®—å­
- å¤šç§é‡åŒ–æ ¼å¼æ”¯æŒ
- æ¨¡å—åŒ–ã€å¯æ‰©å±•çš„è®¾è®¡

**å®æ–½å»ºè®®**ï¼š
æœ¬é¡¹ç›®å·¥ç¨‹é‡å·¨å¤§ï¼ˆé¢„è®¡12-18å‘¨ï¼‰ï¼Œå»ºè®®é‡‡ç”¨**åˆ†é˜¶æ®µå®æ–½ç­–ç•¥**ï¼š
1. å…ˆå®ŒæˆåŸºç¡€æ¶æ„å’Œæ ¸å¿ƒTransformerç»„ä»¶
2. ä½¿ç”¨ç®€å•çš„CPUå®ç°éªŒè¯æ­£ç¡®æ€§
3. å†è¿›è¡ŒSIMDä¼˜åŒ–å’Œé‡åŒ–æ”¯æŒ
4. æœ€åè¿›è¡Œå…¨é¢çš„æ€§èƒ½è°ƒä¼˜

**æ›¿ä»£æ–¹æ¡ˆ**ï¼š
å¦‚æœæ—¶é—´ç´§è¿«ï¼Œå»ºè®®å…ˆé‡‡ç”¨**Pythonæ¡¥æ¥æ–¹æ¡ˆ**æˆ–**é›†æˆllama.cpp**ä½œä¸ºè¿‡æ¸¡ï¼Œå¾…æœ‰å……è¶³èµ„æºåå†æŠ•å…¥è‡ªç ”ã€‚
