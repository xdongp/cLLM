# 请求队列模块设计

## 编程规范

本模块的编码实现遵循以下规范和约定：
- [C++编程规范.md](../../C++编程规范.md)：定义编码风格、命名规范等
- [生成代码规范.md](../生成代码规范.md)：定义代码生成流程、设计文档一致性要求、优化同步机制等

## 0. 要生成的文件

### 0.1 头文件（include/cllm/common/）

根据[C++编程规范.md](../../C++编程规范.md)的命名规范，本模块需要生成以下头文件：

| 文件名 | 对应类/结构体 | 说明 |
|--------|--------------|------|
| `queue.h` | `RequestQueue` | 请求队列主类 |
| `request_state.h` | `RequestState` | 请求状态结构体 |
| `types.h` | 公共类型定义 | 通用类型定义 |
| `utils.h` | 通用工具函数 | 跨模块工具函数 |

**注意**：请求队列功能实现于 `common` 模块中，作为跨模块共享的基础设施。

### 0.2 源文件（src/common/）

| 文件名 | 对应头文件 | 说明 |
|--------|-----------|------|
| `queue.cpp` | `queue.h` | RequestQueue类的实现 |
| `request_state.cpp` | `request_state.h` | RequestState类的实现 |
| `types.cpp` | `types.h` | 类型定义的实现（如有） |
| `utils.cpp` | `utils.h` | 工具函数的实现 |

### 0.3 测试文件（tests/）

| 文件名 | 测试目标 | 说明 |
|--------|---------|------|
| `test_queue.cpp` | RequestQueue | 请求队列模块的单元测试 |

### 0.4 文件命名规范说明

- **头文件名**：使用小写字母+下划线，与类名对应（大驼峰转小写下划线）
- **源文件名**：与对应头文件名保持一致
- **目录结构**：头文件位于 `include/cllm/common/`，源文件位于 `src/common/`
- **特殊性**：作为通用模块，放在common目录下供多个模块共享
- **一致性原则**：所有文件命名遵循[C++编程规范.md](../../C++编程规范.md)第1.1节

## 1. 模块概述

### 1.1 模块定位
请求队列功能目前实现于 `common` 模块中，作为跨模块共享的基础设施。这种设计有以下优势：
- **减少模块间依赖**：避免与其他模块形成复杂的依赖关系
- **提高代码复用性**：被多个模块（如`scheduler`、`batch`）共享使用
- **简化项目结构**：与其他通用组件统一管理
- **降低维护成本**：集中管理通用功能，减少重复代码

### 1.2 模块职责
请求队列模块负责管理所有待处理的推理请求，实现请求的优先级调度、动态批处理形成和队列监控功能。该模块是整个推理系统的核心调度组件，确保请求按照最优顺序和批处理策略进行处理。

### 1.3 核心功能
- 请求优先级管理：根据请求长度、等待时间等因素计算优先级
- 动态批处理形成：根据请求长度和系统负载动态调整批处理大小
- 队列监控：实时监控队列状态，包括请求数量、平均等待时间等
- 请求超时处理：自动移除超时请求
- 并发安全：支持多线程安全地添加和获取请求

### 1.3 设计原则
- 高效性：使用优先队列实现O(log n)的插入和删除操作
- 灵活性：支持多种优先级计算策略
- 可扩展性：易于添加新的调度策略和批处理算法
- 线程安全：使用适当的同步机制保证并发访问安全

## 2. 类设计

### 2.1 RequestState
```cpp
struct RequestState {
    size_t requestId;
    std::vector<int> tokenizedPrompt;
    std::vector<int> generatedTokens;
    int maxTokens;
    float temperature;
    int topK;
    float topP;
    std::string samplingStrategy;
    size_t arrivalTime;
    size_t startTime;
    size_t completionTime;
    size_t priority;
    bool isCompleted;
    bool isRunning;
    bool isFailed;
    std::string errorMessage;
    
    size_t getPromptLength() const {
        return tokenizedPrompt.size();
    }
    
    size_t getTotalLength() const {
        return tokenizedPrompt.size() + generatedTokens.size();
    }
    
    float calculatePriority(size_t currentTime) const;
};
```

### 2.2 RequestQueue
```cpp
class RequestQueue {
public:
    explicit RequestQueue(size_t maxQueueSize = 1000, size_t maxContextLength = 2048);
    ~RequestQueue();
    
    bool addRequest(const RequestState& request);
    bool removeRequest(size_t requestId);
    RequestState getNextRequest();
    std::vector<RequestState> formBatch(size_t maxContextLength);
    std::vector<RequestState> getPendingRequests() const;
    
    size_t getQueueSize() const;
    size_t getRunningRequestsLength() const;
    float getAverageWaitTime() const;
    size_t getAverageRequestLength() const;
    
    void updateRunningRequests(const std::vector<RequestState>& running);
    void clear();
    
private:
    void updatePriorities();
    size_t calculateOptimalBatchSize(const std::vector<RequestState>& requests);
    
    mutable std::priority_queue<RequestState, std::vector<RequestState>, RequestComparator> queue_;
    std::vector<RequestState> runningRequests_;
    mutable std::mutex queueMutex_;
    std::condition_variable condition_;
    
    size_t maxQueueSize_;
    size_t maxContextLength_;
    std::atomic<size_t> totalRequests_;
    std::atomic<size_t> completedRequests_;
    bool stopFlag_;
};
```

### 2.3 RequestComparator
```cpp
struct RequestComparator {
    bool operator()(const RequestState& a, const RequestState& b) const {
        if (a.priority != b.priority) {
            return a.priority > b.priority;
        }
        return a.arrivalTime > b.arrivalTime;
    }
};
```



## 3. 接口设计

### 3.1 RequestQueue接口
```cpp
class RequestQueue {
public:
    explicit RequestQueue(size_t maxQueueSize = 1000);
    ~RequestQueue();
    
    bool addRequest(const RequestState& request);
    bool removeRequest(size_t requestId);
    RequestState getNextRequest();
    std::vector<RequestState> formBatch(size_t maxContextLength);
    
    size_t getQueueSize() const;
    size_t getRunningRequestsLength() const;
    float getAverageWaitTime() const;
    size_t getAverageRequestLength() const;
    
    void updateRunningRequests(const std::vector<RequestState>& running);
    void clear();
    
private:
    void updatePriorities();
    size_t calculateOptimalBatchSize(const std::vector<RequestState>& requests);
    
    std::priority_queue<RequestState, std::vector<RequestState>, RequestComparator> queue_;
    std::vector<RequestState> runningRequests_;
    mutable std::mutex queueMutex_;
    std::condition_variable condition_;
    
    size_t maxQueueSize_;
    size_t maxContextLength_;
    std::atomic<size_t> totalRequests_;
    std::atomic<size_t> completedRequests_;
};
```



## 4. 算法实现

### 4.1 优先级计算算法
```cpp
float RequestState::calculatePriority(size_t currentTime) const {
    size_t waitTime = currentTime - arrivalTime;
    size_t promptLength = getPromptLength();
    
    float lengthFactor = 1.0f / (1.0f + promptLength * 0.01f);
    float waitFactor = 1.0f + waitTime * 0.001f;
    
    return lengthFactor * waitFactor;
}
```

### 4.2 动态批处理形成算法
```cpp
std::vector<RequestState> RequestQueue::formBatch(size_t maxContextLength) {
    std::lock_guard<std::mutex> lock(queueMutex_);
    
    std::vector<RequestState> batch;
    size_t currentBatchLength = 0;
    
    size_t runningLength = 0;
    for (const auto& req : runningRequests_) {
        runningLength += req.getTotalLength();
    }
    
    if (runningLength > maxContextLength * 0.75f) {
        return batch;
    }
    
    std::vector<RequestState> tempQueue;
    while (!queue_.empty()) {
        tempQueue.push_back(queue_.top());
        queue_.pop();
    }
    
    for (auto& req : tempQueue) {
        if (currentBatchLength + req.getPromptLength() <= maxContextLength - runningLength) {
            batch.push_back(req);
            currentBatchLength += req.getPromptLength();
        } else {
            queue_.push(req);
        }
    }
    
    return batch;
}
```

### 4.3 最优批处理大小计算
```cpp
size_t BatchManager::calculateOptimalBatchSize(
    const std::vector<RequestState>& requests,
    size_t avgRequestLength
) {
    if (requests.empty()) {
        return 0;
    }
    
    size_t availableContext = maxContextLength_;
    size_t optimalBatchSize = 0;
    
    if (avgRequestLength < 100) {
        optimalBatchSize = std::min(requests.size(), availableContext / avgRequestLength);
        optimalBatchSize = std::min(optimalBatchSize, size_t(16));
    } else if (avgRequestLength < 500) {
        optimalBatchSize = std::min(requests.size(), availableContext / avgRequestLength);
        optimalBatchSize = std::min(optimalBatchSize, size_t(8));
    } else {
        optimalBatchSize = std::min(requests.size(), availableContext / avgRequestLength);
        optimalBatchSize = std::min(optimalBatchSize, size_t(4));
    }
    
    return optimalBatchSize;
}
```

## 5. 并发设计

### 5.1 线程安全保证
- 使用std::mutex保护队列访问
- 使用std::condition_variable实现生产者-消费者模式
- 使用std::atomic保证统计数据的原子性

### 5.2 请求添加流程
```cpp
bool RequestQueue::addRequest(const RequestState& request) {
    std::lock_guard<std::mutex> lock(queueMutex_);
    
    if (queue_.size() >= maxQueueSize_) {
        return false;
    }
    
    RequestState req = request;
    req.priority = req.calculatePriority(getCurrentTime());
    req.arrivalTime = getCurrentTime();
    
    queue_.push(req);
    totalRequests_++;
    
    condition_.notify_one();
    return true;
}
```

### 5.3 请求获取流程
```cpp
RequestState RequestQueue::getNextRequest() {
    std::unique_lock<std::mutex> lock(queueMutex_);
    
    condition_.wait(lock, [this] {
        return !queue_.empty() || stopFlag_;
    });
    
    if (stopFlag_ && queue_.empty()) {
        return RequestState{};
    }
    
    RequestState request = queue_.top();
    queue_.pop();
    
    return request;
}
```

## 6. 内存管理

### 6.1 内存分配策略
- 使用mimalloc进行高效内存分配
- 使用RAII包装器管理动态数组
- 预分配队列空间减少频繁分配

### 6.2 内存优化
```cpp
class RequestQueue {
private:
    void optimizeMemoryUsage() {
        if (queue_.size() < queue_.capacity() / 4) {
            queue_.shrink_to_fit();
        }
    }
};
```

## 7. 错误处理

### 7.1 错误类型
```cpp
enum class QueueError {
    QUEUE_FULL,
    REQUEST_NOT_FOUND,
    INVALID_REQUEST,
    TIMEOUT
};

class QueueException : public std::runtime_error {
public:
    QueueException(QueueError error, const std::string& message)
        : std::runtime_error(message), error_(error) {}
    
    QueueError getError() const { return error_; }
    
private:
    QueueError error_;
};
```

### 7.2 错误处理策略
- 队列满时返回false，不抛出异常
- 请求不存在时返回空请求
- 使用日志记录错误信息
- 提供错误码供上层处理

## 8. 性能优化

### 8.1 优先队列优化
- 使用std::priority_queue实现O(log n)插入和删除
- 批量更新优先级减少重排序次数
- 定期清理过期请求

### 8.2 批处理优化
- 动态调整批处理大小
- 根据请求长度智能分组
- 优先处理短请求减少延迟

### 8.3 缓存优化
- 缓存平均请求长度
- 缓存批处理大小计算结果
- 使用局部变量减少锁竞争

## 9. 测试策略

### 9.1 单元测试
```cpp
class RequestQueueTest {
public:
    void testAddRequest();
    void testRemoveRequest();
    void testFormBatch();
    void testPriorityCalculation();
    void testConcurrency();
    void testQueueFull();
    void testTimeout();
};
```

### 9.2 性能测试
- 测试大量请求的吞吐量
- 测试并发访问的性能
- 测试批处理形成的效率

### 9.3 集成测试
- 与Scheduler模块集成测试
- 与Model Executor模块集成测试
- 端到端性能测试

## 10. 使用示例

### 10.1 基本使用
```cpp
RequestQueue queue(1000);

RequestState request;
request.requestId = 1;
request.tokenizedPrompt = {1, 2, 3, 4, 5};
request.maxTokens = 100;
request.temperature = 0.7f;

queue.addRequest(request);

auto batch = queue.formBatch(2048);
```

### 10.2 批处理形成
```cpp
BatchManager batchManager(2048);

std::vector<RequestState> pendingRequests = queue.getPendingRequests();
std::vector<RequestState> runningRequests = queue.getRunningRequests();

auto batch = batchManager.formBatch(pendingRequests, runningRequests);
```

## 11. 配置参数

### 11.1 队列配置
```cpp
struct QueueConfig {
    size_t maxQueueSize = 1000;
    size_t maxContextLength = 2048;
    float contextUsageThreshold = 0.75f;
    size_t maxBatchSize = 32;
    size_t minBatchSize = 1;
};
```

### 12.2 优先级配置
```cpp
struct PriorityConfig {
    float lengthWeight = 0.01f;
    float waitWeight = 0.001f;
    size_t maxWaitTime = 10000;
};

## 13. 监控指标

### 13.1 队列指标
- 当前队列大小
- 平均等待时间
- 请求完成率
- 队列满载次数

### 13.2 批处理指标
- 平均批处理大小
- 批处理形成时间
- 上下文利用率
- 批处理成功率

## 14. 依赖关系

### 14.1 外部依赖
- C++标准库（std::priority_queue, std::mutex, std::condition_variable）
- mimalloc（内存管理）
- 日志库（用于错误记录）

### 14.2 内部依赖
- `cllm/common/types.h`: 定义基础数据类型
- `cllm/common/utils.h`: 提供通用工具函数

## 15. 后续优化方向

### 15.1 短期优化
- 实现更复杂的优先级计算策略
- 添加请求预测功能
- 优化批处理形成算法

### 15.2 长期优化
- 实现机器学习驱动的调度策略
- 支持分布式队列
- 添加请求重试机制
