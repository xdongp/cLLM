# 批处理管理器模块设计

## 编程规范

本模块的编码实现遵循以下规范和约定：
- [C++编程规范.md](../../C++编程规范.md)：定义编码风格、命名规范等
- [生成代码规范.md](../生成代码规范.md)：定义代码生成流程、设计文档一致性要求、优化同步机制等

## 0. 要生成的文件

### 0.1 头文件（include/cllm/batch/）

根据[C++编程规范.md](../../C++编程规范.md)的命名规范，本模块需要生成以下头文件：

| 文件名 | 对应类/结构体 | 说明 |
|--------|--------------|------|
| `manager.h` | `BatchManager` | 批处理管理器，协调批处理形成和处理 |
| `input.h` | `BatchInput` | 批处理输入结构体 |
| `output.h` | `BatchOutput` | 批处理输出结构体 |
| `processor.h` | `BatchProcessor` | 批处理处理器 |
| `stats.h` | `BatchStats` | 批处理统计信息结构体 |

### 0.2 源文件（src/batch/）

| 文件名 | 对应头文件 | 说明 |
|--------|-----------|------|
| `manager.cpp` | `manager.h` | BatchManager类的实现 |
| `processor.cpp` | `processor.h` | BatchProcessor类的实现 |
| `stats.cpp` | `stats.h` | BatchStats类的实现 |

### 0.3 测试文件（tests/）

| 文件名 | 测试目标 | 说明 |
|--------|---------|------|
| `test_batch_manager.cpp` | BatchManager, BatchProcessor | 批处理管理器模块的单元测试 |

### 0.4 文件命名规范说明

- **头文件名**：使用小写字母+下划线，与类名对应（大驼峰转小写下划线）
- **源文件名**：与对应头文件名保持一致
- **目录结构**：头文件位于 `include/cllm/batch/`，源文件位于 `src/batch/`
- **一致性原则**：所有文件命名遵循[C++编程规范.md](../../C++编程规范.md)第1.1节

## 1. 模块概述

### 1.1 模块职责
批处理管理器模块负责将多个推理请求组织成高效的批处理，优化模型推理的吞吐量。该模块实现动态批处理大小调整、批处理输入准备、批处理输出处理和批处理状态管理等功能。

### 1.2 核心功能
- 动态批处理大小计算：根据请求长度和系统负载动态调整批处理大小
- 批处理形成：将多个请求组织成最优的批处理组合
- 批处理输入准备：为批处理准备统一的输入格式
- 批处理输出处理：处理模型输出并更新请求状态
- 批处理状态管理：跟踪批处理的执行状态和进度
- 性能监控：监控批处理性能指标

### 1.3 设计原则
- 高效性：最大化模型推理的吞吐量
- 灵活性：支持多种批处理策略
- 可扩展性：易于添加新的批处理算法
- 线程安全：支持多线程并发处理

### 1.4 模块依赖

本模块依赖以下模块：

| 依赖模块 | 依赖类/结构体 | 依赖原因 |
|----------|--------------|----------|
| `common` | `RequestState` | 请求状态管理 |
| `memory` | `FloatArray` | 内存管理 |

**重要**：批处理管理器是调度器的核心组件，负责将多个请求组织成批处理。

### 1.5 命名空间

所有类和函数都在 `cllm` 命名空间下：

```cpp
namespace cllm {
    class BatchManager { ... };
    class BatchProcessor { ... };
    struct BatchInput { ... };
    struct BatchOutput { ... };
    struct BatchStats { ... };
}
```

## 2. 类设计

### 2.1 BatchManager
```cpp
class BatchManager {
public:
    explicit BatchManager(size_t maxContextLength, size_t maxBatchSize = 32);
    ~BatchManager();
    
    std::vector<RequestState> formBatch(
        const std::vector<RequestState>& pendingRequests,
        const std::vector<RequestState>& runningRequests
    );
    
    std::vector<RequestState> formMultipleBatches(
        const std::vector<RequestState>& pendingRequests,
        const std::vector<RequestState>& runningRequests
    );
    
    BatchInput prepareBatchInput(const std::vector<RequestState>& batch);
    void processBatchOutput(
        std::vector<RequestState>& batch,
        const BatchOutput& output
    );
    
    size_t calculateOptimalBatchSize(
        const std::vector<RequestState>& requests,
        size_t avgRequestLength
    );
    
    bool canAddToBatch(
        const RequestState& request,
        const std::vector<RequestState>& currentBatch,
        size_t currentBatchLength,
        size_t dynamicBatchSize
    );
    
    BatchStats getStats() const;
    void resetStats();
    
private:
    size_t calculateRunningRequestsLength(
        const std::vector<RequestState>& runningRequests
    );
    
    size_t calculateAverageRequestLength(
        const std::vector<RequestState>& requests
    );
    
    void updateStats(const std::vector<RequestState>& batch);
    
    size_t maxContextLength_;
    size_t maxBatchSize_;
    float contextUsageThreshold_;
    
    mutable std::mutex statsMutex_;
    BatchStats stats_;
};
```

### 2.2 BatchInput
```cpp
struct BatchInput {
    std::vector<int> inputIds;
    std::vector<std::pair<size_t, size_t>> requestPositions;
    size_t batchSize;
    std::vector<size_t> sequenceIds;
    
    size_t getTotalTokens() const {
        return inputIds.size();
    }
    
    void clear() {
        inputIds.clear();
        requestPositions.clear();
        sequenceIds.clear();
        batchSize = 0;
    }
};
```

### 2.3 BatchOutput
```cpp
struct BatchOutput {
    FloatArray logits;
    std::vector<std::pair<size_t, size_t>> requestPositions;
    std::vector<size_t> sequenceIds;
    
    FloatArray getLogitsForRequest(size_t requestIndex) const;
    
    void clear() {
        logits.clear();
        requestPositions.clear();
        sequenceIds.clear();
    }
};
```

### 2.4 BatchStats
```cpp
struct BatchStats {
    size_t totalBatches;
    size_t totalRequests;
    size_t totalTokens;
    float averageBatchSize;
    float averageBatchLength;
    size_t maxBatchSize;
    size_t minBatchSize;
    
    void update(const std::vector<RequestState>& batch);
    void reset();
    
    std::string toString() const;
};
```

### 2.5 BatchProcessor
```cpp
class BatchProcessor {
public:
    explicit BatchProcessor(BatchManager* manager);
    ~BatchProcessor();
    
    void processBatch(std::vector<RequestState>& batch);
    
    bool isBatchComplete(const std::vector<RequestState>& batch) const;
    
    std::vector<RequestState> getActiveRequests(
        const std::vector<RequestState>& batch
    ) const;
    
    std::vector<RequestState> getCompletedRequests(
        const std::vector<RequestState>& batch
    ) const;
    
private:
    void checkStoppingConditions(RequestState& request, int nextToken);
    
    BatchManager* manager_;
};
```

## 3. 接口设计

### 3.1 BatchManager接口
```cpp
class BatchManager {
public:
    explicit BatchManager(size_t maxContextLength, size_t maxBatchSize = 32);
    ~BatchManager();
    
    std::vector<RequestState> formBatch(
        const std::vector<RequestState>& pendingRequests,
        const std::vector<RequestState>& runningRequests
    );
    
    std::vector<RequestState> formMultipleBatches(
        const std::vector<RequestState>& pendingRequests,
        const std::vector<RequestState>& runningRequests
    );
    
    BatchInput prepareBatchInput(const std::vector<RequestState>& batch);
    void processBatchOutput(
        std::vector<RequestState>& batch,
        const BatchOutput& output
    );
    
    size_t calculateOptimalBatchSize(
        const std::vector<RequestState>& requests,
        size_t avgRequestLength
    );
    
    bool canAddToBatch(
        const RequestState& request,
        const std::vector<RequestState>& currentBatch,
        size_t currentBatchLength,
        size_t dynamicBatchSize
    );
    
    BatchStats getStats() const;
    void resetStats();
    
private:
    size_t calculateRunningRequestsLength(
        const std::vector<RequestState>& runningRequests
    );
    
    size_t calculateAverageRequestLength(
        const std::vector<RequestState>& requests
    );
    
    void updateStats(const std::vector<RequestState>& batch);
    
    void checkStoppingConditions(RequestState& request, int nextToken);
    
    size_t maxContextLength_;
    size_t maxBatchSize_;
    float contextUsageThreshold_;
    
    Sampler sampler_;  // 采样器实例，用于对logits进行采样
    
    mutable std::mutex statsMutex_;
    BatchStats stats_;
};
```

**接口说明**：
- BatchManager现在持有一个Sampler实例，用于在`processBatchOutput`中对模型输出的logits进行采样
- 采样职责已完全委托给Sampler模块，遵循单一职责原则

### 3.2 BatchProcessor接口
```cpp
class BatchProcessor {
public:
    explicit BatchProcessor(BatchManager* manager);
    ~BatchProcessor();
    
    void processBatch(std::vector<RequestState>& batch);
    
    bool isBatchComplete(const std::vector<RequestState>& batch) const;
    
    std::vector<RequestState> getActiveRequests(
        const std::vector<RequestState>& batch
    ) const;
    
    std::vector<RequestState> getCompletedRequests(
        const std::vector<RequestState>& batch
    ) const;
    
private:
    void checkStoppingConditions(RequestState& request, int nextToken);
    
    BatchManager* manager_;
};
```

## 4. 算法实现

### 4.1 动态批处理大小计算
```cpp
size_t BatchManager::calculateOptimalBatchSize(
    const std::vector<RequestState>& requests,
    size_t avgRequestLength
) {
    if (requests.empty()) {
        return 0;
    }
    
    size_t dynamicBatchSize = maxBatchSize_;
    
    if (avgRequestLength > 500) {
        dynamicBatchSize = std::max(size_t(2), maxBatchSize_ / 2);
    } else if (avgRequestLength > 200) {
        dynamicBatchSize = std::max(size_t(3), static_cast<size_t>(maxBatchSize_ / 1.5));
    } else {
        dynamicBatchSize = static_cast<size_t>(maxBatchSize_ * 1.5);
    }
    
    return dynamicBatchSize;
}
```

### 4.2 批处理形成算法
```cpp
std::vector<RequestState> BatchManager::formBatch(
    const std::vector<RequestState>& pendingRequests,
    const std::vector<RequestState>& runningRequests
) {
    std::vector<RequestState> batch;
    size_t currentBatchLength = 0;
    
    size_t runningLength = calculateRunningRequestsLength(runningRequests);
    
    if (runningLength > maxContextLength_ * contextUsageThreshold_) {
        return batch;
    }
    
    size_t avgLength = calculateAverageRequestLength(pendingRequests);
    size_t dynamicBatchSize = calculateOptimalBatchSize(pendingRequests, avgLength);
    
    for (const auto& request : pendingRequests) {
        size_t requestLength = request.getTotalLength();
        size_t totalLength = runningLength + currentBatchLength + requestLength;
        
        if (totalLength <= maxContextLength_ && 
            batch.size() < dynamicBatchSize) {
            batch.push_back(request);
            currentBatchLength += requestLength;
        } else {
            break;
        }
    }
    
    updateStats(batch);
    return batch;
}
```

### 4.3 多批处理形成算法
```cpp
std::vector<RequestState> BatchManager::formMultipleBatches(
    const std::vector<RequestState>& pendingRequests,
    const std::vector<RequestState>& runningRequests
) {
    std::vector<RequestState> allBatches;
    std::vector<RequestState> remaining = pendingRequests;
    
    size_t runningLength = calculateRunningRequestsLength(runningRequests);
    
    while (!remaining.empty()) {
        std::vector<RequestState> batch;
        size_t currentBatchLength = 0;
        
        size_t avgLength = calculateAverageRequestLength(remaining);
        size_t dynamicBatchSize = calculateOptimalBatchSize(remaining, avgLength);
        
        std::vector<size_t> usedIndices;
        
        for (size_t i = 0; i < remaining.size(); ++i) {
            const auto& request = remaining[i];
            size_t requestLength = request.getTotalLength();
            size_t totalLength = runningLength + currentBatchLength + requestLength;
            
            if (totalLength <= maxContextLength_ && 
                batch.size() < dynamicBatchSize) {
                batch.push_back(request);
                currentBatchLength += requestLength;
                usedIndices.push_back(i);
            }
        }
        
        if (batch.empty()) {
            break;
        }
        
        allBatches.insert(allBatches.end(), batch.begin(), batch.end());
        runningLength += currentBatchLength;
        
        std::vector<RequestState> newRemaining;
        for (size_t i = 0; i < remaining.size(); ++i) {
            if (std::find(usedIndices.begin(), usedIndices.end(), i) == usedIndices.end()) {
                newRemaining.push_back(remaining[i]);
            }
        }
        remaining = newRemaining;
    }
    
    return allBatches;
}
```

### 4.4 批处理输入准备
```cpp
BatchInput BatchManager::prepareBatchInput(const std::vector<RequestState>& batch) {
    BatchInput input;
    input.batchSize = batch.size();
    
    size_t currentPos = 0;
    
    for (const auto& request : batch) {
        std::vector<int> inputIds = request.tokenizedPrompt;
        inputIds.insert(inputIds.end(), 
                       request.generatedTokens.begin(), 
                       request.generatedTokens.end());
        
        input.inputIds.insert(input.inputIds.end(), 
                             inputIds.begin(), 
                             inputIds.end());
        
        input.requestPositions.push_back({currentPos, currentPos + inputIds.size()});
        input.sequenceIds.push_back(request.requestId);
        
        currentPos += inputIds.size();
    }
    
    return input;
}
```

### 4.5 批处理输出处理
```cpp
void BatchManager::processBatchOutput(
    std::vector<RequestState>& batch,
    const BatchOutput& output
) {
    for (size_t i = 0; i < batch.size(); ++i) {
        if (batch[i].isCompleted) {
            continue;
        }
        
        FloatArray requestLogits = output.getLogitsForRequest(i);
        
        // 使用Sampler实例进行采样
        int nextToken = sampler_.sample(requestLogits, batch[i].temperature);
        
        batch[i].generatedTokens.push_back(nextToken);
        
        checkStoppingConditions(batch[i], nextToken);
    }
}
```

## 5. 并发设计

### 5.1 线程安全保证
- 使用std::mutex保护统计数据访问
- 批处理操作本身是线程安全的（不修改共享状态）
- 使用局部变量减少锁竞争

### 5.2 并发批处理
```cpp
class ConcurrentBatchProcessor {
public:
    explicit ConcurrentBatchProcessor(BatchManager* manager, ThreadPool* pool);
    ~ConcurrentBatchProcessor();
    
    void processBatchesConcurrently(
        std::vector<std::vector<RequestState>>& batches
    );
    
private:
    void processSingleBatch(std::vector<RequestState>& batch);
    
    BatchManager* manager_;
    ThreadPool* threadPool_;
};
```

## 6. 内存管理

### 6.1 内存分配策略
- 使用mimalloc进行高效内存分配
- 使用RAII包装器管理动态数组
- 预分配批处理缓冲区减少频繁分配

### 6.2 内存优化
```cpp
class BatchManager {
private:
    void optimizeMemoryUsage() {
        if (stats_.totalBatches > 1000) {
            stats_.reset();
        }
    }
    
    std::vector<int> inputBuffer_;
    std::vector<std::pair<size_t, size_t>> positionBuffer_;
};
```

## 7. 错误处理

### 7.1 错误类型
```cpp
enum class BatchError {
    BATCH_TOO_LARGE,
    INVALID_REQUEST,
    OUTPUT_MISMATCH,
    TIMEOUT
};

class BatchException : public std::runtime_error {
public:
    BatchException(BatchError error, const std::string& message)
        : std::runtime_error(message), error_(error) {}
    
    BatchError getError() const { return error_; }
    
private:
    BatchError error_;
};
```

### 7.2 错误处理策略
- 批处理过大时自动分割
- 输出不匹配时记录错误并继续
- 使用日志记录错误信息
- 提供错误码供上层处理

## 8. 性能优化

### 8.1 批处理优化
- 动态调整批处理大小
- 根据请求长度智能分组
- 预分配缓冲区减少分配开销

### 8.2 内存优化
- 重用批处理缓冲区
- 使用内存池管理临时对象
- 减少不必要的拷贝

### 8.3 计算优化
- 批量计算平均长度
- 预计算批处理大小
- 使用SIMD优化批量操作

## 9. 测试策略

### 9.1 单元测试
```cpp
class BatchManagerTest {
public:
    void testFormBatch();
    void testFormMultipleBatches();
    void testPrepareBatchInput();
    void testProcessBatchOutput();
    void testCalculateOptimalBatchSize();
    void testCanAddToBatch();
    void testConcurrency();
};
```

### 9.2 性能测试
- 测试不同批处理大小的吞吐量
- 测试批处理形成的时间
- 测试批处理处理的效率

### 9.3 集成测试
- 与Scheduler模块集成测试
- 与Model Executor模块集成测试
- 端到端性能测试

## 10. 使用示例

### 10.1 基本使用
```cpp
BatchManager manager(2048, 32);

std::vector<RequestState> pendingRequests = queue.getPendingRequests();
std::vector<RequestState> runningRequests = queue.getRunningRequests();

auto batch = manager.formBatch(pendingRequests, runningRequests);

BatchInput input = manager.prepareBatchInput(batch);

BatchOutput output = modelExecutor.forward(input);

manager.processBatchOutput(batch, output);
```

### 10.2 多批处理
```cpp
auto batches = manager.formMultipleBatches(pendingRequests, runningRequests);

for (auto& batch : batches) {
    BatchInput input = manager.prepareBatchInput(batch);
    BatchOutput output = modelExecutor.forward(input);
    manager.processBatchOutput(batch, output);
}
```

## 11. 配置参数

### 11.1 批处理配置
```cpp
struct BatchConfig {
    size_t maxContextLength = 2048;
    size_t maxBatchSize = 32;
    float contextUsageThreshold = 0.75f;
    size_t minBatchSize = 1;
    
    size_t longRequestThreshold = 500;
    size_t mediumRequestThreshold = 200;
    float shortRequestMultiplier = 1.5f;
};
```

### 11.2 性能配置
```cpp
struct PerformanceConfig {
    bool enableBatchReuse = true;
    bool enableBufferReuse = true;
    size_t statsResetInterval = 1000;
    bool enableSIMD = true;
};
```

## 12. 监控指标

### 12.1 批处理指标
- 总批处理数量
- 平均批处理大小
- 平均批处理长度
- 最大/最小批处理大小

### 12.2 性能指标
- 批处理形成时间
- 批处理处理时间
- 上下文利用率
- 吞吐量（tokens/s）

## 13. 依赖关系

### 13.1 外部依赖
- C++标准库（std::vector, std::mutex）
- mimalloc（内存管理）
- SIMD库（SIMD优化）

### 13.2 内部依赖
- RequestState（请求状态）
- FloatArray（RAII包装器）
- ThreadPool（并发处理）

## 14. 后续优化方向

### 14.1 短期优化
- 实现更智能的批处理策略
- 添加批处理预测功能
- 优化批处理输入准备

### 14.2 长期优化
- 实现自适应批处理算法
- 支持动态批处理分割
- 添加批处理缓存机制
- 实现批处理优先级调度
