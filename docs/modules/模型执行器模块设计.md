# 模型执行器模块设计

## 编程规范

本模块的编码实现遵循以下规范和约定：
- [C++编程规范.md](../../C++编程规范.md)：定义编码风格、命名规范等
- [生成代码规范.md](../生成代码规范.md)：定义代码生成流程、设计文档一致性要求、优化同步机制等

## 0. 要生成的文件

### 0.1 头文件（include/cllm/model/）

根据[C++编程规范.md](../../C++编程规范.md)的命名规范，本模块需要生成以下头文件：

| 文件名 | 对应类/结构体 | 说明 |
|--------|--------------|------|
| `executor.h` | `ModelExecutor` | 模型执行器主类 |
| `config.h` | `ModelConfig` | 模型配置结构体 |
| `stats.h` | `ModelStats` | 模型统计信息结构体 |
| `quantization_manager.h` | `QuantizationManager` | 量化管理器 |
| `inference_optimizer.h` | `InferenceOptimizer` | 推理优化器 |
| `batch_processor.h` | `BatchProcessor` | 批处理处理器 |
| `exceptions.h` | `ModelException`, `LoadException` | 模型异常类 |

### 0.2 源文件（src/model/）

| 文件名 | 对应头文件 | 说明 |
|--------|-----------|------|
| `executor.cpp` | `executor.h` | ModelExecutor类的实现 |
| `config.cpp` | `config.h` | ModelConfig类的实现 |
| `stats.cpp` | `stats.h` | ModelStats类的实现 |
| `quantization_manager.cpp` | `quantization_manager.h` | QuantizationManager类的实现 |
| `inference_optimizer.cpp` | `inference_optimizer.h` | InferenceOptimizer类的实现 |
| `batch_processor.cpp` | `batch_processor.h` | BatchProcessor类的实现 |

### 0.3 测试文件（tests/）

| 文件名 | 测试目标 | 说明 |
|--------|---------|------|
| `test_model_executor.cpp` | ModelExecutor, QuantizationManager, InferenceOptimizer | 模型执行器模块的单元测试 |

### 0.4 文件命名规范说明

- **头文件名**：使用小写字母+下划线，与类名对应（大驼峰转小写下划线）
- **源文件名**：与对应头文件名保持一致
- **目录结构**：头文件位于 `include/cllm/model/`，源文件位于 `src/model/`
- **一致性原则**：所有文件命名遵循[C++编程规范.md](../../C++编程规范.md)第1.1节

## 1. 模块概述

### 1.1 模块职责
模型执行器模块负责加载和管理LLM模型，执行模型推理，提供高性能的前向传播接口。该模块是整个推理系统的核心计算组件，负责将输入转换为模型输出。

### 1.2 核心功能
- 模型加载：支持多种量化格式的模型加载
- 前向传播：执行模型推理计算
- 量化支持：支持int8、int4等量化格式
- KV缓存集成：与KV缓存模块集成以优化推理性能
- 采样器集成：提供token采样功能
- 性能优化：应用各种优化技术提升推理速度
- 并发处理：支持多线程并行推理

### 1.3 设计原则
- 高效性：最大化推理速度
- 灵活性：支持多种模型和量化格式
- 可扩展性：易于添加新的优化技术
- 内存效率：优化内存使用

### 1.4 模块依赖

本模块依赖以下模块：

| 依赖模块 | 依赖类/结构体 | 依赖原因 |
|----------|--------------|----------|
| `batch` | `BatchInput`, `BatchOutput` | 批处理输入输出 |
| `kv_cache` | `KVCache` | KV缓存管理 |
| `sampler` | `Sampler` | Token采样 |
| `memory` | `FloatArray` | 内存管理 |
| **外部依赖** | **LibTorch** | 模型加载和推理 |

**重要**：模型执行器使用LibTorch作为深度学习框架，必须正确配置Libtorch库。

### 1.5 命名空间

所有类和函数都在 `cllm` 命名空间下：

```cpp
namespace cllm {
    class ModelExecutor { ... };
    class QuantizationManager { ... };
    class InferenceOptimizer { ... };
    struct ModelConfig { ... };
    struct ModelStats { ... };
}
```

## 2. 类设计

### 2.1 ModelExecutor
```cpp
class ModelExecutor {
public:
    ModelExecutor(
        const std::string& modelPath,
        const std::string& quantization = "",
        bool enableSIMD = true
    );
    
    ~ModelExecutor();
    
    BatchOutput forward(const BatchInput& input);
    std::vector<int> generate(
        const std::vector<int>& inputIds,
        size_t maxNewTokens = 100,
        float temperature = 0.7f
    );
    
    void loadModel();
    void unloadModel();
    
    ModelStats getStats() const;
    void resetStats();
    
    Sampler* getSampler() const;
    KVCache* getKVCache() const;
    
private:
    void loadFullPrecisionModel();
    void loadInt8QuantizedModel();
    void loadInt4QuantizedModel();
    
    void applyInferenceOptimizations();
    void warmupModel();
    
    float* prepareInput(const std::vector<int>& inputIds);
    void processOutput(float* logits, size_t batchSize, size_t vocabSize);
    
    std::string modelPath_;
    std::string quantization_;
    bool enableSIMD_;
    
    void* modelHandle_;
    void* modelWeights_;
    size_t modelSize_;
    
    Sampler* sampler_;
    KVCache* kvCache_;
    
    ModelConfig config_;
    ModelStats stats_;
    
    std::mutex modelMutex_;
};
```

### 2.2 ModelConfig
```cpp
struct ModelConfig {
    std::string modelType;
    size_t vocabSize;
    size_t hiddenSize;
    size_t numLayers;
    size_t numAttentionHeads;
    size_t maxSequenceLength;
    size_t intermediateSize;
    
    bool useKVCache;
    bool useQuantization;
    std::string quantizationType;
    
    void loadFromConfigFile(const std::string& configPath);
    std::string toString() const;
};
```

### 2.3 ModelStats
```cpp
struct ModelStats {
    size_t totalInferences;
    size_t totalTokens;
    float averageInferenceTime;
    float averageTokensPerSecond;
    size_t peakMemoryUsage;
    float cacheHitRate;
    
    void update(float inferenceTime, size_t numTokens);
    void reset();
    
    std::string toString() const;
};
```

### 2.4 QuantizationManager
```cpp
class QuantizationManager {
public:
    explicit QuantizationManager(const std::string& quantizationType);
    ~QuantizationManager();
    
    void quantizeModel(void* modelWeights, size_t modelSize);
    void dequantizeModel(void* quantizedWeights, void* output, size_t size);
    
    float* quantizeTensor(float* tensor, size_t size);
    float* dequantizeTensor(int8_t* quantized, size_t size);
    
    QuantizationStats getStats() const;
    
private:
    void applyInt8Quantization();
    void applyInt4Quantization();
    
    std::string quantizationType_;
    QuantizationStats stats_;
};
```

### 2.5 InferenceOptimizer
```cpp
class InferenceOptimizer {
public:
    explicit InferenceOptimizer(bool enableSIMD = true);
    ~InferenceOptimizer();
    
    void applyOptimizations();
    void optimizeMemoryLayout();
    void optimizeComputeGraph();
    
    void enableSIMDOptimizations();
    void enableMemoryOptimizations();
    
    OptimizerStats getStats() const;
    
private:
    void applyAVX2Optimizations();
    void applyAVX512Optimizations();
    
    bool enableSIMD_;
    OptimizerStats stats_;
};
```

### 2.6 BatchProcessor
```cpp
class BatchProcessor {
public:
    explicit BatchProcessor(ModelExecutor* executor);
    ~BatchProcessor();
    
    BatchOutput processBatch(const BatchInput& input);
    
    void prepareBatchInput(const BatchInput& input);
    void processBatchOutput(BatchOutput& output);
    
private:
    void padBatch(std::vector<int>& inputIds, size_t targetLength);
    void unpadBatch(BatchOutput& output, const std::vector<size_t>& originalLengths);
    
    ModelExecutor* executor_;
};
```

## 3. 接口设计

### 3.1 ModelExecutor接口
```cpp
class ModelExecutor {
public:
    ModelExecutor(
        const std::string& modelPath,
        const std::string& quantization = "",
        bool enableSIMD = true
    );
    
    ~ModelExecutor();
    
    BatchOutput forward(const BatchInput& input);
    std::vector<int> generate(
        const std::vector<int>& inputIds,
        size_t maxNewTokens = 100,
        float temperature = 0.7f
    );
    
    void loadModel();
    void unloadModel();
    
    ModelStats getStats() const;
    void resetStats();
    
    Sampler* getSampler() const;
    KVCache* getKVCache() const;
    
private:
    void loadFullPrecisionModel();
    void loadInt8QuantizedModel();
    void loadInt4QuantizedModel();
    
    void applyInferenceOptimizations();
    void warmupModel();
    
    float* prepareInput(const std::vector<int>& inputIds);
    void processOutput(float* logits, size_t batchSize, size_t vocabSize);
    
    std::string modelPath_;
    std::string quantization_;
    bool enableSIMD_;
    
    void* modelHandle_;
    void* modelWeights_;
    size_t modelSize_;
    
    Sampler* sampler_;
    KVCache* kvCache_;
    
    ModelConfig config_;
    ModelStats stats_;
    
    std::mutex modelMutex_;
};
```

### 3.2 QuantizationManager接口
```cpp
class QuantizationManager {
public:
    explicit QuantizationManager(const std::string& quantizationType);
    ~QuantizationManager();
    
    void quantizeModel(void* modelWeights, size_t modelSize);
    void dequantizeModel(void* quantizedWeights, void* output, size_t size);
    
    float* quantizeTensor(float* tensor, size_t size);
    float* dequantizeTensor(int8_t* quantized, size_t size);
    
    QuantizationStats getStats() const;
    
private:
    void applyInt8Quantization();
    void applyInt4Quantization();
    
    std::string quantizationType_;
    QuantizationStats stats_;
};
```

## 4. 算法实现

### 4.1 模型加载
```cpp
void ModelExecutor::loadModel() {
    if (quantization_ == "int8") {
        loadInt8QuantizedModel();
    } else if (quantization_ == "int4") {
        loadInt4QuantizedModel();
    } else {
        loadFullPrecisionModel();
    }
    
    applyInferenceOptimizations();
    warmupModel();
}
```

### 4.2 前向传播
```cpp
BatchOutput ModelExecutor::forward(const BatchInput& input) {
    auto startTime = std::chrono::high_resolution_clock::now();
    
    std::lock_guard<std::mutex> lock(modelMutex_);
    
    float* inputTensor = prepareInput(input.inputIds);
    
    float* outputTensor = executeModelInference(
        modelHandle_,
        inputTensor,
        input.batchSize,
        config_.maxSequenceLength
    );
    
    processOutput(outputTensor, input.batchSize, config_.vocabSize);
    
    BatchOutput output;
    output.logits = FloatArray(outputTensor, input.batchSize * config_.vocabSize);
    output.requestPositions = input.requestPositions;
    output.sequenceIds = input.sequenceIds;
    
    auto endTime = std::chrono::high_resolution_clock::now();
    float inferenceTime = std::chrono::duration<float>(endTime - startTime).count();
    
    stats_.update(inferenceTime, input.getTotalTokens());
    
    return output;
}
```

### 4.3 生成方法
```cpp
std::vector<int> ModelExecutor::generate(
    const std::vector<int>& inputIds,
    size_t maxNewTokens,
    float temperature
) {
    std::vector<int> generatedTokens;
    std::vector<int> currentInput = inputIds;
    
    for (size_t i = 0; i < maxNewTokens; ++i) {
        BatchInput batchInput;
        batchInput.inputIds = currentInput;
        batchInput.batchSize = 1;
        batchInput.requestPositions = {{0, currentInput.size()}};
        batchInput.sequenceIds = {0};
        
        BatchOutput output = forward(batchInput);
        
        FloatArray logits = output.getLogitsForRequest(0);
        
        int nextToken = sampler_->sample(logits, temperature);
        
        generatedTokens.push_back(nextToken);
        currentInput.push_back(nextToken);
        
        if (nextToken == 0) {
            break;
        }
    }
    
    return generatedTokens;
}
```

### 4.4 量化处理
```cpp
void QuantizationManager::applyInt8Quantization() {
    float* weights = static_cast<float*>(modelWeights_);
    size_t numElements = modelSize_ / sizeof(float);
    
    int8_t* quantizedWeights = new int8_t[numElements];
    
    for (size_t i = 0; i < numElements; ++i) {
        float value = weights[i];
        quantizedWeights[i] = static_cast<int8_t>(std::round(value * 127.0f));
    }
    
    delete[] static_cast<float*>(modelWeights_);
    modelWeights_ = quantizedWeights;
}
```

## 5. 并发设计

### 5.1 线程安全保证
- 使用互斥锁保护模型访问
- 支持多线程并发推理
- 使用线程局部存储优化性能

### 5.2 并发推理
```cpp
class ConcurrentModelExecutor {
public:
    explicit ConcurrentModelExecutor(ModelExecutor* executor, ThreadPool* pool);
    ~ConcurrentModelExecutor();
    
    std::vector<BatchOutput> executeConcurrent(
        const std::vector<BatchInput>& inputs
    );
    
private:
    BatchOutput executeSingle(const BatchInput& input);
    
    ModelExecutor* executor_;
    ThreadPool* threadPool_;
};
```

## 6. 内存管理

### 6.1 内存分配策略
- 使用mimalloc进行高效内存分配
- 使用RAII包装器管理动态数组
- 预分配推理缓冲区减少频繁分配

### 6.2 内存优化
```cpp
class ModelExecutor {
private:
    void optimizeMemoryUsage() {
        if (modelSize_ > 1024 * 1024 * 1024) {
            enableMemoryCompression();
        }
    }
    
    void enableMemoryCompression() {
        quantizationManager_.quantizeModel(modelWeights_, modelSize_);
    }
    
    FloatArray inferenceBuffer_;
    FloatArray inputBuffer_;
};
```

## 7. 错误处理

### 7.1 错误类型
```cpp
enum class ModelError {
    MODEL_LOAD_FAILED,
    MODEL_NOT_LOADED,
    INVALID_INPUT,
    INFERENCE_FAILED,
    QUANTIZATION_FAILED,
    OUT_OF_MEMORY
};

class ModelException : public std::runtime_error {
public:
    ModelException(ModelError error, const std::string& message)
        : std::runtime_error(message), error_(error) {}
    
    ModelError getError() const { return error_; }
    
private:
    ModelError error_;
};
```

### 7.2 错误处理策略
- 模型加载失败时记录错误并抛出异常
- 推理失败时返回空输出
- 使用日志记录错误信息
- 提供错误码供上层处理

## 8. 性能优化

### 8.1 SIMD优化
```cpp
class SIMDInferenceOptimizer {
public:
    void applyAVX2Optimizations();
    void applyAVX512Optimizations();
    
    void optimizeMatMul(float* A, float* B, float* C, size_t M, size_t N, size_t K);
    void optimizeActivation(float* input, float* output, size_t size);
    
private:
    bool hasAVX2();
    bool hasAVX512();
};
```

### 8.2 内存优化
- 重用推理缓冲区
- 使用内存池管理临时对象
- 优化内存布局提升缓存命中率

### 8.3 计算优化
- 批量矩阵乘法
- 算子融合
- 图优化

## 9. 测试策略

### 9.1 单元测试
```cpp
class ModelExecutorTest {
public:
    void testLoadModel();
    void testForward();
    void testGenerate();
    void testQuantization();
    void testSIMDOptimizations();
    void testConcurrency();
    void testErrorHandling();
};
```

### 9.2 性能测试
- 测试推理速度（tokens/s）
- 测试延迟（p50, p95, p99）
- 测试内存使用
- 测试量化效果

### 9.3 集成测试
- 与Scheduler模块集成测试
- 与KV Cache模块集成测试
- 端到端性能测试

## 10. 使用示例

### 10.1 基本使用
```cpp
ModelExecutor executor("model_path", "int8", true);

executor.loadModel();

std::vector<int> inputIds = {1, 2, 3, 4, 5};

auto generated = executor.generate(inputIds, 100, 0.7f);

for (int token : generated) {
    std::cout << token << " ";
}
```

### 10.2 批处理推理
```cpp
BatchInput batchInput;
batchInput.inputIds = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10};
batchInput.batchSize = 2;
batchInput.requestPositions = {{0, 5}, {5, 10}};
batchInput.sequenceIds = {1, 2};

BatchOutput output = executor.forward(batchInput);

for (size_t i = 0; i < output.sequenceIds.size(); ++i) {
    FloatArray logits = output.getLogitsForRequest(i);
    int nextToken = executor.getSampler()->sample(logits, 0.7f);
    std::cout << "Request " << output.sequenceIds[i] << ": " << nextToken << std::endl;
}
```

## 11. 配置参数

### 11.1 模型配置
```cpp
struct ModelConfig {
    std::string modelType;
    size_t vocabSize;
    size_t hiddenSize;
    size_t numLayers;
    size_t numAttentionHeads;
    size_t maxSequenceLength;
    size_t intermediateSize;
    
    bool useKVCache;
    bool useQuantization;
    std::string quantizationType;
};
```

### 11.2 性能配置
```cpp
struct PerformanceConfig {
    bool enableSIMD = true;
    bool enableQuantization = false;
    std::string quantizationType = "int8";
    bool enableMemoryOptimization = true;
    bool enableGraphOptimization = true;
    size_t inferenceThreads = 4;
};
```

## 12. 监控指标

### 12.1 推理指标
- 总推理次数
- 平均推理时间
- 平均tokens/s
- 峰值内存使用

### 12.2 缓存指标
- KV缓存命中率
- 缓存大小
- 缓存使用率

## 13. 依赖关系

### 13.1 外部依赖
- C++标准库（std::vector, std::mutex）
- mimalloc（内存管理）
- SIMD库（SIMD优化）
- 线性代数库（矩阵运算）

### 13.2 内部依赖
- Sampler（采样器）
- KVCache（KV缓存）
- FloatArray（RAII包装器）
- ThreadPool（线程池）

## 14. 后续优化方向

### 14.1 短期优化
- 实现更高效的量化算法
- 添加更多SIMD优化
- 优化内存布局

### 14.2 长期优化
- 支持GPU加速
- 实现分布式推理
- 添加模型压缩技术
- 支持动态批处理
