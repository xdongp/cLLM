# Kylin (éº’éºŸ) æ¨ç†å¼•æ“è®¾è®¡æ–‡æ¡£

## ç¼–ç¨‹è§„èŒƒ

æœ¬æ¨¡å—çš„ç¼–ç å®ç°éµå¾ªä»¥ä¸‹è§„èŒƒå’Œçº¦å®šï¼š
- [C++ç¼–ç¨‹è§„èŒƒ.md](C++ç¼–ç¨‹è§„èŒƒ.md)ï¼šå®šä¹‰ç¼–ç é£æ ¼ã€å‘½åè§„èŒƒç­‰

---

## 0. æ–‡æ¡£æ¦‚è¿°

### 0.1 è®¾è®¡ç›®æ ‡

**Kylin (éº’éºŸ)** æ˜¯ cLLM çš„è‡ªç ”é«˜æ€§èƒ½æ¨ç†å¼•æ“ï¼ŒåŸºäº **GGML** æ„å»ºï¼Œä¸“æ³¨äº CPU æè‡´æ€§èƒ½ä¼˜åŒ–ï¼Œå¯é€‰æ”¯æŒ GPUã€‚

**æ ¸å¿ƒç›®æ ‡**ï¼š
- ğŸ¯ **åŸºäº GGML**ï¼šå¤ç”¨æˆç†Ÿçš„é«˜æ€§èƒ½å¼ é‡è®¡ç®—åº“
- ğŸ¯ **GGUF æ ¼å¼ä¼˜å…ˆ**ï¼šåŸç”Ÿæ”¯æŒé‡åŒ–æ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨é¢„é‡åŒ–æ¨¡å‹
- ğŸ¯ **CPU ä¼˜å…ˆ**ï¼šå……åˆ†åˆ©ç”¨ SIMD æŒ‡ä»¤ï¼ˆAVX2/AVX-512/NEONï¼‰
- ğŸ¯ **GPU å¯é€‰**ï¼šé€šè¿‡ GGML çš„ CUDA/Metal åç«¯æ”¯æŒ GPU åŠ é€Ÿ
- ğŸ¯ **é‡åŒ–æ”¯æŒ**ï¼šåŸç”Ÿæ”¯æŒ Q4_K_Mã€Q8_0 ç­‰å¤šç§é‡åŒ–æ ¼å¼
- ğŸ¯ **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ˜“äºæ‰©å±•å’Œå®šåˆ¶

**å‘½åå«ä¹‰**ï¼š
- **Kylin (éº’éºŸ)**ï¼šä¸­å›½ä¼ ç»Ÿç¥å…½ï¼Œè±¡å¾å‰ç¥¥ã€æ™ºæ…§ã€é€Ÿåº¦
- ä»£è¡¨è‡ªç ”å¼•æ“çš„**é«˜æ€§èƒ½**å’Œ**ä¸­å›½åŸåˆ›**ç‰¹è‰²

### 0.2 ä¸åŸè®¾è®¡çš„ä¸»è¦å˜æ›´

| æ–¹é¢ | åŸè®¾è®¡ | æ–°è®¾è®¡ |
|-----|-------|-------|
| **åº•å±‚è®¡ç®—** | è‡ªç ”ç®—å­ï¼ˆæœ´ç´ å®ç°ï¼‰ | GGMLï¼ˆæˆç†Ÿä¼˜åŒ–ï¼‰ |
| **æ¨¡å‹æ ¼å¼** | æ‰å¹³ .bin | **GGUF**ï¼ˆä¼˜å…ˆï¼‰+ safetensors |
| **é‡åŒ–æ”¯æŒ** | å¾…å¼€å‘ | **åŸç”Ÿæ”¯æŒ**ï¼ˆQ2_K ~ Q8_0ï¼‰ |
| **SIMD ä¼˜åŒ–** | éœ€è‡ªç ” | **GGML å†…ç½®** |
| **GPU æ”¯æŒ** | æ—  | **å¯é€‰**ï¼ˆGGML CUDA/Metalï¼‰ |
| **å¼€å‘å‘¨æœŸ** | 12-18 å‘¨ | **6-10 å‘¨** |

### 0.3 æŠ€æœ¯æŒ‘æˆ˜è¯„ä¼°ï¼ˆæ›´æ–°åï¼‰

| æŠ€æœ¯é¢†åŸŸ | éš¾åº¦ | å·¥ä½œé‡ä¼°ç®— | å…³é”®æŒ‘æˆ˜ |
|---------|------|----------|---------|
| GGML é›†æˆ | â­â­â­ | 1-2å‘¨ | API å°è£…ã€CMake é…ç½® |
| GGUF æ¨¡å‹åŠ è½½å™¨ | â­â­â­ | 2-3å‘¨ | å…ƒæ•°æ®è§£æã€å¼ é‡æ˜ å°„ |
| Transformer é€‚é… | â­â­â­ | 2-3å‘¨ | åŸºäº GGML ç®—å­ç»„è£…æ¨¡å‹ |
| KV Cache ç®¡ç† | â­â­ | 1å‘¨ | ä¸ GGML å†…å­˜ç®¡ç†åè°ƒ |
| GPU æ”¯æŒï¼ˆå¯é€‰ï¼‰ | â­â­â­ | 1-2å‘¨ | GGML CUDA/Metal åç«¯ |
| **æ€»è®¡** | - | **6-10å‘¨** | - |

### 0.4 å¼€å‘è·¯çº¿å›¾ï¼ˆæ›´æ–°åï¼‰

```
é˜¶æ®µ1: GGML é›†æˆ (2å‘¨)
  â”œâ”€ é›†æˆ GGML åº“
  â”œâ”€ CMake é…ç½®
  â”œâ”€ C++ å°è£…å±‚
  â””â”€ åŸºç¡€ç®—å­éªŒè¯

é˜¶æ®µ2: GGUF æ¨¡å‹åŠ è½½ (2-3å‘¨)
  â”œâ”€ GGUF æ ¼å¼è§£æå™¨
  â”œâ”€ æ¨¡å‹å…ƒæ•°æ®è¯»å–
  â”œâ”€ é‡åŒ–å¼ é‡åŠ è½½
  â””â”€ Tokenizer é›†æˆ

é˜¶æ®µ3: Transformer å®ç° (2-3å‘¨)
  â”œâ”€ åŸºäº GGML çš„ Attention
  â”œâ”€ FFN / RMSNorm / RoPE
  â”œâ”€ å®Œæ•´æ¨ç†æµç¨‹
  â””â”€ KV Cache ç®¡ç†

é˜¶æ®µ4: ä¼˜åŒ–ä¸æµ‹è¯• (2å‘¨)
  â”œâ”€ Flash Attentionï¼ˆå¯é€‰ï¼‰
  â”œâ”€ æ€§èƒ½è°ƒä¼˜
  â”œâ”€ GPU æ”¯æŒï¼ˆå¯é€‰ï¼‰
  â””â”€ ä¸ llama.cpp åç«¯å¯¹æ¯”

é˜¶æ®µ5: ç”Ÿäº§å°±ç»ª (1-2å‘¨)
  â”œâ”€ é›†æˆåˆ° cLLM æ¡†æ¶
  â”œâ”€ æ–‡æ¡£å®Œå–„
  â””â”€ å‹åŠ›æµ‹è¯•
```

---

## 1. ç³»ç»Ÿæ¶æ„

### 1.1 æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   InferenceEngine (æ¥å£å±‚)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚      KylinBackend (éº’éºŸ)      â”‚
          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
          â”‚  - GGUFLoader (æ¨¡å‹åŠ è½½)      â”‚
          â”‚  - TransformerModel (æ¨ç†)    â”‚
          â”‚  - KVCacheManager (ç¼“å­˜)      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      GGML è®¡ç®—å±‚                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Tensor æ“ä½œ        â”‚  é‡åŒ–æ”¯æŒ           â”‚  ç¡¬ä»¶åç«¯          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  ggml_mul_mat       â”‚  Q4_0, Q4_1         â”‚  CPU (é»˜è®¤)        â”‚
â”‚  ggml_rms_norm      â”‚  Q5_0, Q5_1         â”‚  â”œâ”€ AVX2          â”‚
â”‚  ggml_rope          â”‚  Q8_0, Q8_1         â”‚  â”œâ”€ AVX-512       â”‚
â”‚  ggml_soft_max      â”‚  Q4_K, Q5_K, Q6_K   â”‚  â””â”€ ARM NEON      â”‚
â”‚  ggml_silu          â”‚  FP16, BF16         â”‚  GPU (å¯é€‰)        â”‚
â”‚  ggml_flash_attn    â”‚                     â”‚  â”œâ”€ CUDA          â”‚
â”‚                     â”‚                     â”‚  â””â”€ Metal         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ¨¡å—ä¾èµ–å…³ç³»

```
cLLM Framework
    â”‚
    â”œâ”€â”€> ModelExecutor
    â”‚       â”‚
    â”‚       â””â”€â”€> InferenceEngine (æ¥å£å±‚)
    â”‚               â”‚
    â”‚               â””â”€â”€> KylinBackend
    â”‚                       â”‚
    â”‚                       â”œâ”€â”€> GGUFLoader
    â”‚                       â”‚       â”œâ”€ è§£æ GGUF æ–‡ä»¶å¤´
    â”‚                       â”‚       â”œâ”€ è¯»å–æ¨¡å‹é…ç½®
    â”‚                       â”‚       â”œâ”€ åŠ è½½é‡åŒ–å¼ é‡
    â”‚                       â”‚       â””â”€ æå– Tokenizer ä¿¡æ¯
    â”‚                       â”‚
    â”‚                       â”œâ”€â”€> GGMLContext
    â”‚                       â”‚       â”œâ”€ å†…å­˜ç®¡ç†
    â”‚                       â”‚       â”œâ”€ è®¡ç®—å›¾æ„å»º
    â”‚                       â”‚       â””â”€ åç«¯è°ƒåº¦ (CPU/GPU)
    â”‚                       â”‚
    â”‚                       â””â”€â”€> TransformerModel
    â”‚                               â”œâ”€ Embedding
    â”‚                               â”œâ”€ TransformerBlock (x N)
    â”‚                               â”‚   â”œâ”€ RMSNorm
    â”‚                               â”‚   â”œâ”€ MultiHeadAttention (GQA)
    â”‚                               â”‚   â”‚   â””â”€ RoPE
    â”‚                               â”‚   â””â”€ FeedForward (SwiGLU)
    â”‚                               â”œâ”€ FinalNorm
    â”‚                               â””â”€ LMHead
    â”‚
    â”œâ”€â”€> KVCache (å¤ç”¨ cLLM ç°æœ‰)
    â”œâ”€â”€> Sampler (å¤ç”¨ cLLM ç°æœ‰)
    â””â”€â”€> Tokenizer (å¤ç”¨ cLLM ç°æœ‰ / æˆ–ä» GGUF æå–)
```

### 1.3 ä¸å…¶ä»–åç«¯çš„å…³ç³»

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚        cLLM Server              â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚         ModelExecutor           â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                           â”‚                           â”‚
        â–¼                           â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  llama.cpp    â”‚         â”‚     Kylin       â”‚         â”‚    LibTorch     â”‚
â”‚   Backend     â”‚         â”‚    Backend      â”‚         â”‚    Backend      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… GGUF       â”‚         â”‚ âœ… GGUF         â”‚         â”‚ âš ï¸ safetensors  â”‚
â”‚ âœ… é‡åŒ–       â”‚         â”‚ âœ… é‡åŒ– (GGML)  â”‚         â”‚ âŒ é‡åŒ–          â”‚
â”‚ âœ… CUDA       â”‚         â”‚ âœ… CPU ä¼˜å…ˆ     â”‚         â”‚ âœ… CUDA         â”‚
â”‚ âœ… ç”Ÿäº§çº§     â”‚         â”‚ ğŸ¯ å¯å®šåˆ¶       â”‚         â”‚ âš ï¸ å¼€å‘ç”¨       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å®šä½å·®å¼‚**ï¼š
- **llama.cpp**ï¼šç”Ÿäº§çº§ï¼Œå¼€ç®±å³ç”¨ï¼Œæ€§èƒ½æœ€ä¼˜
- **Kylin**ï¼šè‡ªç ”å¯æ§ï¼Œå¯æ·±åº¦å®šåˆ¶ï¼Œå­¦ä¹ ç›®çš„
- **LibTorch**ï¼šå¼€å‘è°ƒè¯•ï¼Œå¿«é€ŸåŸå‹éªŒè¯

---

## 2. æ ¸å¿ƒç»„ä»¶è®¾è®¡

### 2.1 GGML é›†æˆå±‚

#### 2.1.1 GGMLContext

**èŒè´£**ï¼šå°è£… GGML çš„ä¸Šä¸‹æ–‡ç®¡ç†ï¼Œæä¾› C++ å‹å¥½çš„æ¥å£ã€‚

```cpp
// include/cllm/inference/ggml_context.h
namespace cllm::inference {

class GGMLContext {
public:
    explicit GGMLContext(size_t memSize);
    ~GGMLContext();
    
    // å¼ é‡åˆ›å»º
    ggml_tensor* newTensor1D(ggml_type type, int64_t ne0);
    ggml_tensor* newTensor2D(ggml_type type, int64_t ne0, int64_t ne1);
    ggml_tensor* newTensor3D(ggml_type type, int64_t ne0, int64_t ne1, int64_t ne2);
    
    // è®¡ç®—å›¾
    ggml_cgraph* buildGraph(ggml_tensor* output);
    void compute(ggml_cgraph* graph);
    
    // åç«¯ç®¡ç†
    void setBackend(BackendType type);  // CPU, CUDA, Metal
    
    ggml_context* raw() { return ctx_; }
    
private:
    ggml_context* ctx_;
    std::vector<uint8_t> buffer_;
    BackendType backend_ = BackendType::CPU;
};

} // namespace cllm::inference
```

#### 2.1.2 åç«¯ç±»å‹

```cpp
enum class BackendType {
    CPU,      // é»˜è®¤ï¼Œæ”¯æŒ AVX2/AVX-512/NEON
    CUDA,     // NVIDIA GPUï¼ˆå¯é€‰ï¼‰
    Metal,    // Apple GPUï¼ˆå¯é€‰ï¼‰
    Auto      // è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜åç«¯
};
```

### 2.2 GGUF æ¨¡å‹åŠ è½½å™¨

#### 2.2.1 GGUF æ ¼å¼æ¦‚è¿°

```
GGUF æ–‡ä»¶ç»“æ„:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Magic Number: "GGUF"               â”‚  4 bytes
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Version: 3                         â”‚  4 bytes
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Tensor Count                       â”‚  8 bytes
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Metadata KV Count                  â”‚  8 bytes
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Metadata Key-Value Pairs           â”‚  Variable
â”‚  â”œâ”€ general.architecture: "qwen2"   â”‚
â”‚  â”œâ”€ general.name: "Qwen3-0.6B"      â”‚
â”‚  â”œâ”€ qwen2.context_length: 32768     â”‚
â”‚  â”œâ”€ qwen2.embedding_length: 1024    â”‚
â”‚  â”œâ”€ qwen2.block_count: 28           â”‚
â”‚  â”œâ”€ tokenizer.ggml.model: "gpt2"    â”‚
â”‚  â””â”€ ...                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Tensor Infos                       â”‚  Variable
â”‚  â”œâ”€ name, dims, type, offset        â”‚
â”‚  â””â”€ ...                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Alignment Padding                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Tensor Data (é‡åŒ–/FP16/FP32)       â”‚  Bulk data
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2.2.2 GGUFLoader æ¥å£

```cpp
// include/cllm/inference/gguf_loader.h
namespace cllm::inference {

struct GGUFModelConfig {
    std::string architecture;      // "qwen2", "llama", etc.
    std::string name;
    size_t contextLength;
    size_t embeddingLength;
    size_t blockCount;
    size_t headCount;
    size_t headCountKV;            // GQA
    size_t feedForwardLength;
    float rmsNormEps;
    float ropeTheta;
    size_t vocabSize;
    // ... å…¶ä»–é…ç½®
};

class GGUFLoader {
public:
    explicit GGUFLoader(const std::string& path);
    ~GGUFLoader();
    
    // æ£€æŸ¥æ–‡ä»¶æœ‰æ•ˆæ€§
    bool isValid() const;
    
    // åŠ è½½æ¨¡å‹é…ç½®ï¼ˆä»å…ƒæ•°æ®ï¼‰
    GGUFModelConfig loadConfig();
    
    // åŠ è½½å¼ é‡åˆ° GGML ä¸Šä¸‹æ–‡
    void loadTensors(GGMLContext* ctx, std::map<std::string, ggml_tensor*>& tensors);
    
    // è·å– Tokenizer ä¿¡æ¯ï¼ˆå¦‚æœå†…åµŒï¼‰
    std::optional<TokenizerInfo> getTokenizerInfo();
    
    // è·å–é‡åŒ–ç±»å‹
    ggml_type getQuantizationType() const;
    
private:
    std::string path_;
    void* mmapData_;      // å†…å­˜æ˜ å°„
    size_t fileSize_;
    
    // GGUF è§£æ
    bool parseHeader();
    bool parseMetadata();
    bool parseTensorInfos();
};

} // namespace cllm::inference
```

#### 2.2.3 æ”¯æŒçš„é‡åŒ–ç±»å‹

| ç±»å‹ | æè¿° | å‹ç¼©æ¯” | ç²¾åº¦æŸå¤± | æ¨èåœºæ™¯ |
|-----|------|-------|---------|---------|
| `Q4_0` | 4-bit å—é‡åŒ– | 4x | ä¸­ | å¿«é€Ÿæµ‹è¯• |
| `Q4_K_M` | 4-bit K-quants | 4x | ä½ | **æ¨è** |
| `Q5_K_M` | 5-bit K-quants | 3.2x | å¾ˆä½ | ç²¾åº¦ä¼˜å…ˆ |
| `Q8_0` | 8-bit é‡åŒ– | 2x | æä½ | é«˜ç²¾åº¦ |
| `F16` | åŠç²¾åº¦æµ®ç‚¹ | 2x | æ—  | åŸºå‡†å¯¹æ¯” |
| `F32` | å•ç²¾åº¦æµ®ç‚¹ | 1x | æ—  | è°ƒè¯• |

### 2.3 Transformer æ¨¡å‹

#### 2.3.1 TransformerModel æ¥å£

```cpp
// include/cllm/inference/transformer_model.h
namespace cllm::inference {

class TransformerModel {
public:
    explicit TransformerModel(const GGUFModelConfig& config, GGMLContext* ctx);
    ~TransformerModel();
    
    // åŠ è½½æƒé‡
    void loadWeights(const std::map<std::string, ggml_tensor*>& tensors);
    
    // å‰å‘ä¼ æ’­
    // è¾“å…¥: token IDs
    // è¾“å‡º: logits [seq_len, vocab_size]
    ggml_tensor* forward(
        const std::vector<int32_t>& inputIds,
        size_t pastLength = 0    // KV Cache å·²æœ‰é•¿åº¦
    );
    
    // å• token ç”Ÿæˆï¼ˆå¢é‡æ¨ç†ï¼‰
    ggml_tensor* forwardOneToken(
        int32_t tokenId,
        size_t position
    );
    
    // KV Cache ç®¡ç†
    void clearKVCache();
    size_t getKVCacheLength() const;
    
private:
    GGUFModelConfig config_;
    GGMLContext* ctx_;
    
    // æ¨¡å‹ç»„ä»¶ï¼ˆä½¿ç”¨ GGML å¼ é‡ï¼‰
    ggml_tensor* embedding_;
    std::vector<TransformerBlock> blocks_;
    ggml_tensor* finalNorm_;
    ggml_tensor* lmHead_;
    
    // KV Cache
    std::vector<ggml_tensor*> kCaches_;
    std::vector<ggml_tensor*> vCaches_;
};

} // namespace cllm::inference
```

#### 2.3.2 æ ¸å¿ƒç®—å­ï¼ˆåŸºäº GGMLï¼‰

| ç®—å­ | GGML å‡½æ•° | è¯´æ˜ |
|-----|----------|-----|
| çŸ©é˜µä¹˜æ³• | `ggml_mul_mat` | è‡ªåŠ¨å¤„ç†é‡åŒ– |
| RMS Norm | `ggml_rms_norm` | æ”¯æŒ eps å‚æ•° |
| RoPE | `ggml_rope` | æ”¯æŒå¤šç§ RoPE å˜ä½“ |
| Softmax | `ggml_soft_max` | æ•°å€¼ç¨³å®šå®ç° |
| SiLU | `ggml_silu` | SwiGLU æ¿€æ´»å‡½æ•° |
| Flash Attention | `ggml_flash_attn_ext` | å¯é€‰ï¼Œé•¿åºåˆ—ä¼˜åŒ– |

#### 2.3.3 Attention è®¡ç®—åŸç†

```
Multi-Head Attention (GQA) æµç¨‹:

1. QKV æŠ•å½±:
   Q = X @ Wq    [seq, num_heads * head_dim]
   K = X @ Wk    [seq, num_kv_heads * head_dim]
   V = X @ Wv    [seq, num_kv_heads * head_dim]

2. é‡å¡‘ä¸ºå¤šå¤´:
   Q: [num_heads, seq, head_dim]
   K: [num_kv_heads, seq, head_dim]
   V: [num_kv_heads, seq, head_dim]

3. åº”ç”¨ RoPE:
   Q, K = RoPE(Q, K, positions)

4. GQA å¹¿æ’­ (å¦‚æœ num_kv_heads < num_heads):
   K, V å¹¿æ’­åˆ° num_heads

5. Attention è®¡ç®—:
   scores = Q @ K^T / sqrt(head_dim)
   scores = scores + causal_mask
   weights = softmax(scores)
   output = weights @ V

6. è¾“å‡ºæŠ•å½±:
   output = concat(heads) @ Wo
```

### 2.4 KV Cache ç®¡ç†

#### 2.4.1 KVCacheManager æ¥å£

```cpp
// include/cllm/inference/kv_cache_manager.h
namespace cllm::inference {

class KVCacheManager {
public:
    KVCacheManager(
        size_t numLayers,
        size_t numKVHeads,
        size_t headDim,
        size_t maxSeqLen,
        GGMLContext* ctx
    );
    
    // è·å–æŒ‡å®šå±‚çš„ KV Cache
    std::pair<ggml_tensor*, ggml_tensor*> getCache(size_t layerIdx);
    
    // æ›´æ–° Cacheï¼ˆè¿½åŠ æ–°çš„ K, Vï¼‰
    void updateCache(
        size_t layerIdx,
        ggml_tensor* newK,
        ggml_tensor* newV,
        size_t position
    );
    
    // æ¸…ç©º Cacheï¼ˆæ–°å¯¹è¯ï¼‰
    void clear();
    
    // è·å–å½“å‰åºåˆ—é•¿åº¦
    size_t getCurrentLength() const;
    
    // å†…å­˜ä½¿ç”¨ç»Ÿè®¡
    size_t getMemoryUsage() const;
    
private:
    std::vector<ggml_tensor*> kCaches_;
    std::vector<ggml_tensor*> vCaches_;
    size_t currentLength_ = 0;
};

} // namespace cllm::inference
```

---

## 3. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 3.1 CPU ä¼˜åŒ–ï¼ˆé»˜è®¤ï¼‰

| ä¼˜åŒ–æŠ€æœ¯ | æ¥æº | è¯´æ˜ |
|---------|------|-----|
| **AVX2/AVX-512** | GGML å†…ç½® | å‘é‡åŒ–çŸ©é˜µè¿ç®— |
| **ARM NEON** | GGML å†…ç½® | Apple Silicon / ARM ä¼˜åŒ– |
| **é‡åŒ–è®¡ç®—** | GGML å†…ç½® | Q4/Q8 ç›´æ¥è®¡ç®—ï¼Œæ— éœ€åé‡åŒ– |
| **å†…å­˜æ˜ å°„** | mmap | å¿«é€ŸåŠ è½½å¤§æ¨¡å‹ |
| **ç¼“å­˜å‹å¥½** | GGML å†…ç½® | åˆ†å—è®¡ç®—ï¼Œæé«˜ç¼“å­˜å‘½ä¸­ |
| **å¤šçº¿ç¨‹** | GGML å†…ç½® | è‡ªåŠ¨åˆ©ç”¨å¤šæ ¸ |

### 3.2 GPU ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰

| åç«¯ | æ”¯æŒå¹³å° | å¯ç”¨æ–¹å¼ |
|-----|---------|---------|
| **CUDA** | NVIDIA GPU | ç¼–è¯‘æ—¶ `-DGGML_CUDA=ON` |
| **Metal** | Apple GPU | ç¼–è¯‘æ—¶ `-DGGML_METAL=ON` |

**GPU åŠ é€Ÿæ•ˆæœ**ï¼ˆå‚è€ƒï¼‰ï¼š
- å°æ¨¡å‹ (<1B): 2-3x åŠ é€Ÿ
- ä¸­æ¨¡å‹ (1-7B): 5-10x åŠ é€Ÿ
- å¤§æ¨¡å‹ (>7B): 10-20x åŠ é€Ÿ

### 3.3 Flash Attention

```
å¯ç”¨æ¡ä»¶:
â”œâ”€ åºåˆ—é•¿åº¦ > 512ï¼ˆçŸ­åºåˆ—æ”¶ç›Šä½ï¼‰
â”œâ”€ éœ€è¦å¤„ç†é•¿ä¸Šä¸‹æ–‡
â””â”€ å†…å­˜å—é™åœºæ™¯

GGML å®ç°: ggml_flash_attn_ext()
â”œâ”€ æ”¯æŒå› æœ mask
â”œâ”€ æ”¯æŒ GQA
â”œâ”€ æ”¯æŒ ALiBi
â””â”€ CPU/GPU å‡å¯ç”¨
```

---

## 4. é…ç½®ä¸ä½¿ç”¨

### 4.1 ç¼–è¯‘é…ç½®

```cmake
# CMakeLists.txt å…³é”®é…ç½®

# GGML é€‰é¡¹
option(KYLIN_ENABLE_CUDA "Enable CUDA support" OFF)
option(KYLIN_ENABLE_METAL "Enable Metal support" OFF)
option(KYLIN_ENABLE_FLASH_ATTN "Enable Flash Attention" ON)

# é›†æˆ GGML
add_subdirectory(third_party/ggml)

# Kylin åç«¯
add_library(kylin_backend
    src/inference/ggml_context.cpp
    src/inference/gguf_loader.cpp
    src/inference/transformer_model.cpp
    src/inference/kv_cache_manager.cpp
    src/inference/kylin_backend.cpp
)
target_link_libraries(kylin_backend PRIVATE ggml)
```

### 4.2 è¿è¡Œæ—¶é…ç½®

```yaml
# config.yaml
backend:
  type: kylin  # ä½¿ç”¨ Kylin åç«¯
  
kylin:
  device: cpu           # cpu / cuda / metal / auto
  threads: 0            # 0 = è‡ªåŠ¨æ£€æµ‹
  use_mmap: true        # å†…å­˜æ˜ å°„åŠ è½½
  use_flash_attn: true  # Flash Attention
  
  # GPU é…ç½®ï¼ˆå¯é€‰ï¼‰
  gpu_layers: 0         # 0 = å…¨ CPUï¼Œ>0 = éƒ¨åˆ†å±‚åœ¨ GPU
```

### 4.3 ä½¿ç”¨ç¤ºä¾‹

```cpp
#include "cllm/inference/kylin_backend.h"

// åˆ›å»º Kylin åç«¯
KylinBackend backend;

// åŠ è½½ GGUF æ¨¡å‹
if (!backend.loadModel("/path/to/model.gguf")) {
    std::cerr << "Failed to load model" << std::endl;
    return -1;
}

// æ¨ç†
std::vector<int32_t> inputIds = {1, 72, 105};  // "Hi"
auto logits = backend.forward(inputIds);

// å¢é‡ç”Ÿæˆ
int32_t nextToken = backend.forwardOneToken(72, 3);
```

---

## 5. ä¸ llama.cpp åç«¯çš„å¯¹æ¯”

| æ–¹é¢ | Kylin Backend | llama.cpp Backend |
|-----|--------------|-------------------|
| **ä»£ç å¤æ‚åº¦** | ä½ï¼ˆå°è£… GGMLï¼‰ | é«˜ï¼ˆç›´æ¥ä½¿ç”¨ llama.cpp APIï¼‰ |
| **å¯å®šåˆ¶æ€§** | â­â­â­â­â­ é«˜ | â­â­â­ ä¸­ |
| **æ€§èƒ½** | â­â­â­â­ æ¥è¿‘ | â­â­â­â­â­ æœ€ä¼˜ |
| **å­¦ä¹ ä»·å€¼** | â­â­â­â­â­ é«˜ | â­â­ ä½ï¼ˆé»‘ç›’ï¼‰ |
| **ç»´æŠ¤æˆæœ¬** | ä¸­ï¼ˆéœ€è·Ÿè¿› GGMLï¼‰ | ä½ï¼ˆç¤¾åŒºç»´æŠ¤ï¼‰ |
| **æ–°åŠŸèƒ½æ”¯æŒ** | éœ€è‡ªè¡Œå®ç° | è‡ªåŠ¨è·å¾— |

**é€‰æ‹©å»ºè®®**ï¼š
- **ç”Ÿäº§ç¯å¢ƒ**ï¼šä¼˜å…ˆä½¿ç”¨ llama.cpp åç«¯
- **å­¦ä¹ ç ”ç©¶**ï¼šä½¿ç”¨ Kylin åç«¯ï¼Œå¯æ·±å…¥ç†è§£æ¨ç†åŸç†
- **å®šåˆ¶éœ€æ±‚**ï¼šä½¿ç”¨ Kylin åç«¯ï¼Œä¾¿äºä¿®æ”¹å’Œæ‰©å±•

---

## 6. å¼€å‘æŒ‡å—

### 6.1 ç›®å½•ç»“æ„

```
src/inference/
â”œâ”€â”€ ggml_context.cpp       # GGML ä¸Šä¸‹æ–‡å°è£…
â”œâ”€â”€ gguf_loader.cpp        # GGUF åŠ è½½å™¨
â”œâ”€â”€ transformer_model.cpp  # Transformer æ¨¡å‹
â”œâ”€â”€ kv_cache_manager.cpp   # KV Cache ç®¡ç†
â””â”€â”€ kylin_backend.cpp      # Kylin åç«¯ä¸»ç±»

include/cllm/inference/
â”œâ”€â”€ ggml_context.h
â”œâ”€â”€ gguf_loader.h
â”œâ”€â”€ transformer_model.h
â”œâ”€â”€ kv_cache_manager.h
â””â”€â”€ kylin_backend.h

third_party/
â””â”€â”€ ggml/                  # GGML åº“ï¼ˆgit submoduleï¼‰
```

### 6.2 ç¼–è¯‘å’Œæµ‹è¯•

```bash
# è·å– GGML
cd third_party
git clone https://github.com/ggerganov/ggml.git

# ç¼–è¯‘ï¼ˆCPUï¼‰
cd ../build
cmake .. -DKYLIN_ENABLE_CUDA=OFF
make -j$(nproc)

# ç¼–è¯‘ï¼ˆCUDAï¼‰
cmake .. -DKYLIN_ENABLE_CUDA=ON
make -j$(nproc)

# æµ‹è¯•
./bin/test_kylin_backend --model /path/to/model.gguf
```

### 6.3 è°ƒè¯•å»ºè®®

1. **æ­£ç¡®æ€§éªŒè¯**ï¼šä¸ llama.cpp åç«¯å¯¹æ¯”è¾“å‡º
2. **æ€§èƒ½åˆ†æ**ï¼šä½¿ç”¨ `perf` æˆ– `Instruments` åˆ†æçƒ­ç‚¹
3. **å†…å­˜æ£€æŸ¥**ï¼šä½¿ç”¨ `valgrind` æˆ– `AddressSanitizer`

---

## 7. å‚è€ƒèµ„æ–™

- [GGML GitHub](https://github.com/ggerganov/ggml) - GGML å¼ é‡è®¡ç®—åº“
- [GGUF è§„èŒƒ](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) - GGUF æ ¼å¼æ–‡æ¡£
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - å‚è€ƒå®ç°
- [æ¨ç†å¼•æ“æ¥å£è®¾è®¡.md](æ¨ç†å¼•æ“æ¥å£è®¾è®¡.md) - cLLM æ¥å£è§„èŒƒ

---

## 8. æ€»ç»“

**Kylin (éº’éºŸ) æ¨ç†å¼•æ“** v2.0 é‡‡ç”¨ GGML ä½œä¸ºåº•å±‚è®¡ç®—åº“ï¼Œå®ç°äº†ï¼š

âœ… **GGUF åŸç”Ÿæ”¯æŒ**ï¼šç›´æ¥åŠ è½½é¢„é‡åŒ–æ¨¡å‹  
âœ… **é«˜æ€§èƒ½è®¡ç®—**ï¼šå¤ç”¨ GGML çš„ SIMD ä¼˜åŒ–  
âœ… **é‡åŒ–æ¨ç†**ï¼šQ4_K_Mã€Q8_0 ç­‰å¤šç§æ ¼å¼  
âœ… **CPU ä¼˜å…ˆ**ï¼šå¼€ç®±å³ç”¨ï¼Œæ— éœ€ GPU  
âœ… **GPU å¯é€‰**ï¼šé€šè¿‡ GGML CUDA/Metal æ”¯æŒ  
âœ… **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ˜“äºç†è§£ã€ä¿®æ”¹å’Œæ‰©å±•  

**è®¾è®¡ç†å¿µ**ï¼š
- ç«™åœ¨å·¨äººè‚©è†€ä¸Šï¼ˆå¤ç”¨ GGMLï¼‰ï¼Œè€Œéé‡å¤é€ è½®å­
- ä¿æŒè‡ªç ”å¯æ§ï¼Œä¾¿äºæ·±å…¥å­¦ä¹ å’Œå®šåˆ¶
- ä¼˜å…ˆå®ç°æ ¸å¿ƒåŠŸèƒ½ï¼Œé€æ­¥å®Œå–„ä¼˜åŒ–
