# Kylin (éº’éºŸ) æ¨ç†å¼•æ“è®¾è®¡æ–‡æ¡£

## ç¼–ç¨‹è§„èŒƒ

æœ¬æ¨¡å—çš„ç¼–ç å®ç°éµå¾ªä»¥ä¸‹è§„èŒƒå’Œçº¦å®šï¼š
- [C++ç¼–ç¨‹è§„èŒƒ.md](C++ç¼–ç¨‹è§„èŒƒ.md)ï¼šå®šä¹‰ç¼–ç é£æ ¼ã€å‘½åè§„èŒƒç­‰

## 0. æ–‡æ¡£æ¦‚è¿°

### 0.1 è®¾è®¡ç›®æ ‡

**Kylin (éº’éºŸ)** æ˜¯ cLLM çš„è‡ªç ”é«˜æ€§èƒ½æ¨ç†å¼•æ“ï¼Œä¸“æ³¨äº CPU æè‡´æ€§èƒ½ä¼˜åŒ–ã€‚

**æ ¸å¿ƒç›®æ ‡**ï¼š
- çº¯ C++ å®ç°ï¼Œæ— ä¾èµ– (é™¤æ ‡å‡†åº“å¤–)
- æ”¯æŒ Qwen3 ç­‰ä¸»æµ Transformer æ¶æ„
- CPU æè‡´ä¼˜åŒ–ï¼Œå……åˆ†åˆ©ç”¨ SIMD æŒ‡ä»¤
- æ”¯æŒå¤šç§é‡åŒ–æ ¼å¼ï¼ˆFP32/FP16/INT8/INT4ï¼‰
- é«˜æ€§èƒ½ï¼Œä½å»¶è¿Ÿï¼Œä½å†…å­˜å ç”¨
- æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•

**å‘½åå«ä¹‰**ï¼š
- **Kylin (éº’éºŸ)**ï¼šä¸­å›½ä¼ ç»Ÿç¥å…½ï¼Œè±¡å¾å‰ç¥¥ã€æ™ºæ…§ã€é€Ÿåº¦
- ä»£è¡¨è‡ªç ”å¼•æ“çš„**é«˜æ€§èƒ½**å’Œ**ä¸­å›½åŸåˆ›**ç‰¹è‰²

### 0.2 æŠ€æœ¯æŒ‘æˆ˜è¯„ä¼°

| æŠ€æœ¯é¢†åŸŸ | éš¾åº¦ | å·¥ä½œé‡ä¼°ç®— | å…³é”®æŒ‘æˆ˜ |
|---------|------|----------|---------|
| Transformer æ¶æ„å®ç° | â­â­â­â­â­ | 4-6å‘¨ | Multi-head attentionã€RoPEã€RMSNorm |
| æ¨¡å‹åŠ è½½å™¨ | â­â­â­â­ | 2-3å‘¨ | æ‰å¹³ .bin è§£æã€æƒé‡æ˜ å°„ |
| SIMD ä¼˜åŒ– | â­â­â­â­â­ | 3-4å‘¨ | AVX2/AVX-512 çŸ©é˜µè¿ç®— |
| é‡åŒ–æ”¯æŒ | â­â­â­â­ | 2-3å‘¨ | INT8/INT4 é‡åŒ–æ¨ç† |
| å†…å­˜ç®¡ç† | â­â­â­ | 1-2å‘¨ | é«˜æ•ˆå†…å­˜åˆ†é…å’Œ KV Cache |
| **æ€»è®¡** | - | **12-18å‘¨** | - |

### 0.3 å¼€å‘è·¯çº¿å›¾

```
é˜¶æ®µ1: MVP åŸºç¡€æ¶æ„ (3å‘¨) - âœ… å·²å®Œæˆ
  â”œâ”€ å¼ é‡æŠ½è±¡å±‚ (FP32, CPU)
  â”œâ”€ ç®€åŒ–å†…å­˜ç®¡ç†
  â””â”€ æ‰å¹³ .bin æ¨¡å‹åŠ è½½å™¨

é˜¶æ®µ2: Transformer æ ¸å¿ƒ (6å‘¨) - ğŸš§ è¿›è¡Œä¸­
  â”œâ”€ Attention æœºåˆ¶
  â”œâ”€ Feed-Forward ç½‘ç»œ
  â”œâ”€ Normalization å±‚
  â””â”€ Position Encoding (RoPE)

é˜¶æ®µ3: ä¼˜åŒ–ä¸é‡åŒ– (5å‘¨) - â³ å¾…å¼€å‘
  â”œâ”€ SIMD ä¼˜åŒ– (AVX2/AVX-512)
  â”œâ”€ é‡åŒ–æ”¯æŒ (INT8/INT4)
  â””â”€ KV Cache ä¼˜åŒ–

é˜¶æ®µ4: é›†æˆä¸æµ‹è¯• (4å‘¨) - ğŸš§ è¿›è¡Œä¸­
  â”œâ”€ é›†æˆåˆ° cLLM æ¡†æ¶ âœ…
  â”œâ”€ å•å…ƒæµ‹è¯•
  â”œâ”€ æ€§èƒ½æµ‹è¯•
  â””â”€ ä¸ LibTorch åç«¯å¯¹æ¯”
```

## 1. ç³»ç»Ÿæ¶æ„

### 1.1 æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              InferenceEngine (æ¥å£å±‚)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   KylinBackend (éº’éºŸ)    â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ - TransformerModel       â”‚
        â”‚ - ModelLoader            â”‚
        â”‚ - è‡ªç ”ç®—å­åº“             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      Layer 1: æ¨¡å‹æŠ½è±¡å±‚                  â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
        â”‚  â”‚ ModelLoader  â”‚  ModelWeights     â”‚    â”‚
        â”‚  â”‚ ModelConfig  â”‚  Tokenizer (å¤ç”¨) â”‚    â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚      Layer 2: Transformer æ ¸å¿ƒå±‚          â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
        â”‚  â”‚ TransformerModel                 â”‚    â”‚
        â”‚  â”‚  â”œâ”€ TransformerBlock (x N)       â”‚    â”‚
        â”‚  â”‚  â”‚   â”œâ”€ MultiHeadAttention       â”‚    â”‚
        â”‚  â”‚  â”‚   â”œâ”€ FeedForwardNetwork       â”‚    â”‚
        â”‚  â”‚  â”‚   â””â”€ RMSNorm                  â”‚    â”‚
        â”‚  â”‚  â”œâ”€ Embedding Layer              â”‚    â”‚
        â”‚  â”‚  â””â”€ LM Head                      â”‚    â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚      Layer 3: ç®—å­å±‚ (Operators)          â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
        â”‚  â”‚ MatMul    â”‚ Softmax â”‚ LayerNorm â”‚    â”‚
        â”‚  â”‚ Embedding â”‚ RoPE    â”‚ SwiGLU    â”‚    â”‚
        â”‚  â”‚ Add/Mul   â”‚ Reshape â”‚ Transpose â”‚    â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚      Layer 4: å¼ é‡ä¸å†…å­˜å±‚                â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
        â”‚  â”‚ Tensor (MVP)  â”‚ TensorView       â”‚    â”‚
        â”‚  â”‚ std::vector   â”‚ Allocator        â”‚    â”‚
        â”‚  â”‚ KVCacheBuffer â”‚ MemoryMonitor    â”‚    â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚      Layer 5: ä¼˜åŒ–å±‚ (å¾…å¼€å‘)             â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
        â”‚  â”‚ SIMD Kernels (AVX2/AVX-512)      â”‚    â”‚
        â”‚  â”‚ Quantization (INT8/INT4)         â”‚    â”‚
        â”‚  â”‚ Kernel Fusion                    â”‚    â”‚
        â”‚  â”‚ Memory Optimization              â”‚    â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ¨¡å—ä¾èµ–å…³ç³»

```
ModelExecutor (ç°æœ‰)
    â”‚
    â”œâ”€â”€> InferenceEngine (æ¥å£å±‚)
    â”‚       â”‚
    â”‚       â””â”€â”€> KylinBackend (Kylin åç«¯)
    â”‚               â”‚
    â”‚               â”œâ”€â”€> ModelLoader (æ‰å¹³ .bin)
    â”‚               â”œâ”€â”€> TransformerModel
    â”‚               â”‚       â”œâ”€â”€> TransformerBlock (x N)
    â”‚               â”‚       â”‚       â”œâ”€â”€> MultiHeadAttention
    â”‚               â”‚       â”‚       â”‚       â”œâ”€â”€> RoPE
    â”‚               â”‚       â”‚       â”‚       â””â”€â”€> Kernels (MatMul, Softmax)
    â”‚               â”‚       â”‚       â”œâ”€â”€> FeedForwardNetwork
    â”‚               â”‚       â”‚       â”‚       â””â”€â”€> SwiGLU
    â”‚               â”‚       â”‚       â””â”€â”€> RMSNorm
    â”‚               â”‚       â”œâ”€â”€> Embedding
    â”‚               â”‚       â””â”€â”€> LMHead
    â”‚               â”‚
    â”‚               â”œâ”€â”€> Tensor / TensorView
    â”‚               â”œâ”€â”€> MemoryAllocator (æœªæ¥)
    â”‚               â””â”€â”€> SIMD Kernels (æœªæ¥)
    â”‚
    â”œâ”€â”€> KVCache (å¤ç”¨ç°æœ‰)
    â”œâ”€â”€> Sampler (å¤ç”¨ç°æœ‰)
    â””â”€â”€> Tokenizer (å¤ç”¨ç°æœ‰)
```

## 2. æ ¸å¿ƒç»„ä»¶è®¾è®¡

### 2.1 å¼ é‡æŠ½è±¡å±‚

#### 2.1.1 Tensor ç±»

**æ–‡ä»¶**: `include/cllm/inference/tensor.h`

**å®ç°çŠ¶æ€**: âœ… å·²å®Œæˆï¼ˆMVP ç®€åŒ–ç‰ˆï¼‰

```cpp
namespace cllm {
namespace inference {

/**
 * @brief æ•°æ®ç±»å‹æšä¸¾ï¼ˆMVP é˜¶æ®µä»…æ”¯æŒ FP32ï¼‰
 */
enum class DataType {
    FP32,   // 32ä½æµ®ç‚¹ï¼ˆå½“å‰å®ç°ï¼‰
    FP16,   // 16ä½æµ®ç‚¹ï¼ˆå¾…æ”¯æŒï¼‰
    INT8,   // 8ä½æ•´æ•°ï¼ˆå¾…æ”¯æŒï¼‰
    INT4    // 4ä½æ•´æ•°ï¼ˆå¾…æ”¯æŒï¼‰
};

/**
 * @brief è®¾å¤‡ç±»å‹æšä¸¾ï¼ˆMVP é˜¶æ®µä»…æ”¯æŒ CPUï¼‰
 */
enum class Device {
    CPU,    // CPUï¼ˆå½“å‰å®ç°ï¼‰
    GPU     // GPUï¼ˆå¾…æ”¯æŒï¼‰
};

/**
 * @brief ç®€åŒ–ç‰ˆå¼ é‡ç±»
 *
 * MVP é˜¶æ®µçš„ç›®æ ‡æ˜¯æä¾›ä¸€ä¸ªè¶³å¤Ÿæ‰¿è½½ Transformer å‰å‘è®¡ç®—çš„æœ€å°å®ç°ï¼š
 * - ä»…æ”¯æŒ float æ•°æ®ç±»å‹
 * - ä»…æ”¯æŒ CPU è®¾å¤‡
 * - ä»¥ row-major æ–¹å¼å­˜å‚¨
 * - å½¢çŠ¶ä¿¡æ¯é€šè¿‡ std::vector<size_t> ç»´æŠ¤
 */
class Tensor {
public:
    /// é»˜è®¤æ„é€ ï¼Œå¾—åˆ°ä¸€ä¸ªç©ºå¼ é‡
    Tensor() = default;

    /// é€šè¿‡å½¢çŠ¶æ„é€ å¼ é‡
    explicit Tensor(const std::vector<size_t>& shape);

    /// é€šè¿‡åˆå§‹åŒ–åˆ—è¡¨æ„é€ å¼ é‡ï¼Œä¾‹å¦‚ Tensor({batch, seq, hidden})
    Tensor(std::initializer_list<size_t> shape);

    /// è·å–å¼ é‡å½¢çŠ¶
    const std::vector<size_t>& shape() const;

    /// è·å–ç»´åº¦ä¸ªæ•°
    size_t ndim() const;

    /// è·å–å…ƒç´ æ€»æ•°
    size_t size() const;

    /// è·å–æ•°æ®æŒ‡é’ˆ
    float* data();
    const float* data() const;

    /// æŒ‰ç´¢å¼•è®¿é—®å…ƒç´ 
    float& operator[](size_t index);
    const float& operator[](size_t index) const;

    /// é‡æ–°è®¾ç½®å½¢çŠ¶ï¼ˆé‡æ–°åˆ†é…å†…å­˜ï¼‰
    void resize(const std::vector<size_t>& newShape);

    /// å¡«å……å€¼
    void fill(float value);

    /// æ‰“å°å¼ é‡ä¿¡æ¯ï¼ˆè°ƒè¯•ç”¨ï¼‰
    void print(const std::string& name = "") const;

private:
    std::vector<size_t> shape_;
    std::vector<float> data_;

    void allocate();
};

} // namespace inference
} // namespace cllm
```

**å®ç°ç¤ºä¾‹**ï¼š

```cpp
// src/inference/tensor.cpp
namespace cllm {
namespace inference {

Tensor::Tensor(const std::vector<size_t>& shape) : shape_(shape) {
    allocate();
}

Tensor::Tensor(std::initializer_list<size_t> shape) : shape_(shape) {
    allocate();
}

void Tensor::allocate() {
    size_t totalSize = 1;
    for (size_t dim : shape_) {
        totalSize *= dim;
    }
    data_.resize(totalSize, 0.0f);
}

const std::vector<size_t>& Tensor::shape() const {
    return shape_;
}

size_t Tensor::ndim() const {
    return shape_.size();
}

size_t Tensor::size() const {
    return data_.size();
}

float* Tensor::data() {
    return data_.data();
}

const float* Tensor::data() const {
    return data_.data();
}

void Tensor::resize(const std::vector<size_t>& newShape) {
    shape_ = newShape;
    allocate();
}

void Tensor::fill(float value) {
    std::fill(data_.begin(), data_.end(), value);
}

} // namespace inference
} // namespace cllm
```

#### 2.1.2 å†…å­˜ç®¡ç†

**å®ç°çŠ¶æ€**: âš ï¸ MVP é˜¶æ®µæš‚æœªå®ç°è‡ªå®šä¹‰å†…å­˜ç®¡ç†

**å½“å‰æ–¹æ¡ˆ**ï¼šä½¿ç”¨ `std::vector` è‡ªåŠ¨ç®¡ç†å†…å­˜

**æœªæ¥è§„åˆ’**ï¼š

**æ–‡ä»¶**: `include/cllm/inference/memory/allocator.h`

```cpp
namespace cllm {
namespace inference {

/**
 * @brief è‡ªå®šä¹‰å†…å­˜åˆ†é…å™¨ï¼ˆæœªæ¥å®ç°ï¼‰
 *
 * ç‰¹æ€§ï¼š
 * - å†…å­˜æ± ç®¡ç†ï¼Œå‡å°‘åˆ†é…/é‡Šæ”¾å¼€é”€
 * - å†…å­˜å¯¹é½ï¼ˆ64 å­—èŠ‚ï¼Œä¼˜åŒ–ç¼“å­˜è¡Œï¼‰
 * - å†…å­˜å¤ç”¨ï¼Œå‡å°‘ç¢ç‰‡
 * - æ”¯æŒ huge pagesï¼ˆå¤§é¡µå†…å­˜ï¼‰
 */
class MemoryAllocator {
public:
    MemoryAllocator(size_t poolSize = 1024 * 1024 * 1024);  // é»˜è®¤ 1GB
    ~MemoryAllocator();
    
    void* allocate(size_t size, size_t alignment = 64);
    void deallocate(void* ptr);
    
    size_t getTotalMemory() const;
    size_t getUsedMemory() const;
    size_t getAvailableMemory() const;
    
private:
    struct MemoryBlock {
        void* ptr;
        size_t size;
        bool inUse;
    };
    
    std::vector<MemoryBlock> blocks_;
    size_t poolSize_;
    size_t usedSize_;
};

} // namespace inference
} // namespace cllm
```

### 2.2 æ¨¡å‹åŠ è½½å™¨

#### 2.2.1 ModelLoader

**æ–‡ä»¶**: `include/cllm/inference/model_loader.h`

**å®ç°çŠ¶æ€**: âœ… å·²å®Œæˆï¼ˆMVP ç‰ˆæœ¬ï¼‰

```cpp
namespace cllm {
namespace inference {

/**
 * @brief æ¨¡å‹åŠ è½½å™¨ï¼ˆæ‰å¹³ .bin æ ¼å¼ï¼‰
 *
 * æ”¯æŒï¼š
 * - ä»æ‰å¹³ .bin æ–‡ä»¶åŠ è½½æƒé‡
 * - FP32/FP16/INT8 æ•°æ®ç±»å‹
 * - Qwen3 GQA æ¶æ„
 */
class ModelLoader {
public:
    explicit ModelLoader(const std::string &binPath);
    ~ModelLoader();

    /**
     * @brief åŠ è½½æ¨¡å‹æƒé‡
     * @param config æ¨¡å‹é…ç½®
     * @return æƒé‡å­—å…¸ {name: Tensor}
     */
    std::map<std::string, Tensor> loadWeights(const ModelConfig &config);

    /**
     * @brief æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å¯è¯»
     */
    bool isValid() const;

    /**
     * @brief è·å–æ–‡ä»¶å¤§å°
     */
    size_t getFileSize() const;

private:
    std::string binPath_;
    
    /**
     * @brief ä»äºŒè¿›åˆ¶æ–‡ä»¶è¯»å–æ•°æ®
     */
    std::vector<float> readBinaryFile(const std::string &path, size_t expectedSize);
    
    /**
     * @brief æƒé‡åç§°æ˜ å°„ï¼ˆHF æ ¼å¼ -> å†…éƒ¨æ ¼å¼ï¼‰
     */
    std::string mapWeightName(const std::string &hfName) const;
};

} // namespace inference
} // namespace cllm
```

**æƒé‡æ–‡ä»¶æ ¼å¼**ï¼ˆæ‰å¹³ .binï¼‰ï¼š

```
æ–‡ä»¶ç»“æ„ï¼š
- æ‰€æœ‰æƒé‡æŒ‰é¡ºåºå­˜å‚¨ä¸º float32
- æ— å…ƒæ•°æ®å¤´ï¼Œçº¯äºŒè¿›åˆ¶æ•°æ®
- æƒé‡é¡ºåºç”±å¯¼å‡ºè„šæœ¬å®šä¹‰

å¯¼å‡ºè„šæœ¬ï¼šmodel/export_weights.py
```

**ä½¿ç”¨æµç¨‹**ï¼š

```cpp
// 1. åˆ›å»ºåŠ è½½å™¨
ModelLoader loader("/path/to/model.bin");

if (!loader.isValid()) {
    std::cerr << "Invalid model file" << std::endl;
    return false;
}

// 2. åŠ è½½æƒé‡
ModelConfig config;
config.loadFromJson("/path/to/config.json");

std::map<std::string, Tensor> weights = loader.loadWeights(config);

// 3. è·å–æƒé‡å¼ é‡
Tensor embedding = weights["embedding"];
Tensor lmHead = weights["lm_head"];
Tensor wq0 = weights["layer.0.attention.wq"];
// ... ç­‰ç­‰
```

#### 2.2.2 GQA æ”¯æŒ

**Grouped Query Attention (GQA)** ç‰¹æ®Šå¤„ç†ï¼š

```cpp
// Qwen3 æ¶æ„ä¸­ï¼š
// numAttentionHeads = 16
// numKeyValueHeads = 2 (GQA)

// æƒé‡å½¢çŠ¶ï¼š
// wq: [hidden_size, num_attention_heads * head_dim]
// wk: [hidden_size, num_key_value_heads * head_dim]
// wv: [hidden_size, num_key_value_heads * head_dim]

// KV å¤´éœ€è¦å¹¿æ’­åˆ°æŸ¥è¯¢å¤´æ•°é‡
// æ¯ä¸ª KV å¤´å¯¹åº” num_attention_heads / num_key_value_heads ä¸ª Q å¤´
```

### 2.3 Transformer æ ¸å¿ƒç»„ä»¶

#### 2.3.1 RMSNorm (Layer Normalization)

**æ–‡ä»¶**: `include/cllm/inference/layers/rms_norm.h`

**å®ç°çŠ¶æ€**: ğŸš§ è¿›è¡Œä¸­

```cpp
namespace cllm {
namespace inference {

/**
 * @brief RMS Normalization
 *
 * å…¬å¼: RMSNorm(x) = x / sqrt(mean(x^2) + eps) * weight
 */
class RMSNorm {
public:
    RMSNorm(size_t hiddenSize, float eps = 1e-6);
    ~RMSNorm();
    
    /**
     * @brief åŠ è½½æƒé‡
     * @param weight å½¢çŠ¶ [hiddenSize]
     */
    void loadWeights(const Tensor& weight);
    
    /**
     * @brief å‰å‘ä¼ æ’­
     * @param input å½¢çŠ¶ [batch, seq, hidden]
     * @return å½¢çŠ¶ [batch, seq, hidden]
     */
    Tensor forward(const Tensor& input);
    
private:
    size_t hiddenSize_;
    float eps_;
    Tensor weight_;  // [hiddenSize]
    
    /**
     * @brief è®¡ç®— RMS å½’ä¸€åŒ–
     */
    void computeRMSNorm(
        const float* input,
        float* output,
        size_t batchSize,
        size_t seqLen
    );
};

} // namespace inference
} // namespace cllm
```

**å®ç°åŸç†**:

```cpp
// æ­¥éª¤ï¼š
// 1. å¯¹æ¯ä¸ª token è®¡ç®— x^2 çš„å‡å€¼
//    rms = sqrt(mean(x^2) + eps)
// 2. å½’ä¸€åŒ–ï¼šx_norm = x / rms
// 3. ç¼©æ”¾ï¼šoutput = x_norm * weight

void RMSNorm::computeRMSNorm(
    const float* input,
    float* output,
    size_t batchSize,
    size_t seqLen
) {
    const float* weightData = weight_.data();
    
    for (size_t b = 0; b < batchSize; ++b) {
        for (size_t s = 0; s < seqLen; ++s) {
            size_t offset = (b * seqLen + s) * hiddenSize_;
            
            // 1. è®¡ç®— mean(x^2)
            float sumSquare = 0.0f;
            for (size_t i = 0; i < hiddenSize_; ++i) {
                float val = input[offset + i];
                sumSquare += val * val;
            }
            float meanSquare = sumSquare / hiddenSize_;
            
            // 2. è®¡ç®— rms
            float rms = std::sqrt(meanSquare + eps_);
            
            // 3. å½’ä¸€åŒ–å¹¶ç¼©æ”¾
            for (size_t i = 0; i < hiddenSize_; ++i) {
                output[offset + i] = (input[offset + i] / rms) * weightData[i];
            }
        }
    }
}
```

**ä¼˜åŒ–æ–¹å‘**ï¼ˆæœªæ¥ï¼‰ï¼š
```cpp
// SIMD ä¼˜åŒ–ï¼ˆAVX2ï¼‰
__m256 sum_vec = _mm256_setzero_ps();
for (size_t i = 0; i < hiddenSize_; i += 8) {
    __m256 x = _mm256_loadu_ps(&input[offset + i]);
    sum_vec = _mm256_fmadd_ps(x, x, sum_vec);  // x^2 ç´¯åŠ 
}
```

#### 2.3.2 RoPE (Rotary Position Embedding)

**æ–‡ä»¶**: `include/cllm/inference/layers/rope.h`

**å®ç°çŠ¶æ€**: ğŸš§ è¿›è¡Œä¸­

```cpp
namespace cllm {
namespace inference {

/**
 * @brief Rotary Position Embedding
 *
 * å°†ä½ç½®ä¿¡æ¯ç¼–ç åˆ°æŸ¥è¯¢å’Œé”®ä¸­ï¼Œä½¿ç”¨æ—‹è½¬çŸ©é˜µ
 */
class RoPE {
public:
    RoPE(size_t dimPerHead, size_t maxSeqLen, float theta = 10000.0f);
    ~RoPE();
    
    /**
     * @brief åº”ç”¨ RoPE åˆ°æŸ¥è¯¢å’Œé”®
     * @param q æŸ¥è¯¢å¼ é‡ [batch, num_heads, seq, head_dim]
     * @param k é”®å¼ é‡ [batch, num_kv_heads, seq, head_dim]
     * @param seqLen åºåˆ—é•¿åº¦
     * @param posOffset ä½ç½®åç§»ï¼ˆç”¨äºå¢é‡ç”Ÿæˆï¼‰
     */
    void apply(
        Tensor& q,
        Tensor& k,
        size_t seqLen,
        size_t posOffset = 0
    );
    
private:
    size_t dimPerHead_;
    size_t maxSeqLen_;
    float theta_;
    
    // é¢„è®¡ç®—çš„ cos/sin è¡¨
    Tensor cosCache_;  // [maxSeqLen, dimPerHead/2]
    Tensor sinCache_;  // [maxSeqLen, dimPerHead/2]
    
    /**
     * @brief é¢„è®¡ç®—é¢‘ç‡è¡¨
     */
    void precomputeFreqs();
    
    /**
     * @brief åº”ç”¨æ—‹è½¬
     */
    void applyRotary(
        float* data,
        size_t batchSize,
        size_t numHeads,
        size_t seqLen,
        size_t posOffset
    );
};

} // namespace inference
} // namespace cllm
```

**å®ç°åŸç†**:

```cpp
// å¯¹äºæ¯ä¸ªä½ç½® pos å’Œç»´åº¦å¯¹ (2i, 2i+1):
// freq = pos / (theta ^ (2i / dim))
// x[2i]'   = x[2i] * cos(freq) - x[2i+1] * sin(freq)
// x[2i+1]' = x[2i] * sin(freq) + x[2i+1] * cos(freq)

void RoPE::precomputeFreqs() {
    cosCache_.resize({maxSeqLen_, dimPerHead_ / 2});
    sinCache_.resize({maxSeqLen_, dimPerHead_ / 2});
    
    float* cosData = cosCache_.data();
    float* sinData = sinCache_.data();
    
    for (size_t pos = 0; pos < maxSeqLen_; ++pos) {
        for (size_t i = 0; i < dimPerHead_ / 2; ++i) {
            float freq = pos / std::pow(theta_, 2.0f * i / dimPerHead_);
            cosData[pos * (dimPerHead_ / 2) + i] = std::cos(freq);
            sinData[pos * (dimPerHead_ / 2) + i] = std::sin(freq);
        }
    }
}

void RoPE::applyRotary(
    float* data,
    size_t batchSize,
    size_t numHeads,
    size_t seqLen,
    size_t posOffset
) {
    const float* cosData = cosCache_.data();
    const float* sinData = sinCache_.data();
    
    for (size_t b = 0; b < batchSize; ++b) {
        for (size_t h = 0; h < numHeads; ++h) {
            for (size_t s = 0; s < seqLen; ++s) {
                size_t pos = posOffset + s;
                size_t offset = ((b * numHeads + h) * seqLen + s) * dimPerHead_;
                
                for (size_t i = 0; i < dimPerHead_ / 2; ++i) {
                    float x0 = data[offset + 2 * i];
                    float x1 = data[offset + 2 * i + 1];
                    
                    float cos_val = cosData[pos * (dimPerHead_ / 2) + i];
                    float sin_val = sinData[pos * (dimPerHead_ / 2) + i];
                    
                    data[offset + 2 * i]     = x0 * cos_val - x1 * sin_val;
                    data[offset + 2 * i + 1] = x0 * sin_val + x1 * cos_val;
                }
            }
        }
    }
}
```

#### 2.3.3 Multi-Head Attention

**æ–‡ä»¶**: `include/cllm/inference/attention.h`

**å®ç°çŠ¶æ€**: ğŸš§ è¿›è¡Œä¸­

```cpp
namespace cllm {
namespace inference {

/**
 * @brief å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
 *
 * æ”¯æŒï¼š
 * - Multi-Head Attention (MHA)
 * - Grouped Query Attention (GQA)
 * - KV Cache
 * - RoPE ä½ç½®ç¼–ç 
 */
class MultiHeadAttention {
public:
    MultiHeadAttention(
        size_t hiddenSize,
        size_t numHeads,
        size_t numKVHeads,  // ç”¨äº GQA
        size_t maxSeqLen,
        float ropeTheta = 10000.0f
    );
    ~MultiHeadAttention();
    
    /**
     * @brief åŠ è½½æƒé‡
     */
    void loadWeights(
        const Tensor& wq,  // [hidden, num_heads * head_dim]
        const Tensor& wk,  // [hidden, num_kv_heads * head_dim]
        const Tensor& wv,  // [hidden, num_kv_heads * head_dim]
        const Tensor& wo   // [num_heads * head_dim, hidden]
    );
    
    /**
     * @brief å‰å‘ä¼ æ’­ï¼ˆå¸¦ KV Cacheï¼‰
     * @param input å½¢çŠ¶ [batch, seq, hidden]
     * @param kCache KV Cacheï¼Œå½¢çŠ¶ [batch, num_kv_heads, max_seq, head_dim]
     * @param vCache KV Cacheï¼Œå½¢çŠ¶ [batch, num_kv_heads, max_seq, head_dim]
     * @param layerIdx å±‚ç´¢å¼•
     * @param seqLen å½“å‰åºåˆ—é•¿åº¦
     * @param posOffset ä½ç½®åç§»ï¼ˆå¢é‡ç”Ÿæˆæ—¶ä½¿ç”¨ï¼‰
     * @return å½¢çŠ¶ [batch, seq, hidden]
     */
    Tensor forward(
        const Tensor& input,
        Tensor* kCache,
        Tensor* vCache,
        size_t layerIdx,
        size_t seqLen,
        size_t posOffset
    );
    
private:
    size_t hiddenSize_;
    size_t numHeads_;
    size_t numKVHeads_;
    size_t headDim_;
    size_t maxSeqLen_;
    
    Tensor wq_;  // [hidden, num_heads * head_dim]
    Tensor wk_;  // [hidden, num_kv_heads * head_dim]
    Tensor wv_;  // [hidden, num_kv_heads * head_dim]
    Tensor wo_;  // [num_heads * head_dim, hidden]
    
    std::unique_ptr<RoPE> rope_;
    
    /**
     * @brief QKV æŠ•å½±
     */
    void projectQKV(
        const Tensor& input,
        Tensor& q,
        Tensor& k,
        Tensor& v
    );
    
    /**
     * @brief è®¡ç®—æ³¨æ„åŠ›
     */
    Tensor computeAttention(
        const Tensor& q,
        const Tensor& k,
        const Tensor& v,
        size_t seqLen
    );
    
    /**
     * @brief è¾“å‡ºæŠ•å½±
     */
    Tensor projectOutput(const Tensor& attnOut);
    
    /**
     * @brief GQA å¹¿æ’­ï¼ˆå°† KV å¤´å¹¿æ’­åˆ° Q å¤´æ•°é‡ï¼‰
     */
    void broadcastKVForGQA(Tensor& k, Tensor& v);
};

} // namespace inference
} // namespace cllm
```

**Attention è®¡ç®—æµç¨‹**:

```cpp
Tensor MultiHeadAttention::forward(
    const Tensor& input,  // [batch, seq, hidden]
    Tensor* kCache,
    Tensor* vCache,
    size_t layerIdx,
    size_t seqLen,
    size_t posOffset
) {
    const size_t batchSize = input.shape()[0];
    
    // 1. QKV æŠ•å½±
    Tensor q({batchSize, seqLen, numHeads_ * headDim_});
    Tensor k({batchSize, seqLen, numKVHeads_ * headDim_});
    Tensor v({batchSize, seqLen, numKVHeads_ * headDim_});
    projectQKV(input, q, k, v);
    
    // 2. é‡å¡‘ä¸ºå¤šå¤´
    // Q: [batch, num_heads, seq, head_dim]
    // K: [batch, num_kv_heads, seq, head_dim]
    // V: [batch, num_kv_heads, seq, head_dim]
    
    // 3. åº”ç”¨ RoPE
    rope_->apply(q, k, seqLen, posOffset);
    
    // 4. æ›´æ–° KV Cache
    // kCache[batch, num_kv_heads, posOffset:posOffset+seqLen, head_dim] = k
    // vCache[batch, num_kv_heads, posOffset:posOffset+seqLen, head_dim] = v
    
    // 5. GQA å¹¿æ’­ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if (numKVHeads_ != numHeads_) {
        broadcastKVForGQA(k, v);
    }
    
    // 6. è®¡ç®— Attention
    Tensor attnOut = computeAttention(q, k, v, seqLen);
    
    // 7. è¾“å‡ºæŠ•å½±
    Tensor output = projectOutput(attnOut);
    
    return output;
}

Tensor MultiHeadAttention::computeAttention(
    const Tensor& q,    // [batch, num_heads, seq, head_dim]
    const Tensor& k,    // [batch, num_heads, ctx, head_dim]
    const Tensor& v,    // [batch, num_heads, ctx, head_dim]
    size_t seqLen
) {
    // scores = (Q @ K^T) / sqrt(head_dim)
    // scores shape: [batch, num_heads, seq, ctx]
    
    // åº”ç”¨å› æœ mask
    // mask[i, j] = -inf if j > i else 0
    
    // attn_weights = softmax(scores)
    // attn_out = attn_weights @ V
    // attn_out shape: [batch, num_heads, seq, head_dim]
    
    // é‡å¡‘ä¸º [batch, seq, num_heads * head_dim]
}
```

#### 2.3.4 Feed-Forward Network (SwiGLU)

**æ–‡ä»¶**: `include/cllm/inference/layers/feed_forward.h`

**å®ç°çŠ¶æ€**: ğŸš§ è¿›è¡Œä¸­

```cpp
namespace cllm {
namespace inference {

/**
 * @brief å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆSwiGLU æ¿€æ´»ï¼‰
 *
 * å…¬å¼: FFN(x) = (x @ W_gate * SiLU(x @ W_up)) @ W_down
 */
class FeedForwardNetwork {
public:
    FeedForwardNetwork(size_t hiddenSize, size_t intermediateSize);
    ~FeedForwardNetwork();
    
    /**
     * @brief åŠ è½½æƒé‡
     */
    void loadWeights(
        const Tensor& wGate,  // [hidden, intermediate]
        const Tensor& wUp,    // [hidden, intermediate]
        const Tensor& wDown   // [intermediate, hidden]
    );
    
    /**
     * @brief å‰å‘ä¼ æ’­
     * @param input å½¢çŠ¶ [batch, seq, hidden]
     * @return å½¢çŠ¶ [batch, seq, hidden]
     */
    Tensor forward(const Tensor& input);
    
private:
    size_t hiddenSize_;
    size_t intermediateSize_;
    
    Tensor wGate_;  // [hidden, intermediate]
    Tensor wUp_;    // [hidden, intermediate]
    Tensor wDown_;  // [intermediate, hidden]
    
    /**
     * @brief SwiGLU æ¿€æ´»å‡½æ•°
     * 
     * SwiGLU(gate, up) = gate * SiLU(up)
     * SiLU(x) = x * sigmoid(x) = x / (1 + exp(-x))
     */
    Tensor swiGLU(const Tensor& gate, const Tensor& up);
};

} // namespace inference
} // namespace cllm
```

**å®ç°ç¤ºä¾‹**:

```cpp
Tensor FeedForwardNetwork::forward(const Tensor& input) {
    // 1. gate = input @ W_gate
    Tensor gate = matmul(input, wGate_);
    
    // 2. up = input @ W_up
    Tensor up = matmul(input, wUp_);
    
    // 3. activated = gate * SiLU(up)
    Tensor activated = swiGLU(gate, up);
    
    // 4. output = activated @ W_down
    Tensor output = matmul(activated, wDown_);
    
    return output;
}

Tensor FeedForwardNetwork::swiGLU(const Tensor& gate, const Tensor& up) {
    Tensor result(gate.shape());
    float* dst = result.data();
    const float* gateData = gate.data();
    const float* upData = up.data();
    
    for (size_t i = 0; i < gate.size(); ++i) {
        float x = upData[i];
        float silu = x / (1.0f + std::exp(-x));  // SiLU(x)
        dst[i] = gateData[i] * silu;
    }
    
    return result;
}
```

#### 2.3.5 Transformer Block

**æ–‡ä»¶**: `include/cllm/inference/transformer_block.h`

**å®ç°çŠ¶æ€**: ğŸš§ è¿›è¡Œä¸­

```cpp
namespace cllm {
namespace inference {

/**
 * @brief Transformer å—ï¼ˆPre-Norm æ¶æ„ï¼‰
 *
 * ç»“æ„ï¼š
 * x = x + Attention(RMSNorm(x))
 * x = x + FFN(RMSNorm(x))
 */
class TransformerBlock {
public:
    TransformerBlock(
        size_t hiddenSize,
        size_t numHeads,
        size_t numKVHeads,
        size_t intermediateSize,
        size_t maxSeqLen,
        float rmsNormEps,
        float ropeTheta
    );
    ~TransformerBlock();
    
    /**
     * @brief åŠ è½½æƒé‡
     */
    void loadWeights(const std::map<std::string, Tensor>& weights);
    
    /**
     * @brief å‰å‘ä¼ æ’­
     */
    Tensor forward(
        const Tensor& input,
        Tensor* kCache,
        Tensor* vCache,
        size_t layerIdx,
        size_t seqLen,
        size_t posOffset
    );
    
private:
    std::unique_ptr<MultiHeadAttention> attention_;
    std::unique_ptr<FeedForwardNetwork> ffn_;
    std::unique_ptr<RMSNorm> inputNorm_;
    std::unique_ptr<RMSNorm> postAttnNorm_;
};

} // namespace inference
} // namespace cllm
```

**å®ç°ç¤ºä¾‹**:

```cpp
Tensor TransformerBlock::forward(
    const Tensor& input,
    Tensor* kCache,
    Tensor* vCache,
    size_t layerIdx,
    size_t seqLen,
    size_t posOffset
) {
    // 1. Pre-Norm Attention
    Tensor residual = input;
    Tensor x = inputNorm_->forward(input);
    x = attention_->forward(x, kCache, vCache, layerIdx, seqLen, posOffset);
    x = add(x, residual);  // æ®‹å·®è¿æ¥
    
    // 2. Pre-Norm FFN
    residual = x;
    x = postAttnNorm_->forward(x);
    x = ffn_->forward(x);
    x = add(x, residual);  // æ®‹å·®è¿æ¥
    
    return x;
}
```

#### 2.3.6 å®Œæ•´ Transformer æ¨¡å‹

**æ–‡ä»¶**: `include/cllm/inference/transformer_model.h`

**å®ç°çŠ¶æ€**: ğŸš§ è¿›è¡Œä¸­

```cpp
namespace cllm {
namespace inference {

/**
 * @brief å®Œæ•´çš„ Transformer æ¨¡å‹
 */
class TransformerModel {
public:
    explicit TransformerModel(const ModelConfig& config);
    ~TransformerModel();
    
    /**
     * @brief åŠ è½½æ‰€æœ‰æƒé‡
     */
    void loadWeights(const std::map<std::string, Tensor>& weights);
    
    /**
     * @brief å‰å‘ä¼ æ’­
     * @param inputIds token id åºåˆ—
     * @param kCache KV Cacheï¼ˆå¯é€‰ï¼Œç”¨äºå¢é‡ç”Ÿæˆï¼‰
     * @param vCache KV Cacheï¼ˆå¯é€‰ï¼‰
     * @param posOffset ä½ç½®åç§»
     * @return [seq_len, vocab_size] logits
     */
    Tensor forward(
        const std::vector<int>& inputIds,
        Tensor* kCache = nullptr,
        Tensor* vCache = nullptr,
        size_t posOffset = 0
    );
    
    ModelConfig getConfig() const { return config_; }
    
private:
    ModelConfig config_;
    
    // ç»„ä»¶
    Tensor embedding_;  // [vocab_size, hidden_size]
    std::vector<std::unique_ptr<TransformerBlock>> layers_;
    std::unique_ptr<RMSNorm> finalNorm_;
    Tensor lmHead_;  // [hidden_size, vocab_size]
    
    /**
     * @brief Embedding æŸ¥è¡¨
     */
    Tensor embed(const std::vector<int>& inputIds);
    
    /**
     * @brief ç”Ÿæˆ logits
     */
    Tensor generateLogits(const Tensor& hiddenStates);
};

} // namespace inference
} // namespace cllm
```

**å®ç°ç¤ºä¾‹**:

```cpp
Tensor TransformerModel::forward(
    const std::vector<int>& inputIds,
    Tensor* kCache,
    Tensor* vCache,
    size_t posOffset
) {
    const size_t seqLen = inputIds.size();
    
    // 1. Embedding
    Tensor x = embed(inputIds);  // [seq_len, hidden_size]
    
    // 2. Transformer Layers
    for (size_t i = 0; i < config_.numLayers; ++i) {
        x = layers_[i]->forward(x, kCache, vCache, i, seqLen, posOffset);
    }
    
    // 3. Final Norm
    x = finalNorm_->forward(x);
    
    // 4. LM Head
    Tensor logits = generateLogits(x);  // [seq_len, vocab_size]
    
    return logits;
}

Tensor TransformerModel::embed(const std::vector<int>& inputIds) {
    const size_t seqLen = inputIds.size();
    Tensor result({seqLen, config_.hiddenSize});
    
    float* dst = result.data();
    const float* embData = embedding_.data();
    
    for (size_t i = 0; i < seqLen; ++i) {
        int tokenId = inputIds[i];
        size_t srcOffset = tokenId * config_.hiddenSize;
        size_t dstOffset = i * config_.hiddenSize;
        
        std::memcpy(dst + dstOffset, embData + srcOffset, 
                    config_.hiddenSize * sizeof(float));
    }
    
    return result;
}

Tensor TransformerModel::generateLogits(const Tensor& hiddenStates) {
    // logits = hiddenStates @ lmHead^T
    // å½¢çŠ¶ï¼š[seq_len, hidden_size] @ [vocab_size, hidden_size]^T
    //     = [seq_len, vocab_size]
    
    return matmul(hiddenStates, lmHead_, false, true);  // transpose B
}
```

### 2.4 é«˜æ€§èƒ½ç®—å­

#### 2.4.1 çŸ©é˜µä¹˜æ³• (GEMM)

**æ–‡ä»¶**: `include/cllm/inference/kernels/matmul.h`

**å®ç°çŠ¶æ€**: â³ å¾…å¼€å‘ï¼ˆå½“å‰ä½¿ç”¨æœ´ç´ å®ç°ï¼‰

```cpp
namespace cllm {
namespace inference {
namespace kernels {

/**
 * @brief é€šç”¨çŸ©é˜µä¹˜æ³•æ¥å£
 * 
 * C = A @ B
 * A: [M, K], B: [K, N], C: [M, N]
 */
void matmul(
    const float* A,
    const float* B,
    float* C,
    size_t M,
    size_t N,
    size_t K,
    bool transposeA = false,
    bool transposeB = false
);

/**
 * @brief SIMD ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆAVX2ï¼‰
 */
void matmul_avx2(
    const float* A,
    const float* B,
    float* C,
    size_t M,
    size_t N,
    size_t K
);

/**
 * @brief SIMD ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆAVX-512ï¼‰
 */
void matmul_avx512(
    const float* A,
    const float* B,
    float* C,
    size_t M,
    size_t N,
    size_t K
);

/**
 * @brief é‡åŒ–çŸ©é˜µä¹˜æ³•ï¼ˆINT8ï¼‰
 */
void matmul_int8(
    const int8_t* A,
    const int8_t* B,
    int32_t* C,
    size_t M,
    size_t N,
    size_t K,
    const float* scaleA,
    const float* scaleB
);

} // namespace kernels
} // namespace inference
} // namespace cllm
```

**æœ´ç´ å®ç°ï¼ˆå½“å‰ï¼‰**:

```cpp
void matmul(
    const float* A,
    const float* B,
    float* C,
    size_t M,
    size_t N,
    size_t K,
    bool transposeA,
    bool transposeB
) {
    for (size_t i = 0; i < M; ++i) {
        for (size_t j = 0; j < N; ++j) {
            float sum = 0.0f;
            for (size_t k = 0; k < K; ++k) {
                size_t aIdx = transposeA ? (k * M + i) : (i * K + k);
                size_t bIdx = transposeB ? (j * K + k) : (k * N + j);
                sum += A[aIdx] * B[bIdx];
            }
            C[i * N + j] = sum;
        }
    }
}
```

**SIMD ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆæœªæ¥ï¼‰**:

```cpp
void matmul_avx2(
    const float* A,
    const float* B,
    float* C,
    size_t M,
    size_t N,
    size_t K
) {
    for (size_t i = 0; i < M; ++i) {
        for (size_t j = 0; j < N; j += 8) {  // 8ä¸ªfloat = 256ä½
            __m256 sum = _mm256_setzero_ps();
            
            for (size_t k = 0; k < K; ++k) {
                __m256 a = _mm256_broadcast_ss(&A[i * K + k]);
                __m256 b = _mm256_loadu_ps(&B[k * N + j]);
                sum = _mm256_fmadd_ps(a, b, sum);  // FMA: a * b + sum
            }
            
            _mm256_storeu_ps(&C[i * N + j], sum);
        }
    }
}
```

**ä¼˜åŒ–ç­–ç•¥**:
1. **å‘é‡åŒ–**ï¼šä½¿ç”¨ AVX2/AVX-512 ä¸€æ¬¡å¤„ç† 8/16 ä¸ªæµ®ç‚¹æ•°
2. **åˆ†å— (Tiling)**ï¼šæé«˜ç¼“å­˜å‘½ä¸­ç‡
3. **æ•°æ®é¢„å– (Prefetching)**ï¼šå‡å°‘å†…å­˜å»¶è¿Ÿ
4. **å¾ªç¯å±•å¼€ (Loop Unrolling)**ï¼šå‡å°‘å¾ªç¯å¼€é”€

#### 2.4.2 Softmax

**æ–‡ä»¶**: `include/cllm/inference/kernels/softmax.h`

**å®ç°çŠ¶æ€**: â³ å¾…å¼€å‘ï¼ˆå½“å‰ä½¿ç”¨æœ´ç´ å®ç°ï¼‰

```cpp
namespace cllm {
namespace inference {
namespace kernels {

/**
 * @brief Softmax æ¿€æ´»å‡½æ•°
 * 
 * è¾“å…¥: [outer_dim, inner_dim]
 * å¯¹æœ€åä¸€ä¸ªç»´åº¦åº”ç”¨ softmax
 */
void softmax(
    const float* input,
    float* output,
    size_t outerDim,
    size_t innerDim
);

/**
 * @brief æ•°å€¼ç¨³å®šç‰ˆæœ¬
 */
void softmax_stable(
    const float* input,
    float* output,
    size_t outerDim,
    size_t innerDim
);

} // namespace kernels
} // namespace inference
} // namespace cllm
```

**æ•°å€¼ç¨³å®šå®ç°**:

```cpp
void softmax_stable(
    const float* input,
    float* output,
    size_t outerDim,
    size_t innerDim
) {
    for (size_t i = 0; i < outerDim; ++i) {
        const float* inRow = input + i * innerDim;
        float* outRow = output + i * innerDim;
        
        // 1. æ‰¾æœ€å¤§å€¼ï¼ˆé¿å…æº¢å‡ºï¼‰
        float maxVal = inRow[0];
        for (size_t j = 1; j < innerDim; ++j) {
            maxVal = std::max(maxVal, inRow[j]);
        }
        
        // 2. è®¡ç®— exp(x - max) å’Œ sum
        float sum = 0.0f;
        for (size_t j = 0; j < innerDim; ++j) {
            outRow[j] = std::exp(inRow[j] - maxVal);
            sum += outRow[j];
        }
        
        // 3. å½’ä¸€åŒ–
        for (size_t j = 0; j < innerDim; ++j) {
            outRow[j] /= sum;
        }
    }
}
```

### 2.5 é‡åŒ–æ”¯æŒ

#### 2.5.1 é‡åŒ–æ–¹æ¡ˆ

**æ–‡ä»¶**: `include/cllm/inference/quantization.h`

**å®ç°çŠ¶æ€**: â³ å¾…å¼€å‘

```cpp
namespace cllm {
namespace inference {

enum class QuantizationType {
    NONE,
    INT8,    // 8ä½æ•´æ•°é‡åŒ–
    INT4,    // 4ä½æ•´æ•°é‡åŒ–
    FP16     // åŠç²¾åº¦æµ®ç‚¹
};

/**
 * @brief é‡åŒ–å™¨
 */
class Quantizer {
public:
    explicit Quantizer(QuantizationType type);
    ~Quantizer();
    
    /**
     * @brief å¯¹ç§°é‡åŒ–
     * 
     * scale = max(abs(x)) / 127
     * x_quant = round(x / scale)
     */
    void quantize_symmetric(
        const float* input,
        int8_t* output,
        float* scale,
        size_t size
    );
    
    /**
     * @brief éå¯¹ç§°é‡åŒ–
     * 
     * scale = (max(x) - min(x)) / 255
     * zero_point = round(-min(x) / scale)
     * x_quant = round(x / scale) + zero_point
     */
    void quantize_asymmetric(
        const float* input,
        int8_t* output,
        float* scale,
        int8_t* zeroPoint,
        size_t size
    );
    
    /**
     * @brief åé‡åŒ–
     */
    void dequantize(
        const int8_t* input,
        float* output,
        const float* scale,
        size_t size
    );
    
    /**
     * @brief é‡åŒ–æƒé‡å¼ é‡
     */
    Tensor quantizeWeights(const Tensor& weights);
    
private:
    QuantizationType type_;
    
    float computeScale(const float* data, size_t size);
    int8_t computeZeroPoint(const float* data, size_t size, float scale);
};

} // namespace inference
} // namespace cllm
```

## 3. æ€§èƒ½ä¼˜åŒ–è·¯çº¿å›¾

### 3.1 å½“å‰çŠ¶æ€ï¼ˆMVPï¼‰

| ç»„ä»¶ | å®ç°çŠ¶æ€ | æ€§èƒ½ |
|------|---------|------|
| Tensor | âœ… FP32, CPU | åŸºå‡† |
| MatMul | æœ´ç´ å®ç° | æ…¢ (~20 GFLOPS) |
| Softmax | æœ´ç´ å®ç° | ä¸­ç­‰ |
| RMSNorm | æœ´ç´ å®ç° | ä¸­ç­‰ |
| Attention | æœ´ç´ å®ç° | æ…¢ |
| å†…å­˜ç®¡ç† | std::vector | ä¸­ç­‰ |

### 3.2 çŸ­æœŸä¼˜åŒ–ï¼ˆ2-4å‘¨ï¼‰

1. **SIMD ä¼˜åŒ–**:
   - âœ… æ£€æµ‹ CPU æŒ‡ä»¤é›†ï¼ˆAVX2/AVX-512ï¼‰
   - ğŸš§ MatMul AVX2 å®ç°
   - ğŸš§ Softmax AVX2 å®ç°
   - ğŸš§ RMSNorm AVX2 å®ç°

2. **å†…å­˜ä¼˜åŒ–**:
   - ğŸš§ å†…å­˜æ± ç®¡ç†
   - ğŸš§ 64å­—èŠ‚å†…å­˜å¯¹é½
   - ğŸš§ å‡å°‘ä¸´æ—¶åˆ†é…

3. **ç®—å­èåˆ**:
   - ğŸš§ RMSNorm + MatMul èåˆ
   - ğŸš§ MatMul + Add èåˆ

**é¢„æœŸæå‡**ï¼š2-3x åŠ é€Ÿ

### 3.3 ä¸­æœŸä¼˜åŒ–ï¼ˆ1-2ä¸ªæœˆï¼‰

1. **é‡åŒ–æ”¯æŒ**:
   - INT8 é‡åŒ–æ¨ç†
   - æ··åˆç²¾åº¦ï¼ˆFP16/INT8ï¼‰
   - æƒé‡é‡åŒ– + æ¿€æ´»é‡åŒ–

2. **é«˜çº§ç®—å­**:
   - Flash Attention å®ç°
   - åˆ†å— MatMulï¼ˆTilingï¼‰
   - é¢„å–ä¼˜åŒ–

3. **å¹¶è¡Œä¼˜åŒ–**:
   - OpenMP å¤šçº¿ç¨‹
   - Token-level å¹¶è¡Œ
   - Layer-level æµæ°´çº¿

**é¢„æœŸæå‡**ï¼š5-8x åŠ é€Ÿï¼ˆç›¸å¯¹MVPï¼‰

### 3.4 é•¿æœŸä¼˜åŒ–ï¼ˆ3-6ä¸ªæœˆï¼‰

1. **æè‡´æ€§èƒ½**:
   - AVX-512 å…¨è¦†ç›–
   - INT4 é‡åŒ–
   - è‡ªå®šä¹‰ç®—å­åº“

2. **æ–°ç¡¬ä»¶æ”¯æŒ**:
   - ARM NEONï¼ˆç§»åŠ¨ç«¯ï¼‰
   - AMD ZEN ä¼˜åŒ–
   - æ¢ç´¢ GPU æ”¯æŒ

3. **é«˜çº§ç‰¹æ€§**:
   - æ¨¡å‹å¹¶è¡Œ
   - Pipeline å¹¶è¡Œ
   - åŠ¨æ€å½¢çŠ¶æ”¯æŒ

**é¢„æœŸæå‡**ï¼š10-15x åŠ é€Ÿï¼ˆç›¸å¯¹ MVPï¼‰ï¼Œæ¥è¿‘ llama.cpp æ€§èƒ½

## 4. ä¸ LibTorch åç«¯å¯¹æ¯”

| ç‰¹æ€§ | Kylin Backend | LibTorch Backend |
|------|---------------|------------------|
| **æ˜“ç”¨æ€§** | â­â­â­ | â­â­â­â­â­ |
| **å¼€å‘é€Ÿåº¦** | â­â­ | â­â­â­â­â­ |
| **CPU æ€§èƒ½ï¼ˆæœªä¼˜åŒ–ï¼‰** | â­â­â­ | â­â­â­ |
| **CPU æ€§èƒ½ï¼ˆä¼˜åŒ–åï¼‰** | â­â­â­â­â­ | â­â­â­ |
| **é‡åŒ–æ”¯æŒ** | â­â­â­â­â­ï¼ˆæœªæ¥ï¼‰ | â­â­â­â­ |
| **å¯å®šåˆ¶æ€§** | â­â­â­â­â­ | â­â­ |
| **å†…å­˜å ç”¨ï¼ˆæœªä¼˜åŒ–ï¼‰** | â­â­â­ | â­â­â­ |
| **å†…å­˜å ç”¨ï¼ˆä¼˜åŒ–åï¼‰** | â­â­â­â­â­ | â­â­â­ |
| **äºŒè¿›åˆ¶å¤§å°** | â­â­â­â­â­ï¼ˆå°ï¼‰ | â­â­ï¼ˆå¤§ï¼‰ |
| **GPU æ”¯æŒ** | â³ å¾…å¼€å‘ | â­â­â­â­â­ |

## 5. å¼€å‘æŒ‡å—

### 5.1 ç¼–è¯‘å’Œæµ‹è¯•

```bash
# ç¼–è¯‘ Kylin åç«¯
cd cpp/cLLM
make clean
make

# è¿è¡Œæµ‹è¯•
./build/tests/test_tensor
./build/tests/test_attention
./build/tests/test_transformer

# æ€§èƒ½æµ‹è¯•
./build/bin/cllm_benchmark --backend kylin
```

### 5.2 ä½¿ç”¨ç¤ºä¾‹

```cpp
#include "cllm/inference/inference_engine.h"

using namespace cllm::inference;

// åˆ›å»º Kylin åç«¯å¼•æ“
ModelConfig config;
config.loadFromJson("/path/to/config.json");

InferenceEngine engine(
    config,
    "/path/to/model.bin",  // æ‰å¹³ .bin æƒé‡
    false  // ä½¿ç”¨ Kylin åç«¯
);

if (!engine.initialize()) {
    std::cerr << "Failed to initialize Kylin engine" << std::endl;
    return -1;
}

// æ¨ç†
std::vector<int> inputIds = {1, 72, 105, 2};  // "Hi"
Tensor logits = engine.forward(inputIds);

std::cout << "Logits shape: [" << logits.shape()[0] 
          << ", " << logits.shape()[1] << "]" << std::endl;
```

## 6. å‚è€ƒæ–‡æ¡£

- [æ¨ç†å¼•æ“æ¥å£è®¾è®¡.md](æ¨ç†å¼•æ“æ¥å£è®¾è®¡.md) - ç»Ÿä¸€æ¥å£å±‚å®šä¹‰
- [LibTorchåç«¯è®¾è®¡.md](LibTorchåç«¯è®¾è®¡.md) - LibTorch åç«¯å®ç°
- [C++ç¼–ç¨‹è§„èŒƒ.md](C++ç¼–ç¨‹è§„èŒƒ.md) - ç¼–ç è§„èŒƒ
- [lesson/5.æ¨¡å‹æ‰§è¡Œå™¨çš„åŸç†.md](/lesson/5.æ¨¡å‹æ‰§è¡Œå™¨çš„åŸç†.md) - æ‰§è¡Œå™¨åŸç†
- [lesson/7.å‰å‘ä¼ æ’­ä¼˜åŒ–æŠ€æœ¯.md](/lesson/7.å‰å‘ä¼ æ’­ä¼˜åŒ–æŠ€æœ¯.md) - ä¼˜åŒ–æŠ€æœ¯

## 7. æ€»ç»“

**Kylin (éº’éºŸ) æ¨ç†å¼•æ“**æ˜¯ cLLM çš„è‡ªç ”é«˜æ€§èƒ½åç«¯ï¼Œä¸“æ³¨äºï¼š

âœ… **çº¯ C++ å®ç°**ï¼šæ— å¤–éƒ¨ä¾èµ–ï¼Œæ˜“äºéƒ¨ç½²  
âœ… **æè‡´ CPU æ€§èƒ½**ï¼šSIMD ä¼˜åŒ–ã€é‡åŒ–æ”¯æŒ  
âœ… **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ˜“äºæ‰©å±•å’Œå®šåˆ¶  
âœ… **å®Œå…¨å¯æ§**ï¼šä»ç®—å­åˆ°ä¼˜åŒ–ç­–ç•¥å…¨æŒæ§  

ğŸš§ **å½“å‰çŠ¶æ€**ï¼šMVP é˜¶æ®µï¼ŒåŸºç¡€åŠŸèƒ½å®Œæˆ  
ğŸ¯ **æœªæ¥ç›®æ ‡**ï¼šé€šè¿‡ SIMD å’Œé‡åŒ–ä¼˜åŒ–ï¼Œè¾¾åˆ° llama.cpp çº§åˆ«æ€§èƒ½  
ğŸ¨ **è®¾è®¡ç†å¿µ**ï¼šå…ˆå®ç°ã€åä¼˜åŒ–ï¼Œé€æ­¥æ¼”è¿›

é€šè¿‡ Kylin åç«¯ï¼Œæˆ‘ä»¬å®ç°äº†ï¼š
- **æŠ€æœ¯è‡ªä¸»**ï¼šå®Œå…¨æŒæ¡æ¨ç†å¼•æ“æ ¸å¿ƒæŠ€æœ¯
- **æ€§èƒ½å¯æ§**ï¼šæ ¹æ®éœ€æ±‚å®šåˆ¶ä¼˜åŒ–ç­–ç•¥
- **é•¿æœŸä»·å€¼**ï¼šä¸ºæœªæ¥çš„ç¡¬ä»¶å’Œç®—æ³•æ¼”è¿›æ‰“ä¸‹åŸºç¡€
