# Kylin (éº’éºŸ) æ¨ç†å¼•æ“è®¾è®¡æ–‡æ¡£

## ç¼–ç¨‹è§„èŒƒ

æœ¬æ¨¡å—çš„ç¼–ç å®ç°éµå¾ªä»¥ä¸‹è§„èŒƒå’Œçº¦å®šï¼š
- [C++ç¼–ç¨‹è§„èŒƒ.md](C++ç¼–ç¨‹è§„èŒƒ.md)ï¼šå®šä¹‰ç¼–ç é£æ ¼ã€å‘½åè§„èŒƒç­‰

---

## 0. æ–‡æ¡£æ¦‚è¿°

### 0.1 è®¾è®¡ç›®æ ‡

**Kylin (éº’éºŸ)** æ˜¯ cLLM çš„è‡ªç ”é«˜æ€§èƒ½æ¨ç†å¼•æ“ï¼ŒåŸºäº **GGML** æ„å»ºï¼Œä¸“æ³¨äº CPU æè‡´æ€§èƒ½ä¼˜åŒ–ï¼Œå¯é€‰æ”¯æŒ GPUã€‚

**æ ¸å¿ƒç›®æ ‡**ï¼š
- ğŸ¯ **åŸºäº GGML**ï¼šå¤ç”¨æˆç†Ÿçš„é«˜æ€§èƒ½å¼ é‡è®¡ç®—åº“
- ğŸ¯ **HF æ ¼å¼ä¼˜å…ˆ**ï¼šsafetensors ä¸ºä¸»è·¯å¾„ï¼ŒGGUF ä¸ºå¯é€‰æ”¯æŒï¼ˆæš‚æœªè°ƒé€šï¼‰
- ğŸ¯ **CPU ä¼˜å…ˆ**ï¼šå……åˆ†åˆ©ç”¨ SIMD æŒ‡ä»¤ï¼ˆAVX2/AVX-512/NEONï¼‰
- ğŸ¯ **GPU å¯é€‰**ï¼šé€šè¿‡ GGML çš„ CUDA/Metal åç«¯æ”¯æŒ GPU åŠ é€Ÿ
- ğŸ¯ **é‡åŒ–ä¼˜å…ˆ HF**ï¼šä¼˜å…ˆæ”¯æŒ HF çš„ FP32/FP16/BF16 é‡åŒ–è·¯å¾„
- ğŸ¯ **GGUF éä¸»è·¯å¾„**ï¼šä»…ä¿æŒå¯ç”¨æ€§ï¼Œç»´æŠ¤ä¼˜å…ˆçº§è¾ƒä½
- ğŸ¯ **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ˜“äºæ‰©å±•å’Œå®šåˆ¶

**å‘½åå«ä¹‰**ï¼š
- **Kylin (éº’éºŸ)**ï¼šä¸­å›½ä¼ ç»Ÿç¥å…½ï¼Œè±¡å¾å‰ç¥¥ã€æ™ºæ…§ã€é€Ÿåº¦
- ä»£è¡¨è‡ªç ”å¼•æ“çš„**é«˜æ€§èƒ½**å’Œ**ä¸­å›½åŸåˆ›**ç‰¹è‰²

### 0.2 ä¸åŸè®¾è®¡çš„ä¸»è¦å˜æ›´

| æ–¹é¢ | åŸè®¾è®¡ | æ–°è®¾è®¡ |
|-----|-------|-------|
| **åº•å±‚è®¡ç®—** | è‡ªç ”ç®—å­ï¼ˆæœ´ç´ å®ç°ï¼‰ | GGMLï¼ˆæˆç†Ÿä¼˜åŒ–ï¼‰ |
| **æ¨¡å‹æ ¼å¼** | æ‰å¹³ .bin | **safetensors(HFä¼˜å…ˆ)** + GGUF(å¯é€‰ï¼Œéä¸»è·¯å¾„/ä½ä¼˜å…ˆçº§) |
| **é‡åŒ–æ”¯æŒ** | å¾…å¼€å‘ | **HF é‡åŒ–** âœ…ï¼ˆFP32/FP16/INT8ï¼‰ï¼ŒGGUF é‡åŒ–å¯é€‰ï¼ˆä½ä¼˜å…ˆçº§ï¼‰ |
| **SIMD ä¼˜åŒ–** | éœ€è‡ªç ” | **GGML å†…ç½®** |
| **GPU æ”¯æŒ** | æ—  | **å¯é€‰**ï¼ˆGGML CUDA/Metalï¼‰ |
| **å¼€å‘å‘¨æœŸ** | 12-18 å‘¨ | **6-10 å‘¨** |

### 0.3 æŠ€æœ¯æŒ‘æˆ˜è¯„ä¼°ï¼ˆæ›´æ–°åï¼‰

| æŠ€æœ¯é¢†åŸŸ | éš¾åº¦ | å·¥ä½œé‡ä¼°ç®— | å…³é”®æŒ‘æˆ˜ |
|---------|------|----------|---------|
| GGML é›†æˆ | â­â­â­ | 1-2å‘¨ | API å°è£…ã€CMake é…ç½® |
| GGUF æ¨¡å‹åŠ è½½å™¨ | â­â­â­ | 2-3å‘¨ | å…ƒæ•°æ®è§£æã€å¼ é‡æ˜ å°„ |
| Transformer é€‚é… | â­â­â­ | 2-3å‘¨ | åŸºäº GGML ç®—å­ç»„è£…æ¨¡å‹ |
| KV Cache ç®¡ç† | â­â­ | 1å‘¨ | ä¸ GGML å†…å­˜ç®¡ç†åè°ƒ |
| GPU æ”¯æŒï¼ˆå¯é€‰ï¼‰ | â­â­â­ | 1-2å‘¨ | GGML CUDA/Metal åç«¯ |
| **æ€»è®¡** | - | **6-10å‘¨** | - |

### 0.4 å¼€å‘è·¯çº¿å›¾ï¼ˆæ›´æ–°åï¼‰

```
é˜¶æ®µ1: GGML é›†æˆ (2å‘¨)
  â”œâ”€ é›†æˆ GGML åº“
  â”œâ”€ CMake é…ç½®
  â”œâ”€ C++ å°è£…å±‚
  â””â”€ åŸºç¡€ç®—å­éªŒè¯

é˜¶æ®µ2: HF æ¨¡å‹åŠ è½½ä¸é‡åŒ– (2-3å‘¨)
  â”œâ”€ safetensors è§£æ
  â”œâ”€ HF config.json è¯»å–
  â”œâ”€ HF é‡åŒ–è·¯å¾„ï¼ˆFP32/FP16/INT8 âœ…ï¼‰
  â””â”€ Tokenizer é›†æˆ

é˜¶æ®µ2.5: GGUF å…¼å®¹ï¼ˆä½ä¼˜å…ˆçº§ï¼Œéä¸»è·¯å¾„ï¼‰
  â”œâ”€ GGUF æ ¼å¼è§£æå™¨
  â”œâ”€ å…ƒæ•°æ®è¯»å–
  â””â”€ é‡åŒ–å¼ é‡åŠ è½½

é˜¶æ®µ3: Transformer å®ç° (2-3å‘¨)
  â”œâ”€ åŸºäº GGML çš„ Attention
  â”œâ”€ FFN / RMSNorm / RoPE
  â”œâ”€ å®Œæ•´æ¨ç†æµç¨‹
  â””â”€ KV Cache ç®¡ç†

é˜¶æ®µ4: ä¼˜åŒ–ä¸æµ‹è¯• (2å‘¨)
  â”œâ”€ Flash Attentionï¼ˆå¯é€‰ï¼‰
  â”œâ”€ æ€§èƒ½è°ƒä¼˜
  â”œâ”€ GPU æ”¯æŒï¼ˆå¯é€‰ï¼‰
  â””â”€ ä¸ llama.cpp åç«¯å¯¹æ¯”

é˜¶æ®µ5: ç”Ÿäº§å°±ç»ª (1-2å‘¨)
  â”œâ”€ é›†æˆåˆ° cLLM æ¡†æ¶
  â”œâ”€ æ–‡æ¡£å®Œå–„
  â””â”€ å‹åŠ›æµ‹è¯•
```

---

## 1. ç³»ç»Ÿæ¶æ„

### 1.1 æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   InferenceEngine (æ¥å£å±‚)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚      KylinBackend (éº’éºŸ)      â”‚
          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
          â”‚  - HF Safetensors Loader     â”‚
          â”‚  - HFTransformerModel (æ¨ç†) â”‚
          â”‚  - GGUFLoader (å¯é€‰/éä¸»è·¯å¾„)â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      GGML è®¡ç®—å±‚                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Tensor æ“ä½œ        â”‚  é‡åŒ–æ”¯æŒ           â”‚  ç¡¬ä»¶åç«¯          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  ggml_mul_mat       â”‚  Q4_0, Q4_1         â”‚  CPU (é»˜è®¤)        â”‚
â”‚  ggml_rms_norm      â”‚  Q5_0, Q5_1         â”‚  â”œâ”€ AVX2          â”‚
â”‚  ggml_rope          â”‚  Q8_0, Q8_1         â”‚  â”œâ”€ AVX-512       â”‚
â”‚  ggml_soft_max      â”‚  Q4_K, Q5_K, Q6_K   â”‚  â””â”€ ARM NEON      â”‚
â”‚  ggml_silu          â”‚  FP16, BF16         â”‚  GPU (å¯é€‰)        â”‚
â”‚  ggml_flash_attn    â”‚                     â”‚  â”œâ”€ CUDA          â”‚
â”‚                     â”‚                     â”‚  â””â”€ Metal         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ¨¡å—ä¾èµ–å…³ç³»

```
cLLM Framework
    â”‚
    â”œâ”€â”€> ModelExecutor
    â”‚       â”‚
    â”‚       â””â”€â”€> InferenceEngine (æ¥å£å±‚)
    â”‚               â”‚
    â”‚               â””â”€â”€> KylinBackend
    â”‚                       â”‚
    â”‚                       â”œâ”€â”€> HF Safetensors Loader
    â”‚                       â”‚       â”œâ”€ è§£æ safetensors
    â”‚                       â”‚       â”œâ”€ è¯»å– HF config.json
    â”‚                       â”‚       â””â”€ HF é‡åŒ–æƒé‡å‡†å¤‡
    â”‚                       â”œâ”€â”€> GGUFLoader (å¯é€‰/éä¸»è·¯å¾„/ä½ä¼˜å…ˆçº§)
    â”‚                       â”‚       â”œâ”€ è§£æ GGUF æ–‡ä»¶å¤´
    â”‚                       â”‚       â”œâ”€ è¯»å–æ¨¡å‹é…ç½®
    â”‚                       â”‚       â”œâ”€ åŠ è½½é‡åŒ–å¼ é‡
    â”‚                       â”‚       â””â”€ æå– Tokenizer ä¿¡æ¯
    â”‚                       â”‚
    â”‚                       â”œâ”€â”€> GGMLContext
    â”‚                       â”‚       â”œâ”€ å†…å­˜ç®¡ç†
    â”‚                       â”‚       â”œâ”€ è®¡ç®—å›¾æ„å»º
    â”‚                       â”‚       â””â”€ åç«¯è°ƒåº¦ (CPU/GPU)
    â”‚                       â”‚
    â”‚                       â””â”€â”€> TransformerModel
    â”‚                               â”œâ”€ Embedding
    â”‚                               â”œâ”€ TransformerBlock (x N)
    â”‚                               â”‚   â”œâ”€ RMSNorm
    â”‚                               â”‚   â”œâ”€ MultiHeadAttention (GQA)
    â”‚                               â”‚   â”‚   â””â”€ RoPE
    â”‚                               â”‚   â””â”€ FeedForward (SwiGLU)
    â”‚                               â”œâ”€ FinalNorm
    â”‚                               â””â”€ LMHead
    â”‚
    â”œâ”€â”€> KVCache (å¤ç”¨ cLLM ç°æœ‰)
    â”œâ”€â”€> Sampler (å¤ç”¨ cLLM ç°æœ‰)
    â””â”€â”€> Tokenizer (å¤ç”¨ cLLM ç°æœ‰ / æˆ–ä» GGUF æå–)
```

### 1.3 ä¸å…¶ä»–åç«¯çš„å…³ç³»

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚        cLLM Server              â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚         ModelExecutor           â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                           â”‚                           â”‚
        â–¼                           â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  llama.cpp    â”‚         â”‚     Kylin       â”‚         â”‚    LibTorch     â”‚
â”‚   Backend     â”‚         â”‚    Backend      â”‚         â”‚    Backend      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… GGUF       â”‚         â”‚ âš ï¸ GGUF(å¯é€‰/ä½ä¼˜å…ˆçº§) â”‚   â”‚ âš ï¸ safetensors  â”‚
â”‚ âœ… é‡åŒ–       â”‚         â”‚ âœ… HF é‡åŒ–ä¼˜å…ˆ        â”‚   â”‚ âŒ é‡åŒ–          â”‚
â”‚ âœ… CUDA       â”‚         â”‚ âœ… CPU ä¼˜å…ˆ     â”‚         â”‚ âœ… CUDA         â”‚
â”‚ âœ… ç”Ÿäº§çº§     â”‚         â”‚ ğŸ¯ å¯å®šåˆ¶       â”‚         â”‚ âš ï¸ å¼€å‘ç”¨       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å®šä½å·®å¼‚**ï¼š
- **llama.cpp**ï¼šç”Ÿäº§çº§ï¼Œå¼€ç®±å³ç”¨ï¼Œæ€§èƒ½æœ€ä¼˜
- **Kylin**ï¼šè‡ªç ”å¯æ§ï¼Œå¯æ·±åº¦å®šåˆ¶ï¼Œå­¦ä¹ ç›®çš„
- **LibTorch**ï¼šå¼€å‘è°ƒè¯•ï¼Œå¿«é€ŸåŸå‹éªŒè¯

---

## 2. æ ¸å¿ƒç»„ä»¶è®¾è®¡

### 2.1 GGML é›†æˆå±‚

#### 2.1.1 GGMLContext

**èŒè´£**ï¼šå°è£… GGML çš„ä¸Šä¸‹æ–‡ç®¡ç†ï¼Œæä¾› C++ å‹å¥½çš„æ¥å£ã€‚

```cpp
// include/cllm/inference/ggml_context.h
namespace cllm::inference {

class GGMLContext {
public:
    explicit GGMLContext(size_t memSize);
    ~GGMLContext();
    
    // å¼ é‡åˆ›å»º
    ggml_tensor* newTensor1D(ggml_type type, int64_t ne0);
    ggml_tensor* newTensor2D(ggml_type type, int64_t ne0, int64_t ne1);
    ggml_tensor* newTensor3D(ggml_type type, int64_t ne0, int64_t ne1, int64_t ne2);
    
    // è®¡ç®—å›¾
    ggml_cgraph* buildGraph(ggml_tensor* output);
    void compute(ggml_cgraph* graph);
    
    // åç«¯ç®¡ç†
    void setBackend(BackendType type);  // CPU, CUDA, Metal
    
    ggml_context* raw() { return ctx_; }
    
private:
    ggml_context* ctx_;
    std::vector<uint8_t> buffer_;
    BackendType backend_ = BackendType::CPU;
};

} // namespace cllm::inference
```

#### 2.1.2 åç«¯ç±»å‹

```cpp
enum class BackendType {
    CPU,      // é»˜è®¤ï¼Œæ”¯æŒ AVX2/AVX-512/NEON
    CUDA,     // NVIDIA GPUï¼ˆå¯é€‰ï¼‰
    Metal,    // Apple GPUï¼ˆå¯é€‰ï¼‰
    Auto      // è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜åç«¯
};
```

### 2.2 GGUF æ¨¡å‹åŠ è½½å™¨ï¼ˆå¯é€‰/éä¸»è·¯å¾„/ç»´æŠ¤è¾ƒä½ä¼˜å…ˆçº§ï¼‰

#### 2.2.1 GGUF æ ¼å¼æ¦‚è¿°

```
GGUF æ–‡ä»¶ç»“æ„:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Magic Number: "GGUF"               â”‚  4 bytes
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Version: 3                         â”‚  4 bytes
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Tensor Count                       â”‚  8 bytes
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Metadata KV Count                  â”‚  8 bytes
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Metadata Key-Value Pairs           â”‚  Variable
â”‚  â”œâ”€ general.architecture: "qwen2"   â”‚
â”‚  â”œâ”€ general.name: "Qwen3-0.6B"      â”‚
â”‚  â”œâ”€ qwen2.context_length: 32768     â”‚
â”‚  â”œâ”€ qwen2.embedding_length: 1024    â”‚
â”‚  â”œâ”€ qwen2.block_count: 28           â”‚
â”‚  â”œâ”€ tokenizer.ggml.model: "gpt2"    â”‚
â”‚  â””â”€ ...                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Tensor Infos                       â”‚  Variable
â”‚  â”œâ”€ name, dims, type, offset        â”‚
â”‚  â””â”€ ...                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Alignment Padding                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Tensor Data (é‡åŒ–/FP16/FP32)       â”‚  Bulk data
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2.2.2 GGUFLoader æ¥å£

```cpp
// include/cllm/inference/gguf_loader.h
namespace cllm::inference {

struct GGUFModelConfig {
    std::string architecture;      // "qwen2", "llama", etc.
    std::string name;
    size_t contextLength;
    size_t embeddingLength;
    size_t blockCount;
    size_t headCount;
    size_t headCountKV;            // GQA
    size_t feedForwardLength;
    float rmsNormEps;
    float ropeTheta;
    size_t vocabSize;
    // ... å…¶ä»–é…ç½®
};

class GGUFLoader {
public:
    explicit GGUFLoader(const std::string& path);
    ~GGUFLoader();
    
    // æ£€æŸ¥æ–‡ä»¶æœ‰æ•ˆæ€§
    bool isValid() const;
    
    // åŠ è½½æ¨¡å‹é…ç½®ï¼ˆä»å…ƒæ•°æ®ï¼‰
    GGUFModelConfig loadConfig();
    
    // åŠ è½½å¼ é‡åˆ° GGML ä¸Šä¸‹æ–‡
    void loadTensors(GGMLContext* ctx, std::map<std::string, ggml_tensor*>& tensors);
    
    // è·å– Tokenizer ä¿¡æ¯ï¼ˆå¦‚æœå†…åµŒï¼‰
    std::optional<TokenizerInfo> getTokenizerInfo();
    
    // è·å–é‡åŒ–ç±»å‹
    ggml_type getQuantizationType() const;
    
private:
    std::string path_;
    void* mmapData_;      // å†…å­˜æ˜ å°„
    size_t fileSize_;
    
    // GGUF è§£æ
    bool parseHeader();
    bool parseMetadata();
    bool parseTensorInfos();
};

} // namespace cllm::inference
```

#### 2.2.3 æ”¯æŒçš„é‡åŒ–ç±»å‹ï¼ˆå¯é€‰/ä½ä¼˜å…ˆçº§ï¼‰

| ç±»å‹ | æè¿° | å‹ç¼©æ¯” | ç²¾åº¦æŸå¤± | æ¨èåœºæ™¯ |
|-----|------|-------|---------|---------|
| `Q4_0` | 4-bit å—é‡åŒ– | 4x | ä¸­ | å¿«é€Ÿæµ‹è¯• |
| `Q4_K_M` | 4-bit K-quants | 4x | ä½ | **æ¨è** |
| `Q5_K_M` | 5-bit K-quants | 3.2x | å¾ˆä½ | ç²¾åº¦ä¼˜å…ˆ |
| `Q8_0` | 8-bit é‡åŒ– | 2x | æä½ | é«˜ç²¾åº¦ |
| `F16` | åŠç²¾åº¦æµ®ç‚¹ | 2x | æ—  | åŸºå‡†å¯¹æ¯” |
| `F32` | å•ç²¾åº¦æµ®ç‚¹ | 1x | æ—  | è°ƒè¯• |

### 2.3 Transformer æ¨¡å‹

#### 2.3.1 TransformerModel æ¥å£

```cpp
// include/cllm/inference/transformer_model.h
namespace cllm::inference {

class TransformerModel {
public:
    explicit TransformerModel(const GGUFModelConfig& config, GGMLContext* ctx);
    ~TransformerModel();
    
    // åŠ è½½æƒé‡
    void loadWeights(const std::map<std::string, ggml_tensor*>& tensors);
    
    // å‰å‘ä¼ æ’­
    // è¾“å…¥: token IDs
    // è¾“å‡º: logits [seq_len, vocab_size]
    ggml_tensor* forward(
        const std::vector<int32_t>& inputIds,
        size_t pastLength = 0    // KV Cache å·²æœ‰é•¿åº¦
    );
    
    // å• token ç”Ÿæˆï¼ˆå¢é‡æ¨ç†ï¼‰
    ggml_tensor* forwardOneToken(
        int32_t tokenId,
        size_t position
    );
    
    // KV Cache ç®¡ç†
    void clearKVCache();
    size_t getKVCacheLength() const;
    
private:
    GGUFModelConfig config_;
    GGMLContext* ctx_;
    
    // æ¨¡å‹ç»„ä»¶ï¼ˆä½¿ç”¨ GGML å¼ é‡ï¼‰
    ggml_tensor* embedding_;
    std::vector<TransformerBlock> blocks_;
    ggml_tensor* finalNorm_;
    ggml_tensor* lmHead_;
    
    // KV Cache
    std::vector<ggml_tensor*> kCaches_;
    std::vector<ggml_tensor*> vCaches_;
};

} // namespace cllm::inference
```

#### 2.3.2 æ ¸å¿ƒç®—å­ï¼ˆåŸºäº GGMLï¼‰

| ç®—å­ | GGML å‡½æ•° | è¯´æ˜ |
|-----|----------|-----|
| çŸ©é˜µä¹˜æ³• | `ggml_mul_mat` | è‡ªåŠ¨å¤„ç†é‡åŒ– |
| RMS Norm | `ggml_rms_norm` | æ”¯æŒ eps å‚æ•° |
| RoPE | `ggml_rope` | æ”¯æŒå¤šç§ RoPE å˜ä½“ |
| Softmax | `ggml_soft_max` | æ•°å€¼ç¨³å®šå®ç° |
| SiLU | `ggml_silu` | SwiGLU æ¿€æ´»å‡½æ•° |
| Flash Attention | `ggml_flash_attn_ext` | å¯é€‰ï¼Œé•¿åºåˆ—ä¼˜åŒ– |

#### 2.3.3 Attention è®¡ç®—åŸç†

```
Multi-Head Attention (GQA) æµç¨‹:

1. QKV æŠ•å½±:
   Q = X @ Wq    [seq, num_heads * head_dim]
   K = X @ Wk    [seq, num_kv_heads * head_dim]
   V = X @ Wv    [seq, num_kv_heads * head_dim]

2. é‡å¡‘ä¸ºå¤šå¤´:
   Q: [num_heads, seq, head_dim]
   K: [num_kv_heads, seq, head_dim]
   V: [num_kv_heads, seq, head_dim]

3. åº”ç”¨ RoPE:
   Q, K = RoPE(Q, K, positions)

4. GQA å¹¿æ’­ (å¦‚æœ num_kv_heads < num_heads):
   K, V å¹¿æ’­åˆ° num_heads

5. Attention è®¡ç®—:
   scores = Q @ K^T / sqrt(head_dim)
   scores = scores + causal_mask
   weights = softmax(scores)
   output = weights @ V

6. è¾“å‡ºæŠ•å½±:
   output = concat(heads) @ Wo
```

### 2.4 HF æ¨¡å‹é‡åŒ–ï¼ˆsafetensorsï¼Œä¼˜å…ˆè·¯å¾„ï¼‰

æœ¬èŠ‚æè¿° `src/kylin/hf` ä¸‹çš„ HuggingFace safetensors é‡åŒ–è·¯å¾„ï¼Œè¦†ç›–å…¥å£ã€æƒé‡è½¬æ¢ã€è¿è¡Œæ—¶è®¡ç®—ä¸å·²çŸ¥é™åˆ¶ã€‚

#### 2.4.1 å…¥å£ä¸é…ç½®

- å…¥å£ï¼š`KylinBackend::initializeHFModel()` æ£€æµ‹ HuggingFace ç›®å½•ååˆ›å»º `HFTransformerModel`ã€‚
- é‡åŒ–ç±»å‹ï¼šæ¥è‡ªé…ç½® `model.quantization`ï¼Œé€šè¿‡ `parseQuantType()` è§£æä¸º `QuantType`ã€‚
- é»˜è®¤å€¼ï¼š`Config::serverQuantization()` é»˜è®¤è¿”å› `fp16`ï¼Œå¯é€šè¿‡é…ç½®åˆ‡æ¢åˆ° `fp32`/`bf16`ã€‚

#### 2.4.2 æƒé‡åŠ è½½ä¸æ•°æ®å‡è®¾

- `SafetensorsLoader` é‡‡ç”¨å†…å­˜æ˜ å°„è¯»å–æƒé‡ï¼Œå¹¶è§£æ `dtype/shape/data_offsets` å…ƒä¿¡æ¯ã€‚
- `HFTransformerModel::loadWeights()` ç›´æ¥ä¿å­˜æŒ‡å‘åŸå§‹æ•°æ®çš„ `uint16_t*` æŒ‡é’ˆï¼ˆ`embedTokens_` ç­‰ï¼‰ã€‚
- é‡è¦å‡è®¾ï¼šæƒé‡ä¸º BF16ï¼ˆ`torch_dtype` å¸¸è§ä¸º `bfloat16`ï¼‰ã€‚å½“å‰å®ç°æœªåœ¨ `loadWeights()` æ ¡éªŒ dtypeï¼Œè‹¥æ¨¡å‹ä¸º F16 æˆ– F32ï¼Œä¼šå¯¼è‡´è§£é‡Šé”™è¯¯æˆ–æ•°å€¼å¼‚å¸¸ã€‚

#### 2.4.3 é‡åŒ–æ¨¡å¼ä¸è®¡ç®—è·¯å¾„

1) FP32ï¼ˆæ¨èç¨³å®šè·¯å¾„ï¼‰
- åˆå§‹åŒ–æ—¶è°ƒç”¨ `preconvertWeights()`ï¼šBF16 -> F32ã€‚
- è¿è¡Œæ—¶ç”¨ `matmulF32()` ä¸ `ggml_kernels` çš„ F32 å†…æ ¸ã€‚
- RoPEã€RMSNormã€Softmax å‡ä¸º F32 è®¡ç®—ã€‚

2) FP16ï¼ˆåŠç²¾åº¦è·¯å¾„ï¼Œå·²å®ç°ä½†ä»åœ¨ä¿®å¤ç¨³å®šæ€§ï¼‰
- åˆå§‹åŒ–æ—¶è°ƒç”¨ `convertWeightsToFP16()`ï¼š
  - BF16 -> F32 -> FP16ï¼ˆå¤§æƒé‡çŸ©é˜µï¼‰ã€‚
  - Norm æƒé‡ä¿ç•™ F32ã€‚
  - QKVã€Gate/Up èåˆæƒé‡ä½¿ç”¨ FP16 å­˜å‚¨ã€‚
- è¿è¡Œæ—¶ï¼š
  - `matmulFP16()` ä½¿ç”¨ FP16 æƒé‡ + F32 è¾“å…¥è¾“å‡ºã€‚
  - Embedding ä» FP16 è¡¨ä¸­æŸ¥æ‰¾å¹¶è½¬å› F32ã€‚
  - LM Head å¯ä½¿ç”¨ FP16 æƒé‡ã€‚

3) BF16 / INT8
- BF16 åœ¨ HF è·¯å¾„ä¸­ç›®å‰è¢«è§†ä¸º FP32 è®¡ç®—è·¯å¾„ï¼ˆå…ˆè½¬ F32ï¼‰ï¼Œæœªå®ç° BF16 è®¡ç®—å†…æ ¸ã€‚
- INT8 å·²å®ç° âœ…ï¼šper-tensor å¯¹ç§°é‡åŒ–ï¼ŒNEON ä¼˜åŒ–å†…æ ¸ï¼Œè¯¦è§ 2.4.7 èŠ‚ã€‚

#### 2.4.4 å†…æ ¸ä¾èµ–

- F32/BF16 è®¡ç®—ä¾èµ– `ggml_kernels`ï¼ˆ`matmul_f32`ã€`rms_norm`ã€`dot_product` ç­‰ï¼‰ã€‚
- FP16 è®¡ç®—ä¾èµ– `quant_kernels`ï¼ˆ`convert_f32_to_fp16`ã€`convert_fp16_to_f32`ã€`matmul_fp16_f32`ï¼‰ã€‚
- é‡åŒ–å†…æ ¸éœ€è¦å¯¹é½å¹³å° SIMD èƒ½åŠ›ï¼ˆx86 F16C / ARM NEONï¼‰ï¼Œéœ€è¦ç¡®ä¿å®ç°ä¸å¹³å°é€‚é…ã€‚

#### 2.4.5 å·²çŸ¥é™åˆ¶ä¸é£é™©

- dtype å‡è®¾é£é™©ï¼š`loadWeights()` æœªéªŒè¯ safetensors çš„ `dtype`ï¼ŒF16 æƒé‡ä¼šè¢«å½“ä½œ BF16 è§£é‡Šã€‚
- å¹¶å‘æ¨ç†è·¯å¾„å·®å¼‚ï¼š`forwardSingle()`ï¼ˆper-request KV cacheï¼‰å·²ä¿®å¤æ”¯æŒ FP16 æƒé‡è·¯å¾„ã€‚
- FP16 çš„ç¨³å®šæ€§ä¾èµ– `quant_kernels` å®ç°è´¨é‡ï¼Œè‹¥è½¬æ¢æˆ–çŸ©é˜µä¹˜å®ç°å­˜åœ¨è¾¹ç•Œé—®é¢˜ï¼Œä¼šå¯¼è‡´å´©æºƒæˆ–è¾“å‡ºå¼‚å¸¸ã€‚
- FP16 å´©æºƒé—®é¢˜å·²ä¿®å¤ï¼ˆ2026-01-25ï¼‰ï¼š
  - æ·»åŠ äº† embedding æŒ‡é’ˆåç§»è¾¹ç•Œæ£€æŸ¥
  - ä¿®å¤äº† per-request è·¯å¾„çš„ FP16 æƒé‡é€‰æ‹©é€»è¾‘
  - ç»Ÿä¸€äº† `forwardSingle()` ä¸­ LM Head çš„é‡åŒ–è·¯å¾„

#### 2.4.6 FP16 æ€§èƒ½åˆ†æï¼ˆ2026-01-25 NEON ä¼˜åŒ–åï¼‰

##### æµ‹è¯•ç»“æœ

| æ¨¡å¼ | å†…å­˜ä½¿ç”¨ | ååé‡ (tok/s) | çŠ¶æ€ |
|------|----------|----------------|------|
| FP32 | 2161 MB | ~22 tok/s | âœ… ç¨³å®š |
| **FP16 (ä¼˜åŒ–å)** | **1080 MB** | **~29 tok/s** | **âœ… +32% æå‡** |

##### æ¨ç†ç»“æœéªŒè¯

FP16 æ¨¡å¼çš„æ¨ç†è¾“å‡ºå·²éªŒè¯æ­£ç¡®ï¼Œç”Ÿæˆçš„æ–‡æœ¬è¿è´¯ä¸”æœ‰æ„ä¹‰ï¼Œä¸ FP32 è¾“å‡ºè´¨é‡ä¸€è‡´ã€‚

##### NEON ä¼˜åŒ–å†…å®¹

é’ˆå¯¹ ARM NEON FP16 matmul è¿›è¡Œäº†ä»¥ä¸‹ä¼˜åŒ–ï¼ˆ`quantization.cpp:matmul_fp16_f32`ï¼‰ï¼š

```cpp
// ä¼˜åŒ–åçš„æ ¸å¿ƒå¾ªç¯ï¼ˆ2x å±•å¼€ + é¢„å– + åŒç´¯åŠ å™¨ï¼‰
for (; k + 16 <= K; k += 16) {
    __builtin_prefetch(row + k + 64, 0, 3);  // é¢„å–ä¸‹ä¸€æ‰¹æ•°æ®
    
    // ç¬¬ä¸€ç»„ 8 ä¸ªå…ƒç´ 
    float16x8_t h0 = vld1q_f16(...);
    vsum0 = vfmaq_f32(vsum0, vcvt_f32_f16(vget_low_f16(h0)), in0_lo);
    vsum1 = vfmaq_f32(vsum1, vcvt_f32_f16(vget_high_f16(h0)), in0_hi);
    
    // ç¬¬äºŒç»„ 8 ä¸ªå…ƒç´ 
    float16x8_t h1 = vld1q_f16(...);
    vsum0 = vfmaq_f32(vsum0, vcvt_f32_f16(vget_low_f16(h1)), in1_lo);
    vsum1 = vfmaq_f32(vsum1, vcvt_f32_f16(vget_high_f16(h1)), in1_hi);
}
```

**ä¼˜åŒ–æŠ€æœ¯**ï¼š
1. **2x å¾ªç¯å±•å¼€** - æ¯æ¬¡å¤„ç† 16 ä¸ªå…ƒç´ ï¼ˆåŸ 8 ä¸ªï¼‰
2. **æ•°æ®é¢„å–** - ä½¿ç”¨ `__builtin_prefetch` é¢„å–åˆ° L1 ç¼“å­˜
3. **åŒç´¯åŠ å™¨** - å‡å°‘æ•°æ®ä¾èµ–ï¼Œæé«˜æµæ°´çº¿æ•ˆç‡
4. **èåˆä¹˜åŠ ** - ä½¿ç”¨ `vfmaq_f32` æ›¿ä»£ `vmlaq_f32`

##### FP16 ç»¼åˆæ”¶ç›Š

| æ”¶ç›Šé¡¹ | æ•ˆæœ |
|--------|------|
| æƒé‡å†…å­˜ | **-50%** (1080 MB vs 2161 MB) |
| é¡ºåºååé‡ | **+32%** (~29 vs ~22 tok/s) |
| å¹¶å‘ååé‡ | **+50%** (~30 vs ~20 tok/s) |
| å“åº”æ—¶é—´ | **-24%** (1.71s vs 2.24s) |

##### æœªæ¥ä¼˜åŒ–æ–¹å‘

1. **Apple AMX**ï¼šåˆ©ç”¨ Apple Silicon çš„ AMX çŸ©é˜µåŠ é€Ÿå•å…ƒ
2. **é™æ€è®¡ç®—å›¾**ï¼šå‡å°‘è°ƒåº¦å¼€é”€ï¼Œæ”¯æŒç®—å­èåˆ
3. **INT4 é‡åŒ–**ï¼šè¿›ä¸€æ­¥å‹ç¼©å†…å­˜ï¼Œé€‚åˆéƒ¨ç½²

#### 2.4.7 INT8 é‡åŒ–å®ç°ï¼ˆ2026-01-25ï¼‰

##### æŠ€æœ¯æ–¹æ¡ˆ

INT8 é‡åŒ–é‡‡ç”¨ **per-tensor symmetric quantization**ï¼ˆå¯¹ç§°é‡åŒ–ï¼‰ç­–ç•¥ï¼š

```cpp
// é‡åŒ–å…¬å¼
int8_value = round(fp32_value / scale)
scale = max(abs(min_val), abs(max_val)) / 127

// åé‡åŒ–å…¬å¼
fp32_value = int8_value * scale
```

##### å®ç°ç»†èŠ‚

1. **æƒé‡è½¬æ¢æµç¨‹**ï¼š
   - BF16 æƒé‡ â†’ F32 ä¸´æ—¶ç¼“å†² â†’ è®¡ç®—é‡åŒ–å‚æ•° â†’ INT8 æƒé‡
   - Embedding å’Œ Norm æƒé‡ä¿æŒ F32ï¼ˆç²¾åº¦æ•æ„Ÿï¼‰
   - Projection æƒé‡ï¼ˆQ/K/V/O/Gate/Up/Downï¼‰è½¬æ¢ä¸º INT8

2. **NEON ä¼˜åŒ–å†…æ ¸**ï¼ˆ`quantization.cpp`ï¼‰ï¼š
   ```cpp
   // ä¸»å¾ªç¯ï¼šæ¯æ¬¡å¤„ç† 32 ä¸ªå…ƒç´ ï¼ˆ2x16 å±•å¼€ï¼‰
   for (; k + 32 <= K; k += 32) {
       // é¢„å–æ•°æ®
       __builtin_prefetch(row + k + 64, 0, 3);
       
       // INT8 â†’ INT16 â†’ INT32 â†’ FP32 è½¬æ¢
       int8x16_t w0 = vld1q_s8(row + k);
       int16x8_t w0_lo = vmovl_s8(vget_low_s8(w0));
       float32x4_t fw00 = vcvtq_f32_s32(vmovl_s16(vget_low_s16(w0_lo)));
       
       // FMA ç´¯åŠ 
       vsum0 = vfmaq_f32(vsum0, fw00, in00);
   }
   ```

3. **ä¼˜åŒ–æŠ€æœ¯**ï¼š
   - **é¢„è®¡ç®— inputSum**ï¼šé¿å…æ¯è¡Œé‡å¤è®¡ç®— `zeroPoint * sum(input)`
   - **2x å¾ªç¯å±•å¼€**ï¼šæ¯æ¬¡å¤„ç† 32 ä¸ªå…ƒç´ 
   - **åŒç´¯åŠ å™¨**ï¼š`vsum0` å’Œ `vsum1` éšè—å»¶è¿Ÿ
   - **æ•°æ®é¢„å–**ï¼š`__builtin_prefetch` å‡å°‘ç¼“å­˜æœªå‘½ä¸­

##### æ€§èƒ½æµ‹è¯•ç»“æœï¼ˆQwen3-0.6B, Apple M1, requests=20, max_tokens=50ï¼‰

| æŒ‡æ ‡ | INT8 | FP16 | FP32 | INT8 vs FP16 | INT8 vs FP32 |
|------|------|------|------|--------------|--------------|
| **æƒé‡å†…å­˜** | 336 MB | 540 MB | 1080 MB | **-38%** | **-69%** |
| **æ€»å†…å­˜** | 930 MB | 1080 MB | 2161 MB | **-14%** | **-57%** |
| **é¡ºåºåå** | 24.78 tok/s | 22 tok/s | 17 tok/s | **+12.6%** | **+46%** |
| **å¹¶å‘åå** | 31.79 tok/s | 26 tok/s | 19 tok/s | **+22.3%** | **+67%** |
| **å“åº”æ—¶é—´** | 2.02s | 2.24s | 2.95s | **-10%** | **-31%** |

##### ç»¼åˆæ”¶ç›Š

| ç»´åº¦ | æ•ˆæœ |
|------|------|
| **å†…å­˜æ•ˆç‡** | æƒé‡å‡å°‘ 69%ï¼ˆç›¸æ¯” FP32ï¼‰ï¼Œæ€»å†…å­˜å‡å°‘ 57% |
| **è®¡ç®—æ€§èƒ½** | é¡ºåºæå‡ 46%ï¼Œå¹¶å‘æå‡ 67%ï¼ˆç›¸æ¯” FP32ï¼‰ |
| **ç²¾åº¦æŸå¤±** | å¯æ¥å—èŒƒå›´ï¼Œæ¨ç†ç»“æœè¯­ä¹‰æ­£ç¡® |
| **éƒ¨ç½²å‹å¥½** | é€‚åˆå†…å­˜å—é™çš„è¾¹ç¼˜è®¾å¤‡ |

#### 2.4.8 é…ç½®å»ºè®®

```yaml
# æ€§èƒ½ä¼˜å…ˆï¼ˆæ¨èç”Ÿäº§ç¯å¢ƒï¼‰
model:
  quantization: "int8"   # å†…å­˜å‡å°‘ 57%ï¼Œæ€§èƒ½æå‡ 50%+

# å¹³è¡¡æ¨¡å¼ï¼ˆå†…å­˜/æ€§èƒ½å‡è¡¡ï¼‰
model:
  quantization: "fp16"   # å†…å­˜å‡å°‘ 50%ï¼Œæ€§èƒ½æå‡ 30%+

# ç²¾åº¦ä¼˜å…ˆï¼ˆè°ƒè¯•/å¯¹æ¯”æµ‹è¯•ï¼‰
model:
  quantization: "fp32"   # åŸºå‡†ç²¾åº¦ï¼Œæ— é‡åŒ–æŸå¤±
```

**é‡åŒ–ç±»å‹é€‰æ‹©æŒ‡å—**ï¼š

| åœºæ™¯ | æ¨èé‡åŒ– | ç†ç”± |
|------|---------|------|
| ç”Ÿäº§éƒ¨ç½² | `int8` | æœ€ä½³æ€§èƒ½/å†…å­˜æ¯” |
| å†…å­˜å—é™ | `int8` | æœ€å°å†…å­˜å ç”¨ |
| ç²¾åº¦æ•æ„Ÿ | `fp16` | å¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½ |
| è°ƒè¯•æµ‹è¯• | `fp32` | åŸºå‡†å‚è€ƒ |

**ä½¿ç”¨å‰æ**ï¼š
- safetensors æƒé‡ dtype ä¸º BF16ï¼ˆå¸¸è§çš„ HuggingFace æ¨¡å‹é»˜è®¤æ ¼å¼ï¼‰
- INT8/FP16 å·²éªŒè¯æ¨ç†ç»“æœæ­£ç¡®æ€§ï¼ˆ20 è¯·æ±‚ Ã— 4 å¹¶å‘ï¼Œ100% æˆåŠŸç‡ï¼‰
- é€‚ç”¨äº Apple Silicon (NEON) å’Œ x86 (AVX2/AVX-512) æ¶æ„

**æ³¨æ„äº‹é¡¹**ï¼š
- INT8 çš„ Embedding å’Œ Norm æƒé‡ä¿æŒ FP32 ä»¥ç¡®ä¿ç²¾åº¦
- å½“å‰ INT8 ä½¿ç”¨ per-tensor å¯¹ç§°é‡åŒ–ï¼Œæœªæ¥å¯å‡çº§åˆ° per-channel

### 2.5 KV Cache ç®¡ç†

#### 2.5.1 KVCacheManager æ¥å£

```cpp
// include/cllm/inference/kv_cache_manager.h
namespace cllm::inference {

class KVCacheManager {
public:
    KVCacheManager(
        size_t numLayers,
        size_t numKVHeads,
        size_t headDim,
        size_t maxSeqLen,
        GGMLContext* ctx
    );
    
    // è·å–æŒ‡å®šå±‚çš„ KV Cache
    std::pair<ggml_tensor*, ggml_tensor*> getCache(size_t layerIdx);
    
    // æ›´æ–° Cacheï¼ˆè¿½åŠ æ–°çš„ K, Vï¼‰
    void updateCache(
        size_t layerIdx,
        ggml_tensor* newK,
        ggml_tensor* newV,
        size_t position
    );
    
    // æ¸…ç©º Cacheï¼ˆæ–°å¯¹è¯ï¼‰
    void clear();
    
    // è·å–å½“å‰åºåˆ—é•¿åº¦
    size_t getCurrentLength() const;
    
    // å†…å­˜ä½¿ç”¨ç»Ÿè®¡
    size_t getMemoryUsage() const;
    
private:
    std::vector<ggml_tensor*> kCaches_;
    std::vector<ggml_tensor*> vCaches_;
    size_t currentLength_ = 0;
};

} // namespace cllm::inference
```

---

## 3. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 3.1 CPU ä¼˜åŒ–ï¼ˆé»˜è®¤ï¼‰

| ä¼˜åŒ–æŠ€æœ¯ | æ¥æº | è¯´æ˜ |
|---------|------|-----|
| **AVX2/AVX-512** | GGML å†…ç½® | å‘é‡åŒ–çŸ©é˜µè¿ç®— |
| **ARM NEON** | GGML å†…ç½® | Apple Silicon / ARM ä¼˜åŒ– |
| **é‡åŒ–è®¡ç®—** | GGML å†…ç½® | Q4/Q8 ç›´æ¥è®¡ç®—ï¼Œæ— éœ€åé‡åŒ– |
| **å†…å­˜æ˜ å°„** | mmap | å¿«é€ŸåŠ è½½å¤§æ¨¡å‹ |
| **ç¼“å­˜å‹å¥½** | GGML å†…ç½® | åˆ†å—è®¡ç®—ï¼Œæé«˜ç¼“å­˜å‘½ä¸­ |
| **å¤šçº¿ç¨‹** | GGML å†…ç½® | è‡ªåŠ¨åˆ©ç”¨å¤šæ ¸ |

#### 3.1.1 CPU ç«¯è¿›ä¸€æ­¥ä¼˜åŒ–ç‚¹ï¼ˆå»ºè®®è¡¥å……å®ç°ï¼‰

**SIMD ä¸å¾®å†…æ ¸ä¼˜åŒ–**
- **å¯¹é½ä¸å¸ƒå±€**ï¼šç¡®ä¿ `head_dim` / `embedding_dim` çš„å†…å­˜å¯¹é½ï¼ˆ32/64 å­—èŠ‚ï¼‰ï¼Œé¿å…è·¨ cache line çš„åŠ è½½ï¼Œæå‡ SIMD ååã€‚
- **å‘é‡åŒ–å‹å¥½å¸ƒå±€**ï¼šå¯¹ GEMM è¾“å…¥é‡‡ç”¨ `K` ç»´è¿ç»­å¸ƒå±€ï¼Œå‡å°‘ strideï¼›å¯¹ KV cache ä¿æŒè¿ç»­å†™å…¥è·¯å¾„ã€‚
- **æ··åˆç²¾åº¦ç­–ç•¥**ï¼šEmbedding/LMHead ä½¿ç”¨ F16ï¼ŒFFN/Attention ä½¿ç”¨ Q4_K_M æˆ– Q5_K_Mï¼Œä»¥å‡å°‘å¸¦å®½å‹åŠ›ã€‚

**ç®—æ³•çº§ä¼˜åŒ–**
- **Prefill ä¸ Decode åˆ†ç¦»**ï¼šé¢„å¡«å……é˜¶æ®µä¸€æ¬¡æ„å›¾ã€æ‰¹é‡ç®—å­èåˆï¼›è§£ç é˜¶æ®µä½¿ç”¨è½»é‡å›¾æˆ–å• token è®¡ç®—è·¯å¾„ï¼Œå‡å°‘æ„å›¾å¼€é”€ã€‚
- **æ³¨æ„åŠ›ä¼˜åŒ–**ï¼šçŸ­åºåˆ—èµ°æ ‡å‡† softmaxï¼›é•¿åºåˆ—å¯ç”¨ `ggml_flash_attn_ext`ï¼Œé¿å…ä¸­é—´ `QK^T` å…¨é‡ç‰©åŒ–ã€‚
- **FFN ç»“æ„ä¼˜åŒ–**ï¼šå¯¹ SwiGLU ä¸­ `gate`/`up` çš„å¹¶è¡Œ GEMM è¿›è¡Œèåˆï¼Œå‡å°‘ä¸€æ¬¡ä¸­é—´å¼ é‡å†™å›ã€‚

**çº¿ç¨‹ä¸è°ƒåº¦ä¼˜åŒ–**
- **çº¿ç¨‹æ•°ç­–ç•¥**ï¼šé»˜è®¤å–ç‰©ç†æ ¸æ•°ï¼›å½“æ¨¡å‹è¾ƒå°æˆ– batch size å¾ˆå°ï¼Œé™åˆ¶çº¿ç¨‹æ•°é¿å…è°ƒåº¦å¼€é”€å‹è¿‡è®¡ç®—æ”¶ç›Šã€‚
- **çº¿ç¨‹äº²å’Œ**ï¼šåœ¨ NUMA/å¤šæ’æ§½ç¯å¢ƒå»ºè®®ç»‘å®šçº¿ç¨‹ä¸å†…å­˜ï¼ˆå¦‚ `numactl` æˆ–å¹³å°ç»‘å®šç­–ç•¥ï¼‰ã€‚
- **åˆ†å—å°ºå¯¸è°ƒä¼˜**ï¼šåŸºäº `L2/L3` å¤§å°é€‰æ‹© block sizeï¼Œå‡å°‘ç¼“å­˜æŠ–åŠ¨ã€‚

**å†…å­˜ä¸ç¼“å­˜ä¼˜åŒ–**
- **KV Cache åˆ†å—/åˆ†é¡µ**ï¼šæŒ‰å›ºå®š block å¤§å°ï¼ˆå¦‚ 256/512 tokenï¼‰ç®¡ç† KVï¼Œå‡å°‘ç¼“å­˜ miss ä¸ç¢ç‰‡åŒ–ã€‚
- **é‡å¤å¼ é‡å¤ç”¨**ï¼šæ„å›¾é˜¶æ®µå¤ç”¨ä¸´æ—¶å¼ é‡ï¼Œé™ä½åˆ†é…æ¬¡æ•°ä¸å†…å­˜å³°å€¼ã€‚
- **é¢„å–ä¸æµæ°´**ï¼šå…³é”®è·¯å¾„å¯æ˜¾å¼é¢„å–ä¸‹ä¸€å±‚æƒé‡æˆ– KV blockï¼Œæå‡æµæ°´çº¿æ•ˆç‡ã€‚

### 3.2 GPU ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰

| åç«¯ | æ”¯æŒå¹³å° | å¯ç”¨æ–¹å¼ |
|-----|---------|---------|
| **CUDA** | NVIDIA GPU | ç¼–è¯‘æ—¶ `-DGGML_CUDA=ON` |
| **Metal** | Apple GPU | ç¼–è¯‘æ—¶ `-DGGML_METAL=ON` |

**GPU åŠ é€Ÿæ•ˆæœ**ï¼ˆå‚è€ƒï¼‰ï¼š
- å°æ¨¡å‹ (<1B): 2-3x åŠ é€Ÿ
- ä¸­æ¨¡å‹ (1-7B): 5-10x åŠ é€Ÿ
- å¤§æ¨¡å‹ (>7B): 10-20x åŠ é€Ÿ

### 3.3 Flash Attention

```
å¯ç”¨æ¡ä»¶:
â”œâ”€ åºåˆ—é•¿åº¦ > 512ï¼ˆçŸ­åºåˆ—æ”¶ç›Šä½ï¼‰
â”œâ”€ éœ€è¦å¤„ç†é•¿ä¸Šä¸‹æ–‡
â””â”€ å†…å­˜å—é™åœºæ™¯

GGML å®ç°: ggml_flash_attn_ext()
â”œâ”€ æ”¯æŒå› æœ mask
â”œâ”€ æ”¯æŒ GQA
â”œâ”€ æ”¯æŒ ALiBi
â””â”€ CPU/GPU å‡å¯ç”¨
```

---

## 4. é…ç½®ä¸ä½¿ç”¨

### 4.1 ç¼–è¯‘é…ç½®

```cmake
# CMakeLists.txt å…³é”®é…ç½®

# GGML é€‰é¡¹
option(KYLIN_ENABLE_CUDA "Enable CUDA support" OFF)
option(KYLIN_ENABLE_METAL "Enable Metal support" OFF)
option(KYLIN_ENABLE_FLASH_ATTN "Enable Flash Attention" ON)

# é›†æˆ GGML
add_subdirectory(third_party/ggml)

# Kylin åç«¯
add_library(kylin_backend
    src/inference/ggml_context.cpp
    src/inference/gguf_loader.cpp
    src/inference/transformer_model.cpp
    src/inference/kv_cache_manager.cpp
    src/inference/kylin_backend.cpp
)
target_link_libraries(kylin_backend PRIVATE ggml)
```

### 4.2 è¿è¡Œæ—¶é…ç½®

```yaml
# config.yaml
backend:
  type: kylin  # ä½¿ç”¨ Kylin åç«¯
  
kylin:
  device: cpu           # cpu / cuda / metal / auto
  threads: 0            # 0 = è‡ªåŠ¨æ£€æµ‹
  use_mmap: true        # å†…å­˜æ˜ å°„åŠ è½½
  use_flash_attn: true  # Flash Attention
  
  # GPU é…ç½®ï¼ˆå¯é€‰ï¼‰
  gpu_layers: 0         # 0 = å…¨ CPUï¼Œ>0 = éƒ¨åˆ†å±‚åœ¨ GPU
```

### 4.3 ä½¿ç”¨ç¤ºä¾‹

```cpp
#include "cllm/inference/kylin_backend.h"

// åˆ›å»º Kylin åç«¯
KylinBackend backend;

// åŠ è½½ HF safetensors æ¨¡å‹ï¼ˆä¼˜å…ˆï¼‰
if (!backend.loadModel("/path/to/hf_model_dir")) {
    std::cerr << "Failed to load model" << std::endl;
    return -1;
}

// æ¨ç†
std::vector<int32_t> inputIds = {1, 72, 105};  // "Hi"
auto logits = backend.forward(inputIds);

// å¢é‡ç”Ÿæˆ
int32_t nextToken = backend.forwardOneToken(72, 3);
```

---

## 5. ä¸ llama.cpp åç«¯çš„å¯¹æ¯”

| æ–¹é¢ | Kylin Backend | llama.cpp Backend |
|-----|--------------|-------------------|
| **ä»£ç å¤æ‚åº¦** | ä½ï¼ˆå°è£… GGMLï¼‰ | é«˜ï¼ˆç›´æ¥ä½¿ç”¨ llama.cpp APIï¼‰ |
| **å¯å®šåˆ¶æ€§** | â­â­â­â­â­ é«˜ | â­â­â­ ä¸­ |
| **æ€§èƒ½** | â­â­â­â­ æ¥è¿‘ | â­â­â­â­â­ æœ€ä¼˜ |
| **å­¦ä¹ ä»·å€¼** | â­â­â­â­â­ é«˜ | â­â­ ä½ï¼ˆé»‘ç›’ï¼‰ |
| **ç»´æŠ¤æˆæœ¬** | ä¸­ï¼ˆéœ€è·Ÿè¿› GGMLï¼‰ | ä½ï¼ˆç¤¾åŒºç»´æŠ¤ï¼‰ |
| **æ–°åŠŸèƒ½æ”¯æŒ** | éœ€è‡ªè¡Œå®ç° | è‡ªåŠ¨è·å¾— |

**é€‰æ‹©å»ºè®®**ï¼š
- **ç”Ÿäº§ç¯å¢ƒ**ï¼šä¼˜å…ˆä½¿ç”¨ llama.cpp åç«¯
- **å­¦ä¹ ç ”ç©¶**ï¼šä½¿ç”¨ Kylin åç«¯ï¼Œå¯æ·±å…¥ç†è§£æ¨ç†åŸç†
- **å®šåˆ¶éœ€æ±‚**ï¼šä½¿ç”¨ Kylin åç«¯ï¼Œä¾¿äºä¿®æ”¹å’Œæ‰©å±•

---

## 6. å¼€å‘æŒ‡å—

### 6.1 ç›®å½•ç»“æ„

```
src/inference/
â”œâ”€â”€ ggml_context.cpp       # GGML ä¸Šä¸‹æ–‡å°è£…
â”œâ”€â”€ gguf_loader.cpp        # GGUF åŠ è½½å™¨
â”œâ”€â”€ transformer_model.cpp  # Transformer æ¨¡å‹
â”œâ”€â”€ kv_cache_manager.cpp   # KV Cache ç®¡ç†
â””â”€â”€ kylin_backend.cpp      # Kylin åç«¯ä¸»ç±»

include/cllm/inference/
â”œâ”€â”€ ggml_context.h
â”œâ”€â”€ gguf_loader.h
â”œâ”€â”€ transformer_model.h
â”œâ”€â”€ kv_cache_manager.h
â””â”€â”€ kylin_backend.h

third_party/
â””â”€â”€ ggml/                  # GGML åº“ï¼ˆgit submoduleï¼‰
```

### 6.2 ç¼–è¯‘å’Œæµ‹è¯•

```bash
# è·å– GGML
cd third_party
git clone https://github.com/ggerganov/ggml.git

# ç¼–è¯‘ï¼ˆCPUï¼‰
cd ../build
cmake .. -DKYLIN_ENABLE_CUDA=OFF
make -j$(nproc)

# ç¼–è¯‘ï¼ˆCUDAï¼‰
cmake .. -DKYLIN_ENABLE_CUDA=ON
make -j$(nproc)

# æµ‹è¯•
./bin/test_kylin_backend --model /path/to/model.gguf
```

### 6.3 è°ƒè¯•å»ºè®®

1. **æ­£ç¡®æ€§éªŒè¯**ï¼šä¸ llama.cpp åç«¯å¯¹æ¯”è¾“å‡º
2. **æ€§èƒ½åˆ†æ**ï¼šä½¿ç”¨ `perf` æˆ– `Instruments` åˆ†æçƒ­ç‚¹
3. **å†…å­˜æ£€æŸ¥**ï¼šä½¿ç”¨ `valgrind` æˆ– `AddressSanitizer`

---

## 7. å‚è€ƒèµ„æ–™

- [GGML GitHub](https://github.com/ggerganov/ggml) - GGML å¼ é‡è®¡ç®—åº“
- [GGUF è§„èŒƒ](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) - GGUF æ ¼å¼æ–‡æ¡£
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - å‚è€ƒå®ç°
- [æ¨ç†å¼•æ“æ¥å£è®¾è®¡.md](æ¨ç†å¼•æ“æ¥å£è®¾è®¡.md) - cLLM æ¥å£è§„èŒƒ

---

## 8. æ€»ç»“

**Kylin (éº’éºŸ) æ¨ç†å¼•æ“** v2.0 é‡‡ç”¨ GGML ä½œä¸ºåº•å±‚è®¡ç®—åº“ï¼Œå®ç°äº†ï¼š

âœ… **HF æ ¼å¼ä¼˜å…ˆ**ï¼šsafetensors ä½œä¸ºä¸»è·¯å¾„ï¼ŒGGUF å¯é€‰  
âœ… **é«˜æ€§èƒ½è®¡ç®—**ï¼šå¤ç”¨ GGML çš„ SIMD ä¼˜åŒ–  
âœ… **é‡åŒ–ä¼˜å…ˆ HF**ï¼šä¼˜å…ˆæ”¯æŒ FP32/FP16/BF16  
âœ… **CPU ä¼˜å…ˆ**ï¼šå¼€ç®±å³ç”¨ï¼Œæ— éœ€ GPU  
âœ… **GPU å¯é€‰**ï¼šé€šè¿‡ GGML CUDA/Metal æ”¯æŒ  
âœ… **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ˜“äºç†è§£ã€ä¿®æ”¹å’Œæ‰©å±•  
âœ… **GGUF éä¸»è·¯å¾„**ï¼šä»…ä¿æŒå¯ç”¨æ€§ï¼Œç»´æŠ¤ä¼˜å…ˆçº§è¾ƒä½  

**è®¾è®¡ç†å¿µ**ï¼š
- ç«™åœ¨å·¨äººè‚©è†€ä¸Šï¼ˆå¤ç”¨ GGMLï¼‰ï¼Œè€Œéé‡å¤é€ è½®å­
- ä¿æŒè‡ªç ”å¯æ§ï¼Œä¾¿äºæ·±å…¥å­¦ä¹ å’Œå®šåˆ¶
- ä¼˜å…ˆå®ç°æ ¸å¿ƒåŠŸèƒ½ï¼Œé€æ­¥å®Œå–„ä¼˜åŒ–
