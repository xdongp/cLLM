# 推理引擎接口设计文档

## 编程规范

本模块的编码实现遵循以下规范和约定：
- [C++编程规范.md](C++编程规范.md)：定义编码风格、命名规范等

## 0. 文档概述

### 0.1 设计目标

本文档定义了 cLLM 推理引擎的**统一接口层**，为不同的后端实现提供一致的抽象。

**核心目标**：
- 定义统一的推理接口，支持多后端切换
- 抽象底层实现细节，简化上层调用
- 支持批处理、流式生成等通用功能
- 提供清晰的配置和生命周期管理

**支持的后端**：
1. **LibTorch Backend**: 基于 PyTorch C++ API 的推理后端
2. **Kylin Backend (麒麟)**: 自研高性能推理引擎

### 0.2 架构分层

```
┌─────────────────────────────────────────────────────────┐
│                   cLLM Server                           │
│  ┌────────────┐  ┌────────────┐  ┌────────────┐       │
│  │ Scheduler  │─→│BatchManager│─→│ModelExecutor│       │
│  └────────────┘  └────────────┘  └─────┬──────┘       │
└─────────────────────────────────────────┼──────────────┘
                                          │
                    ┌─────────────────────▼──────────────────────┐
                    │    InferenceEngine (统一接口层)            │
                    ├────────────────────────────────────────────┤
                    │  - 模型配置管理                            │
                    │  - 批处理接口                              │
                    │  - 流式生成接口                            │
                    │  - 生命周期管理                            │
                    └─────────────────────┬──────────────────────┘
                                          │
              ┌───────────────────────────┴───────────────────────────┐
              │                                                       │
    ┌─────────▼──────────┐                            ┌──────────────▼─────────┐
    │  LibTorch Backend  │                            │   Kylin Backend        │
    │  (TorchScript)     │                            │   (麒麟自研引擎)       │
    ├────────────────────┤                            ├────────────────────────┤
    │ - .pt 模型加载     │                            │ - TransformerModel     │
    │ - LibTorch 推理    │                            │ - 自研算子库           │
    │ - 内置量化支持     │                            │ - SIMD 优化            │
    └────────────────────┘                            └────────────────────────┘
```

## 1. 核心接口定义

### 1.1 InferenceEngine 接口

**文件**: `include/cllm/inference/inference_engine.h`

**实现状态**: ✅ 已完成

```cpp
namespace cllm {
namespace inference {

/**
 * @brief 推理引擎统一接口
 *
 * 提供：
 * - 统一的前向推理接口
 * - 多后端支持（LibTorch、Kylin）
 * - 批处理和流式生成
 * - 配置和生命周期管理
 */
class InferenceEngine {
public:
    /**
     * @brief 构造函数
     * @param config 模型配置
     * @param modelPath 模型路径（.pt 用于 LibTorch，.bin 用于 Kylin）
     * @param useLibTorch 是否使用 LibTorch 后端（默认 false，使用 Kylin）
     */
    explicit InferenceEngine(
        const ModelConfig &config, 
        const std::string &modelPath = std::string(), 
        bool useLibTorch = false
    );

    /**
     * @brief 初始化推理引擎
     * 
     * 根据 useLibTorch 参数选择后端：
     * - LibTorch: 加载 TorchScript 模型
     * - Kylin: 加载自研引擎权重
     * 
     * @return true 成功，false 失败
     */
    bool initialize();

    /**
     * @brief 单序列前向推理
     * 
     * 输入 token id 序列，输出 logits 张量
     * 
     * @param inputIds 输入 token id 序列
     * @return [seq_len, vocab_size] logits 张量
     */
    Tensor forward(const std::vector<int> &inputIds) const;

    /**
     * @brief 批处理前向推理
     * 
     * 与 BatchInput 语义对齐，支持多请求并行处理
     * 
     * @param flatInputIds 展平后的所有 token id
     * @param requestPositions 每个请求在 flatInputIds 中的起止位置 [start, end)
     * @param batchSize 批大小（请求数）
     * @return 形状为 [total_tokens, vocab_size] 的 logits Tensor
     */
    Tensor forwardBatch(
        const std::vector<int> &flatInputIds,
        const std::vector<std::pair<size_t, size_t>> &requestPositions,
        size_t batchSize
    ) const;

    /**
     * @brief 获取模型配置
     */
    const ModelConfig &getConfig() const { return externalConfig_; }

    /**
     * @brief 获取后端类型
     */
    std::string getBackendType() const {
        return useLibTorch_ ? "LibTorch" : "Kylin";
    }

    /**
     * @brief 检查是否已初始化
     */
    bool isInitialized() const { return initialized_; }

private:
    // 外部传入的模型配置（来自 ModelExecutor）
    ModelConfig externalConfig_;

    // 内部精简配置（用于 Kylin 占位权重）
    ModelConfig internalConfig_;

    // 模型路径
    std::string modelPath_;
    
    // 后端选择标志
    bool useLibTorch_;
    
    // 初始化状态
    bool initialized_;

    // LibTorch 后端实例（可选）
    std::unique_ptr<LibTorchBackend> libtorchBackend_;

    // Kylin 后端实例（可选）
    std::unique_ptr<KylinBackend> kylinBackend_;
};

} // namespace inference
} // namespace cllm
```

### 1.2 后端接口规范

所有后端必须实现以下接口：

```cpp
namespace cllm {
namespace inference {

/**
 * @brief 后端接口基类（抽象）
 */
class IBackend {
public:
    virtual ~IBackend() = default;

    /**
     * @brief 初始化后端
     * @return true 成功，false 失败
     */
    virtual bool initialize() = 0;

    /**
     * @brief 单序列前向推理
     * @param inputIds 输入 token id 序列
     * @return [seq_len, vocab_size] logits 张量
     */
    virtual Tensor forward(const std::vector<int> &inputIds) = 0;

    /**
     * @brief 批处理前向推理
     * @param flatInputIds 展平的 token id 数组
     * @param requestPositions 每个请求的起止位置
     * @param batchSize 批大小
     * @return [total_tokens, vocab_size] logits 张量
     */
    virtual Tensor forwardBatch(
        const std::vector<int> &flatInputIds,
        const std::vector<std::pair<size_t, size_t>> &requestPositions,
        size_t batchSize
    ) = 0;

    /**
     * @brief 获取后端名称
     */
    virtual std::string getName() const = 0;

    /**
     * @brief 检查是否已初始化
     */
    virtual bool isInitialized() const = 0;

    /**
     * @brief 获取后端配置
     */
    virtual const ModelConfig &getConfig() const = 0;
};

} // namespace inference
} // namespace cllm
```

## 2. 通用数据结构

### 2.1 ModelConfig 模型配置

**文件**: `include/cllm/model/config.h`

```cpp
namespace cllm {

struct ModelConfig {
    std::string modelType;          // 模型类型（如 "Qwen3"）
    size_t vocabSize;               // 词汇表大小
    size_t hiddenSize;              // 隐藏层维度
    size_t numLayers;               // Transformer 层数
    size_t numAttentionHeads;       // 注意力头数
    size_t numKeyValueHeads;        // KV 头数（用于 GQA）
    size_t maxSeqLen;               // 最大序列长度
    size_t intermediateSize;        // FFN 中间层维度
    float rmsNormEps;               // RMSNorm epsilon
    float ropeTheta;                // RoPE theta 参数
    
    // 运行时配置
    bool useKVCache;                // 是否使用 KV Cache
    bool useMemoryCompression;      // 是否使用内存压缩
    size_t maxBatchSize;            // 最大批大小
    
    /**
     * @brief 从 JSON 文件加载配置
     */
    void loadFromJson(const std::string& jsonPath);
    
    /**
     * @brief 转换为字符串表示
     */
    std::string toString() const;
};

} // namespace cllm
```

### 2.2 Tensor 张量抽象

**文件**: `include/cllm/inference/tensor.h`

**实现状态**: ✅ 已完成（MVP 简化版）

```cpp
namespace cllm {
namespace inference {

/**
 * @brief 数据类型枚举
 */
enum class DataType {
    FP32,   // 32位浮点（MVP 阶段）
    FP16,   // 16位浮点（待支持）
    INT8,   // 8位整数（待支持）
    INT4    // 4位整数（待支持）
};

/**
 * @brief 设备类型枚举
 */
enum class Device {
    CPU,    // CPU（MVP 阶段）
    GPU     // GPU（待支持）
};

/**
 * @brief 简化版张量类
 *
 * 提供跨后端的统一张量抽象：
 * - 支持多种数据类型（FP32/FP16/INT8/INT4）
 * - 支持多种设备（CPU/GPU）
 * - Row-major 内存布局
 */
class Tensor {
public:
    /// 默认构造
    Tensor() = default;

    /// 通过形状构造
    explicit Tensor(const std::vector<size_t>& shape);

    /// 通过初始化列表构造
    Tensor(std::initializer_list<size_t> shape);

    /// 获取张量形状
    const std::vector<size_t>& shape() const;

    /// 获取维度个数
    size_t ndim() const;

    /// 获取元素总数
    size_t size() const;

    /// 获取数据指针
    float* data();
    const float* data() const;

    /// 按索引访问元素
    float& operator[](size_t index);
    const float& operator[](size_t index) const;

    /// 重新设置形状
    void resize(const std::vector<size_t>& newShape);

    /// 填充值
    void fill(float value);

private:
    std::vector<size_t> shape_;
    std::vector<float> data_;

    void allocate();
};

} // namespace inference
} // namespace cllm
```

## 3. 接口使用示例

### 3.1 创建和初始化

```cpp
#include "cllm/inference/inference_engine.h"

using namespace cllm::inference;

// 创建 LibTorch 后端引擎
ModelConfig config;
config.vocabSize = 151936;
config.hiddenSize = 1024;
config.numLayers = 28;
// ... 其他配置

InferenceEngine libtorchEngine(
    config,
    "/path/to/model.pt",
    true  // 使用 LibTorch
);

if (!libtorchEngine.initialize()) {
    std::cerr << "Failed to initialize LibTorch engine" << std::endl;
    return -1;
}

// 创建 Kylin 后端引擎
InferenceEngine kylinEngine(
    config,
    "/path/to/model.bin",
    false  // 使用 Kylin
);

if (!kylinEngine.initialize()) {
    std::cerr << "Failed to initialize Kylin engine" << std::endl;
    return -1;
}
```

### 3.2 单序列推理

```cpp
// 准备输入
std::vector<int> inputIds = {1, 72, 105, 2};  // "Hi" 的 token ids

// 执行推理
Tensor logits = engine.forward(inputIds);

// 输出形状: [seq_len, vocab_size]
std::cout << "Logits shape: [" << logits.shape()[0] 
          << ", " << logits.shape()[1] << "]" << std::endl;
```

### 3.3 批处理推理

```cpp
// 准备批处理输入
std::vector<int> flatInputIds = {
    1, 72, 105, 2,      // 请求1: "Hi"
    1, 72, 101, 108     // 请求2: "Hel"
};

std::vector<std::pair<size_t, size_t>> requestPositions = {
    {0, 4},   // 请求1: [0, 4)
    {4, 8}    // 请求2: [4, 8)
};

size_t batchSize = 2;

// 执行批处理推理
Tensor batchLogits = engine.forwardBatch(
    flatInputIds,
    requestPositions,
    batchSize
);

// 输出形状: [total_tokens, vocab_size]
std::cout << "Batch logits shape: [" << batchLogits.shape()[0] 
          << ", " << batchLogits.shape()[1] << "]" << std::endl;
```

## 4. 与 ModelExecutor 集成

### 4.1 集成代码

**文件**: `src/model/executor.cpp`

```cpp
#include "cllm/inference/inference_engine.h"

namespace cllm {

ModelExecutor::ModelExecutor(
    const std::string& modelPath,
    const std::string& quantization,
    bool enableSIMD,
    bool useLibTorch
) {
    std::cout << "[ModelExecutor] Initializing with "
              << (useLibTorch ? "LibTorch" : "Kylin") 
              << " backend" << std::endl;
    
    // 创建推理引擎
    inferenceEngine_ = std::make_unique<inference::InferenceEngine>(
        config_, 
        modelPath, 
        useLibTorch
    );
    
    if (!inferenceEngine_->initialize()) {
        throw std::runtime_error("Failed to initialize inference engine");
    }
}

FloatArray ModelExecutor::_executeModelInference(const BatchInput& input) {
    // 调用推理引擎
    inference::Tensor logitsTensor = inferenceEngine_->forwardBatch(
        input.inputIds,
        input.requestPositions,
        input.batchSize
    );
    
    // 转换为 FloatArray
    const float* src = logitsTensor.data();
    size_t outputSize = logitsTensor.size();
    FloatArray outputTensor(outputSize);
    
    for (size_t i = 0; i < outputSize; ++i) {
        outputTensor[i] = src[i];
    }
    
    return outputTensor;
}

} // namespace cllm
```

## 5. 后端选择策略

### 5.1 运行时选择

通过命令行参数或配置文件选择后端：

```bash
# 使用 LibTorch 后端
./cllm_server --model-path model.pt --use-libtorch

# 使用 Kylin 后端（默认）
./cllm_server --model-path model.bin
```

### 5.2 性能对比

| 特性 | LibTorch Backend | Kylin Backend |
|------|------------------|---------------|
| 易用性 | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ |
| 开发速度 | ⭐⭐⭐⭐⭐ | ⭐⭐ |
| CPU 性能 | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| 量化支持 | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| 可定制性 | ⭐⭐ | ⭐⭐⭐⭐⭐ |
| 内存占用 | ⭐⭐⭐ | ⭐⭐⭐⭐ |

### 5.3 推荐使用场景

**LibTorch Backend**:
- ✅ 快速原型开发
- ✅ 模型验证和调试
- ✅ 需要 GPU 加速
- ✅ 对性能要求不高的场景

**Kylin Backend**:
- ✅ 生产环境部署
- ✅ CPU 密集型推理
- ✅ 需要极致性能优化
- ✅ 定制化算子开发

## 6. 接口扩展性

### 6.1 未来扩展方向

1. **新增后端支持**:
   - ONNX Runtime Backend
   - TensorRT Backend
   - 其他推理框架

2. **高级功能**:
   - 流式生成接口
   - 动态批处理
   - 请求优先级调度
   - 模型热更新

3. **性能优化**:
   - 算子融合接口
   - 自定义内存分配器
   - 多设备协同

### 6.2 接口版本管理

接口遵循语义化版本控制（Semantic Versioning）：
- **主版本号**：不兼容的 API 修改
- **次版本号**：向下兼容的功能性新增
- **修订号**：向下兼容的问题修正

当前版本：`v1.0.0`

## 7. 参考文档

- [LibTorch后端设计.md](LibTorch后端设计.md) - LibTorch 后端实现细节
- [Kylin推理引擎设计.md](Kylin推理引擎设计.md) - Kylin 麒麟后端实现细节
- [C++编程规范.md](C++编程规范.md) - 编码规范

## 8. 总结

本文档定义了 cLLM 推理引擎的统一接口层，提供了：

✅ **清晰的抽象**：隔离上层业务逻辑与底层实现细节  
✅ **多后端支持**：LibTorch 和 Kylin 两种后端可切换  
✅ **一致的接口**：批处理、流式生成等通用功能  
✅ **易于扩展**：支持未来新增后端和功能

通过这套接口设计，我们实现了：
- **灵活性**：根据场景选择最合适的后端
- **可维护性**：接口稳定，实现独立演进
- **高性能**：支持优化后端的无缝切换
