# Kylin Backend 批处理优化分析报告

## 执行摘要

本报告分析了 cLLM Kylin 后端的批处理实现现状，发现了关键的架构问题，并提出了优化方案。

### 关键发现

1. **GPU 路径未启用**：`HFTransformerModel` 虽然初始化了 `GGMLGPUBackend`，但 GPU forward 被禁用，实际使用 CPU 计算
2. **伪批处理**：当前 `forwardBatch` 使用 OpenMP 并行调用 `forwardSingle`，是 CPU 级并行，非真正 GPU 批处理
3. **性能差距**：CPU 路径 ~25 tok/s，GPU 路径 ~51 tok/s（单请求），潜在提升 2x

---

## 1. 当前架构分析

### 1.1 调用链路

```
KylinBackend::forwardBatch
    └── hfModel_->forwardWithRequestId (OpenMP 并行)
            └── HFTransformerModel::forwardSingle
                    ├── [禁用] gpuBackend_->forward(tokenId, position)
                    └── [启用] CPU 实现：embedding → attention → ffn → lmHead
```

### 1.2 代码证据

**GPU 路径被禁用** (`transformer.cpp:748-757`)：
```cpp
// GPU forward 暂时禁用 - CPU BLAS 已经很快
// 真正的 GPU 加速需要使用 GGML 计算图，但 API 复杂
// 当前 CPU 实现约 25 tok/s，已达到目标
// if (useGPU_ && gpuBackend_ && seqLen == 1) {
//     auto logits = gpuBackend_->forward(inputIds[0], startPos);
//     ...
// }
```

**CPU 批处理** (`transformer.cpp:1328-1331`)：
```cpp
// 并行处理每个请求
#pragma omp parallel for schedule(dynamic) if(batchSize > 1)
for (size_t i = 0; i < batchSize; ++i) {
    results[i].resize(config_.vocabSize);
    forwardSingle(batchInputIds[i], requestIds[i], results[i]);
}
```

### 1.3 资源管理现状

| 组件 | 位置 | 多请求支持 |
|------|------|-----------|
| KVCachePool | CPU | ✅ 支持多槽位 |
| WorkBufferPool | CPU | ✅ 支持多槽位 |
| GGMLGPUBackend | GPU | ❌ 单份 KV Cache |

---

## 2. 性能瓶颈分析

### 2.1 当前性能基线

| 模式 | 吞吐量 | 备注 |
|------|--------|------|
| CPU 单请求 | ~25 tok/s | 使用 BLAS 优化 |
| GPU 单请求 | ~51 tok/s | Metal 加速 |
| CPU 批处理 (4请求) | ~25 tok/s/请求 | OpenMP 并行，但共享 CPU |

### 2.2 瓶颈分析

1. **CPU 路径瓶颈**：
   - 矩阵乘法受限于 CPU 带宽
   - OpenMP 线程竞争 L3 Cache
   - 多请求无法利用 GPU

2. **GPU 路径问题**：
   - 被禁用
   - 只有单份 KV Cache，不支持多请求

3. **Apple Silicon 特点**：
   - Unified Memory：CPU/GPU 共享内存，传输代价低
   - GPU 计算能力强于 CPU BLAS
   - 并发 GPU 调用受益于高带宽

---

## 3. 优化方案

### 方案 A：启用 GPU 路径（推荐）

**目标**：让每个请求使用 GPU 加速，预期性能 ~51 tok/s/请求

**改动范围**：
1. 取消 GPU forward 的注释
2. 添加 GPU KV Cache 与 CPU KVCachePool 的同步

**优点**：
- 改动最小
- 立即获得 2x 性能提升
- 保持现有架构

**缺点**：
- GPU 串行执行多请求
- KV Cache 需要 CPU↔GPU 同步

### 方案 B：GPU 多槽位 KV Cache

**目标**：GPU 原生支持多请求，消除 CPU↔GPU KV Cache 同步

**架构改动**：

```
GGMLGPUBackend
├── 原布局：k_cache[layer][MAX_SEQ_LEN][kvSize]
└── 新布局：k_cache[layer][MAX_SLOTS][MAX_SEQ_LEN][kvSize]
```

**API 改动**：
```cpp
// 原 API
std::vector<float> forward(int tokenId, int position);

// 新 API
std::vector<float> forward(int tokenId, int position, int slotId);
```

**优点**：
- GPU 原生多请求支持
- 消除 CPU↔GPU KV Cache 传输
- 架构更清晰

**缺点**：
- 改动较大
- 需要重新分配 GPU 内存

### 方案 C：真正的 GPU 批处理

**目标**：一次 GPU 调用处理多个 token

**架构改动**：

```
原计算图：input[1] → ... → output[vocabSize]
新计算图：input[batchSize] → ... → output[batchSize, vocabSize]
```

**优点**：
- 最高性能潜力
- 充分利用 GPU 并行性

**缺点**：
- 需要重构整个计算图
- GGML 对批处理支持有限
- 开发周期长

---

## 4. 推荐实施路径

### 阶段 1：启用 GPU 路径（立即可做）

**预期收益**：单请求 25 tok/s → 51 tok/s (+104%)

**步骤**：
1. 取消 `transformer.cpp:748-757` 的注释
2. 确保 `gpuBackend_` 正确初始化
3. 添加 KV Cache 长度同步

### 阶段 2：GPU 多槽位支持（中期优化）

**预期收益**：多请求场景消除 KV Cache 传输开销

**步骤**：
1. 修改 `GGMLGPUBackend::initWeights` 分配多槽位 KV Cache
2. 添加 `slotId` 参数到 `forward` API
3. 修改 `buildFullGraph` 支持槽位偏移

### 阶段 3：真正批处理（长期优化）

**预期收益**：批处理吞吐量线性提升

**步骤**：
1. 研究 GGML batch tensor 支持
2. 重构计算图支持 batch dimension
3. 修改 Attention 支持多 query

---

## 5. 阶段 1 详细实施方案

### 5.1 启用 GPU Forward

```cpp
// transformer.cpp:748-757
// 修改前：被注释
// 修改后：
if (useGPU_ && gpuBackend_ && seqLen == 1) {
    auto logits = gpuBackend_->forward(inputIds[0], startPos);
    if (!logits.empty()) {
        kvCacheLen_ = startPos + 1;
        return logits;
    }
}
```

### 5.2 KV Cache 长度同步

需要在 GPU 和 CPU KV Cache 之间同步长度信息：

```cpp
void HFTransformerModel::forwardSingle(
    const std::vector<int32_t>& inputIds,
    size_t requestId,
    std::vector<float>& logits) {
    
    KVCacheSlot* kvSlot = kvCachePool_->getOrAllocate(requestId);
    int position = kvSlot->currentLen;
    
    // GPU 路径
    if (useGPU_ && gpuBackend_ && inputIds.size() == 1) {
        // 设置 GPU 后端的 KV Cache 长度
        gpuBackend_->setKVCacheLen(position);
        
        auto gpuLogits = gpuBackend_->forward(inputIds[0], position);
        if (!gpuLogits.empty()) {
            std::copy(gpuLogits.begin(), gpuLogits.end(), logits.begin());
            kvSlot->currentLen = position + 1;
            return;
        }
    }
    
    // CPU 回退路径...
}
```

### 5.3 问题：GPU KV Cache 与 CPU KV Cache 独立

**当前状态**：
- `GGMLGPUBackend` 有自己的 GPU KV Cache
- `HFTransformerModel` 通过 `KVCachePool` 管理 CPU KV Cache
- 两者独立，不同步

**解决方案**（阶段 1 简化版）：
- GPU 路径使用全局 KV Cache（`kvCacheLen_` 变量）
- 仅支持单请求 GPU 加速
- 多请求仍用 CPU 路径

```cpp
if (useGPU_ && gpuBackend_ && inputIds.size() == 1 && 
    requestId == 0) {  // 只有 request 0 使用 GPU
    // GPU 路径
} else {
    // CPU 路径
}
```

---

## 6. 阶段 2 详细实施方案

### 6.1 GPU 多槽位 KV Cache 设计

**内存布局**：

```cpp
// ggml_backend.h
struct KVCacheGPU {
    static constexpr int MAX_SLOTS = 8;  // 最大并发请求数
    
    // [MAX_SLOTS * MAX_SEQ_LEN * kvSize]
    std::vector<ggml_tensor*> kCacheLayers;
    std::vector<ggml_tensor*> vCacheLayers;
    
    // 每个槽位的当前长度
    std::array<int, MAX_SLOTS> slotLengths{};
};
```

**偏移计算**：

```cpp
size_t getSlotOffset(int slotId, int position) const {
    const size_t slotBase = slotId * MAX_SEQ_LEN * kvSize;
    const size_t posOffset = position * kvSize;
    return (slotBase + posOffset) * sizeof(float);
}
```

### 6.2 API 改动

```cpp
// ggml_backend.h
class GGMLGPUBackend {
public:
    // 新 API
    std::vector<float> forward(int tokenId, int position, int slotId = 0);
    
    // 槽位管理
    int allocateSlot();
    void releaseSlot(int slotId);
    void resetSlot(int slotId);
    
private:
    KVCacheGPU kvCacheGPU_;
};
```

### 6.3 与 KVCachePool 集成

```cpp
void HFTransformerModel::forwardSingle(...) {
    // 获取 CPU KV Cache 槽位
    KVCacheSlot* kvSlot = kvCachePool_->getOrAllocate(requestId);
    
    // 映射到 GPU 槽位（简单取模）
    int gpuSlotId = requestId % GGMLGPUBackend::KVCacheGPU::MAX_SLOTS;
    
    // 使用 GPU 路径
    if (useGPU_ && gpuBackend_) {
        auto logits = gpuBackend_->forward(inputIds[0], kvSlot->currentLen, gpuSlotId);
        // ...
    }
}
```

---

## 7. 性能预测

| 场景 | 当前 | 阶段 1 | 阶段 2 |
|------|------|--------|--------|
| 单请求 | 25 tok/s | 51 tok/s | 51 tok/s |
| 4 并发请求（总吞吐） | ~50 tok/s | ~51 tok/s | ~150 tok/s |
| 8 并发请求（总吞吐） | ~50 tok/s | ~51 tok/s | ~200 tok/s |

注：
- 阶段 1：单请求用 GPU，多请求竞争 GPU
- 阶段 2：多槽位减少 KV Cache 传输，但 GPU 仍串行执行
- 真正批处理（阶段 3）可实现近线性扩展

---

## 8. GPU 路径问题分析

### 8.1 问题描述

在尝试启用 `HFTransformerModel::forward` 中的 GPU 路径时，发现 `GGMLGPUBackend::buildFullGraph` 中存在 Flash Attention 形状不匹配问题：

```
GGML_ASSERT(ggml_can_mul_mat(k, q)) failed
```

### 8.2 根因分析

问题出在 `buildFullGraph` 第 857-868 行的 Flash Attention 输入形状：

**当前代码（错误）**：
```cpp
// k_cache 形状: (headDim, nKVHeads, totalLen)
ggml_tensor* k_cache = ggml_new_tensor_3d(graphCtx_, GGML_TYPE_F32, headDim, nKVHeads, totalLen);

// reshape 后: (headDim, 1, nKVHeads, totalLen)
// permute (0, 2, 1, 3) 后: (headDim, nKVHeads, 1, totalLen)
ggml_tensor* k4 = ggml_permute(graphCtx_, 
    ggml_reshape_4d(graphCtx_, k_cache_upd, headDim, 1, nKVHeads, totalLen), 
    0, 2, 1, 3);
```

**ggml_flash_attn_ext 期望的形状**：
- Q: `(head_dim, n_queries, n_heads, batch_size)`
- K: `(head_dim, n_kv, n_kv_heads, batch_size)`
- V: `(head_dim, n_kv, n_kv_heads, batch_size)`

**当前 k4 形状**: `(headDim, nKVHeads, 1, totalLen)` ≠ 期望的 `(headDim, totalLen, nKVHeads, 1)`

### 8.3 修复方案

**方案 1：修改 k_cache 形状定义**
```cpp
// 修改为: (headDim, totalLen, nKVHeads)
ggml_tensor* k_cache = ggml_new_tensor_3d(graphCtx_, GGML_TYPE_F32, headDim, totalLen, nKVHeads);
```

**方案 2：修改 reshape/permute 逻辑**
```cpp
// 调整 reshape 和 permute 以匹配 flash_attn_ext 期望
ggml_tensor* k4 = ggml_permute(graphCtx_, 
    ggml_reshape_4d(graphCtx_, k_cache_upd, headDim, nKVHeads, totalLen, 1), 
    0, 2, 1, 3);
```

### 8.4 状态

- **当前状态**：GPU 路径已禁用，回退到 CPU 路径
- **CPU 性能**：~35 tok/s
- **预期 GPU 性能**：~51 tok/s（待修复后验证）

---

## 9. 结论与建议

### 立即行动（阶段 1）

1. **启用 GPU 路径**：取消注释，支持单请求 GPU 加速
2. **预期收益**：单请求性能翻倍（25→51 tok/s）
3. **风险**：低，改动小

### 中期优化（阶段 2）

1. **GPU 多槽位**：支持多请求独立 KV Cache
2. **预期收益**：减少 CPU↔GPU 同步开销
3. **开发周期**：约 2-3 天

### 长期优化（阶段 3）

1. **真正批处理**：需要重构计算图
2. **预期收益**：吞吐量线性提升
3. **开发周期**：约 2-4 周

---

## 附录：相关文件清单

| 文件 | 描述 |
|------|------|
| `src/kylin/hf/transformer.cpp` | HFTransformerModel 实现 |
| `src/kylin/hf/ggml_backend.cpp` | GGMLGPUBackend GPU 加速 |
| `include/cllm/kylin/hf/kv_cache_pool.h` | KV Cache 池管理 |
| `src/inference/kylin_backend.cpp` | KylinBackend 入口 |

---

## 9. 实施记录

### 2026-02-05 实施尝试

#### 尝试 1：启用 GPU 路径

**修改内容**：
1. 取消 `transformer.cpp:748-757` 的 GPU forward 注释
2. 修改 `kylin_backend.cpp` 单请求场景使用 `hfModel_->forward()`

**结果**：失败

**错误信息**：
```
GGML_ASSERT(ggml_can_mul_mat(k, q)) failed
```

**问题分析**：
`buildFullGraph` 中的 Flash Attention 形状不匹配：
- `k_cache` 定义为 `(headDim, nKVHeads, totalLen)`
- Flash Attention 要求 K 形状为 `(headDim, kvLen, nKVHeads, batchSize)`
- 当前 reshape 和 permute 逻辑产生的形状与要求不符

**代码位置**：`ggml_backend.cpp:857-868`

```cpp
// 问题代码
ggml_tensor* k4 = ggml_permute(graphCtx_, 
                               ggml_reshape_4d(graphCtx_, k_cache_upd, headDim, 1, nKVHeads, totalLen), 
                               0, 2, 1, 3);
// 当前结果：(headDim, nKVHeads, 1, totalLen)
// 期望结果：(headDim, totalLen, nKVHeads, 1)
```

**修复方案（待实施）**：
1. 修正 `k_cache` 形状定义为 `(headDim, totalLen, nKVHeads)`
2. 或修正 reshape/permute 逻辑

#### 回退

由于 GPU 计算图存在 bug，已回退所有更改：
- `transformer.cpp`: GPU forward 保持注释状态
- `kylin_backend.cpp`: 恢复原有的 per-request KV Cache 逻辑

### 当前状态

| 指标 | 值 |
|------|-----|
| 单请求性能 | ~35 tok/s |
| 实现方式 | CPU (FP16 + Accelerate BLAS) |
| OpenMP 线程数 | 8 |

### 下一步计划

1. **修复 GPU 计算图**（优先级高）
   - 修正 Flash Attention 的 K/V 形状
   - 验证 GPU 路径正确性

2. **CPU 批处理优化**（优先级中）
   - 优化 SIMD 向量化
   - 改进缓存局部性

---

*报告生成时间：2026-02-05*
*最后更新：2026-02-05*