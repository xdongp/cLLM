# HF æ ¼å¼æ€§èƒ½ä¼˜åŒ–æŒ‡å—

## æ¦‚è¿°

æœ¬æ–‡æ¡£é’ˆå¯¹ cLLM çš„ Kylin å¼•æ“ä¸­ HuggingFace (HF) åŸç”Ÿæ ¼å¼ï¼ˆsafetensorsï¼‰çš„ CPU/GPU æ¨ç†æ€§èƒ½ä¼˜åŒ–æä¾›ç³»ç»Ÿæ€§åˆ†æå’Œä¼˜åŒ–æ–¹æ¡ˆã€‚

**ç›®æ ‡**ï¼šåœ¨ä¸å¢åŠ å†…å­˜å¼€é”€çš„å‰æä¸‹ï¼Œå®ç° HF æ ¼å¼çš„æè‡´æ€§èƒ½ä¼˜åŒ–ã€‚

---

## 1. ç°çŠ¶åˆ†æ

### 1.1 å½“å‰æ¶æ„

```
HF Safetensors â†’ BF16 æƒé‡ â†’ è¿è¡Œæ—¶è½¬æ¢ â†’ FP32/FP16/INT8 â†’ æ¨ç†
```

### 1.2 æ€§èƒ½ç“¶é¢ˆè¯†åˆ«

#### 1.2.1 CPU æ¨ç†ç“¶é¢ˆ

| ç“¶é¢ˆç‚¹ | å½±å“ | ä¸¥é‡ç¨‹åº¦ | ä»£ç ä½ç½® |
|--------|------|----------|----------|
| **BF16â†’FP32 è½¬æ¢** | æ¯æ¬¡æ¨ç†éƒ½è¦è½¬æ¢ï¼Œè€—æ—¶ 10-20% | ğŸ”´ é«˜ | `ggml_kernels.cpp:200-220` |
| **çŸ©é˜µä¹˜æ³•æœªåˆ†å—** | ç¼“å­˜å‘½ä¸­ç‡ä½ï¼Œæœªç”¨ BLAS | ï¿½ ä¸­ | `ggml_kernels.cpp:300+` |
| **é‡å¤å†…å­˜åˆ†é…** | æ¨ç†æ—¶åŠ¨æ€åˆ†é…ç¼“å†²åŒº | ğŸŸ¡ ä¸­ | `transformer.cpp:100-125` |
| **SIMD å·²å®ç°ä½†å¯ä¼˜åŒ–** | AVX2/NEON å·²å®ç°ï¼Œä½†å¯ç”¨ BLAS æ›¿ä»£ | ï¿½ ä¸­ | `ggml_kernels.cpp:250+` |
| **OpenMP å¹¶è¡Œå·²å¯ç”¨** | å·²ä½¿ç”¨ï¼Œä½†é˜ˆå€¼è®¾ç½®å¯èƒ½ä¸åˆç† | ï¿½ ä½ | `ggml_kernels.cpp:30-35` |

#### 1.2.2 GPU æ¨ç†ç“¶é¢ˆ

| ç“¶é¢ˆç‚¹ | å½±å“ | ä¸¥é‡ç¨‹åº¦ |
|--------|------|----------|
| **CPU-GPU æ•°æ®ä¼ è¾“** | æ¯ token éƒ½è¦ä¼ è¾“ï¼Œå¸¦å®½ç“¶é¢ˆ | ğŸ”´ é«˜ |
| **è®¡ç®—å›¾é‡å»º** | æ¯æ¬¡æ¨ç†é‡å»º GGML å›¾ | ğŸ”´ é«˜ |
| **åŒæ­¥ç­‰å¾…** | CPU ç­‰å¾… GPU å®Œæˆ | ğŸŸ¡ ä¸­ |
| **å°æ‰¹é‡åˆ©ç”¨ç‡ä½** | batch=1 æ—¶ GPU åˆ©ç”¨ç‡ä¸è¶³ | ğŸŸ¡ ä¸­ |

#### 1.2.3 å†…å­˜ä½¿ç”¨é—®é¢˜

```cpp
// å½“å‰å†…å­˜åˆ†é…æ¨¡å¼ï¼ˆé—®é¢˜ç¤ºä¾‹ï¼‰
// src/kylin/hf/transformer.cpp:81-125
ropeFreqsCos_.resize(kMaxSeqLen * headDim / 2);      // é¢„åˆ†é… âœ…
kCache_.resize(kvSize, 0.0f);                        // é¢„åˆ†é… âœ…
hiddenStates_.resize(config_.hiddenSize);            // å•çº¿ç¨‹ âŒ
qkvBuffer_.resize(qSize + 2 * kvSize2);              // å•çº¿ç¨‹ âŒ
// ... æ›´å¤šå•çº¿ç¨‹ç¼“å†²åŒº
```

**é—®é¢˜**ï¼š
- å·¥ä½œç¼“å†²åŒºæ˜¯æˆå‘˜å˜é‡ï¼Œå¤šçº¿ç¨‹éœ€è¦ç«äº‰æˆ–å¤åˆ¶
- æ¯ä¸ªè¯·æ±‚éœ€è¦ç‹¬ç«‹çš„ KV Cacheï¼Œä½†é¢„åˆ†é…ç­–ç•¥ä¸å¤Ÿçµæ´»
- æƒé‡å­˜å‚¨å¤šä»½ï¼ˆBF16 + FP32/FP16/INT8ï¼‰

---

## 2. ä¼˜åŒ–ç­–ç•¥

### 2.1 æ ¸å¿ƒåŸåˆ™

1. **é›¶é¢å¤–å†…å­˜**ï¼šä¸å¢åŠ å†…å­˜å ç”¨ï¼Œé€šè¿‡ç®—æ³•ä¼˜åŒ–æå‡æ€§èƒ½
2. **å»¶è¿Ÿæœ€å°åŒ–**ï¼šå‡å°‘ CPU-GPU æ•°æ®ä¼ è¾“å’ŒåŒæ­¥
3. **è®¡ç®—å¹¶è¡ŒåŒ–**ï¼šå……åˆ†åˆ©ç”¨å¤šæ ¸ CPU å’Œ GPU å¹¶è¡Œèƒ½åŠ›
4. **å†…å­˜å±€éƒ¨æ€§**ï¼šä¼˜åŒ–æ•°æ®å¸ƒå±€ï¼Œæé«˜ç¼“å­˜å‘½ä¸­ç‡

### 2.2 CPU ä¼˜åŒ–æ–¹æ¡ˆ

#### 2.2.1 çŸ©é˜µä¹˜æ³•ä¼˜åŒ–ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰

**ç°çŠ¶åˆ†æ**ï¼š

```cpp
// å½“å‰å®ç°ï¼ˆggml_kernels.cpp:300+ï¼‰
// å·²å®ç° SIMD + OpenMPï¼Œä½†ä»æœ‰ä¼˜åŒ–ç©ºé—´ï¼š
// 1. ä½¿ç”¨ Apple Accelerate BLASï¼ˆå·²å¯ç”¨ï¼‰
// 2. ä½†æœªä½¿ç”¨åˆ†å—ï¼ˆTilingï¼‰ç­–ç•¥
// 3. çŸ©é˜µ-å‘é‡ä¹˜æ³•ä¸ºä¸»ï¼Œæœªä¼˜åŒ–çŸ©é˜µ-çŸ©é˜µä¹˜æ³•

void matmul_f32(const float* weight, const float* input,
                float* output, int M, int K) {
    // å½“å‰ï¼šè¡Œä¸»åºéå† + SIMD
    #pragma omp parallel for if(useParallel)
    for (int m = 0; m < M; ++m) {
        // AVX2/NEON ç‚¹ç§¯è®¡ç®—
        output[m] = dot_product_simd(weight + m*K, input, K);
    }
}
```

**ä¼˜åŒ–æ–¹æ¡ˆ 1: ä½¿ç”¨ BLAS åº“ï¼ˆæ¨èï¼‰**

```cpp
// åˆ©ç”¨ Apple Accelerate (vecLib) æˆ– OpenBLAS
// ggml_kernels.cpp ä¿®æ”¹ï¼š

#if USE_BLAS
void matmul_f32_blas(const float* A, const float* B, float* C, 
                     int M, int N, int K) {
    // cblas_sgemm: C = A * B
    // å¯¹äºçŸ©é˜µ-å‘é‡: cblas_sgemv
    cblas_sgemv(CblasRowMajor, CblasNoTrans, 
                M, K, 1.0f, A, K, B, 1, 0.0f, C, 1);
}
#endif
```

**ä¼˜åŒ–æ–¹æ¡ˆ 2: åˆ†å— + SIMDï¼ˆå¦‚æœä¸ç”¨ BLASï¼‰**

```cpp
// åˆ†å—çŸ©é˜µä¹˜æ³•ï¼Œæé«˜ L1/L2 ç¼“å­˜å‘½ä¸­ç‡
void matmul_blocked(const float* A, const float* B, float* C,
                    int M, int N, int K) {
    constexpr int BM = 64;  // L1 ç¼“å­˜å¯å®¹çº³
    constexpr int BN = 64;
    constexpr int BK = 256;
    
    #pragma omp parallel for collapse(2)
    for (int i0 = 0; i0 < M; i0 += BM) {
        for (int j0 = 0; j0 < N; j0 += BN) {
            // å¤„ç† BM x BN å—
            for (int k0 = 0; k0 < K; k0 += BK) {
                simd_block_mul(A, B, C, i0, j0, k0, 
                              std::min(BM, M-i0),
                              std::min(BN, N-j0),
                              std::min(BK, K-k0));
            }
        }
    }
}
```
```

**å®ç°æ­¥éª¤**ï¼š
1. ä½¿ç”¨ OpenMP å¹¶è¡ŒåŒ–å¤–å±‚å¾ªç¯
2. å®ç°åˆ†å—ï¼ˆTilingï¼‰ç­–ç•¥ï¼Œä¼˜åŒ– L1/L2 ç¼“å­˜ä½¿ç”¨
3. ä½¿ç”¨ SIMD æŒ‡ä»¤ï¼ˆAVX2/AVX-512/NEONï¼‰åŠ é€Ÿå†…å±‚è®¡ç®—
4. è€ƒè™‘ä½¿ç”¨ BLAS åº“ï¼ˆOpenBLAS/MKLï¼‰æ›¿ä»£è‡ªç ”å®ç°

#### 2.2.2 BF16 å¿«é€Ÿè½¬æ¢ä¼˜åŒ–

**ç°çŠ¶**ï¼šå·²å®ç° SIMD ä¼˜åŒ– âœ…

```cpp
// å½“å‰å®ç°ï¼ˆggml_kernels.cpp:200-220ï¼‰
// å·²å®ç° AVX2/NEON SIMD ä¼˜åŒ–

void convert_bf16_to_f32(const uint16_t* src, float* dst, size_t count) {
#if USE_AVX2
    size_t i = 0;
    for (; i + 8 <= count; i += 8) {
        __m128i bf16 = _mm_loadu_si128((const __m128i*)(src + i));
        __m256i bf16_32 = _mm256_cvtepu16_epi32(bf16);
        __m256 f32 = _mm256_castsi256_ps(_mm256_slli_epi32(bf16_32, 16));
        _mm256_storeu_ps(dst + i, f32);
    }
    // å¤„ç†å‰©ä½™...
#elif USE_NEON
    // NEON å®ç°...
#endif
}
```

**çŠ¶æ€**ï¼šâœ… **å·²å®Œæˆ** - å·²å®ç° AVX2/NEON SIMD ä¼˜åŒ–

**è¿›ä¸€æ­¥ä¼˜åŒ–å»ºè®®**ï¼š
- è€ƒè™‘ä½¿ç”¨ Intel AMX æŒ‡ä»¤ï¼ˆBF16 åŸç”Ÿæ”¯æŒï¼‰
- å¯¹äºå¤§æ‰¹é‡è½¬æ¢ï¼Œå¯ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œ

#### 2.2.3 æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–

**ç°çŠ¶**ï¼šæ ‡å‡† Attention å®ç°

```cpp
// Q @ K^T / sqrt(d_k)
// softmax
// @ V
```

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

1. **Flash Attention ç®—æ³•**
   - åˆ†å—è®¡ç®—ï¼Œå‡å°‘ HBM è®¿é—®
   - åœ¨çº¿ softmaxï¼Œé¿å…å­˜å‚¨å®Œæ•´ attention çŸ©é˜µ
   - é€‚åˆé•¿åºåˆ—ï¼ˆ>1024 tokensï¼‰

2. **GQA (Grouped Query Attention) ä¼˜åŒ–**
   - å…±äº« K/Vï¼Œå‡å°‘å†…å­˜å¸¦å®½
   - å·²éƒ¨åˆ†å®ç°ï¼Œå¯è¿›ä¸€æ­¥ä¼˜åŒ–

```cpp
// Flash Attention ä¼ªä»£ç 
void flash_attention(const float* Q, const float* K, const float* V, float* O,
                     int seq_len, int head_dim, int block_size) {
    for (int i = 0; i < seq_len; i += block_size) {
        for (int j = 0; j < seq_len; j += block_size) {
            // åŠ è½½ Q[i:i+block], K[j:j+block], V[j:j+block] åˆ° SRAM
            // è®¡ç®—å±€éƒ¨ attention
            // åœ¨çº¿ softmax æ›´æ–°
        }
    }
}
```

#### 2.2.4 å†…å­˜æ± ä¼˜åŒ–

**ç°çŠ¶**ï¼šKV Cache Pool å’Œ WorkBuffer Pool å·²å®ç°ï¼Œä½†å¯è¿›ä¸€æ­¥ä¼˜åŒ–

```cpp
// å½“å‰ï¼šæ¯ä¸ªè¯·æ±‚ç‹¬ç«‹åˆ†é…
KVCacheSlot* slot = kvCachePool_->allocate(requestId);
WorkBufferSlot* workBuf = workBufferPool_->allocate();
```

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

1. **å†…å­˜å¯¹é½**ï¼šç¡®ä¿æ‰€æœ‰ç¼“å†²åŒº 64 å­—èŠ‚å¯¹é½ï¼ˆç¼“å­˜è¡Œå¤§å°ï¼‰
2. **NUMA æ„ŸçŸ¥**ï¼šåœ¨å¤šè·¯ CPU ä¸Šï¼Œå°†å†…å­˜åˆ†é…åˆ°ä½¿ç”¨å®ƒçš„ NUMA èŠ‚ç‚¹
3. **Huge Pages**ï¼šä½¿ç”¨å¤§é¡µå‡å°‘ TLB miss

```cpp
// å¯¹é½åˆ†é…
void* aligned_malloc(size_t size, size_t alignment = 64) {
    void* ptr = nullptr;
    posix_memalign(&ptr, alignment, size);
    return ptr;
}
```

### 2.3 GPU ä¼˜åŒ–æ–¹æ¡ˆ

#### 2.3.1 è®¡ç®—å›¾ç¼“å­˜ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰

**ç°çŠ¶**ï¼šå½“å‰å®ç°ä½¿ç”¨ CPU å›é€€ï¼Œæœªå……åˆ†åˆ©ç”¨ GPU

```cpp
// å½“å‰å®ç°ï¼ˆggml_backend.cpp:380+ï¼‰
// å®é™…æ˜¯ CPU è·¯å¾„ï¼Œæƒé‡å·²ç¼“å­˜åˆ° CPU
std::vector<float> GGMLGPUBackend::forward(int tokenId, int position) {
    // 1. ä» CPU ç¼“å­˜è·å– embedding
    // 2. ä½¿ç”¨ cpuMatmul (BLAS/SIMD) è¿›è¡ŒçŸ©é˜µä¹˜æ³•
    // 3. æ‰€æœ‰è®¡ç®—åœ¨ CPU ä¸Šå®Œæˆ
}
```

**é—®é¢˜åˆ†æ**ï¼š
1. âœ… æƒé‡å·²ç¼“å­˜åˆ° CPU (`weightsCached_`)
2. âœ… ä½¿ç”¨ BLAS/SIMD ä¼˜åŒ–çš„ `cpuMatmul`
3. âŒ **æœªçœŸæ­£ä½¿ç”¨ GPU** - å½“å‰å®ç°æ˜¯ CPU è·¯å¾„
4. âŒ æ¯æ¬¡æ¨ç†éƒ½æœ‰å†…å­˜æ‹·è´å¼€é”€

**ä¼˜åŒ–æ–¹æ¡ˆ 1: çœŸæ­£çš„ GPU è®¡ç®—å›¾ï¼ˆMetal/CUDAï¼‰**

```cpp
// åˆ›å»ºæŒä¹…åŒ–çš„ GGML è®¡ç®—å›¾
class GGMLGPUBackend {
    // å½“å‰å·²æœ‰ï¼š
    ggml_backend_t backend_;      // Metal/CUDA backend
    ggml_backend_sched_t graphSched_;  // è°ƒåº¦å™¨
    
    // éœ€è¦æ·»åŠ ï¼š
    struct CachedGraph {
        ggml_cgraph* graph = nullptr;
        ggml_context* ctx = nullptr;
        int max_seq_len = 0;
        bool initialized = false;
    };
    std::vector<CachedGraph> layer_graphs_;  // æ¯å±‚ä¸€ä¸ªå›¾
    
    // é¢„æ„å»º Transformer Layer çš„è®¡ç®—å›¾
    bool buildLayerGraph(int layer_idx, int max_seq_len) {
        CachedGraph& cg = layer_graphs_[layer_idx];
        
        // åˆ›å»ºä¸Šä¸‹æ–‡
        struct ggml_init_params params = {
            .mem_size = 1024 * 1024 * 10,  // 10MB å›¾å†…å­˜
            .mem_buffer = nullptr,
            .no_alloc = true,
        };
        cg.ctx = ggml_init(params);
        
        // åˆ›å»ºè¾“å…¥å¼ é‡ï¼ˆå ä½ç¬¦ï¼‰
        ggml_tensor* input = ggml_new_tensor_1d(cg.ctx, GGML_TYPE_F32, hiddenSize);
        ggml_tensor* position = ggml_new_tensor_1d(cg.ctx, GGML_TYPE_I32, 1);
        
        // æ„å»ºè®¡ç®—å›¾ï¼šRMSNorm -> QKV Proj -> RoPE -> Attention -> FFN
        ggml_tensor* norm_out = ggml_rms_norm(cg.ctx, input, eps);
        ggml_tensor* q = ggml_mul_mat(cg.ctx, q_proj_weight, norm_out);
        ggml_tensor* k = ggml_mul_mat(cg.ctx, k_proj_weight, norm_out);
        ggml_tensor* v = ggml_mul_mat(cg.ctx, v_proj_weight, norm_out);
        
        // RoPE ä½ç½®ç¼–ç 
        q = ggml_rope(cg.ctx, q, position, head_dim, 10000.0f);
        k = ggml_rope(cg.ctx, k, position, head_dim, 10000.0f);
        
        // Attention: Q @ K^T / sqrt(d_k)
        ggml_tensor* attn_weights = ggml_soft_max(cg.ctx,
            ggml_scale(cg.ctx, ggml_mul_mat(cg.ctx, q, k), 1.0f / sqrt(head_dim)));
        
        // @ V
        ggml_tensor* attn_out = ggml_mul_mat(cg.ctx, attn_weights, v);
        
        // O Projection
        ggml_tensor* output = ggml_mul_mat(cg.ctx, o_proj_weight, attn_out);
        
        // æ„å»ºå›¾
        cg.graph = ggml_new_graph(cg.ctx);
        ggml_build_forward_expand(cg.graph, output);
        
        // åˆ†é…åç«¯ç¼“å†²åŒº
        ggml_backend_alloc_ctx_tensors(cg.ctx, backend_);
        
        cg.max_seq_len = max_seq_len;
        cg.initialized = true;
        
        return true;
    }
};
```

**ä¼˜åŒ–æ–¹æ¡ˆ 2: ä½¿ç”¨ GGML çš„å›¾è°ƒåº¦å™¨**

```cpp
// åˆ©ç”¨ GGML çš„è‡ªåŠ¨è°ƒåº¦åŠŸèƒ½
bool forwardGPU(int tokenId, int position) {
    // 1. å‡†å¤‡è¾“å…¥å¼ é‡
    ggml_tensor* input = ggml_new_tensor_1d(computeCtx_, GGML_TYPE_F32, hiddenSize);
    
    // 2. å¤åˆ¶ embedding åˆ° GPU
    const float* embed = weightsGPU_["embed_tokens"] + tokenId * hiddenSize;
    ggml_backend_tensor_set(backend_, input, embed, hiddenSize * sizeof(float));
    
    // 3. ä½¿ç”¨è°ƒåº¦å™¨è‡ªåŠ¨åˆ†é…è®¡ç®—åˆ° GPU/CPU
    ggml_backend_sched_t sched = ggml_backend_sched_new(
        backends_.data(), bufts_.data(), backends_.size(), 
        1024 * 1024 * 100  // 100MB è°ƒåº¦ç¼“å†²åŒº
    );
    
    // 4. è®¾ç½®è¯„ä¼°å›è°ƒï¼Œç›‘æ§ GPU åˆ©ç”¨ç‡
    ggml_backend_sched_set_eval_callback(sched, sched_eval_cb, &stats);
    
    // 5. æ‰§è¡Œè®¡ç®—
    ggml_backend_sched_graph_compute(sched, gf);
    
    // 6. è·å–ç»“æœ
    ggml_backend_tensor_get(backend_, output, result.data(), output_size);
}
```

**é¢„æœŸæå‡**ï¼š
- çœŸæ­£çš„ GPU åŠ é€Ÿï¼š5-10x é€Ÿåº¦æå‡ï¼ˆvs CPUï¼‰
- å›¾ç¼“å­˜ï¼šå‡å°‘ 30-50% çš„å‡†å¤‡æ—¶é—´
- æ‰¹å¤„ç†ï¼šå¤šåºåˆ—å¹¶è¡Œæ—¶æ•ˆç‡æ›´é«˜

#### 2.3.2 å¼‚æ­¥æ•°æ®ä¼ è¾“

**ç°çŠ¶**ï¼šåŒæ­¥ä¼ è¾“ï¼ŒCPU ç­‰å¾… GPU

```cpp
// å½“å‰ï¼šåŒæ­¥ä¼ è¾“
memcpy(cpu_buffer, gpu_buffer, size);  // é˜»å¡
```

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

```cpp
// æ–¹æ¡ˆï¼šåŒç¼“å†² + å¼‚æ­¥ä¼ è¾“
class AsyncTransfer {
    void* buffer_[2];  // åŒç¼“å†²
    int current_ = 0;
    
    void* get_buffer() { return buffer_[current_]; }
    
    void swap_and_transfer() {
        int next = 1 - current_;
        // å¼‚æ­¥å¯åŠ¨ä¼ è¾“åˆ° buffer_[next]
        cudaMemcpyAsync(buffer_[next], gpu_buffer, size, cudaMemcpyDeviceToHost, stream_);
        current_ = next;
    }
};
```

#### 2.3.3 æ‰¹å¤„ç†ä¼˜åŒ–

**ç°çŠ¶**ï¼šå•åºåˆ—æ¨ç†ï¼ŒGPU åˆ©ç”¨ç‡ä½

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

1. **åŠ¨æ€æ‰¹å¤„ç†**ï¼šå°†å¤šä¸ªè¯·æ±‚åˆå¹¶æˆä¸€ä¸ª batch
2. **è¿ç»­æ‰¹å¤„ç†ï¼ˆContinuous Batchingï¼‰**ï¼šæ–°è¯·æ±‚éšæ—¶åŠ å…¥å½“å‰ batch

```cpp
// è¿ç»­æ‰¹å¤„ç†
void continuous_batching() {
    while (has_active_requests()) {
        // æ”¶é›†æ‰€æœ‰éœ€è¦ forward çš„è¯·æ±‚
        std::vector<Request*> batch = collect_ready_requests(max_batch_size);
        
        // æ‰“åŒ…æˆ batch tensor
        BatchTensors tensors = pack_batch(batch);
        
        // å•æ¬¡ GPU æ¨ç†
        forward_batch(tensors);
        
        // åˆ†å‘ç»“æœ
        for (auto* req : batch) {
            req->update_state();
        }
    }
}
```

#### 2.3.4 æ··åˆç²¾åº¦æ¨ç†

**ç°çŠ¶**ï¼šFP32 æ¨ç†ï¼Œæœªåˆ©ç”¨ Tensor Core

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

```cpp
// ä½¿ç”¨ FP16/BF16 æ··åˆç²¾åº¦
// æƒé‡ï¼šFP16
// æ¿€æ´»ï¼šFP16
// ç´¯åŠ ï¼šFP32ï¼ˆé¿å…ç²¾åº¦æŸå¤±ï¼‰

#ifdef GGML_USE_METAL
// Metal è‡ªåŠ¨ä½¿ç”¨æ··åˆç²¾åº¦
#endif
```

### 2.4 å†…å­˜ä¼˜åŒ–æ–¹æ¡ˆ

#### 2.4.1 æƒé‡å…±äº«ç­–ç•¥

**ç°çŠ¶**ï¼šåŒæ—¶å­˜å‚¨ BF16ï¼ˆåŸå§‹ï¼‰+ FP32/FP16/INT8ï¼ˆè½¬æ¢åï¼‰

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

```cpp
// æ–¹æ¡ˆï¼šæŒ‰éœ€åŠ è½½ï¼Œä¸ä¿ç•™åŸå§‹ BF16
class WeightManager {
    enum class Format { BF16, FP32, FP16, INT8 };
    
    // åªä¿ç•™ä¸€ç§æ ¼å¼
    Format current_format_;
    std::vector<uint8_t> weight_data_;
    
    void convert_to(Format target) {
        if (current_format_ == target) return;
        
        // åŸåœ°è½¬æ¢ï¼Œä¸åˆ†é…æ–°å†…å­˜
        convert_inplace(weight_data_, current_format_, target);
        current_format_ = target;
    }
};
```

#### 2.4.2 å†…å­˜æ˜ å°„åŠ è½½

**ç°çŠ¶**ï¼šå°†æ•´ä¸ªæ¨¡å‹åŠ è½½åˆ°å†…å­˜

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

```cpp
// ä½¿ç”¨ mmap å»¶è¿ŸåŠ è½½
class MmappedWeights {
    int fd_;
    void* mapped_;
    size_t size_;
    
    void* get_weight(const std::string& name) {
        // æŒ‰éœ€ä»ç£ç›˜åŠ è½½ï¼ŒOS è‡ªåŠ¨ç®¡ç†ç¼“å­˜
        return (char*)mapped_ + offset_table_[name];
    }
};
```

#### 2.4.3 KV Cache å‹ç¼©

**æ–¹æ¡ˆ 1ï¼šé‡åŒ– KV Cache**

```cpp
// KV Cache INT8 é‡åŒ–
struct QuantizedKVCache {
    std::vector<int8_t> k_cache_q;
    std::vector<int8_t> v_cache_q;
    std::vector<float> k_scales;
    std::vector<float> v_scales;
    
    void quantize(const float* k, const float* v, int size) {
        // åŠ¨æ€è®¡ç®— scale
        float k_max = max_abs(k, size);
        float k_scale = k_max / 127.0f;
        
        for (int i = 0; i < size; ++i) {
            k_cache_q[i] = round(k[i] / k_scale);
        }
        k_scales.push_back(k_scale);
    }
};
```

**æ–¹æ¡ˆ 2ï¼šæ»‘åŠ¨çª—å£ Attention**

```cpp
// åªä¿ç•™æœ€è¿‘ N ä¸ª token çš„ KV
constexpr int SLIDING_WINDOW = 2048;

void sliding_window_kv_cache(float* k_cache, float* v_cache, 
                              int seq_len, int head_dim) {
    if (seq_len > SLIDING_WINDOW) {
        // ä¸¢å¼ƒæœ€æ—©çš„ token
        memmove(k_cache, k_cache + head_dim, 
                (SLIDING_WINDOW - 1) * head_dim * sizeof(float));
    }
}
```

---

## 3. å®æ–½è·¯çº¿å›¾

### Phase 1: CPU æ ¸å¿ƒä¼˜åŒ–ï¼ˆ2-3 å‘¨ï¼‰

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | é¢„æœŸæ”¶ç›Š | å·¥ä½œé‡ |
|------|--------|----------|--------|
| çŸ©é˜µä¹˜æ³• SIMD ä¼˜åŒ– | ğŸ”´ é«˜ | 3-5x åŠ é€Ÿ | 1 å‘¨ |
| BF16 SIMD è½¬æ¢ | ğŸ”´ é«˜ | 5-10x åŠ é€Ÿ | 3 å¤© |
| OpenMP å¹¶è¡ŒåŒ– | ğŸ”´ é«˜ | çº¿æ€§æ‰©å±• | 3 å¤© |
| å†…å­˜å¯¹é½ä¼˜åŒ– | ğŸŸ¡ ä¸­ | 10-20% æå‡ | 2 å¤© |

### Phase 2: GPU æ ¸å¿ƒä¼˜åŒ–ï¼ˆ2-3 å‘¨ï¼‰

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | é¢„æœŸæ”¶ç›Š | å·¥ä½œé‡ |
|------|--------|----------|--------|
| è®¡ç®—å›¾ç¼“å­˜ | ğŸ”´ é«˜ | 30-50% æå‡ | 1 å‘¨ |
| å¼‚æ­¥æ•°æ®ä¼ è¾“ | ğŸ”´ é«˜ | å‡å°‘å»¶è¿Ÿ | 3 å¤© |
| æ··åˆç²¾åº¦æ¨ç† | ğŸŸ¡ ä¸­ | 2x åå | 3 å¤© |
| æ‰¹å¤„ç†ä¼˜åŒ– | ğŸŸ¡ ä¸­ | æå‡åˆ©ç”¨ç‡ | 4 å¤© |

### Phase 3: å†…å­˜ä¼˜åŒ–ï¼ˆ1-2 å‘¨ï¼‰

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | é¢„æœŸæ”¶ç›Š | å·¥ä½œé‡ |
|------|--------|----------|--------|
| æƒé‡åŸåœ°è½¬æ¢ | ğŸŸ¡ ä¸­ | å‡å°‘ 50% å†…å­˜ | 3 å¤© |
| KV Cache é‡åŒ– | ğŸŸ¢ ä½ | å‡å°‘ 75% å†…å­˜ | 4 å¤© |
| å†…å­˜æ˜ å°„åŠ è½½ | ğŸŸ¢ ä½ | å¿«é€Ÿå¯åŠ¨ | 3 å¤© |

### Phase 4: é«˜çº§ä¼˜åŒ–ï¼ˆ2-3 å‘¨ï¼‰

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | é¢„æœŸæ”¶ç›Š | å·¥ä½œé‡ |
|------|--------|----------|--------|
| Flash Attention | ğŸ”´ é«˜ | é•¿åºåˆ— 2-4x | 2 å‘¨ |
| è¿ç»­æ‰¹å¤„ç† | ğŸŸ¡ ä¸­ | ååæå‡ | 1 å‘¨ |
| NUMA ä¼˜åŒ– | ğŸŸ¢ ä½ | å¤šè·¯ CPU ä¼˜åŒ– | 3 å¤© |

---

## 4. æ€§èƒ½åŸºå‡†

### 4.1 ç›®æ ‡æ€§èƒ½æŒ‡æ ‡

#### CPU ç›®æ ‡ï¼ˆApple M3 Proï¼‰

| æ¨¡å‹ | å½“å‰ | ç›®æ ‡ | ä¼˜åŒ–å |
|------|------|------|--------|
| Qwen3-0.6B | 20 t/s | 60 t/s | 3x |
| Qwen3-1.7B | 10 t/s | 30 t/s | 3x |
| Qwen3-7B | 3 t/s | 10 t/s | 3x |

#### GPU ç›®æ ‡ï¼ˆMetalï¼‰

| æ¨¡å‹ | å½“å‰ | ç›®æ ‡ | ä¼˜åŒ–å |
|------|------|------|--------|
| Qwen3-0.6B | 40 t/s | 100 t/s | 2.5x |
| Qwen3-1.7B | 20 t/s | 60 t/s | 3x |
| Qwen3-7B | 8 t/s | 25 t/s | 3x |

### 4.2 å†…å­˜ç›®æ ‡

| æŒ‡æ ‡ | å½“å‰ | ç›®æ ‡ |
|------|------|------|
| æƒé‡å†…å­˜ | 2x (BF16+FP32) | 1x (ä»… FP16) |
| KV Cache | FP32 | INT8 (75% å‡å°‘) |
| å·¥ä½œç¼“å†² | æ¯è¯·æ±‚ç‹¬ç«‹ | å†…å­˜æ± å¤ç”¨ |

---

## 5. ä»£ç å®ç°å»ºè®®

### 5.1 çŸ©é˜µä¹˜æ³•ä¼˜åŒ–ç¤ºä¾‹

```cpp
// include/cllm/kylin/core/optimized_matmul.h
#pragma once

#include <cstddef>

namespace cllm {
namespace kylin {

// å¹³å°æ£€æµ‹
#if defined(__AVX512F__)
    #define KYLIN_USE_AVX512
#elif defined(__AVX2__)
    #define KYLIN_USE_AVX2
#elif defined(__ARM_NEON)
    #define KYLIN_USE_NEON
#endif

// ä¼˜åŒ–çš„çŸ©é˜µä¹˜æ³•æ¥å£
void matmul_optimized(const float* A, const float* B, float* C,
                      int M, int N, int K, bool transB = false);

// BF16 å¿«é€Ÿè½¬æ¢
void convert_bf16_to_f32_fast(const uint16_t* src, float* dst, size_t count);
void convert_f32_to_bf16_fast(const float* src, uint16_t* dst, size_t count);

} // namespace kylin
} // namespace cllm
```

### 5.2 GPU è®¡ç®—å›¾ç¼“å­˜ç¤ºä¾‹

```cpp
// src/kylin/hf/ggml_backend.cpp

class GraphCache {
public:
    struct GraphKey {
        int seq_len;
        int batch_size;
        
        bool operator==(const GraphKey& other) const {
            return seq_len == other.seq_len && batch_size == other.batch_size;
        }
    };
    
    struct GraphKeyHash {
        size_t operator()(const GraphKey& k) const {
            return std::hash<int>()(k.seq_len) ^ 
                   (std::hash<int>()(k.batch_size) << 1);
        }
    };
    
    ggml_cgraph* get_or_create(const GraphKey& key, 
                                std::function<ggml_cgraph*()> creator);
    void clear();
    
private:
    std::unordered_map<GraphKey, std::unique_ptr<ggml_cgraph>, GraphKeyHash> cache_;
};
```

---

## 6. æµ‹è¯•ä¸éªŒè¯

### 6.1 æ€§èƒ½æµ‹è¯•

```bash
# åŸºå‡†æµ‹è¯•
./bin/cllm_benchmark --backend kylin --model Qwen3-1.7B --device cpu
./bin/cllm_benchmark --backend kylin --model Qwen3-1.7B --device gpu

# å¯¹æ¯”æµ‹è¯•
./bin/cllm_benchmark --backend llama_cpp --model Qwen3-1.7B --device cpu
```

### 6.2 æ­£ç¡®æ€§éªŒè¯

```cpp
// tests/test_kylin_optimized.cpp
TEST(KylinOptimized, MatmulCorrectness) {
    // å¯¹æ¯”ä¼˜åŒ–å‰åçš„ç»“æœ
    std::vector<float> A(1024*1024), B(1024*1024), C_ref(1024*1024), C_opt(1024*1024);
    
    // å¡«å……éšæœºæ•°æ®
    fill_random(A.data(), A.size());
    fill_random(B.data(), B.size());
    
    // å‚è€ƒå®ç°
    matmul_reference(A.data(), B.data(), C_ref.data(), 1024, 1024, 1024);
    
    // ä¼˜åŒ–å®ç°
    matmul_optimized(A.data(), B.data(), C_opt.data(), 1024, 1024, 1024);
    
    // éªŒè¯è¯¯å·®
    EXPECT_LT(max_relative_error(C_ref, C_opt), 1e-5);
}
```

---

## 7. æ€»ç»“

### å…³é”®ä¼˜åŒ–ç‚¹

1. **CPU æ ¸å¿ƒ**ï¼šçŸ©é˜µä¹˜æ³• SIMD + OpenMP å¹¶è¡Œ
2. **GPU æ ¸å¿ƒ**ï¼šè®¡ç®—å›¾ç¼“å­˜ + å¼‚æ­¥ä¼ è¾“
3. **å†…å­˜ä¼˜åŒ–**ï¼šåŸåœ°è½¬æ¢ + KV Cache é‡åŒ–
4. **ç®—æ³•ä¼˜åŒ–**ï¼šFlash Attention + è¿ç»­æ‰¹å¤„ç†

### é¢„æœŸæˆæœ

- **æ€§èƒ½æå‡**ï¼šCPU 3xï¼ŒGPU 2.5-3x
- **å†…å­˜ä¼˜åŒ–**ï¼šæƒé‡ 50%ï¼ŒKV Cache 75%
- **å»¶è¿Ÿé™ä½**ï¼šé¦– token æ—¶é—´å‡å°‘ 30-50%

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. ç«‹å³å¼€å§‹ Phase 1ï¼ˆCPU çŸ©é˜µä¹˜æ³•ä¼˜åŒ–ï¼‰
2. å¹¶è¡Œå‡†å¤‡ Phase 2ï¼ˆGPU è®¡ç®—å›¾ç¼“å­˜ï¼‰
3. å»ºç«‹æ€§èƒ½åŸºå‡†æµ‹è¯•æ¡†æ¶
4. æ¯å‘¨è¿›è¡Œæ€§èƒ½å›å½’æµ‹è¯•

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**ä½œè€…**: cLLM Team  
**æ—¥æœŸ**: 2026-02-05
