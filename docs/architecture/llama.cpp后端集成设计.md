# llama.cppåç«¯é›†æˆè®¾è®¡

> **çŠ¶æ€æ›´æ–°**ï¼šå½“å‰ `KylinBackend` å·²æ”¯æŒ GGUF æ ¼å¼åŠ è½½ï¼Œ`GGUFTokenizer` å·²å®ç°å®Œæ•´ BPE ç¼–ç é€»è¾‘ã€‚æœ¬æ–‡æ¡£æè¿°å¦‚ä½•æ–°å¢ç‹¬ç«‹çš„ `llama.cpp` åç«¯ï¼ˆå¯é€‰ï¼‰ï¼Œä»¥åŠ GGUF tokenizer é›†æˆæ–¹æ¡ˆã€‚

---

## 1. å½“å‰æ¶æ„çŠ¶æ€

### 1.1 ç°æœ‰åç«¯æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ cLLM ä¸»ç³»ç»Ÿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  ITokenizer  â”‚      â”‚      IBackend        â”‚       â”‚InferenceEngineâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚          â”‚                      â”‚                                 â”‚      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ GGUFTokenizer  â”‚     â”‚KylinBackend â”‚              â”‚ LibTorchBackendâ”‚ â”‚
â”‚  â”‚ (å®Œæ•´BPEå®ç°)  â”‚     â”‚ (æ”¯æŒGGUF)  â”‚              â”‚  (TorchScript) â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  KylinBackend é€šè¿‡ ModelLoaderFactory è‡ªåŠ¨æ£€æµ‹å¹¶åŠ è½½ GGUF æ ¼å¼    â”‚  â”‚
â”‚  â”‚  - ä½¿ç”¨ GGUFLoader è§£æ metadata å’Œæƒé‡                          â”‚  â”‚
â”‚  â”‚  - æ”¯æŒ Q4_K_M, Q8_0, F16, F32 ç­‰é‡åŒ–æ ¼å¼                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 å®é™…åç«¯æ¥å£å®šä¹‰ï¼ˆIBackendï¼‰

```cpp
// æ–‡ä»¶ï¼šinclude/cllm/inference/backend_interface.h
class IBackend {
public:
    virtual ~IBackend() = default;

    // åˆå§‹åŒ–åç«¯ï¼ˆåŠ è½½æ¨¡å‹æƒé‡ã€åˆå§‹åŒ–æ•°æ®ç»“æ„ï¼‰
    virtual bool initialize() = 0;

    // å•åºåˆ—å‰å‘æ¨ç†
    virtual Tensor forward(const std::vector<int> &inputIds) = 0;

    // æ‰¹å¤„ç†å‰å‘æ¨ç†
    virtual Tensor forwardBatch(
        const std::vector<int> &flatInputIds,
        const std::vector<std::pair<size_t, size_t>> &requestPositions,
        size_t batchSize
    ) = 0;

    // è·å–åç«¯åç§°
    virtual std::string getName() const = 0;

    // æ£€æŸ¥æ˜¯å¦å·²åˆå§‹åŒ–
    virtual bool isInitialized() const = 0;

    // è·å–æ¨¡å‹é…ç½®
    virtual const ModelConfig &getConfig() const = 0;
};
```

### 1.3 å½“å‰åç«¯å®ç°çŠ¶æ€

| åç«¯ | çŠ¶æ€ | æ”¯æŒæ ¼å¼ | Tokenizer |
|------|------|---------|-----------|
| **KylinBackend** | âœ… å·²å®ç° | GGUF, .bin | GGUFTokenizerï¼ˆå®Œæ•´BPEï¼‰ |
| **LibTorchBackend** | âœ… å·²å®ç° | TorchScript (.pt) | HFTokenizer |
| **LlamaCppBackend** | âŒ æœªå®ç° | GGUFï¼ˆè®¡åˆ’ï¼‰ | llama.cpp å†…ç½®æˆ– GGUFTokenizer |

### 1.4 å¯é€‰ï¼šLlamaCppBackend å®šä¹‰ï¼ˆå¦‚æœå®ç°ï¼‰

```cpp
// æ³¨æ„ï¼šå½“å‰ä»£ç ä¸­ä¸å­˜åœ¨æ­¤ç±»ï¼Œè¿™æ˜¯è®¾è®¡å»ºè®®
class LlamaCppBackend : public IBackend {
public:
    explicit LlamaCppBackend(const ModelConfig& config, const std::string& modelPath);
    ~LlamaCppBackend() override;

    // IBackend æ¥å£å®ç°
    bool initialize() override;
    Tensor forward(const std::vector<int> &inputIds) override;
    Tensor forwardBatch(...) override;
    std::string getName() const override { return "llama.cpp"; }
    bool isInitialized() const override;
    const ModelConfig &getConfig() const override;

private:
    struct llama_model* model_ = nullptr;
    struct llama_context* ctx_ = nullptr;
    std::unique_ptr<ITokenizer> tokenizer_;  // GGUFTokenizer æˆ– llama.cpp å†…ç½®
    ModelConfig config_;
    bool initialized_ = false;
};
```

### 1.5 Tokenizer æ¨¡å—æ¥å£ä¸é›†æˆçº¦æŸ

```cpp
// æ–‡ä»¶ï¼šinclude/cllm/tokenizer/i_tokenizer.h
class ITokenizer {
public:
    virtual ~ITokenizer() = default;
    virtual bool load(const std::string& path) = 0;
    virtual std::vector<int> encode(const std::string& text, bool addSpecialTokens = true) = 0;
    virtual std::string decode(const std::vector<int>& ids, bool skipSpecialTokens = true) = 0;
    virtual int getVocabSize() const = 0;
    virtual std::string idToToken(int id) const = 0;
    virtual int tokenToId(const std::string& token) const = 0;
    virtual int getBosId() const = 0;
    virtual int getEosId() const = 0;
    virtual int getPadId() const = 0;
    virtual int getUnkId() const = 0;
    virtual ModelType getModelType() const = 0;
};
```

**å¼ºåˆ¶çº¦æŸ**ï¼š
- âœ… **`*.gguf` å¿…é¡»ä½¿ç”¨ GGUF åŒæº tokenizer**ï¼ˆ`GGUFTokenizer` æˆ– `llama.cpp` å†…ç½®ï¼‰ã€‚
- âŒ **ç¦æ­¢ `HFTokenizer` ä½œä¸º GGUF ç¼–ç å™¨**ï¼ˆé¿å… vocab/merge ä¸ä¸€è‡´å¯¼è‡´ä¹±ç ï¼‰ã€‚
- âœ… **`GGUFTokenizer` å·²å®ç°å®Œæ•´ BPE ç¼–ç é€»è¾‘**ï¼ˆ`preTokenize` â†’ `bpe()` â†’ `tokenToId`ï¼‰ï¼Œä¸è§£ç å®Œå…¨å¯¹é½ã€‚

---

## 2. Tokenizer è¯¦ç»†é›†æˆæ–¹æ¡ˆ

### 2.1 å½“å‰å®ç°çŠ¶æ€

**âœ… GGUFTokenizer å®Œæ•´ BPE å®ç°**ï¼ˆå·²å®Œæˆï¼‰ï¼š
- `buildByteEncoder()` - å­—èŠ‚ç¼–ç å™¨æ˜ å°„ï¼ˆ0-255ï¼‰
- `preTokenize()` - UTF-8 é¢„åˆ†è¯ï¼ˆç©ºç™½ç¬¦åˆ†å‰²ï¼Œå¯æ‰©å±•ä¸ºæ­£åˆ™ï¼‰
- `bpe()` - BPE åˆå¹¶ç®—æ³•ï¼ˆè´ªå¿ƒç®—æ³•åº”ç”¨ merge rulesï¼‰
- `encode()` - å®Œæ•´ç¼–ç æµç¨‹ï¼ˆç‰¹æ®Š token å¤„ç† â†’ é¢„åˆ†è¯ â†’ BPE â†’ tokenToIdï¼‰
- `decode()` - ä¼˜åŒ–è§£ç ï¼ˆæ­£ç¡®å¤„ç† byte-level tokens å’Œç‰¹æ®Š tokensï¼‰

### 2.2 é€‰æ‹©ç­–ç•¥

| æ¨¡å‹æ ¼å¼ | ç¼–ç å™¨ | è§£ç å™¨ | è¯´æ˜ |
|---|---|---|---|
| **GGUF** | âœ… **GGUFTokenizer**ï¼ˆå®Œæ•´BPEï¼‰ | âœ… **GGUFTokenizer** | **å¿…é¡»åŒæºï¼Œå·²å®ç°** |
| TorchScript (.pt) | HFTokenizer | HFTokenizer | ä¿æŒåŸé€»è¾‘ |
| .bin (Kylin) | GGUFTokenizer æˆ– HFTokenizer | åŒæº | æ ¹æ®é…ç½®é€‰æ‹© |

### 2.3 æ¥å£è°ƒç”¨æµç¨‹ï¼ˆKylinBackend å®é™…å®ç°ï¼‰

```cpp
// KylinBackend é€šè¿‡ ModelLoaderFactory è‡ªåŠ¨æ£€æµ‹æ ¼å¼
// æ–‡ä»¶ï¼šsrc/inference/kylin_backend.cpp

KylinBackend::KylinBackend(const ModelConfig &config, const std::string &modelPath)
    : externalConfig_(config), modelPath_(modelPath) {
    
    if (!modelPath_.empty()) {
        // è‡ªåŠ¨æ£€æµ‹æ ¼å¼ï¼ˆGGUF/.binï¼‰
        loader_ = ModelLoaderFactory::createLoader(modelPath_, externalConfig_);
        // GGUFLoader ä¼šè‡ªåŠ¨è§£æ metadata å’Œæƒé‡
    }
}

bool KylinBackend::initialize() {
    // 1. åŠ è½½æƒé‡ï¼ˆGGUFLoader æˆ– BinLoaderï¼‰
    if (loader_) {
        loadRealWeights();  // å†…éƒ¨ä½¿ç”¨ GGUFLoader æˆ– BinLoader
    }
    
    // 2. ç»‘å®šæƒé‡åˆ° TransformerModel
    bindWeightsToModel();
    
    // æ³¨æ„ï¼šTokenizer ç”±ä¸Šå±‚ï¼ˆModelExecutor/InferenceEngineï¼‰ç®¡ç†
    // ç¡®ä¿ä½¿ç”¨ GGUFTokenizer è¿›è¡Œç¼–ç /è§£ç 
}
```

**å…³é”®ç‚¹**ï¼š
- `KylinBackend` ä¸ç›´æ¥ç®¡ç† tokenizerï¼Œç”±ä¸Šå±‚ç»Ÿä¸€ç®¡ç†
- ä¸Šå±‚å¿…é¡»ç¡®ä¿ GGUF æ¨¡å‹ä½¿ç”¨ `GGUFTokenizer`
- `GGUFTokenizer` å·²å®ç°å®Œæ•´ BPEï¼Œç¼–ç /è§£ç å®Œå…¨å¯¹é½

### 2.4 æ€§èƒ½è€ƒé‡

- **ç¼–ç /è§£ç å¼€é”€**ï¼šå¯¹äºå°ä¸Šä¸‹æ–‡ï¼Œtokenizer å æ¯”é«˜ã€‚å»ºè®®ï¼š
  - å¤ç”¨ tokenizer å®ä¾‹ï¼ˆåŒæ¨¡å‹å•ä¾‹ï¼‰
  - å¤ç”¨é¢„åˆ†è¯ç¼“å­˜ï¼ˆçŸ­ prompt å¸¸è§ï¼‰
- **åˆå¹¶è§„åˆ™æŸ¥æ‰¾**ï¼š
  - `merge_rules` ä½¿ç”¨ `unordered_map<std::string, int>`ï¼ˆå¦‚ä»¥ `"a b"` ä½œä¸º keyï¼‰å®ç° O(1) æŸ¥æ‰¾
  - è‹¥éœ€ `pair` ä½œä¸º keyï¼Œè¯·æä¾›è‡ªå®šä¹‰ hash
  - é¿å…é¢‘ç¹å­—ç¬¦ä¸²æ‹¼æ¥ï¼Œä½¿ç”¨å±€éƒ¨ buffer

### 2.5 å¼‚å¸¸å¤„ç†

- è¯»å– GGUF metadata å¤±è´¥ â†’ ç«‹å³è¿”å›é”™è¯¯å¹¶é˜»æ–­æ¨ç†
- vocab_size ä¸ä¸€è‡´ â†’ ç›´æ¥æŠ¥é”™å¹¶è®°å½•æ—¥å¿—ï¼ˆé˜²æ­¢ä¹±ç è¾“å‡ºï¼‰
- tokenizer tokens/merges ç¼ºå¤± â†’ é™çº§ä¸º `llama.cpp` å†…ç½® tokenizerï¼ˆè‹¥å¯ç”¨ï¼‰ï¼Œå¦åˆ™å¤±è´¥

### 2.6 Tokenizer å®ç°è¦ç‚¹ï¼ˆæŠ€æœ¯ç»†èŠ‚ - å·²å®ç°ï¼‰

**âœ… å·²å®ç°çš„ BPE åŠŸèƒ½**ï¼š
- **BPE/ByteLevel å¯¹é½**ï¼š`GGUFTokenizer` å·²å®ç°å®Œæ•´ BPE ç®—æ³•ï¼Œä¸ llama.cpp å¯¹é½
  - `buildByteEncoder()` - å­—èŠ‚ç¼–ç å™¨ï¼ˆ0-255 æ˜ å°„ï¼‰
  - `preTokenize()` - UTF-8 é¢„åˆ†è¯ï¼ˆå½“å‰ä¸ºç©ºç™½ç¬¦åˆ†å‰²ï¼Œå¯æ‰©å±•ä¸º GPT-2 æ­£åˆ™ï¼‰
  - `bpe()` - è´ªå¿ƒ BPE åˆå¹¶ï¼ˆåº”ç”¨ merge rulesï¼Œé€‰æ‹©ä¼˜å…ˆçº§æœ€é«˜çš„ pairï¼‰
  - `encode()` - å®Œæ•´æµç¨‹ï¼šç‰¹æ®Š token â†’ é¢„åˆ†è¯ â†’ BPE â†’ tokenToId
  - `decode()` - æ­£ç¡®å¤„ç† byte-level tokens å’Œç‰¹æ®Š tokens

**âš ï¸ å¾…å®Œå–„çš„åŠŸèƒ½**ï¼š
- **æ­£åˆ™é¢„åˆ†è¯**ï¼šå½“å‰ `preTokenize()` ä½¿ç”¨ç®€å•ç©ºç™½ç¬¦åˆ†å‰²ï¼Œå¯æ‰©å±•ä¸º GPT-2/Qwen é£æ ¼çš„æ­£åˆ™è¡¨è¾¾å¼
- **token_type æ”¯æŒ**ï¼šè‹¥ GGUF æä¾› `tokenizer.ggml.token_type`ï¼Œéœ€æ­£ç¡®å¤„ç†æ§åˆ¶ç±» token
- **added_tokens**ï¼šè‹¥å­˜åœ¨ `tokenizer.ggml.added_tokens`ï¼Œéœ€ç¡®ä¿ token id å¯¹é½
- **çº¿ç¨‹å®‰å…¨**ï¼šå½“å‰å®ç°å¯å¤ç”¨ï¼Œä½†è‹¥æ·»åŠ ç¼“å­˜éœ€è€ƒè™‘çº¿ç¨‹å®‰å…¨

**ç‰¹æ®Š token å¤„ç†**ï¼ˆå·²å®ç°ï¼‰ï¼š
- encodeï¼šé‡åˆ°ç‰¹æ®Š token å­—ç¬¦ä¸²ï¼ˆå¦‚ `<|...|>`ï¼‰ç›´æ¥æ˜ å°„ä¸º token idï¼Œä¸å‚ä¸ BPE åˆå¹¶
- decodeï¼šæ ¹æ® `skipSpecialTokens` è·³è¿‡æˆ–åŸæ ·è¾“å‡º

---

## 3. GGUF æ ¼å¼åŠ è½½æµç¨‹ï¼ˆå½“å‰å®ç°ï¼‰

### 3.1 KylinBackend çš„ GGUF åŠ è½½æµç¨‹

```
KylinBackend::initialize()
  â”œâ”€ ModelLoaderFactory::createLoader(modelPath)  // è‡ªåŠ¨æ£€æµ‹æ ¼å¼
  â”œâ”€ GGUFLoader::load()                           // è§£æ GGUF æ–‡ä»¶
  â”‚   â”œâ”€ è¯»å– metadata (tokenizer.ggml.* / rope / vocab_size)
  â”‚   â”œâ”€ åŠ è½½æƒé‡å¼ é‡ï¼ˆæ”¯æŒ Q4_K_M, Q8_0, F16, F32 ç­‰ï¼‰
  â”‚   â””â”€ è§£ææ¨¡å‹é…ç½®ï¼ˆhidden_size, num_layers, num_heads ç­‰ï¼‰
  â”œâ”€ loadRealWeights()                            // ä» GGUFLoader æå–æƒé‡
  â”œâ”€ bindWeightsToModel()                         // ç»‘å®šåˆ° TransformerModel
  â””â”€ éªŒè¯é…ç½®ä¸€è‡´æ€§ï¼ˆvocab_size, hidden_size ç­‰ï¼‰
```

### 3.2 æ—¶åºå›¾ï¼ˆKylinBackend åŠ è½½ GGUFï¼‰

```
Client -> InferenceEngine: initialize(config, modelPath)
InferenceEngine -> KylinBackend: create(config, modelPath)
KylinBackend -> ModelLoaderFactory: createLoader(modelPath)
ModelLoaderFactory -> GGUFLoader: create
KylinBackend -> KylinBackend: initialize()
KylinBackend -> GGUFLoader: load()
GGUFLoader --> KylinBackend: metadata + weights
KylinBackend -> KylinBackend: loadRealWeights()
KylinBackend -> KylinBackend: bindWeightsToModel()
KylinBackend --> InferenceEngine: ready

// Tokenizer ç”±ä¸Šå±‚ç®¡ç†
InferenceEngine -> GGUFTokenizer: load(modelPath)
GGUFTokenizer -> GGUFLoader: loadVocabulary() + loadMergeRules()
GGUFTokenizer -> GGUFTokenizer: initializeEncoding()  // æ„å»º BPE ranks
GGUFTokenizer --> InferenceEngine: ready
```

### 3.3 å¯é€‰ï¼šLlamaCppBackend çš„ GGUF åŠ è½½æµç¨‹ï¼ˆå¦‚æœå®ç°ï¼‰

```
LlamaCppBackend::initialize()
  â”œâ”€ llama_model_load_from_file(gguf_path, params)
  â”œâ”€ llama_new_context_with_model(model, ctxParams)
  â”œâ”€ GGUFTokenizer::load(gguf_path)  // æˆ–ä½¿ç”¨ llama.cpp å†…ç½® tokenizer
  â”œâ”€ æ ¡éªŒ vocab_size (llama_n_vocab vs tokenizer->getVocabSize())
  â””â”€ é¢„çƒ­æ¨ç†ï¼ˆå¯é€‰ï¼‰
```

### 3.4 GGUF metadata æ ¡éªŒæ¸…å•

- `tokenizer.ggml.model`ï¼šç¡®å®š tokenizer ç±»å‹ï¼ˆ`llama/replit/gpt2`ï¼‰
- `tokenizer.ggml.tokens`ï¼šå¿…é¡»å­˜åœ¨å¹¶ä¸ vocab_size ä¸€è‡´
- `tokenizer.ggml.merges`ï¼šè‹¥ `model=gpt2` å¿…é¡»å­˜åœ¨
- `tokenizer.ggml.token_type`ï¼šè‹¥å­˜åœ¨éœ€ä¸ tokens ç­‰é•¿
- `tokenizer.ggml.added_tokens`ï¼šå­˜åœ¨æ—¶éœ€åˆå¹¶åˆ°è¯è¡¨è§†å›¾
- `tokenizer.ggml.*_token_id`ï¼šç‰¹æ®Š token å¿…é¡»åˆæ³•ä¸”å°äº vocab_size

---

## 4. åç«¯æ¶æ„å®Œæ•´æ€§ä¸ä¸€è‡´æ€§

### 4.1 ç»„ä»¶äº¤äº’ä¸€è‡´æ€§ï¼ˆå®é™…å®ç°ï¼‰

- âœ… `InferenceEngine` åªé¢å‘ `IBackend`ï¼Œä¸æ„ŸçŸ¥åç«¯å®ç°å·®å¼‚
- âœ… `KylinBackend` å’Œ `LibTorchBackend` éƒ½å®ç° `IBackend` æ¥å£
- âœ… Tokenizer ç”± `InferenceEngine` æˆ–ä¸Šå±‚ç»Ÿä¸€ç®¡ç†ï¼Œç¡®ä¿ç¼–ç /è§£ç ä¸€è‡´
- âš ï¸ **å…³é”®**ï¼šGGUF æ¨¡å‹å¿…é¡»ä½¿ç”¨ `GGUFTokenizer`ï¼Œç¦æ­¢ä½¿ç”¨ `HFTokenizer`

### 4.2 çŠ¶æ€ç®¡ç†ï¼ˆå®é™…å®ç°ï¼‰

**å½“å‰å®ç°**ï¼ˆç®€åŒ–çŠ¶æ€æœºï¼‰ï¼š
```
[CREATED] (æ„é€ å‡½æ•°)
   â”‚ initialize()
   â–¼
[INITIALIZED] (initialized_ = true)
   â”‚ forward() / forwardBatch()
   â–¼
[READY] (å¯é‡å¤è°ƒç”¨ forward)
```

**å…³é”®çº¦æŸ**ï¼š
- `forward()` åªèƒ½åœ¨ `initialized_ = true` æ—¶æ‰§è¡Œ
- åˆå§‹åŒ–å¤±è´¥æ—¶ `initialized_ = false`ï¼Œåç»­è°ƒç”¨ä¼šæŠ›å‡ºå¼‚å¸¸
- èµ„æºé‡Šæ”¾ç”±ææ„å‡½æ•°è‡ªåŠ¨å¤„ç†ï¼ˆRAIIï¼‰

**æ³¨æ„**ï¼šå½“å‰å®ç°æ²¡æœ‰æ˜¾å¼çš„ `release()` æ–¹æ³•ï¼Œä½¿ç”¨ RAII è‡ªåŠ¨ç®¡ç†èµ„æºã€‚

---

## 5. æ€§èƒ½æŒ‡æ ‡ä¸èµ„æºå ç”¨è¯„ä¼°

### 5.1 æ€§èƒ½æŒ‡æ ‡ï¼ˆå»ºè®®é‡‡é›†ï¼‰

| æŒ‡æ ‡ | è¯´æ˜ | é‡‡æ ·ä½ç½® |
|---|---|---|
| ç¼–ç è€—æ—¶ | tokenizer encode æ—¶é—´ | `encode()` å‰å |
| è§£ç è€—æ—¶ | tokenizer decode æ—¶é—´ | `decode()` å‰å |
| é¦– token å»¶è¿Ÿ | prompt -> first token | `run()` å†… |
| token/s | å¹³å‡ç”Ÿæˆé€Ÿç‡ | `run()` ç»Ÿè®¡ |
| å³°å€¼å†…å­˜ | æ¨¡å‹+KV+ä¸´æ—¶ buffer | ç³»ç»Ÿç›‘æ§ |

### 5.2 èµ„æºå ç”¨ç²—ä¼°ï¼ˆQwen3-0.6Bï¼‰

| é‡åŒ– | æ¨¡å‹å¤§å° | å…¸å‹å†…å­˜å ç”¨ | å¤‡æ³¨ |
|---|---|---|---|
| F16 | ~1.2GB | 1.5~2.0GB | ç²¾åº¦é«˜ã€æ…¢ |
| Q4_K_M | ~0.5GB | 0.8~1.2GB | æ¨èé»˜è®¤ |

> å®é™…å–å†³äº `n_ctx`ã€KV cache å¤§å°ä¸ batchã€‚

### 5.3 è¯„ä¼°æ–¹æ³•ä¸é‡‡æ ·å»ºè®®

- **æ¨¡å‹åŠ è½½æ—¶é—´**ï¼šä» `load()` å¼€å§‹åˆ° `prepare()` å®Œæˆçš„æ—¶é•¿
- **æ¨ç†æ€§èƒ½**ï¼š
  - `first_token_latency`ï¼ˆé¦– token å»¶è¿Ÿï¼‰
  - `tokens_per_second`ï¼ˆå¹³å‡ç”Ÿæˆé€Ÿç‡ï¼‰
- **å†…å­˜å ç”¨**ï¼šå»ºè®®è®°å½•æ¨¡å‹åŠ è½½åä¸æ¨ç†å³°å€¼ä¸¤é˜¶æ®µ
- **CPU/GPU ä½¿ç”¨ç‡**ï¼šç”¨äºè¯„ä¼° `n_threads` æˆ– GPU offload å‚æ•°çš„åˆç†æ€§

---

## 6. é”™è¯¯å¤„ç†ä¸æ—¥å¿—ç³»ç»Ÿè®¾è®¡

### 6.1 é”™è¯¯åˆ†ç±»

| åˆ†ç±» | å…¸å‹é”™è¯¯ | å¤„ç†æ–¹å¼ |
|---|---|---|
| å‚æ•°é”™è¯¯ | modelPath ä¸ºç©º / ä¸å­˜åœ¨ | è¿”å›é”™è¯¯ç  + æ—¥å¿— |
| æ¨¡å‹åŠ è½½å¤±è´¥ | gguf è§£æå¤±è´¥ | ç»ˆæ­¢åˆå§‹åŒ– |
| tokenizer ä¸ä¸€è‡´ | vocab_size mismatch | ç»ˆæ­¢æ¨ç† |
| è¿è¡ŒæœŸé”™è¯¯ | llama_eval å¤±è´¥ | è¿”å›é”™è¯¯ç  |

### 6.2 æ—¥å¿—è§„èŒƒ

- **INFO**ï¼šæ¨¡å‹åŠ è½½æˆåŠŸã€tokenizer ç»‘å®šæˆåŠŸã€å…³é”®é…ç½®æ‰“å°
- **WARN**ï¼šå¯å›é€€çš„é—®é¢˜ï¼ˆä¾‹å¦‚ merges ç¼ºå¤±ä½†å¯ fallbackï¼‰
- **ERROR**ï¼šä¸å¯æ¢å¤é”™è¯¯ï¼ˆvocab mismatch / model load failï¼‰

ç¤ºä¾‹ï¼š

```
[INFO] LlamaCppBackend: model loaded, vocab=151936
[INFO] Tokenizer: gguf tokens=151936 merges=XXXX
[ERROR] Tokenizer: vocab mismatch, tokenizer=151669 model=151936
```

### 6.3 é”™è¯¯ç ä¸å¼‚å¸¸ç­–ç•¥ï¼ˆå®é™…å®ç°ï¼‰

- **åˆå§‹åŒ–å¤±è´¥**ï¼ˆ`initialize()`ï¼‰ï¼šè¿”å› `false` å¹¶è®°å½• `ERROR` æ—¥å¿—
- **è¿è¡ŒæœŸå¤±è´¥**ï¼ˆ`forward()`ï¼‰ï¼šæŠ›å‡º `std::runtime_error` å¼‚å¸¸
- **èµ„æºç®¡ç†**ï¼šä½¿ç”¨ RAIIï¼Œææ„å‡½æ•°è‡ªåŠ¨é‡Šæ”¾èµ„æºï¼ˆæ— éœ€æ˜¾å¼ `release()`ï¼‰

### 6.4 å…³é”®é”™è¯¯åœºæ™¯

| é”™è¯¯åœºæ™¯ | å½“å‰å¤„ç† | å»ºè®®æ”¹è¿› |
|---------|---------|---------|
| GGUF æ¨¡å‹ä½¿ç”¨ HFTokenizer | âš ï¸ å¯èƒ½å‘ç”Ÿï¼ˆTokenizerManager æœªæ£€æµ‹ï¼‰ | **å¿…é¡»ä¿®å¤**ï¼šæ·»åŠ  GGUF æ£€æµ‹ |
| vocab_size ä¸ä¸€è‡´ | âš ï¸ å¯èƒ½æœªæ ¡éªŒ | åœ¨ `ModelExecutor` æˆ– `InferenceEngine` ä¸­æ·»åŠ æ ¡éªŒ |
| GGUFTokenizer åŠ è½½å¤±è´¥ | æŠ›å‡ºå¼‚å¸¸ | âœ… å·²å¤„ç† |
| merge rules ç¼ºå¤± | è­¦å‘Šä½†ç»§ç»­ | âš ï¸ å¯èƒ½å¯¼è‡´ç¼–ç é”™è¯¯ï¼Œå»ºè®®å¤±è´¥ |

---

## 7. å…³é”®æµç¨‹ä¸ç¤ºä¾‹ä»£ç 

### 7.1 KylinBackend çš„ GGUF åŠ è½½ï¼ˆå®é™…å®ç°ï¼‰

```cpp
// æ–‡ä»¶ï¼šsrc/inference/kylin_backend.cpp

bool KylinBackend::initialize() {
    // 1. åˆ›å»ºæ¨¡å‹åŠ è½½å™¨ï¼ˆè‡ªåŠ¨æ£€æµ‹æ ¼å¼ï¼‰
    if (!modelPath_.empty()) {
        loader_ = ModelLoaderFactory::createLoader(modelPath_, externalConfig_);
        // å¯¹äº .gguf æ–‡ä»¶ï¼Œä¼šåˆ›å»º GGUFLoader
    }

    // 2. åŠ è½½æƒé‡
    if (loader_) {
        if (!loadRealWeights()) {
            return false;
        }
    } else {
        // å ä½æƒé‡æ¨¡å¼
        allocatePlaceholderWeights();
    }

    // 3. ç»‘å®šæƒé‡åˆ° TransformerModel
    bindWeightsToModel();

    initialized_ = true;
    return true;
}

// loadRealWeights() å†…éƒ¨ä¼šè°ƒç”¨ GGUFLoader
void KylinBackend::loadRealWeights() {
    // GGUFLoader è‡ªåŠ¨è§£æ metadata å’Œæƒé‡
    // æ”¯æŒ Q4_K_M, Q8_0, F16, F32 ç­‰é‡åŒ–æ ¼å¼
    loader_->loadWeights(...);
}
```

### 7.2 GGUFTokenizer ä½¿ç”¨ç¤ºä¾‹ï¼ˆå®é™…å®ç°ï¼‰

```cpp
// æ–‡ä»¶ï¼šsrc/tokenizer/gguf_tokenizer.cpp

// ç¼–ç æµç¨‹ï¼ˆå®Œæ•´ BPEï¼‰
std::vector<int> GGUFTokenizer::encode(const std::string& text, bool addSpecialTokens) {
    // 1. å¤„ç†ç‰¹æ®Š tokens
    // 2. é¢„åˆ†è¯ï¼špreTokenize(text) -> words
    // 3. å¯¹æ¯ä¸ª word åº”ç”¨ BPEï¼šbpe(word) -> tokens
    // 4. tokenToId(tokens) -> tokenIds
    // 5. æ·»åŠ  BOS/EOSï¼ˆå¦‚éœ€è¦ï¼‰
    return tokenIds;
}

// è§£ç æµç¨‹
std::string GGUFTokenizer::decode(const std::vector<int>& ids, bool skipSpecialTokens) {
    // 1. idToToken(ids) -> tokens
    // 2. è·³è¿‡ç‰¹æ®Š tokensï¼ˆå¦‚éœ€è¦ï¼‰
    // 3. æ‹¼æ¥ tokens -> text
    return text;
}
```

### 7.3 å¯é€‰ï¼šLlamaCppBackend å®ç°ç¤ºä¾‹ï¼ˆå¦‚æœå®ç°ï¼‰

```cpp
// æ³¨æ„ï¼šè¿™æ˜¯è®¾è®¡å»ºè®®ï¼Œå½“å‰ä»£ç ä¸­ä¸å­˜åœ¨

bool LlamaCppBackend::initialize() {
    // 1. åŠ è½½æ¨¡å‹
    llama_model_params mparams = llama_model_default_params();
    mparams.n_gpu_layers = 0;  // CPU only
    model_ = llama_load_model_from_file(config_.modelPath.c_str(), mparams);
    if (!model_) {
        return false;
    }

    // 2. åˆ›å»ºä¸Šä¸‹æ–‡
    llama_context_params cparams = llama_context_default_params();
    cparams.n_ctx = config_.maxSequenceLength;
    ctx_ = llama_new_context_with_model(model_, cparams);
    if (!ctx_) {
        return false;
    }

    // 3. åŠ è½½ tokenizerï¼ˆå¿…é¡»ä½¿ç”¨ GGUFTokenizerï¼‰
    tokenizer_ = std::make_unique<GGUFTokenizer>();
    if (!tokenizer_->load(config_.modelPath)) {
        return false;
    }

    // 4. æ ¡éªŒ vocab_size
    size_t modelVocab = llama_n_vocab(model_);
    if (tokenizer_->getVocabSize() != static_cast<int>(modelVocab)) {
        CLLM_ERROR("vocab mismatch: tokenizer=%d model=%zu",
                   tokenizer_->getVocabSize(), modelVocab);
        return false;
    }

    initialized_ = true;
    return true;
}

Tensor LlamaCppBackend::forward(const std::vector<int> &inputIds) {
    // 1. è½¬æ¢ä¸º llama_token
    std::vector<llama_token> tokens(inputIds.begin(), inputIds.end());

    // 2. åˆ›å»º batch
    llama_batch batch = llama_batch_init(tokens.size(), 0, 1);
    for (size_t i = 0; i < tokens.size(); ++i) {
        batch.token[i] = tokens[i];
        batch.pos[i] = i;
        batch.seq_id[i] = 0;
        batch.logits[i] = (i == tokens.size() - 1);  // åªè®¡ç®—æœ€åä¸€ä¸ªä½ç½®çš„ logits
    }
    batch.n_tokens = tokens.size();

    // 3. æ¨ç†
    if (llama_decode(ctx_, batch) != 0) {
        llama_batch_free(batch);
        throw std::runtime_error("llama_decode failed");
    }

    // 4. æå– logits
    size_t vocabSize = llama_n_vocab(model_);
    Tensor logits({1, vocabSize});
    float* logitsPtr = llama_get_logits(ctx_);
    std::memcpy(logits.data(), logitsPtr, vocabSize * sizeof(float));

    llama_batch_free(batch);
    return logits;
}
```

---

## 8. è§£å†³å½“å‰ GGUF æµ‹è¯•å¤±è´¥é—®é¢˜ï¼ˆå·²å®Œæˆé¡¹ï¼‰

### 8.1 å·²å®Œæˆçš„ä¿®å¤

1. âœ… **GGUFTokenizer å®Œæ•´ BPE å®ç°**ï¼š
   - å®ç°äº† `buildByteEncoder()`, `preTokenize()`, `bpe()`, `encode()`, `decode()`
   - ç¼–ç å’Œè§£ç éƒ½ä½¿ç”¨ GGUF tokenizerï¼Œç¡®ä¿ token ID ä¸€è‡´

2. âœ… **KylinBackend æ”¯æŒ GGUF**ï¼š
   - é€šè¿‡ `ModelLoaderFactory` è‡ªåŠ¨æ£€æµ‹å¹¶åŠ è½½ GGUF æ ¼å¼
   - æ”¯æŒå¤šç§é‡åŒ–æ ¼å¼ï¼ˆQ4_K_M, Q8_0, F16, F32ï¼‰

3. âœ… **å¼ºåˆ¶ GGUF ä½¿ç”¨åŒæº tokenizer**ï¼š
   - `GGUFTokenizer` ä» GGUF metadata åŠ è½½ tokens å’Œ merges
   - ç¼–ç å’Œè§£ç ä½¿ç”¨ç›¸åŒçš„ BPE ç®—æ³•

4. âš ï¸ **å¾…ä¿®å¤ï¼ˆå…³é”®ï¼‰**ï¼š
   - **TokenizerManager ç¼ºå°‘ GGUF æ£€æµ‹**ï¼šå½“å‰ `TokenizerManager` åªæ£€æŸ¥ `tokenizer.json` å’Œ `tokenizer.model`ï¼Œ**æ²¡æœ‰æ£€æµ‹ `.gguf` æ–‡ä»¶**
   - **é£é™©**ï¼šGGUF æ¨¡å‹å¯èƒ½é”™è¯¯åœ°ä½¿ç”¨ `HFTokenizer`ï¼Œå¯¼è‡´ç¼–ç /è§£ç ä¸ä¸€è‡´
   - **ä¿®å¤ä½ç½®**ï¼š`src/tokenizer/manager.cpp` çš„ `TokenizerManager` æ„é€ å‡½æ•°
   - **ä¿®å¤æ–¹æ¡ˆ**ï¼šæ·»åŠ  `.gguf` æ–‡ä»¶æ£€æµ‹ï¼Œè‡ªåŠ¨ä½¿ç”¨ `GGUFTokenizer`
   - vocab size æ ¡éªŒï¼ˆéœ€è¦åœ¨åŠ è½½æ—¶éªŒè¯ï¼‰

### 8.2 å¾…å®Œæˆé¡¹ï¼ˆå…³é”®ä¿®å¤ï¼‰

1. **âš ï¸ å…³é”®ï¼šä¿®å¤ TokenizerManager çš„è‡ªåŠ¨é€‰æ‹©é€»è¾‘**ï¼š
   - **é—®é¢˜**ï¼š`TokenizerManager` å½“å‰åªæ£€æŸ¥ `tokenizer.json` å’Œ `tokenizer.model`ï¼Œ**æ²¡æœ‰æ£€æŸ¥ GGUF æ ¼å¼**
   - **é£é™©**ï¼šGGUF æ¨¡å‹å¯èƒ½é”™è¯¯åœ°ä½¿ç”¨ `HFTokenizer` æˆ– `NativeTokenizer`ï¼Œå¯¼è‡´ç¼–ç /è§£ç ä¸ä¸€è‡´
   - **ä¿®å¤æ–¹æ¡ˆ**ï¼š
     ```cpp
     // src/tokenizer/manager.cpp
     // åœ¨ TokenizerManager æ„é€ å‡½æ•°ä¸­æ·»åŠ  GGUF æ£€æµ‹
     if (isGgufFile(modelPath)) {
         CLLM_INFO("âœ… Detected GGUF format, using GGUFTokenizer");
         tokenizer_ = new GGUFTokenizer();
     } else if (hasTokenizerJson(modelPath)) {
         // ... ç°æœ‰é€»è¾‘
     }
     ```

2. **ä»£ç æ£€æŸ¥**ï¼šç¡®ä¿æ‰€æœ‰ GGUF æ¨¡å‹åŠ è½½è·¯å¾„éƒ½ä½¿ç”¨ `GGUFTokenizer`
3. **æµ‹è¯•éªŒè¯**ï¼šè¿è¡Œ `test_hello_inference` éªŒè¯ç¼–ç /è§£ç ä¸€è‡´æ€§
4. **æ—¥å¿—å¢å¼º**ï¼šè¾“å‡º tokenizer/gguf vocab ä¸ merges/tokens ç»Ÿè®¡

---

## 9. é…ç½®å…¼å®¹ä¸è½åœ°å»ºè®®

### 9.1 å½“å‰é…ç½®æ–¹å¼

**InferenceEngine ä½¿ç”¨æ–¹å¼**ï¼š
```cpp
// ä½¿ç”¨ KylinBackendï¼ˆæ”¯æŒ GGUFï¼‰
ModelConfig config;
config.vocabSize = 151936;
config.hiddenSize = 1024;
// ... å…¶ä»–é…ç½®

InferenceEngine engine(config, "model/Qwen/qwen3-0.6b-q4_k_m.gguf", false);  // false = Kylin
engine.initialize();

// ç¡®ä¿ä½¿ç”¨ GGUFTokenizer
// ï¼ˆéœ€è¦åœ¨ InferenceEngine æˆ– ModelExecutor ä¸­å®ç°è‡ªåŠ¨é€‰æ‹©ï¼‰
```

### 9.2 å¯é€‰ï¼šLlamaCppBackend å®ç°æ­¥éª¤ï¼ˆå¦‚æœå®ç°ï¼‰

1. **åˆ›å»º `LlamaCppBackend` ç±»**ï¼š
   - å®ç° `IBackend` æ¥å£
   - ä½¿ç”¨ `llama.cpp` C APIï¼ˆ`llama.h`ï¼‰

2. **æ³¨å†Œåˆ° BackendFactory**ï¼š
   ```cpp
   // src/inference/backend_factory.cpp
   if (backendType == "llama_cpp" || backendType == "llama.cpp") {
       return std::make_unique<LlamaCppBackend>(config, modelPath);
   }
   ```

3. **CMake é›†æˆ**ï¼š
   - é“¾æ¥ `third_party/llama.cpp` çš„åº“
   - åŒ…å« `llama.h` å¤´æ–‡ä»¶

4. **é…ç½®æ”¯æŒ**ï¼š
   ```yaml
   backend:
     type: llama_cpp  # æˆ– kylin, libtorch
     llama_cpp:
       n_ctx: 4096
       n_batch: 512
       n_threads: 8
       use_mmap: true
       use_mlock: false
   ```

### 9.3 å½“å‰æ¨èæ–¹æ¡ˆ

**âœ… æ¨èä½¿ç”¨ KylinBackend + GGUFTokenizer**ï¼š
- KylinBackend å·²æ”¯æŒ GGUF æ ¼å¼
- GGUFTokenizer å·²å®ç°å®Œæ•´ BPE ç¼–ç 
- æ— éœ€é¢å¤–ä¾èµ– `llama.cpp` C API
- æ€§èƒ½å¯æ§ï¼Œæ˜“äºè°ƒè¯•

**å¯é€‰ï¼šLlamaCppBackend**ï¼š
- å¦‚æœéœ€è¦ç›´æ¥ä½¿ç”¨ `llama.cpp` çš„ä¼˜åŒ–å®ç°
- å¦‚æœéœ€è¦ GPU åŠ é€Ÿï¼ˆMetal/CUDAï¼‰
- å¦‚æœéœ€è¦æ›´å®Œæ•´çš„ GGUF æ”¯æŒï¼ˆæŸäº›ç‰¹æ®Šæ ¼å¼ï¼‰

---

## 10. é£é™©ä¸è¾¹ç•Œ

### 10.1 å½“å‰å®ç°çš„é£é™©

- âœ… **GGUF tokenizer metadata å®Œæ•´æ€§**ï¼š`GGUFTokenizer` å·²å®ç°ä» GGUF metadata åŠ è½½ tokens å’Œ merges
- âš ï¸ **ä¸åŒ tokenizer ç±»å‹**ï¼šå½“å‰å®ç°ä¸»è¦é’ˆå¯¹ BPEï¼Œå¯¹äº `tokenizer.ggml.model` ä¸º `llama/replit/gpt2` çš„æƒ…å†µéœ€è¦éªŒè¯
- âœ… **BPE å¯¹é½**ï¼š`GGUFTokenizer` çš„ BPE å®ç°å·²å¯¹é½ llama.cpp çš„æ ¸å¿ƒé€»è¾‘
- âš ï¸ **é¢„åˆ†è¯æ­£åˆ™**ï¼šå½“å‰ `preTokenize()` ä½¿ç”¨ç®€å•ç©ºç™½ç¬¦åˆ†å‰²ï¼Œå¯èƒ½éœ€è¦æ‰©å±•ä¸º GPT-2/Qwen é£æ ¼çš„æ­£åˆ™è¡¨è¾¾å¼

### 10.2 å¾…éªŒè¯é¡¹

1. **ç¼–ç /è§£ç ä¸€è‡´æ€§æµ‹è¯•**ï¼š
   - ä½¿ç”¨ç›¸åŒæ–‡æœ¬ï¼ŒéªŒè¯ `encode()` å’Œ `decode()` çš„ round-trip ä¸€è‡´æ€§
   - ä¸ `llama.cpp` çš„è¾“å‡ºå¯¹æ¯”éªŒè¯

2. **ç‰¹æ®Š token å¤„ç†**ï¼š
   - éªŒè¯ç‰¹æ®Š tokenï¼ˆå¦‚ `<|im_start|>`, `<|im_end|>`ï¼‰çš„æ­£ç¡®ç¼–ç /è§£ç 

3. **è¾¹ç•Œæƒ…å†µ**ï¼š
   - ç©ºå­—ç¬¦ä¸²ã€è¶…é•¿æ–‡æœ¬ã€åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„æ–‡æœ¬
   - æœªçŸ¥ tokenï¼ˆUNKï¼‰çš„å¤„ç†

### 10.3 å¯é€‰ï¼šLlamaCppBackend çš„é£é™©

- **ä¾èµ–ç®¡ç†**ï¼šéœ€è¦æ­£ç¡®é“¾æ¥ `llama.cpp` åº“ï¼Œå¤„ç†ç‰ˆæœ¬å…¼å®¹æ€§
- **API å˜åŒ–**ï¼š`llama.cpp` API å¯èƒ½åœ¨ä¸åŒç‰ˆæœ¬é—´å˜åŒ–
- **æ€§èƒ½æƒè¡¡**ï¼šç›´æ¥ä½¿ç”¨ `llama.cpp` å¯èƒ½æ€§èƒ½æ›´å¥½ï¼Œä½†å¤±å»å¯¹å†…éƒ¨å®ç°çš„å®Œå…¨æ§åˆ¶

---

## 11. æ€»ç»“ä¸å»ºè®®

### 11.1 å½“å‰çŠ¶æ€

âœ… **å·²å®Œæˆ**ï¼š
- `KylinBackend` æ”¯æŒ GGUF æ ¼å¼åŠ è½½
- `GGUFTokenizer` å®ç°å®Œæ•´ BPE ç¼–ç é€»è¾‘
- ç¼–ç å’Œè§£ç ä½¿ç”¨ç›¸åŒçš„ GGUF tokenizer

âš ï¸ **å¾…éªŒè¯**ï¼š
- ç¡®ä¿æ‰€æœ‰ GGUF æ¨¡å‹åŠ è½½è·¯å¾„éƒ½ä½¿ç”¨ `GGUFTokenizer`
- æµ‹è¯•éªŒè¯ç¼–ç /è§£ç çš„ token ID ä¸€è‡´æ€§
- æ€§èƒ½æµ‹è¯•å’Œä¼˜åŒ–

### 11.2 æ¨èæ–¹æ¡ˆ

**æ–¹æ¡ˆ Aï¼ˆæ¨èï¼‰**ï¼šç»§ç»­ä½¿ç”¨ `KylinBackend + GGUFTokenizer`
- âœ… æ— éœ€é¢å¤–ä¾èµ–
- âœ… å®Œå…¨æ§åˆ¶å®ç°
- âœ… æ˜“äºè°ƒè¯•å’Œä¼˜åŒ–
- âš ï¸ éœ€è¦ç¡®ä¿ BPE å®ç°å®Œå…¨æ­£ç¡®

**æ–¹æ¡ˆ Bï¼ˆå¯é€‰ï¼‰**ï¼šå®ç° `LlamaCppBackend`
- âœ… ç›´æ¥ä½¿ç”¨ `llama.cpp` çš„æˆç†Ÿå®ç°
- âœ… æ›´å¥½çš„æ€§èƒ½ï¼ˆå¯èƒ½ï¼‰
- âœ… GPU åŠ é€Ÿæ”¯æŒ
- âš ï¸ å¢åŠ ä¾èµ–å’Œå¤æ‚åº¦

### 11.3 ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼ˆä¼˜å…ˆçº§æ’åºï¼‰

**ğŸ”´ P0 - ç«‹å³ä¿®å¤ï¼ˆé˜»å¡æ€§é—®é¢˜ï¼‰**ï¼š
1. **ä¿®å¤ TokenizerManager çš„ GGUF æ£€æµ‹**ï¼š
   - æ–‡ä»¶ï¼š`src/tokenizer/manager.cpp`
   - é—®é¢˜ï¼š`TokenizerManager` æœªæ£€æµ‹ `.gguf` æ–‡ä»¶ï¼Œå¯èƒ½é”™è¯¯ä½¿ç”¨ `HFTokenizer`
   - ä¿®å¤ï¼šåœ¨ `AUTO` æ¨¡å¼ä¸‹ï¼Œä¼˜å…ˆæ£€æŸ¥æ˜¯å¦ä¸º `.gguf` æ–‡ä»¶ï¼Œå¦‚æœæ˜¯åˆ™ä½¿ç”¨ `GGUFTokenizer`
   ```cpp
   // åœ¨ TokenizerManager æ„é€ å‡½æ•°ä¸­æ·»åŠ 
   if (modelPath.ends_with(".gguf")) {
       CLLM_INFO("âœ… Detected GGUF format, using GGUFTokenizer");
       tokenizer_ = new GGUFTokenizer();
   } else if (hasTokenizerJson(modelPath)) {
       // ... ç°æœ‰é€»è¾‘
   }
   ```

2. **æ·»åŠ  vocab_size æ ¡éªŒ**ï¼š
   - åœ¨ `ModelExecutor` æˆ– `InferenceEngine` ä¸­ï¼ŒåŠ è½½æ¨¡å‹åéªŒè¯ `tokenizer->getVocabSize() == model->getVocabSize()`
   - å¦‚æœä¸ä¸€è‡´ï¼Œç«‹å³æŠ¥é”™å¹¶ç»ˆæ­¢

**ğŸŸ¡ P1 - éªŒè¯æµ‹è¯•**ï¼š
1. **ç¼–ç /è§£ç ä¸€è‡´æ€§æµ‹è¯•**ï¼šè¿è¡Œ `test_hello_inference`ï¼ŒéªŒè¯ `GGUFTokenizer` çš„ round-trip ä¸€è‡´æ€§
2. **ä¸ llama.cpp å¯¹æ¯”æµ‹è¯•**ï¼šä½¿ç”¨ç›¸åŒæ–‡æœ¬ï¼Œå¯¹æ¯” token IDs æ˜¯å¦ä¸€è‡´

**ğŸŸ¢ P2 - ä¼˜åŒ–æ”¹è¿›**ï¼š
1. **é¢„åˆ†è¯æ­£åˆ™æ‰©å±•**ï¼šå°† `preTokenize()` æ‰©å±•ä¸ºæ”¯æŒ GPT-2/Qwen é£æ ¼çš„æ­£åˆ™è¡¨è¾¾å¼
2. **æ€§èƒ½æµ‹è¯•**ï¼šå¯¹æ¯” `KylinBackend` å’Œ `llama.cpp` çš„æ€§èƒ½ï¼ˆå¦‚æœå®ç° LlamaCppBackendï¼‰
3. **æ—¥å¿—å¢å¼º**ï¼šè¾“å‡º tokenizer/gguf vocab ä¸ merges/tokens ç»Ÿè®¡

---

## 12. å·²çŸ¥é—®é¢˜ä¸ä¿®å¤å»ºè®®

### 12.1 å…³é”®æ¼æ´ï¼ˆå¿…é¡»ä¿®å¤ï¼‰

| é—®é¢˜ | ä½ç½® | é£é™© | ä¿®å¤ä¼˜å…ˆçº§ |
|------|------|------|-----------|
| **TokenizerManager æœªæ£€æµ‹ GGUF** | `src/tokenizer/manager.cpp:112-128` | ğŸ”´ **é«˜**ï¼šGGUF æ¨¡å‹å¯èƒ½ä½¿ç”¨é”™è¯¯çš„ tokenizer | **P0** |
| **ç¼ºå°‘ vocab_size æ ¡éªŒ** | `ModelExecutor` æˆ– `InferenceEngine` | ğŸŸ¡ **ä¸­**ï¼šå¯èƒ½å¯¼è‡´é‡‡æ ·é”™è¯¯ | **P0** |
| **preTokenize è¿‡äºç®€åŒ–** | `src/tokenizer/gguf_tokenizer.cpp` | ğŸŸ¢ **ä½**ï¼šå¯èƒ½å½±å“æŸäº›æ–‡æœ¬çš„åˆ†è¯å‡†ç¡®æ€§ | **P2** |

### 12.2 ä¿®å¤ä»£ç ç¤ºä¾‹

**ä¿®å¤ TokenizerManager**ï¼š
```cpp
// src/tokenizer/manager.cpp
TokenizerManager::TokenizerManager(...) {
    // ... ç°æœ‰ä»£ç  ...
    
    case TokenizerImpl::AUTO:
    default:
        // âœ… ä¼˜å…ˆæ£€æµ‹ GGUF æ ¼å¼
        if (modelPath.ends_with(".gguf") || 
            (fs::is_regular_file(modelPath) && 
             modelPath.find(".gguf") != std::string::npos)) {
            CLLM_INFO("âœ… Detected GGUF format, using GGUFTokenizer");
            tokenizer_ = new GGUFTokenizer();
        } else if (hasTokenizerJson(modelPath)) {
            CLLM_INFO("âœ… Detected HuggingFace format (tokenizer.json), using HFTokenizer");
            tokenizer_ = new HFTokenizer(modelType);
        } else if (hasTokenizerModel(modelPath)) {
            // ... ç°æœ‰é€»è¾‘ ...
        }
        break;
}
```

---

**ç»“è®º**ï¼šå½“å‰ `KylinBackend + GGUFTokenizer` çš„ç»„åˆå·²ç»èƒ½å¤Ÿè§£å†³ GGUF è¾“å‡ºä¹±ç ä¸ vocab ä¸ä¸€è‡´é—®é¢˜ã€‚**ä½†å¿…é¡»ä¿®å¤ `TokenizerManager` çš„ GGUF æ£€æµ‹é€»è¾‘**ï¼Œç¡®ä¿æ‰€æœ‰ GGUF æ¨¡å‹éƒ½ä½¿ç”¨ `GGUFTokenizer`ã€‚`LlamaCppBackend` æ˜¯ä¸€ä¸ªå¯é€‰çš„åç«¯é€‰é¡¹ï¼Œå¯ä»¥æä¾›é¢å¤–çš„æ€§èƒ½å’ŒåŠŸèƒ½æ”¯æŒã€‚