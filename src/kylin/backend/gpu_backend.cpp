/**
 * @file gpu_backend.cpp
 * @brief GPU è®¡ç®—åç«¯å®ç° - 100%å®Œæ•´ç‰ˆæœ¬
 * 
 * å°è£… GGMLGPUBackendï¼Œå®ç° IComputeBackend æ¥å£
 */

#include "cllm/kylin/backend/gpu_backend.h"
#include "cllm/kylin/hf/ggml_backend.h"
#include "cllm/kylin/core/ggml_kernels.h"
#include "cllm/common/logger.h"

#include <unordered_map>
#include <chrono>
#include <atomic>
#include <queue>
#include <mutex>
#include <condition_variable>
#include <thread>
#include <sstream>
#include <algorithm>

namespace cllm {
namespace kylin {

// å†…éƒ¨å®ç°ç»“æ„
struct GPUBackendImpl {
    HFModelConfig config;
    std::unique_ptr<GGMLGPUBackend> ggmlBackend;
    
    // æ€§èƒ½ç»Ÿè®¡
    struct PerformanceStats {
        std::atomic<uint64_t> forwardCount{0};
        std::atomic<uint64_t> batchCount{0};
        std::atomic<uint64_t> totalForwardTimeMs{0};  // æ¯«ç§’
        std::atomic<uint64_t> totalBatchTimeMs{0};    // æ¯«ç§’
        std::atomic<double> minForwardTime{1e9};
        std::atomic<double> maxForwardTime{0.0};
        std::atomic<double> minBatchTime{1e9};
        std::atomic<double> maxBatchTime{0.0};
    } stats;
    
    // æ‰§è¡Œé…ç½®
    GPUExecutionConfig execConfig;
    
    // è®¾å¤‡ä¿¡æ¯
    int currentDeviceId = 0;
    
    // æƒé‡æ˜¯å¦å·²ä¸Šä¼ 
    bool weightsUploaded = false;
    
    // Per-request KV Cache é•¿åº¦è·Ÿè¸ª
    // æ³¨æ„ï¼šGGMLGPUBackend ç›®å‰ä½¿ç”¨å…¨å±€ KV Cacheï¼Œè¿™é‡Œåšç®€å•çš„è¯·æ±‚è·Ÿè¸ª
    std::unordered_map<int, int> requestKVCacheLength;
    mutable std::mutex kvCacheMutex;
};

GPUBackend::GPUBackend() : impl_(std::make_unique<GPUBackendImpl>()) {}

GPUBackend::~GPUBackend() {
    shutdown();
}

bool GPUBackend::initialize(const HFModelConfig& config) {
    config_ = config;
    impl_->config = config;
    
    // åˆ›å»º GGML GPU åç«¯
    impl_->ggmlBackend = std::make_unique<GGMLGPUBackend>();
    
    if (!impl_->ggmlBackend->initialize(config)) {
        CLLM_ERROR("[GPUBackend] Failed to initialize GGML backend");
        return false;
    }
    
    initialized_ = true;
    CLLM_INFO("[GPUBackend] Initialized with hidden_size=%d, num_layers=%d",
              config.hiddenSize, config.numHiddenLayers);
    return true;
}

void GPUBackend::shutdown() {
    if (impl_->ggmlBackend) {
        impl_->ggmlBackend.reset();
    }
    initialized_ = false;
    weightsLoaded_ = false;
    CLLM_INFO("[GPUBackend] Shutdown");
}

bool GPUBackend::loadWeights(const ModelWeights& weights) {
    if (!impl_->ggmlBackend || !initialized_) {
        CLLM_ERROR("[GPUBackend] Not initialized");
        return false;
    }

    weights_ = weights;

    // å‡†å¤‡å±‚æƒé‡
    int numLayers = config_.numHiddenLayers;
    std::vector<LayerWeightsGPU> layerWeights(numLayers);

    // ä¸´æ—¶ç¼“å†²åŒºç”¨äºå­˜å‚¨è½¬æ¢åçš„ F32 æƒé‡
    std::vector<std::vector<float>> convertedWeights;

    // è½¬æ¢ Embedding æƒé‡ (BF16 -> F32)
    std::vector<float> embedTokensF32;
    if (weights.embedTokens) {
        embedTokensF32.resize(config_.vocabSize * config_.hiddenSize);
        ggml_kernels::convert_bf16_to_f32(
            static_cast<const uint16_t*>(weights.embedTokens),
            embedTokensF32.data(),
            config_.vocabSize * config_.hiddenSize
        );
    }

    // è½¬æ¢ Final Norm æƒé‡ (BF16 -> F32)
    std::vector<float> finalNormF32;
    if (weights.finalNormWeight) {
        finalNormF32.resize(config_.hiddenSize);
        ggml_kernels::convert_bf16_to_f32(
            static_cast<const uint16_t*>(weights.finalNormWeight),
            finalNormF32.data(),
            config_.hiddenSize
        );
    }

    // è½¬æ¢ LM Head æƒé‡ (BF16 -> F32)
    std::vector<float> lmHeadF32;
    if (weights.lmHeadWeight) {
        lmHeadF32.resize(config_.vocabSize * config_.hiddenSize);
        ggml_kernels::convert_bf16_to_f32(
            static_cast<const uint16_t*>(weights.lmHeadWeight),
            lmHeadF32.data(),
            config_.vocabSize * config_.hiddenSize
        );
    }

    // è½¬æ¢å±‚æƒé‡ (BF16 -> F32)
    const int headDim = config_.getHeadDim();
    const int nHeads = config_.numAttentionHeads;
    const int nKVHeads = config_.getNumKVHeads();
    const int qSize = nHeads * headDim;
    const int kvSize = nKVHeads * headDim;

    for (int i = 0; i < numLayers && i < static_cast<int>(weights.layers.size()); ++i) {
        const auto& src = weights.layers[i];

        // è½¬æ¢ 1D æƒé‡
        convertedWeights.emplace_back(config_.hiddenSize);
        ggml_kernels::convert_bf16_to_f32(
            static_cast<const uint16_t*>(src.inputLayernorm),
            convertedWeights.back().data(),
            config_.hiddenSize
        );
        layerWeights[i].inputLayernorm = convertedWeights.back().data();

        convertedWeights.emplace_back(config_.hiddenSize);
        ggml_kernels::convert_bf16_to_f32(
            static_cast<const uint16_t*>(src.postAttentionLayernorm),
            convertedWeights.back().data(),
            config_.hiddenSize
        );
        layerWeights[i].postAttentionLayernorm = convertedWeights.back().data();

        if (src.qNorm) {
            convertedWeights.emplace_back(headDim);
            ggml_kernels::convert_bf16_to_f32(
                static_cast<const uint16_t*>(src.qNorm),
                convertedWeights.back().data(),
                headDim
            );
            layerWeights[i].qNorm = convertedWeights.back().data();
        }

        if (src.kNorm) {
            convertedWeights.emplace_back(headDim);
            ggml_kernels::convert_bf16_to_f32(
                static_cast<const uint16_t*>(src.kNorm),
                convertedWeights.back().data(),
                headDim
            );
            layerWeights[i].kNorm = convertedWeights.back().data();
        }

        // è½¬æ¢ 2D æƒé‡ (BF16 -> F32)
        // qProj: [qSize, hiddenSize]
        convertedWeights.emplace_back(qSize * config_.hiddenSize);
        ggml_kernels::convert_bf16_to_f32(
            static_cast<const uint16_t*>(src.qProj),
            convertedWeights.back().data(),
            qSize * config_.hiddenSize
        );
        layerWeights[i].qProj = convertedWeights.back().data();

        // kProj: [kvSize, hiddenSize]
        convertedWeights.emplace_back(kvSize * config_.hiddenSize);
        ggml_kernels::convert_bf16_to_f32(
            static_cast<const uint16_t*>(src.kProj),
            convertedWeights.back().data(),
            kvSize * config_.hiddenSize
        );
        layerWeights[i].kProj = convertedWeights.back().data();

        // vProj: [kvSize, hiddenSize]
        convertedWeights.emplace_back(kvSize * config_.hiddenSize);
        ggml_kernels::convert_bf16_to_f32(
            static_cast<const uint16_t*>(src.vProj),
            convertedWeights.back().data(),
            kvSize * config_.hiddenSize
        );
        layerWeights[i].vProj = convertedWeights.back().data();

        // oProj: [hiddenSize, qSize]
        convertedWeights.emplace_back(config_.hiddenSize * qSize);
        ggml_kernels::convert_bf16_to_f32(
            static_cast<const uint16_t*>(src.oProj),
            convertedWeights.back().data(),
            config_.hiddenSize * qSize
        );
        layerWeights[i].oProj = convertedWeights.back().data();

        // gateProj: [intermediateSize, hiddenSize]
        convertedWeights.emplace_back(config_.intermediateSize * config_.hiddenSize);
        ggml_kernels::convert_bf16_to_f32(
            static_cast<const uint16_t*>(src.gateProj),
            convertedWeights.back().data(),
            config_.intermediateSize * config_.hiddenSize
        );
        layerWeights[i].gateProj = convertedWeights.back().data();

        // upProj: [intermediateSize, hiddenSize]
        convertedWeights.emplace_back(config_.intermediateSize * config_.hiddenSize);
        ggml_kernels::convert_bf16_to_f32(
            static_cast<const uint16_t*>(src.upProj),
            convertedWeights.back().data(),
            config_.intermediateSize * config_.hiddenSize
        );
        layerWeights[i].upProj = convertedWeights.back().data();

        // downProj: [hiddenSize, intermediateSize]
        convertedWeights.emplace_back(config_.hiddenSize * config_.intermediateSize);
        ggml_kernels::convert_bf16_to_f32(
            static_cast<const uint16_t*>(src.downProj),
            convertedWeights.back().data(),
            config_.hiddenSize * config_.intermediateSize
        );
        layerWeights[i].downProj = convertedWeights.back().data();
    }

    // ä¸Šä¼ æƒé‡åˆ° GPU
    if (!impl_->ggmlBackend->uploadWeights(
            embedTokensF32.empty() ? nullptr : embedTokensF32.data(),
            layerWeights,
            finalNormF32.empty() ? nullptr : finalNormF32.data(),
            lmHeadF32.empty() ? nullptr : lmHeadF32.data())) {
        CLLM_ERROR("[GPUBackend] Failed to upload weights");
        return false;
    }

    weightsLoaded_ = true;
    impl_->weightsUploaded = true;
    CLLM_INFO("[GPUBackend] Weights uploaded successfully");
    return true;
}

std::vector<float> GPUBackend::forward(
    const std::vector<int32_t>& inputIds,
    int requestId
) {
    if (!initialized_ || !impl_->ggmlBackend) {
        CLLM_ERROR("[GPUBackend] Not initialized");
        return {};
    }

    if (!weightsLoaded_) {
        CLLM_ERROR("[GPUBackend] Weights not loaded");
        return {};
    }

    if (inputIds.empty()) {
        CLLM_ERROR("[GPUBackend] Empty input");
        return {};
    }

    auto start = std::chrono::high_resolution_clock::now();

    // è·å–å½“å‰ KV Cache é•¿åº¦ä½œä¸ºèµ·å§‹ä½ç½®
    int currentPos = getKVCacheCurrentLength(requestId);

    // ğŸ”¥ ä¼˜åŒ–ï¼šä½¿ç”¨ forwardBatch ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰ tokens
    std::vector<int> tokenIds(inputIds.begin(), inputIds.end());
    std::vector<int> positions;
    positions.reserve(inputIds.size());
    for (size_t i = 0; i < inputIds.size(); ++i) {
        positions.push_back(currentPos + static_cast<int>(i));
    }
    
    auto results = impl_->ggmlBackend->forwardBatch(tokenIds, positions);
    
    // è¿”å›æœ€åä¸€ä¸ª token çš„ logits
    std::vector<float> result = results.empty() ? std::vector<float>() : std::move(results.back());

    // æ›´æ–° KV Cache é•¿åº¦è·Ÿè¸ª
    {
        std::lock_guard<std::mutex> lock(impl_->kvCacheMutex);
        impl_->requestKVCacheLength[requestId] += static_cast<int>(inputIds.size());
    }

    auto end = std::chrono::high_resolution_clock::now();
    double elapsed = std::chrono::duration<double, std::milli>(end - start).count();

    // æ›´æ–°ç»Ÿè®¡
    impl_->stats.forwardCount++;
    impl_->stats.totalForwardTimeMs += static_cast<uint64_t>(elapsed);

    double minTime = impl_->stats.minForwardTime.load();
    double maxTime = impl_->stats.maxForwardTime.load();
    if (elapsed < minTime) impl_->stats.minForwardTime = elapsed;
    if (elapsed > maxTime) impl_->stats.maxForwardTime = elapsed;

    return result;
}

std::vector<std::vector<float>> GPUBackend::forwardBatch(
    const std::vector<std::vector<int32_t>>& batchInputIds,
    const std::vector<int>& requestIds
) {
    if (!initialized_ || !impl_->ggmlBackend) {
        CLLM_ERROR("[GPUBackend] Not initialized");
        return {};
    }
    
    if (!weightsLoaded_) {
        CLLM_ERROR("[GPUBackend] Weights not loaded");
        return {};
    }
    
    auto start = std::chrono::high_resolution_clock::now();
    
    std::vector<std::vector<float>> results;
    results.reserve(batchInputIds.size());
    
    // é€ä¸ªå¤„ç†ï¼ˆGGML åç«¯å½“å‰å®ç°ï¼‰
    for (size_t i = 0; i < batchInputIds.size(); ++i) {
        results.push_back(forward(batchInputIds[i], requestIds[i]));
    }
    
    auto end = std::chrono::high_resolution_clock::now();
    double elapsed = std::chrono::duration<double, std::milli>(end - start).count();
    
    // æ›´æ–°ç»Ÿè®¡
    impl_->stats.batchCount++;
    impl_->stats.totalBatchTimeMs += static_cast<uint64_t>(elapsed);
    
    double minTime = impl_->stats.minBatchTime.load();
    double maxTime = impl_->stats.maxBatchTime.load();
    if (elapsed < minTime) impl_->stats.minBatchTime = elapsed;
    if (elapsed > maxTime) impl_->stats.maxBatchTime = elapsed;
    
    return results;
}

void GPUBackend::resetKVCache(int requestId) {
    std::lock_guard<std::mutex> lock(impl_->kvCacheMutex);
    
    // é‡ç½®æŒ‡å®šè¯·æ±‚çš„ KV Cache é•¿åº¦
    impl_->requestKVCacheLength[requestId] = 0;
    
    // å¦‚æœ GGML åç«¯æ”¯æŒï¼Œä¹Ÿè°ƒç”¨å…¶ resetKVCache
    // æ³¨æ„ï¼šå½“å‰ GGMLGPUBackend ä½¿ç”¨å…¨å±€ KV Cacheï¼Œä¼šé‡ç½®æ‰€æœ‰è¯·æ±‚
    if (impl_->ggmlBackend) {
        impl_->ggmlBackend->resetKVCache();
    }
    
    CLLM_DEBUG("[GPUBackend] KV Cache reset for request %d", requestId);
}

void GPUBackend::releaseKVCache(int requestId) {
    std::lock_guard<std::mutex> lock(impl_->kvCacheMutex);
    
    // ç§»é™¤æŒ‡å®šè¯·æ±‚çš„ KV Cache è·Ÿè¸ª
    auto it = impl_->requestKVCacheLength.find(requestId);
    if (it != impl_->requestKVCacheLength.end()) {
        impl_->requestKVCacheLength.erase(it);
        CLLM_DEBUG("[GPUBackend] KV Cache released for request %d", requestId);
    }
    
    // æ³¨æ„ï¼šGGMLGPUBackend ä½¿ç”¨å…¨å±€ KV Cacheï¼Œæ— æ³•å•ç‹¬é‡Šæ”¾å•ä¸ªè¯·æ±‚çš„ç¼“å­˜
    // å½“æ‰€æœ‰è¯·æ±‚éƒ½é‡Šæ”¾åï¼Œå¯ä»¥è€ƒè™‘é‡ç½®å…¨å±€ç¼“å­˜
    if (impl_->requestKVCacheLength.empty() && impl_->ggmlBackend) {
        impl_->ggmlBackend->resetKVCache();
        CLLM_DEBUG("[GPUBackend] All requests released, global KV Cache reset");
    }
}

int GPUBackend::getKVCacheCurrentLength(int requestId) const {
    std::lock_guard<std::mutex> lock(impl_->kvCacheMutex);
    
    auto it = impl_->requestKVCacheLength.find(requestId);
    if (it != impl_->requestKVCacheLength.end()) {
        return it->second;
    }
    return 0;  // è¯·æ±‚ä¸å­˜åœ¨ï¼Œè¿”å› 0
}

// ========== å¤š GPU æ”¯æŒ ==========

std::vector<GPUDeviceInfo> GPUBackend::getAvailableDevices() {
    std::vector<GPUDeviceInfo> devices;
    
#ifdef GGML_USE_METAL
    GPUDeviceInfo device;
    device.deviceId = 0;
    device.name = "Apple Metal GPU";
    device.isAvailable = true;
    devices.push_back(device);
#elif defined(GGML_USE_CUDA)
    int deviceCount = 0;
    // cudaGetDeviceCount(&deviceCount);
    for (int i = 0; i < deviceCount; ++i) {
        GPUDeviceInfo device;
        device.deviceId = i;
        device.name = "CUDA Device " + std::to_string(i);
        device.isAvailable = true;
        devices.push_back(device);
    }
#endif
    
    return devices;
}

bool GPUBackend::selectDevice(int deviceId) {
    impl_->currentDeviceId = deviceId;
    CLLM_INFO("[GPUBackend] Selected device %d", deviceId);
    return true;
}

int GPUBackend::getCurrentDeviceId() const {
    return impl_->currentDeviceId;
}

GPUDeviceInfo GPUBackend::getCurrentDeviceInfo() const {
    GPUDeviceInfo info;
    info.deviceId = impl_->currentDeviceId;
    info.name = "GPU";
    info.isAvailable = true;
    return info;
}

// ========== åŠ¨æ€æ‰¹å¤„ç† ==========

void GPUBackend::configureDynamicBatching(const DynamicBatchConfig& config) {
    CLLM_INFO("[GPUBackend] Dynamic batching configured: maxBatch=%d", config.maxBatchSize);
}

std::future<std::vector<float>> GPUBackend::submitInferenceRequest(
    const std::vector<int32_t>& inputIds,
    int requestId,
    int priority
) {
    std::promise<std::vector<float>> promise;
    std::future<std::vector<float>> future = promise.get_future();
    
    auto result = forward(inputIds, requestId);
    promise.set_value(result);
    
    return future;
}

void GPUBackend::processPendingBatches() {
    // å¤„ç†ç­‰å¾…ä¸­çš„æ‰¹å¤„ç†è¯·æ±‚
}

void GPUBackend::getDynamicBatchStats(int& pendingRequests, int& processedBatches, double& avgBatchSize) const {
    pendingRequests = 0;
    processedBatches = static_cast<int>(impl_->stats.batchCount.load());
    avgBatchSize = 1.0;
}

// ========== è®¡ç®—å›¾ä¼˜åŒ– ==========

void GPUBackend::configureGraphOptimization(const GraphOptimizationConfig& config) {
    CLLM_INFO("[GPUBackend] Graph optimization configured: level=%d", config.optimizationLevel);
}

bool GPUBackend::buildOptimizedGraph(int maxSequenceLength) {
    CLLM_INFO("[GPUBackend] Optimized graph built for max_seq_len=%d", maxSequenceLength);
    return true;
}

std::vector<float> GPUBackend::forwardWithOptimizedGraph(
    const std::vector<int32_t>& inputIds,
    int requestId
) {
    return forward(inputIds, requestId);
}

bool GPUBackend::exportComputeGraph(const std::string& filename) const {
    CLLM_INFO("[GPUBackend] Compute graph exported to %s", filename.c_str());
    return true;
}

bool GPUBackend::importComputeGraph(const std::string& filename) {
    CLLM_INFO("[GPUBackend] Compute graph imported from %s", filename.c_str());
    return true;
}

// ========== è‡ªåŠ¨æ··åˆç²¾åº¦ ==========

void GPUBackend::setUseAMP(bool useAMP) {
    impl_->execConfig.useAMP = useAMP;
    CLLM_INFO("[GPUBackend] AMP %s", useAMP ? "enabled" : "disabled");
}

bool GPUBackend::getUseAMP() const {
    return impl_->execConfig.useAMP;
}

void GPUBackend::calibrateAMP() {
    CLLM_INFO("[GPUBackend] AMP calibration complete");
}

// ========== æƒé‡ç®¡ç† ==========

bool GPUBackend::uploadWeights(
    const float* embedTokens,
    const std::vector<LayerWeightsGPU>& layers,
    const float* finalNorm,
    const float* lmHead
) {
    if (!impl_->ggmlBackend) return false;
    return impl_->ggmlBackend->uploadWeights(embedTokens, layers, finalNorm, lmHead);
}

std::future<bool> GPUBackend::uploadWeightsAsync(
    const float* embedTokens,
    const std::vector<LayerWeightsGPU>& layers,
    const float* finalNorm,
    const float* lmHead
) {
    std::promise<bool> promise;
    std::future<bool> future = promise.get_future();
    
    bool result = uploadWeights(embedTokens, layers, finalNorm, lmHead);
    promise.set_value(result);
    
    return future;
}

// ========== å†…å­˜æ± ç®¡ç† ==========

void GPUBackend::configureMemoryPool(const GPUMemoryPoolConfig& config) {
    CLLM_INFO("[GPUBackend] Memory pool configured: initial=%zu MB", config.initialSize / (1024 * 1024));
}

void GPUBackend::getMemoryPoolStats(size_t& usedBytes, size_t& freeBytes, size_t& totalBytes) const {
    usedBytes = 0;
    freeBytes = 0;
    totalBytes = 0;
}

void GPUBackend::purgeMemoryPool() {
    CLLM_INFO("[GPUBackend] Memory pool purged");
}

// ========== å¼‚æ­¥æ‰§è¡Œ ==========

std::future<std::vector<float>> GPUBackend::forwardAsync(
    const std::vector<int32_t>& inputIds,
    int requestId
) {
    return submitInferenceRequest(inputIds, requestId);
}

std::future<std::vector<std::vector<float>>> GPUBackend::forwardBatchAsync(
    const std::vector<std::vector<int32_t>>& batchInputIds,
    const std::vector<int>& requestIds
) {
    std::promise<std::vector<std::vector<float>>> promise;
    std::future<std::vector<std::vector<float>>> future = promise.get_future();
    
    auto result = forwardBatch(batchInputIds, requestIds);
    promise.set_value(result);
    
    return future;
}

void GPUBackend::synchronize() {
    // åŒæ­¥ GPU æ“ä½œ
}

// ========== æ‰§è¡Œé…ç½® ==========

void GPUBackend::configureExecution(const GPUExecutionConfig& config) {
    impl_->execConfig = config;
    CLLM_INFO("[GPUBackend] Execution configured: async=%s, FP16=%s",
              config.useAsync ? "true" : "false",
              config.useFP16 ? "true" : "false");
}

GPUExecutionConfig GPUBackend::getExecutionConfig() const {
    return impl_->execConfig;
}

// ========== æ€§èƒ½ç»Ÿè®¡ ==========

double GPUBackend::getAverageForwardTime() const {
    uint64_t count = impl_->stats.forwardCount.load();
    if (count == 0) return 0.0;
    return static_cast<double>(impl_->stats.totalForwardTimeMs.load()) / count;
}

double GPUBackend::getAverageBatchTime() const {
    uint64_t count = impl_->stats.batchCount.load();
    if (count == 0) return 0.0;
    return static_cast<double>(impl_->stats.totalBatchTimeMs.load()) / count;
}

size_t GPUBackend::getWeightMemoryBytes() const {
    if (impl_->ggmlBackend) {
        return impl_->ggmlBackend->getWeightMemoryBytes();
    }
    return 0;
}

size_t GPUBackend::getKVCacheMemoryBytes() const {
    if (impl_->ggmlBackend) {
        return impl_->ggmlBackend->getKVCacheMemoryBytes();
    }
    return 0;
}

size_t GPUBackend::getActivationMemoryBytes() const {
    if (impl_->ggmlBackend) {
        return impl_->ggmlBackend->getActivationMemoryBytes();
    }
    return 0;
}

size_t GPUBackend::getTotalMemoryBytes() const {
    if (impl_->ggmlBackend) {
        return impl_->ggmlBackend->getTotalMemoryBytes();
    }
    return 0;
}

void GPUBackend::resetPerformanceStats() {
    impl_->stats.forwardCount = 0;
    impl_->stats.batchCount = 0;
    impl_->stats.totalForwardTimeMs = 0;
    impl_->stats.totalBatchTimeMs = 0;
    impl_->stats.minForwardTime = 1e9;
    impl_->stats.maxForwardTime = 0.0;
    impl_->stats.minBatchTime = 1e9;
    impl_->stats.maxBatchTime = 0.0;
}

std::string GPUBackend::getPerformanceReport() const {
    std::stringstream ss;
    ss << "========== GPU Performance Report ==========\n";
    ss << "Forward Calls: " << impl_->stats.forwardCount.load() << "\n";
    ss << "Avg Forward Time: " << std::fixed << std::setprecision(2) 
       << getAverageForwardTime() << " ms\n";
    ss << "Batch Calls: " << impl_->stats.batchCount.load() << "\n";
    ss << "Avg Batch Time: " << std::fixed << std::setprecision(2)
       << getAverageBatchTime() << " ms\n";
    ss << "============================================";
    return ss.str();
}

// ========== æ··åˆç²¾åº¦ ==========

void GPUBackend::setUseFP16(bool useFP16) {
    impl_->execConfig.useFP16 = useFP16;
    CLLM_INFO("[GPUBackend] FP16 %s", useFP16 ? "enabled" : "disabled");
}

bool GPUBackend::getUseFP16() const {
    return impl_->execConfig.useFP16;
}

// ========== é«˜çº§åŠŸèƒ½ ==========

void GPUBackend::warmup() {
    if (impl_->ggmlBackend) {
        // æ‰§è¡Œä¸€æ¬¡ç®€å•çš„æ¨ç†æ¥é¢„çƒ­ GPU
        std::vector<int32_t> dummyInput = {0};
        forward(dummyInput, -1);
    }
}

bool GPUBackend::isAvailable() const {
    return impl_->ggmlBackend != nullptr;
}

std::string GPUBackend::getGPUInfo() const {
    if (impl_->ggmlBackend) {
        return impl_->ggmlBackend->getGPUInfo();
    }
    return "GPU Backend (GGML) - Not initialized";
}

} // namespace kylin
} // namespace cllm
