/**
 * @file cpu_backend.cpp
 * @brief CPU è®¡ç®—åç«¯å®ç° - å®Œæ•´ç‰ˆæœ¬
 * 
 * ä» transformer.cpp è¿ç§»çš„ CPU è®¡ç®—é€»è¾‘
 * æ”¯æŒ BF16/F32/FP16/INT8 æƒé‡
 */

#include "cllm/kylin/backend/cpu_backend.h"
#include "cllm/kylin/core/ggml_kernels.h"
#include "cllm/common/logger.h"

#include <cmath>
#include <algorithm>
#include <cstring>

#ifdef _OPENMP
#include <omp.h>
#endif

// SIMD æ”¯æŒ
#if defined(__ARM_NEON) || defined(__aarch64__)
    #define USE_NEON 1
    #include <arm_neon.h>
#endif

namespace cllm {
namespace kylin {

// å†…éƒ¨å®ç°ç»“æ„
struct CPUBackendImpl {
    // æ¨¡å‹é…ç½®
    HFModelConfig config;
    
    // æƒé‡æ•°æ® - F32 æ ¼å¼ï¼ˆé¢„è½¬æ¢åï¼‰
    struct LayerWeights {
        std::vector<float> inputLayernorm;
        std::vector<float> qProj;
        std::vector<float> kProj;
        std::vector<float> vProj;
        std::vector<float> oProj;
        std::vector<float> qNorm;
        std::vector<float> kNorm;
        std::vector<float> postAttentionLayernorm;
        std::vector<float> gateProj;
        std::vector<float> upProj;
        std::vector<float> downProj;
    };
    std::vector<LayerWeights> layers;
    
    // å…¨å±€æƒé‡
    std::vector<float> embedTokens;
    std::vector<float> lmHeadWeight;
    std::vector<float> finalNormWeight;
    
    // KV Cache ç®¡ç†
    struct KVCache {
        std::vector<float> kCache;
        std::vector<float> vCache;
        int currentLen = 0;
    };
    std::unordered_map<int, KVCache> kvCaches;
    
    // å·¥ä½œç¼“å†²åŒºï¼ˆé¢„åˆ†é…ï¼‰
    std::vector<float> hiddenStates;
    std::vector<float> residual;
    std::vector<float> normOutput;
    std::vector<float> attnOutput;
    std::vector<float> ffnOutput;
    std::vector<float> qkvBuffer;
    std::vector<float> qBuffer;
    std::vector<float> kBuffer;
    std::vector<float> vBuffer;
    std::vector<float> attnScores;
    std::vector<float> attnOutBuffer;
    std::vector<float> gateBuffer;
    std::vector<float> upBuffer;
    std::vector<float> gateUpBuffer;
    std::vector<float> logitsBuffer;
    std::vector<float> normWeightBuffer;
    std::vector<float> qkNormBuffer;
    
    // RoPE é¢‘ç‡
    std::vector<float> ropeFreqsCos;
    std::vector<float> ropeFreqsSin;
    
    // å¸¸é‡
    static constexpr int kMaxSeqLen = 4096;
    
    void allocateBuffers() {
        int hiddenSize = config.hiddenSize;
        int intermediateSize = config.intermediateSize;
        int numHeads = config.numAttentionHeads;
        int numKVHeads = config.getNumKVHeads();
        int headDim = config.getHeadDim();
        int maxSeqLen = kMaxSeqLen;
        
        hiddenStates.resize(hiddenSize);
        residual.resize(hiddenSize);
        normOutput.resize(hiddenSize);
        attnOutput.resize(hiddenSize);
        ffnOutput.resize(hiddenSize);
        
        int qSize = numHeads * headDim;
        int kvSize = numKVHeads * headDim;
        qkvBuffer.resize(qSize + 2 * kvSize);
        qBuffer.resize(qSize);
        kBuffer.resize(kvSize);
        vBuffer.resize(kvSize);
        attnScores.resize(numHeads * maxSeqLen);
        attnOutBuffer.resize(qSize);
        
        gateBuffer.resize(intermediateSize);
        upBuffer.resize(intermediateSize);
        gateUpBuffer.resize(intermediateSize * 2);
        logitsBuffer.resize(config.vocabSize);
        normWeightBuffer.resize(hiddenSize);
        qkNormBuffer.resize(headDim);
    }
    
    void precomputeRoPE() {
        int headDim = config.getHeadDim();
        ropeFreqsCos.resize(kMaxSeqLen * headDim / 2);
        ropeFreqsSin.resize(kMaxSeqLen * headDim / 2);
        
        for (int pos = 0; pos < kMaxSeqLen; ++pos) {
            for (int i = 0; i < headDim / 2; ++i) {
                float freq = 1.0f / std::pow(config.ropeTheta, 2.0f * i / headDim);
                float angle = pos * freq;
                ropeFreqsCos[pos * headDim / 2 + i] = std::cos(angle);
                ropeFreqsSin[pos * headDim / 2 + i] = std::sin(angle);
            }
        }
    }
    
    // è·å–æˆ–åˆ›å»º KV Cache
    KVCache& getOrCreateKVCache(int requestId) {
        auto it = kvCaches.find(requestId);
        if (it == kvCaches.end()) {
            int numLayers = config.numHiddenLayers;
            int numKVHeads = config.getNumKVHeads();
            int headDim = config.getHeadDim();
            size_t kvSize = static_cast<size_t>(numLayers) * kMaxSeqLen * numKVHeads * headDim;
            
            KVCache cache;
            cache.kCache.resize(kvSize, 0.0f);
            cache.vCache.resize(kvSize, 0.0f);
            cache.currentLen = 0;
            kvCaches[requestId] = std::move(cache);
            return kvCaches[requestId];
        }
        return it->second;
    }
    
    // RMS Norm
    void rmsNorm(const float* input, const float* weight, float* output, int size, float eps) {
        ggml_kernels::rms_norm(input, weight, output, size, eps);
    }
    
    // çŸ©é˜µä¹˜æ³• F32
    void matmulF32(const float* weight, const float* input, float* output, int outFeatures, int inFeatures) {
        ggml_kernels::matmul_f32(weight, input, output, outFeatures, inFeatures);
    }
    
    // çŸ©é˜µä¹˜æ³• BF16
    void matmulBF16(const uint16_t* weight, const float* input, float* output, int outFeatures, int inFeatures) {
        // å…ˆå°† BF16 æƒé‡è½¬æ¢ä¸º F32ï¼Œç„¶ååšçŸ©é˜µä¹˜æ³•
        std::vector<float> weightF32(outFeatures * inFeatures);
        ggml_kernels::convert_bf16_to_f32(weight, weightF32.data(), outFeatures * inFeatures);
        matmulF32(weightF32.data(), input, output, outFeatures, inFeatures);
    }
    
    // åº”ç”¨ RoPE
    void applyRoPE(float* q, float* k, int headDim, int nHeads, int nKVHeads, int seqLen, int startPos) {
        const int halfDim = headDim / 2;
        
        for (int pos = 0; pos < seqLen; ++pos) {
            const int actualPos = startPos + pos;
            const float* cosPtr = ropeFreqsCos.data() + actualPos * halfDim;
            const float* sinPtr = ropeFreqsSin.data() + actualPos * halfDim;
            
            // Q å¤´
            for (int h = 0; h < nHeads; ++h) {
                float* head = q + h * headDim;
                for (int i = 0; i < halfDim; ++i) {
                    float x0 = head[i];
                    float x1 = head[i + halfDim];
                    float cos = cosPtr[i];
                    float sin = sinPtr[i];
                    head[i] = x0 * cos - x1 * sin;
                    head[i + halfDim] = x0 * sin + x1 * cos;
                }
            }
            
            // K å¤´
            for (int h = 0; h < nKVHeads; ++h) {
                float* head = k + h * headDim;
                for (int i = 0; i < halfDim; ++i) {
                    float x0 = head[i];
                    float x1 = head[i + halfDim];
                    float cos = cosPtr[i];
                    float sin = sinPtr[i];
                    head[i] = x0 * cos - x1 * sin;
                    head[i + halfDim] = x0 * sin + x1 * cos;
                }
            }
        }
    }
    
    // Softmax
    void softmax(float* scores, int size) {
        float maxVal = scores[0];
        for (int i = 1; i < size; ++i) {
            if (scores[i] > maxVal) maxVal = scores[i];
        }
        
        float sum = 0.0f;
        for (int i = 0; i < size; ++i) {
            scores[i] = std::exp(scores[i] - maxVal);
            sum += scores[i];
        }
        
        for (int i = 0; i < size; ++i) {
            scores[i] /= sum;
        }
    }
    
    // SiLU æ¿€æ´»å‡½æ•°
    float silu(float x) {
        return x / (1.0f + std::exp(-x));
    }
    
    // Embedding
    void embedding(int tokenId, float* output) {
        if (tokenId < 0 || tokenId >= config.vocabSize) {
            std::fill(output, output + config.hiddenSize, 0.0f);
            return;
        }
        const float* embRow = embedTokens.data() + tokenId * config.hiddenSize;
        std::copy(embRow, embRow + config.hiddenSize, output);
    }
    
    // LM Head
    void lmHead(const float* input, float* output) {
        matmulF32(lmHeadWeight.data(), input, output, config.vocabSize, config.hiddenSize);
    }
    
    // å•å±‚çš„ Attention è®¡ç®—
    void attention(int layerIdx, const float* input, float* output, int seqLen, int startPos, KVCache& kvCache) {
        const LayerWeights& layer = layers[layerIdx];
        const int headDim = config.getHeadDim();
        const int nHeads = config.numAttentionHeads;
        const int nKVHeads = config.getNumKVHeads();
        const int qSize = nHeads * headDim;
        const int kvSize = nKVHeads * headDim;
        
        float* q = qBuffer.data();
        float* k = kBuffer.data();
        float* v = vBuffer.data();
        
        // QKV æŠ•å½±
        matmulF32(layer.qProj.data(), input, q, qSize, config.hiddenSize);
        matmulF32(layer.kProj.data(), input, k, kvSize, config.hiddenSize);
        matmulF32(layer.vProj.data(), input, v, kvSize, config.hiddenSize);
        
        // Q/K Normï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if (!layer.qNorm.empty()) {
            for (int h = 0; h < nHeads; ++h) {
                float* qHead = q + h * headDim;
                float sumSq = ggml_kernels::dot_product(qHead, qHead, headDim);
                float invRms = 1.0f / std::sqrt(sumSq / headDim + config.rmsNormEps);
                for (int i = 0; i < headDim; ++i) {
                    qHead[i] = qHead[i] * invRms * layer.qNorm[i];
                }
            }
        }
        if (!layer.kNorm.empty()) {
            for (int h = 0; h < nKVHeads; ++h) {
                float* kHead = k + h * headDim;
                float sumSq = ggml_kernels::dot_product(kHead, kHead, headDim);
                float invRms = 1.0f / std::sqrt(sumSq / headDim + config.rmsNormEps);
                for (int i = 0; i < headDim; ++i) {
                    kHead[i] = kHead[i] * invRms * layer.kNorm[i];
                }
            }
        }
        
        // åº”ç”¨ RoPE
        applyRoPE(q, k, headDim, nHeads, nKVHeads, seqLen, startPos);
        
        // å†™å…¥ KV Cache
        int numLayers = config.numHiddenLayers;
        size_t layerOffset = static_cast<size_t>(layerIdx) * kMaxSeqLen * nKVHeads * headDim;
        size_t posOffset = static_cast<size_t>(startPos) * nKVHeads * headDim;
        
        float* kCacheLayer = kvCache.kCache.data() + layerOffset;
        float* vCacheLayer = kvCache.vCache.data() + layerOffset;
        
        std::memcpy(kCacheLayer + posOffset, k, kvSize * sizeof(float));
        std::memcpy(vCacheLayer + posOffset, v, kvSize * sizeof(float));
        
        // Attention è®¡ç®—
        const int kvLen = startPos + seqLen;
        const float scale = 1.0f / std::sqrt(static_cast<float>(headDim));
        const int gqa = nHeads / nKVHeads;
        const int vStride = nKVHeads * headDim;
        
        float* attnOut = attnOutBuffer.data();
        std::fill(attnOut, attnOut + qSize, 0.0f);
        
        // å¹¶è¡Œå¤„ç†æ¯ä¸ª attention head
        #pragma omp parallel for schedule(static) if(nHeads >= 4)
        for (int h = 0; h < nHeads; ++h) {
            const int kvHead = h / gqa;
            const float* qHead = q + h * headDim;
            float* localScores = attnScores.data() + h * kMaxSeqLen;
            
            // è®¡ç®— attention scores + softmax
            float maxScore = -1e30f;
            for (int t = 0; t < kvLen; ++t) {
                const float* kRow = kCacheLayer + t * vStride + kvHead * headDim;
                float dot = ggml_kernels::dot_product(qHead, kRow, headDim) * scale;
                localScores[t] = dot;
                maxScore = (dot > maxScore) ? dot : maxScore;
            }
            
            float sumExp = 0.0f;
            for (int t = 0; t < kvLen; ++t) {
                float e = std::exp(localScores[t] - maxScore);
                localScores[t] = e;
                sumExp += e;
            }
            const float invSum = 1.0f / sumExp;
            for (int t = 0; t < kvLen; ++t) {
                localScores[t] *= invSum;
            }
            
            // Weighted sum of V
            float* outHead = attnOut + h * headDim;
            std::memset(outHead, 0, headDim * sizeof(float));
            
            for (int t = 0; t < kvLen; ++t) {
                const float* vRow = vCacheLayer + t * vStride + kvHead * headDim;
                const float weight = localScores[t];
                for (int d = 0; d < headDim; ++d) {
                    outHead[d] += weight * vRow[d];
                }
            }
        }
        
        // Output æŠ•å½±
        matmulF32(layer.oProj.data(), attnOut, output, config.hiddenSize, qSize);
    }
    
    // å•å±‚çš„ FFN è®¡ç®—
    void ffn(int layerIdx, const float* input, float* output) {
        const LayerWeights& layer = layers[layerIdx];
        
        // Gate å’Œ Up æŠ•å½±
        matmulF32(layer.gateProj.data(), input, gateBuffer.data(), config.intermediateSize, config.hiddenSize);
        matmulF32(layer.upProj.data(), input, upBuffer.data(), config.intermediateSize, config.hiddenSize);
        
        // DEBUG: æ‰“å° Gate å’Œ Up å€¼
        if (layerIdx == 0) {
            CLLM_DEBUG_CPU("[CPU DEBUG] FFN Gate - first 5=[%.6f, %.6f, %.6f, %.6f, %.6f]",
                      gateBuffer[0], gateBuffer[1], gateBuffer[2], gateBuffer[3], gateBuffer[4]);
            CLLM_DEBUG_CPU("[CPU DEBUG] FFN Up - first 5=[%.6f, %.6f, %.6f, %.6f, %.6f]",
                      upBuffer[0], upBuffer[1], upBuffer[2], upBuffer[3], upBuffer[4]);
        }
        
        // SiLU(gate) * up
        for (int i = 0; i < config.intermediateSize; ++i) {
            gateBuffer[i] = silu(gateBuffer[i]) * upBuffer[i];
        }
        
        // DEBUG: æ‰“å° SiLU(gate) * up å€¼
        if (layerIdx == 0) {
            CLLM_DEBUG_CPU("[CPU DEBUG] FFN Gate*Up - first 5=[%.6f, %.6f, %.6f, %.6f, %.6f]",
                      gateBuffer[0], gateBuffer[1], gateBuffer[2], gateBuffer[3], gateBuffer[4]);
        }
        
        // Down æŠ•å½±
        matmulF32(layer.downProj.data(), gateBuffer.data(), output, config.hiddenSize, config.intermediateSize);
        
        // DEBUG: æ‰“å° Down è¾“å‡ºå€¼
        if (layerIdx == 0) {
            CLLM_DEBUG_CPU("[CPU DEBUG] FFN Down - first 5=[%.6f, %.6f, %.6f, %.6f, %.6f]",
                      output[0], output[1], output[2], output[3], output[4]);
        }
    }
};

CPUBackend::CPUBackend() = default;

CPUBackend::~CPUBackend() {
    shutdown();
}

bool CPUBackend::initialize(const HFModelConfig& config) {
    config_ = config;
    
    impl_ = std::make_unique<CPUBackendImpl>();
    impl_->config = config;
    impl_->allocateBuffers();
    impl_->precomputeRoPE();
    
    initialized_ = true;
    CLLM_INFO("[CPUBackend] Initialized with hidden_size=%d, num_layers=%d",
              config.hiddenSize, config.numHiddenLayers);
    return true;
}

void CPUBackend::shutdown() {
    impl_.reset();
    initialized_ = false;
    weightsLoaded_ = false;
    CLLM_INFO("[CPUBackend] Shutdown");
}

bool CPUBackend::loadWeights(const ModelWeights& weights) {
    if (!impl_) {
        CLLM_ERROR("[CPUBackend] Not initialized");
        return false;
    }
    
    weights_ = weights;
    
    // è½¬æ¢å¹¶åŠ è½½åµŒå…¥æƒé‡
    if (weights.embedTokens) {
        impl_->embedTokens.resize(config_.vocabSize * config_.hiddenSize);
        ggml_kernels::convert_bf16_to_f32(
            static_cast<const uint16_t*>(weights.embedTokens),
            impl_->embedTokens.data(),
            config_.vocabSize * config_.hiddenSize
        );
    }
    
    // è½¬æ¢å¹¶åŠ è½½ LM Head æƒé‡
    if (weights.lmHeadWeight) {
        impl_->lmHeadWeight.resize(config_.vocabSize * config_.hiddenSize);
        ggml_kernels::convert_bf16_to_f32(
            static_cast<const uint16_t*>(weights.lmHeadWeight),
            impl_->lmHeadWeight.data(),
            config_.vocabSize * config_.hiddenSize
        );
    }
    
    // è½¬æ¢å¹¶åŠ è½½ Final Norm æƒé‡
    if (weights.finalNormWeight) {
        impl_->finalNormWeight.resize(config_.hiddenSize);
        ggml_kernels::convert_bf16_to_f32(
            static_cast<const uint16_t*>(weights.finalNormWeight),
            impl_->finalNormWeight.data(),
            config_.hiddenSize
        );
    }
    
    // è½¬æ¢å¹¶åŠ è½½æ¯å±‚æƒé‡
    int numLayers = config_.numHiddenLayers;
    impl_->layers.resize(numLayers);
    
    for (int i = 0; i < numLayers && i < static_cast<int>(weights.layers.size()); ++i) {
        const auto& srcLayer = weights.layers[i];
        auto& dstLayer = impl_->layers[i];
        
        int hiddenSize = config_.hiddenSize;
        int intermediateSize = config_.intermediateSize;
        int numHeads = config_.numAttentionHeads;
        int numKVHeads = config_.getNumKVHeads();
        int headDim = config_.getHeadDim();
        
        // Input LayerNorm
        if (srcLayer.inputLayernorm) {
            dstLayer.inputLayernorm.resize(hiddenSize);
            ggml_kernels::convert_bf16_to_f32(
                static_cast<const uint16_t*>(srcLayer.inputLayernorm),
                dstLayer.inputLayernorm.data(),
                hiddenSize
            );
        }
        
        // Q, K, V, O Proj
        if (srcLayer.qProj) {
            dstLayer.qProj.resize(numHeads * headDim * hiddenSize);
            ggml_kernels::convert_bf16_to_f32(
                static_cast<const uint16_t*>(srcLayer.qProj),
                dstLayer.qProj.data(),
                numHeads * headDim * hiddenSize
            );
        }
        
        if (srcLayer.kProj) {
            dstLayer.kProj.resize(numKVHeads * headDim * hiddenSize);
            ggml_kernels::convert_bf16_to_f32(
                static_cast<const uint16_t*>(srcLayer.kProj),
                dstLayer.kProj.data(),
                numKVHeads * headDim * hiddenSize
            );
        }
        
        if (srcLayer.vProj) {
            dstLayer.vProj.resize(numKVHeads * headDim * hiddenSize);
            ggml_kernels::convert_bf16_to_f32(
                static_cast<const uint16_t*>(srcLayer.vProj),
                dstLayer.vProj.data(),
                numKVHeads * headDim * hiddenSize
            );
        }
        
        if (srcLayer.oProj) {
            dstLayer.oProj.resize(hiddenSize * numHeads * headDim);
            ggml_kernels::convert_bf16_to_f32(
                static_cast<const uint16_t*>(srcLayer.oProj),
                dstLayer.oProj.data(),
                hiddenSize * numHeads * headDim
            );
        }
        
        // Q/K Norm (optional)
        if (srcLayer.qNorm) {
            dstLayer.qNorm.resize(headDim);
            ggml_kernels::convert_bf16_to_f32(
                static_cast<const uint16_t*>(srcLayer.qNorm),
                dstLayer.qNorm.data(),
                headDim
            );
        }
        
        if (srcLayer.kNorm) {
            dstLayer.kNorm.resize(headDim);
            ggml_kernels::convert_bf16_to_f32(
                static_cast<const uint16_t*>(srcLayer.kNorm),
                dstLayer.kNorm.data(),
                headDim
            );
        }
        
        // Post-Attention LayerNorm
        if (srcLayer.postAttentionLayernorm) {
            dstLayer.postAttentionLayernorm.resize(hiddenSize);
            ggml_kernels::convert_bf16_to_f32(
                static_cast<const uint16_t*>(srcLayer.postAttentionLayernorm),
                dstLayer.postAttentionLayernorm.data(),
                hiddenSize
            );
        }
        
        // FFN weights
        if (srcLayer.gateProj) {
            dstLayer.gateProj.resize(intermediateSize * hiddenSize);
            ggml_kernels::convert_bf16_to_f32(
                static_cast<const uint16_t*>(srcLayer.gateProj),
                dstLayer.gateProj.data(),
                intermediateSize * hiddenSize
            );
        }
        
        if (srcLayer.upProj) {
            dstLayer.upProj.resize(intermediateSize * hiddenSize);
            ggml_kernels::convert_bf16_to_f32(
                static_cast<const uint16_t*>(srcLayer.upProj),
                dstLayer.upProj.data(),
                intermediateSize * hiddenSize
            );
        }
        
        if (srcLayer.downProj) {
            dstLayer.downProj.resize(hiddenSize * intermediateSize);
            ggml_kernels::convert_bf16_to_f32(
                static_cast<const uint16_t*>(srcLayer.downProj),
                dstLayer.downProj.data(),
                hiddenSize * intermediateSize
            );
        }
    }
    
    weightsLoaded_ = true;
    CLLM_INFO("[CPUBackend] Weights loaded successfully (%d layers)", numLayers);
    return true;
}

std::vector<float> CPUBackend::forward(
    const std::vector<int32_t>& inputIds,
    int requestId
) {
    if (!initialized_ || !impl_) {
        CLLM_ERROR("[CPUBackend] Not initialized");
        return {};
    }
    
    if (!weightsLoaded_) {
        CLLM_ERROR("[CPUBackend] Weights not loaded");
        return {};
    }
    
    // è·å– KV Cache
    auto& kvCache = impl_->getOrCreateKVCache(requestId);
    int startPos = kvCache.currentLen;
    
    // ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šæ”¯æŒå¤š token è¾“å…¥ï¼ˆæ‰¹é‡ prefillï¼‰
    // å½“ inputIds.size() > 1 æ—¶ï¼Œé€ä¸ªå¤„ç†æ‰€æœ‰ tokens
    // ä½†åªè¿”å›æœ€åä¸€ä¸ª token çš„ logits
    if (inputIds.empty()) {
        CLLM_ERROR("[CPUBackend] Empty input");
        return {};
    }
    
    // é€ä¸ªå¤„ç†æ‰€æœ‰ input tokens
    for (size_t tokenIdx = 0; tokenIdx < inputIds.size(); ++tokenIdx) {
        int tokenId = inputIds[tokenIdx];
        int currentPos = startPos + static_cast<int>(tokenIdx);
        
        // 1. Embedding
        impl_->embedding(tokenId, impl_->hiddenStates.data());

    // DEBUG: æ‰“å° Embedding è¾“å‡º
    CLLM_DEBUG_CPU("[CPU DEBUG] Embedding - first 5=[%.6f, %.6f, %.6f, %.6f, %.6f]",
              impl_->hiddenStates[0], impl_->hiddenStates[1], impl_->hiddenStates[2],
              impl_->hiddenStates[3], impl_->hiddenStates[4]);

    // ä¿å­˜æ®‹å·®
    std::copy(impl_->hiddenStates.begin(), impl_->hiddenStates.end(), impl_->residual.begin());
    
    // 2. Transformer å±‚
    for (int layerIdx = 0; layerIdx < impl_->config.numHiddenLayers; ++layerIdx) {
        const auto& layer = impl_->layers[layerIdx];
        
        // 2.1 RMS Norm
        impl_->rmsNorm(impl_->hiddenStates.data(), layer.inputLayernorm.data(),
                       impl_->normOutput.data(), impl_->config.hiddenSize, impl_->config.rmsNormEps);

        // DEBUG: æ‰“å° Attention Input (RMS Norm åçš„è¾“å‡º)
        if (layerIdx == 0) {
            CLLM_DEBUG_CPU("[CPU DEBUG] Attention Input - first 5=[%.6f, %.6f, %.6f, %.6f, %.6f]",
                      impl_->normOutput[0], impl_->normOutput[1], impl_->normOutput[2],
                      impl_->normOutput[3], impl_->normOutput[4]);
        }

        // 2.2 Attention - ä½¿ç”¨ currentPos ä½œä¸ºå½“å‰ä½ç½®
        impl_->attention(layerIdx, impl_->normOutput.data(), impl_->attnOutput.data(), 1, currentPos, kvCache);

        // DEBUG: æ‰“å° Attention è¾“å‡º
        if (layerIdx == 0) {
            CLLM_DEBUG_CPU("[CPU DEBUG] Attention Output - first 5=[%.6f, %.6f, %.6f, %.6f, %.6f]",
                      impl_->attnOutput[0], impl_->attnOutput[1], impl_->attnOutput[2],
                      impl_->attnOutput[3], impl_->attnOutput[4]);
        }

        // 2.3 æ®‹å·®è¿æ¥
        ggml_kernels::vector_add(impl_->residual.data(), impl_->attnOutput.data(),
                                 impl_->hiddenStates.data(), impl_->config.hiddenSize);

        // DEBUG: æ‰“å° FFN Input (Attention + Residual åçš„è¾“å‡º)
        if (layerIdx == 0) {
            CLLM_DEBUG_CPU("[CPU DEBUG] FFN Input - first 5=[%.6f, %.6f, %.6f, %.6f, %.6f]",
                      impl_->hiddenStates[0], impl_->hiddenStates[1], impl_->hiddenStates[2],
                      impl_->hiddenStates[3], impl_->hiddenStates[4]);
        }

        // ä¿å­˜æ®‹å·®
        std::copy(impl_->hiddenStates.begin(), impl_->hiddenStates.end(), impl_->residual.begin());

        // 2.4 Post-Attention RMS Norm
        impl_->rmsNorm(impl_->hiddenStates.data(), layer.postAttentionLayernorm.data(),
                       impl_->normOutput.data(), impl_->config.hiddenSize, impl_->config.rmsNormEps);

        // DEBUG: æ‰“å° Post-Attention RMS Norm è¾“å‡º
        if (layerIdx == 0) {
            CLLM_DEBUG_CPU("[CPU DEBUG] Post Attention RMS Norm - first 5=[%.6f, %.6f, %.6f, %.6f, %.6f]",
                      impl_->normOutput[0], impl_->normOutput[1], impl_->normOutput[2],
                      impl_->normOutput[3], impl_->normOutput[4]);
        }

        // 2.5 FFN
        impl_->ffn(layerIdx, impl_->normOutput.data(), impl_->ffnOutput.data());
        
        // 2.6 æ®‹å·®è¿æ¥
        ggml_kernels::vector_add(impl_->residual.data(), impl_->ffnOutput.data(),
                                 impl_->hiddenStates.data(), impl_->config.hiddenSize);

        // DEBUG: æ‰“å° Layer 0 Output
        if (layerIdx == 0) {
            CLLM_DEBUG_CPU("[CPU DEBUG] Layer 0 Output - first 5=[%.6f, %.6f, %.6f, %.6f, %.6f]",
                      impl_->hiddenStates[0], impl_->hiddenStates[1], impl_->hiddenStates[2],
                      impl_->hiddenStates[3], impl_->hiddenStates[4]);
        }

        // DEBUG: æ‰“å° Layer 1 å…³é”®æ­¥éª¤
        if (layerIdx == 1) {
            CLLM_DEBUG_CPU("[CPU DEBUG] Layer 1 Attention Input - first 5=[%.6f, %.6f, %.6f, %.6f, %.6f]",
                      impl_->normOutput[0], impl_->normOutput[1], impl_->normOutput[2],
                      impl_->normOutput[3], impl_->normOutput[4]);
            CLLM_DEBUG_CPU("[CPU DEBUG] Layer 1 Attention Output - first 5=[%.6f, %.6f, %.6f, %.6f, %.6f]",
                      impl_->attnOutput[0], impl_->attnOutput[1], impl_->attnOutput[2],
                      impl_->attnOutput[3], impl_->attnOutput[4]);
            CLLM_DEBUG_CPU("[CPU DEBUG] Layer 1 FFN Input - first 5=[%.6f, %.6f, %.6f, %.6f, %.6f]",
                      impl_->hiddenStates[0], impl_->hiddenStates[1], impl_->hiddenStates[2],
                      impl_->hiddenStates[3], impl_->hiddenStates[4]);
            CLLM_DEBUG_CPU("[CPU DEBUG] Layer 1 FFN Output - first 5=[%.6f, %.6f, %.6f, %.6f, %.6f]",
                      impl_->ffnOutput[0], impl_->ffnOutput[1], impl_->ffnOutput[2],
                      impl_->ffnOutput[3], impl_->ffnOutput[4]);
            CLLM_DEBUG_CPU("[CPU DEBUG] Layer 1 Output - first 5=[%.6f, %.6f, %.6f, %.6f, %.6f]",
                      impl_->hiddenStates[0], impl_->hiddenStates[1], impl_->hiddenStates[2],
                      impl_->hiddenStates[3], impl_->hiddenStates[4]);
        }

        // ä¿å­˜æ®‹å·®ç”¨äºä¸‹ä¸€å±‚
        std::copy(impl_->hiddenStates.begin(), impl_->hiddenStates.end(), impl_->residual.begin());
    }
    
        // 3. Final RMS Norm
        impl_->rmsNorm(impl_->hiddenStates.data(), impl_->finalNormWeight.data(),
                       impl_->normOutput.data(), impl_->config.hiddenSize, impl_->config.rmsNormEps);
        
        // 4. LM Head - åªåœ¨æœ€åä¸€ä¸ª token æ—¶è®¡ç®— logits
        if (tokenIdx == inputIds.size() - 1) {
            impl_->lmHead(impl_->normOutput.data(), impl_->logitsBuffer.data());
        }
        
        // æ›´æ–° KV Cache é•¿åº¦
        kvCache.currentLen = currentPos + 1;
    }
    
    return impl_->logitsBuffer;
}

std::vector<std::vector<float>> CPUBackend::forwardBatch(
    const std::vector<std::vector<int32_t>>& batchInputIds,
    const std::vector<int>& requestIds
) {
    if (!initialized_ || !impl_) {
        CLLM_ERROR("[CPUBackend] Not initialized");
        return {};
    }
    
    std::vector<std::vector<float>> results;
    results.reserve(batchInputIds.size());
    
    for (size_t i = 0; i < batchInputIds.size(); ++i) {
        results.push_back(forward(batchInputIds[i], requestIds[i]));
    }
    
    return results;
}

void CPUBackend::resetKVCache(int requestId) {
    if (!impl_) return;
    
    auto it = impl_->kvCaches.find(requestId);
    if (it != impl_->kvCaches.end()) {
        it->second.currentLen = 0;
        std::fill(it->second.kCache.begin(), it->second.kCache.end(), 0.0f);
        std::fill(it->second.vCache.begin(), it->second.vCache.end(), 0.0f);
    }
}

void CPUBackend::releaseKVCache(int requestId) {
    if (!impl_) return;
    impl_->kvCaches.erase(requestId);
}

int CPUBackend::getKVCacheCurrentLength(int requestId) const {
    if (!impl_) return 0;
    
    auto it = impl_->kvCaches.find(requestId);
    if (it != impl_->kvCaches.end()) {
        return it->second.currentLen;
    }
    return 0;
}

} // namespace kylin
} // namespace cllm
