#include "cllm/batch/manager.h"
#include "cllm/model/executor.h"
#include <algorithm>
#include <cmath>
#include <random>
#include <numeric>
#include <stdexcept>

namespace cllm {

BatchManager::BatchManager(size_t maxContextLength, size_t maxBatchSize)
    : maxContextLength_((maxContextLength != 0) ? maxContextLength : Config::instance().serverMaxContextLength())
    , maxBatchSize_((maxBatchSize != 0) ? maxBatchSize : Config::instance().serverMaxBatchSize())
    , contextUsageThreshold_(Config::instance().schedulerContextUsageThreshold())
    , executor_(nullptr)
    , lastBatchProcessingTimeMs_(0)
    , adaptiveBatchSize_(8)
    , minAdaptiveBatchSize_(1)
    , maxAdaptiveBatchSize_(64) {
}

BatchManager::BatchManager(size_t maxContextLength, size_t maxBatchSize, ModelExecutor* executor)
    : maxContextLength_((maxContextLength != 0) ? maxContextLength : Config::instance().serverMaxContextLength())
    , maxBatchSize_((maxBatchSize != 0) ? maxBatchSize : Config::instance().serverMaxBatchSize())
    , contextUsageThreshold_(Config::instance().schedulerContextUsageThreshold())
    , executor_(executor)
    , lastBatchProcessingTimeMs_(0)
    , adaptiveBatchSize_(8)
    , minAdaptiveBatchSize_(1)
    , maxAdaptiveBatchSize_(64) {
}

BatchManager::~BatchManager() {
}

std::vector<RequestState> BatchManager::formBatch(
    const std::vector<RequestState>& pendingRequests,
    const std::vector<RequestState>& runningRequests,
    size_t availableSeqIds
) {
    std::vector<RequestState> batch;
    size_t currentBatchLength = 0;
    
    // ğŸ”¥ ç“¶é¢ˆåˆ†æ: è¯¦ç»†æ—¥å¿—è®°å½•æ‰¹å¤„ç†å½¢æˆè¿‡ç¨‹
    CLLM_DEBUG("[BatchManager::formBatch] === æ‰¹å¤„ç†å½¢æˆåˆ†æ ===");
    CLLM_DEBUG("[BatchManager::formBatch] pendingRequests.size()=%zu, runningRequests.size()=%zu, availableSeqIds=%zu",
              pendingRequests.size(), runningRequests.size(), availableSeqIds);
    
    size_t runningLength = calculateRunningRequestsLength(runningRequests);
    CLLM_DEBUG("[BatchManager::formBatch] runningLength=%zu, maxContextLength_=%zu, threshold=%.1f%%",
              runningLength, maxContextLength_, (runningLength * 100.0 / maxContextLength_));
    
    // ğŸ”¥ ä¼˜åŒ–: æ”¾å®½è¿è¡Œä¸­è¯·æ±‚çš„é•¿åº¦é™åˆ¶ï¼Œå…è®¸æ›´å¤šå¹¶å‘
    // ä¹‹å‰è¿‡äºä¿å®ˆï¼Œå¯¼è‡´æ— æ³•å½¢æˆæ–°çš„æ‰¹å¤„ç†
    if (runningLength > maxContextLength_ * 0.9) {  // ä»0.75æ”¾å®½åˆ°0.9
        CLLM_DEBUG("[BatchManager] Running length (%zu) > 90%% of maxContextLength_ (%zu), skipping batch formation",
                  runningLength, maxContextLength_);
        return batch;
    }
    
    size_t avgLength = calculateAverageRequestLength(pendingRequests);
    size_t dynamicBatchSize = calculateOptimalBatchSize(pendingRequests, avgLength);
    
    // ğŸ”¥ ç¦ç”¨åŠ¨æ€æ‰¹å¤„ç†ï¼šç”±äºæ€§èƒ½ä¸¥é‡ä¸‹é™ï¼Œæš‚æ—¶ç¦ç”¨adaptiveBatchSize
    // size_t adaptiveSize = adaptiveBatchSize(pendingRequests.size(), runningRequests.size());
    // dynamicBatchSize = std::min(dynamicBatchSize, adaptiveSize);
    
    CLLM_DEBUG("[BatchManager::formBatch] avgLength=%zu, calculated dynamicBatchSize=%zu, maxBatchSize_=%zu",
              avgLength, dynamicBatchSize, maxBatchSize_);
    
    // ğŸ”¥ ä¼˜åŒ–: è€ƒè™‘åºåˆ—IDå¯ç”¨æ€§ï¼Œä½†æ›´çµæ´»
    // å…³é”®ï¼šå³ä½¿availableSeqIdsè¾ƒå°ï¼Œä¹Ÿå…è®¸å½¢æˆè¾ƒå¤§çš„æ‰¹å¤„ç†ï¼ˆå› ä¸ºåºåˆ—IDä¼šåœ¨è¯·æ±‚å®Œæˆæ—¶ç«‹å³é‡Šæ”¾ï¼‰
    if (availableSeqIds > 0) {
        // ğŸ”¥ å…³é”®ä¼˜åŒ–: ä¸è¦è¿‡åº¦é™åˆ¶æ‰¹å¤„ç†å¤§å°
        // å¦‚æœavailableSeqIdsè¾ƒå°ï¼Œå¯èƒ½æ˜¯æš‚æ—¶çš„ï¼ˆæœ‰è¯·æ±‚æ­£åœ¨å®Œæˆï¼‰ï¼Œå…è®¸ç¨å¾®è¶…è¿‡
        // åªæœ‰åœ¨availableSeqIdséå¸¸å°ï¼ˆ<4ï¼‰æ—¶æ‰é™åˆ¶
        if (availableSeqIds < 4) {
            CLLM_DEBUG("[BatchManager] availableSeqIds (%zu) < 4, limiting dynamicBatchSize to %zu",
                      availableSeqIds, availableSeqIds);
            dynamicBatchSize = std::min(dynamicBatchSize, availableSeqIds);
        } else {
            // availableSeqIds >= 4ï¼Œå…è®¸dynamicBatchSizeç¨å¾®è¶…è¿‡ï¼ˆæœ€å¤š1.5å€ï¼‰
            // å› ä¸ºåºåˆ—IDä¼šåœ¨è¯·æ±‚å®Œæˆæ—¶ç«‹å³é‡Šæ”¾
            size_t maxAllowed = static_cast<size_t>(availableSeqIds * 1.5);
            if (dynamicBatchSize > maxAllowed) {
                CLLM_DEBUG("[BatchManager] dynamicBatchSize (%zu) > maxAllowed (%zu), limiting to %zu",
                          dynamicBatchSize, maxAllowed, maxAllowed);
                dynamicBatchSize = maxAllowed;
            }
        }
    }
    
    // ğŸ”¥ ä¼˜åŒ–: æ›´æ¿€è¿›çš„æ‰¹å¤„ç†å¤§å°ç­–ç•¥ï¼Œä¼˜å…ˆè¾¾åˆ°dynamicBatchSize
    for (const auto& request : pendingRequests) {
        size_t requestLength = request.getTotalLength();
        size_t totalLength = runningLength + currentBatchLength + requestLength;
        
        // ğŸ”¥ å…³é”®ä¼˜åŒ–: éå¸¸æ¿€è¿›çš„æ‰¹å¤„ç†å½¢æˆç­–ç•¥ï¼ˆå‚è€ƒllama-benchçš„ç›´æ¥æ–¹å¼ï¼‰
        // 1. ä¼˜å…ˆè¾¾åˆ°dynamicBatchSizeï¼ˆå……åˆ†åˆ©ç”¨GPUï¼‰
        // 2. å…è®¸ä¸Šä¸‹æ–‡é•¿åº¦å¤§å¹…è¶…é™ï¼ˆæœ€å¤š50%ï¼‰
        // 3. å°æ‰¹å¤„ç†æ—¶éå¸¸å®½æ¾çš„é™åˆ¶
        bool withinContext = (totalLength <= maxContextLength_);
        bool withinBatchSize = (batch.size() < dynamicBatchSize);
        bool contextBoost = (totalLength <= maxContextLength_ * 1.5);  // å…è®¸50%è¶…é™ï¼ˆéå¸¸æ¿€è¿›ï¼‰
        bool smallBatchBoost = (batch.size() < 16 && contextBoost);  // å°æ‰¹å¤„ç†æ—¶å…è®¸50%è¶…é™
        
        // ğŸ”¥ å…³é”®: åªè¦åœ¨æ‰¹å¤„ç†å¤§å°é™åˆ¶å†…ï¼Œå°±å…è®¸åŠ å…¥ï¼ˆå³ä½¿ä¸Šä¸‹æ–‡é•¿åº¦è¶…é™ï¼‰
        if (withinBatchSize && (withinContext || contextBoost)) {
            batch.push_back(request);
            currentBatchLength += requestLength;
            CLLM_DEBUG("[BatchManager] âœ“ Added request to batch: batchSize=%zu, requestLength=%zu, totalLength=%zu/%zu, dynamicBatchSize=%zu",
                      batch.size(), requestLength, totalLength, maxContextLength_, dynamicBatchSize);
        } else {
            CLLM_DEBUG("[BatchManager] âœ— Stopped adding requests: batchSize=%zu, totalLength=%zu/%zu, dynamicBatchSize=%zu",
                      batch.size(), totalLength, maxContextLength_, dynamicBatchSize);
            CLLM_DEBUG("[BatchManager]   åŸå› : withinBatchSize=%d, withinContext=%d, contextBoost=%d",
                      withinBatchSize, withinContext, contextBoost);
            break;
        }
    }
    
    CLLM_DEBUG("[BatchManager::formBatch] === æ‰¹å¤„ç†å½¢æˆå®Œæˆ: size=%zu, totalLength=%zu ===",
              batch.size(), currentBatchLength);
    
    updateStats(batch);
    return batch;
}

std::vector<RequestState> BatchManager::formMultipleBatches(
    const std::vector<RequestState>& pendingRequests,
    const std::vector<RequestState>& runningRequests
) {
    std::vector<RequestState> allBatches;
    std::vector<RequestState> remaining = pendingRequests;
    
    size_t runningLength = calculateRunningRequestsLength(runningRequests);
    
    while (!remaining.empty()) {
        std::vector<RequestState> batch;
        size_t currentBatchLength = 0;
        
        size_t avgLength = calculateAverageRequestLength(remaining);
        size_t dynamicBatchSize = calculateOptimalBatchSize(remaining, avgLength);
        
        std::vector<size_t> usedIndices;
        
        for (size_t i = 0; i < remaining.size(); ++i) {
            const auto& request = remaining[i];
            size_t requestLength = request.getTotalLength();
            size_t totalLength = runningLength + currentBatchLength + requestLength;
            
            if (totalLength <= maxContextLength_ && 
                batch.size() < dynamicBatchSize) {
                batch.push_back(request);
                currentBatchLength += requestLength;
                usedIndices.push_back(i);
            }
        }
        
        if (batch.empty()) {
            break;
        }
        
        allBatches.insert(allBatches.end(), batch.begin(), batch.end());
        runningLength += currentBatchLength;
        
        std::vector<RequestState> newRemaining;
        for (size_t i = 0; i < remaining.size(); ++i) {
            if (std::find(usedIndices.begin(), usedIndices.end(), i) == usedIndices.end()) {
                newRemaining.push_back(remaining[i]);
            }
        }
        remaining = newRemaining;
    }
    
    return allBatches;
}

BatchInput BatchManager::prepareBatchInput(const std::vector<RequestState>& batch) {
    BatchInput input;
    input.batchSize = batch.size();
    
    // ğŸ”¥ ä¼˜åŒ–ï¼šé¢„åˆ†é…å†…å­˜ï¼Œå‡å°‘é‡æ–°åˆ†é…å¼€é”€
    size_t totalTokens = 0;
    for (const auto& request : batch) {
        totalTokens += request.getTotalLength();
    }
    input.inputIds.reserve(totalTokens);
    input.requestPositions.reserve(batch.size());
    input.sequenceIds.reserve(batch.size());
    
    size_t currentPos = 0;
    
    for (const auto& request : batch) {
        // ğŸ”¥ ä¼˜åŒ–ï¼šå¯¹äºå•tokenç”Ÿæˆåœºæ™¯ï¼Œåªä¼ å…¥æ–°tokenï¼ˆå¢é‡ç”Ÿæˆï¼‰
        // è¿™æ ·å¯ä»¥é¿å…é‡æ–°æ„å»ºæ•´ä¸ªinputIdsï¼ˆåŒ…æ‹¬promptå’Œæ‰€æœ‰å·²ç”Ÿæˆçš„tokensï¼‰
        size_t promptSize = request.tokenizedPrompt.size();
        size_t generatedSize = request.generatedTokens.size();
        
        // ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šå¦‚æœgeneratedTokensä¸ä¸ºç©ºï¼Œè¯´æ˜è¿™æ˜¯å¢é‡ç”Ÿæˆï¼Œåªä¼ å…¥æœ€åä¸€ä¸ªtoken
        // llama.cppæ”¯æŒå¢é‡æ¨ç†ï¼Œåªéœ€è¦ä¼ å…¥æ–°tokenå³å¯
        if (generatedSize > 0) {
            // å¢é‡ç”Ÿæˆï¼šåªä¼ å…¥æœ€åä¸€ä¸ªtokenï¼ˆæ–°ç”Ÿæˆçš„tokenï¼‰
            input.inputIds.push_back(request.generatedTokens.back());
            input.requestPositions.push_back({currentPos, currentPos + 1});
        } else {
            // é¦–æ¬¡ç”Ÿæˆï¼šä¼ å…¥å®Œæ•´çš„prompt
            input.inputIds.insert(input.inputIds.end(), 
                                 request.tokenizedPrompt.begin(), 
                                 request.tokenizedPrompt.end());
            input.requestPositions.push_back({currentPos, currentPos + promptSize});
        }
        
        input.sequenceIds.push_back(request.requestId);
        currentPos = input.inputIds.size();
    }
    
    return input;
}

BatchInput BatchManager::prepareBatchInputIncremental(
    const std::vector<RequestState>& batch,
    const BatchInput& previousInput,
    const std::vector<size_t>& previousTokenCounts
) {
    // ğŸ”¥ ä¼˜åŒ–2: çœŸæ­£çš„å¢é‡è¾“å…¥å‡†å¤‡ - åªè¿½åŠ æ–°tokensï¼Œä¸é‡æ–°å¤åˆ¶å·²æœ‰tokens
    BatchInput input;
    input.batchSize = batch.size();
    
    // æ£€æŸ¥æ˜¯å¦å¯ä»¥å¢é‡æ›´æ–°
    bool canReuse = (previousInput.batchSize == batch.size()) && 
                    (previousTokenCounts.size() == batch.size());
    
    if (canReuse) {
        // éªŒè¯æ¯ä¸ªè¯·æ±‚çš„tokenæ•°é‡æ˜¯å¦åªå¢åŠ äº†
        for (size_t i = 0; i < batch.size(); ++i) {
            size_t currentTokenCount = batch[i].getTotalLength();
            if (i >= previousTokenCounts.size() || 
                currentTokenCount < previousTokenCounts[i]) {
                canReuse = false;
                break;
            }
        }
    }
    
    if (canReuse && batch.size() == 1) {
        // ğŸ”¥ å…³é”®ä¼˜åŒ–: å•è¯·æ±‚åœºæ™¯ï¼Œå¯¹äºå•tokenå¢é‡ç”Ÿæˆï¼Œç›´æ¥æ„å»ºåªåŒ…å«æ–°tokençš„BatchInput
        // è¿™æ ·å¯ä»¥é¿å…ä»previousInputæ‹·è´æ•´ä¸ªvectorï¼ˆè¿™æ˜¯æ€§èƒ½ç“¶é¢ˆï¼‰
        size_t i = 0;
        size_t currentTokenCount = batch[i].getTotalLength();
        size_t previousTokenCount = previousTokenCounts[i];
        
        if (currentTokenCount > previousTokenCount) {
            // ğŸ”¥ å•tokenæˆ–å¤štokenå¢é‡ç”Ÿæˆï¼šç›´æ¥æ„å»ºåªåŒ…å«æ–°tokençš„BatchInputï¼ˆé›¶æ‹·è´previousInputï¼‰
            // æ³¨æ„ï¼šllama.cppæ”¯æŒå¢é‡æ¨ç†ï¼Œåªéœ€è¦ä¼ å…¥æ–°tokenå³å¯
            size_t newTokensCount = currentTokenCount - previousTokenCount;
            
            input.inputIds.clear();
            input.inputIds.reserve(newTokensCount);
            
            // åªè¿½åŠ æ–°ç”Ÿæˆçš„tokensï¼ˆå¢é‡æ›´æ–°ï¼‰
            size_t promptLength = batch[i].tokenizedPrompt.size();
            size_t generatedStartIdx = previousTokenCount - promptLength;
            
            input.inputIds.insert(input.inputIds.end(),
                                 batch[i].generatedTokens.begin() + generatedStartIdx,
                                 batch[i].generatedTokens.end());
            
            input.requestPositions = {{0, newTokensCount}};  // åªæœ‰æ–°tokens
            input.sequenceIds = previousInput.sequenceIds;  // é‡ç”¨sequenceIdsï¼ˆé¿å…æ‹·è´ï¼‰
            input.batchSize = 1;
            
            #ifdef CLLM_DEBUG_MODE
            CLLM_DEBUG("[BatchManager] Incremental batch input prepared (single request, %zu new tokens, zero-copy previousInput)", newTokensCount);
            #endif
            return input;
        }
        // å¦‚æœæ²¡æœ‰æ–°å¢tokensï¼Œç»§ç»­ä¸‹é¢çš„é€»è¾‘
    }
    
    if (canReuse) {
        // ğŸ”¥ å…³é”®ä¼˜åŒ–: å¯¹äºå¤šè¯·æ±‚åœºæ™¯ï¼Œåªæ„å»ºæ–°tokensï¼Œä¸é‡ç”¨previousInput.inputIds
        // è¿™æ ·å¯ä»¥é¿å…æ‹·è´æ•´ä¸ªpreviousInput.inputIdsï¼ˆå¯èƒ½åŒ…å«å¤§é‡tokensï¼‰
        // llama.cppæ”¯æŒå¢é‡æ¨ç†ï¼Œåªéœ€è¦ä¼ å…¥æ–°tokenså³å¯
        
        // ğŸ”¥ ä¼˜åŒ–ï¼šè®¡ç®—æ€»çš„æ–°tokensæ•°é‡ï¼Œé¢„åˆ†é…å†…å­˜
        size_t totalNewTokens = 0;
        for (size_t i = 0; i < batch.size(); ++i) {
            size_t currentTokenCount = batch[i].getTotalLength();
            size_t previousTokenCount = previousTokenCounts[i];
            if (currentTokenCount > previousTokenCount) {
                totalNewTokens += (currentTokenCount - previousTokenCount);
            }
        }
        
        input.inputIds.clear();
        input.inputIds.reserve(totalNewTokens);
        input.requestPositions.clear();
        input.requestPositions.reserve(batch.size());
        input.sequenceIds = previousInput.sequenceIds;  // é‡ç”¨sequenceIdsï¼ˆé¿å…æ‹·è´ï¼‰
        
        size_t currentPos = 0;
        
        for (size_t i = 0; i < batch.size(); ++i) {
            size_t currentTokenCount = batch[i].getTotalLength();
            size_t previousTokenCount = previousTokenCounts[i];
            
            // ğŸ”¥ ä¼˜åŒ–: å¦‚æœåªæ˜¯æ–°å¢äº†tokensï¼Œåªè¿½åŠ æ–°éƒ¨åˆ†
            if (currentTokenCount > previousTokenCount) {
                // æœ‰æ–°å¢tokensï¼Œåªè¿½åŠ æ–°token
                size_t promptLength = batch[i].tokenizedPrompt.size();
                size_t generatedStartIdx = previousTokenCount - promptLength;
                
                // åªè¿½åŠ æ–°ç”Ÿæˆçš„tokensï¼ˆå¢é‡æ›´æ–°ï¼‰
                input.inputIds.insert(input.inputIds.end(),
                                     batch[i].generatedTokens.begin() + generatedStartIdx,
                                     batch[i].generatedTokens.end());
                
                // æ›´æ–°requestPositionsï¼ˆåªåŒ…å«æ–°tokensçš„ä½ç½®ï¼‰
                size_t newTokensCount = currentTokenCount - previousTokenCount;
                input.requestPositions.push_back({currentPos, currentPos + newTokensCount});
                currentPos += newTokensCount;
            } else {
                // å¦‚æœæ²¡æœ‰æ–°å¢tokensï¼ŒrequestPositionsä¸ºç©ºï¼ˆè¡¨ç¤ºè¯¥è¯·æ±‚å·²å®Œæˆæˆ–ä¸éœ€è¦å¤„ç†ï¼‰
                input.requestPositions.push_back({currentPos, currentPos});
            }
        }
        
        #ifdef CLLM_DEBUG_MODE
        CLLM_DEBUG("[BatchManager] Incremental batch input prepared (new tokens only, %zu new tokens, zero-copy previousInput)",
                  input.inputIds.size());
        #endif
    } else {
        // æ— æ³•é‡ç”¨ï¼Œå®Œæ•´é‡æ–°æ„å»º
        input = prepareBatchInput(batch);
        #ifdef CLLM_DEBUG_MODE
        CLLM_DEBUG("[BatchManager] Full batch input prepared (cannot reuse)");
        #endif
    }
    
    return input;
}

void BatchManager::processBatchOutput(
    std::vector<RequestState>& batch,
    const BatchOutput& output
) {
    for (size_t i = 0; i < batch.size(); ++i) {
        if (batch[i].isCompleted) {
            continue;
        }
        
        size_t vocabSize = executor_ ? executor_->getConfig().vocabSize : 32000;
        FloatArray requestLogits = output.getLogitsForRequest(i, vocabSize);
        
        float temperature = batch[i].temperature;
        int topK = batch[i].topK;
        float topP = batch[i].topP;
        
        int nextToken = sampler_.sample(requestLogits, temperature, topK, topP);
        
        batch[i].generatedTokens.push_back(nextToken);
        
        checkStoppingConditions(batch[i], nextToken);
    }
}

size_t BatchManager::calculateOptimalBatchSize(
    const std::vector<RequestState>& requests,
    size_t avgRequestLength
) {
    if (requests.empty()) {
        return 0;
    }
    
    // ğŸ”¥ ä¼˜åŒ–2: éå¸¸æ¿€è¿›çš„æ‰¹å¤„ç†å¤§å°è®¡ç®—ç­–ç•¥
    // ç›®æ ‡ï¼šå……åˆ†åˆ©ç”¨GPUå¹¶è¡Œèƒ½åŠ›ï¼Œå°½å¯èƒ½å½¢æˆå¤§æ‰¹å¤„ç†
    // å…³é”®ï¼šä¸è¦å› ä¸ºè¯·æ±‚é•¿åº¦è€Œè¿‡åº¦é™åˆ¶æ‰¹å¤„ç†å¤§å°
    size_t dynamicBatchSize = maxBatchSize_;
    
    // ğŸ”¥ å…³é”®ä¼˜åŒ–: æ ¹æ®å¹³å‡è¯·æ±‚é•¿åº¦è°ƒæ•´ï¼Œä½†éå¸¸æ¿€è¿›
    // å³ä½¿è¯·æ±‚è¾ƒé•¿ï¼Œä¹Ÿè¦å…è®¸éå¸¸å¤§çš„æ‰¹å¤„ç†ï¼Œå……åˆ†åˆ©ç”¨GPU
    if (avgRequestLength > 500) {
        // âš ï¸ ä¼˜åŒ–å‰: æœ€å¤š2ä¸ªè¯·æ±‚ï¼ˆè¿‡äºä¿å®ˆï¼‰
        // ğŸ”¥ ä¼˜åŒ–å: è‡³å°‘32ä¸ªè¯·æ±‚ï¼ˆéå¸¸æ¿€è¿›ï¼‰ï¼Œå……åˆ†åˆ©ç”¨GPUå¹¶è¡Œèƒ½åŠ›
        dynamicBatchSize = std::max(size_t(32), maxBatchSize_);  // è‡³å°‘32ä¸ª
        CLLM_DEBUG("[BatchManager] avgLength > 500: dynamicBatchSize = %zu (min 32, maxBatchSize_=%zu)", 
                  dynamicBatchSize, maxBatchSize_);
    } else if (avgRequestLength > 200) {
        // ğŸ”¥ ä¼˜åŒ–å: è‡³å°‘48ä¸ªè¯·æ±‚
        dynamicBatchSize = std::max(size_t(48), static_cast<size_t>(maxBatchSize_ * 2));
        CLLM_DEBUG("[BatchManager] avgLength > 200: dynamicBatchSize = %zu (min 48)", dynamicBatchSize);
    } else {
        // å°è¯·æ±‚ï¼Œå¯ä»¥ä½¿ç”¨æ›´å¤§çš„æ‰¹å¤„ç†ï¼ˆmaxBatchSize_çš„ä¸¤å€æˆ–æ›´å¤šï¼‰
        dynamicBatchSize = std::min(static_cast<size_t>(maxBatchSize_ * 3), requests.size());
        CLLM_DEBUG("[BatchManager] avgLength <= 200: dynamicBatchSize = %zu", dynamicBatchSize);
    }
    
    // ç¡®ä¿ä¸è¶…è¿‡è¯·æ±‚æ•°é‡
    dynamicBatchSize = std::min(dynamicBatchSize, requests.size());
    
    CLLM_DEBUG("[BatchManager] calculateOptimalBatchSize: avgLength=%zu, dynamicBatchSize=%zu, maxBatchSize_=%zu, requests.size()=%zu",
              avgRequestLength, dynamicBatchSize, maxBatchSize_, requests.size());
    
    return dynamicBatchSize;
}

size_t BatchManager::adaptiveBatchSize(size_t queueSize, size_t runningCount) {
    if (lastBatchProcessingTimeMs_ > 500) {
        adaptiveBatchSize_ = std::max(minAdaptiveBatchSize_, adaptiveBatchSize_ / 2);
        CLLM_DEBUG("[BatchManager::adaptiveBatchSize] Last batch processing time too long (%zu ms), reducing batch size to %zu",
                  lastBatchProcessingTimeMs_, adaptiveBatchSize_);
    } else if (lastBatchProcessingTimeMs_ < 100 && queueSize > adaptiveBatchSize_ * 2) {
        adaptiveBatchSize_ = std::min(maxAdaptiveBatchSize_, adaptiveBatchSize_ * 2);
        CLLM_DEBUG("[BatchManager::adaptiveBatchSize] Last batch processing time short (%zu ms) and queue large (%zu), increasing batch size to %zu",
                  lastBatchProcessingTimeMs_, queueSize, adaptiveBatchSize_);
    }
    
    return adaptiveBatchSize_;
}

void BatchManager::updateBatchProcessingTime(size_t processingTimeMs) {
    lastBatchProcessingTimeMs_ = processingTimeMs;
    CLLM_DEBUG("[BatchManager::updateBatchProcessingTime] Updated batch processing time to %zu ms", processingTimeMs);
}

bool BatchManager::canAddToBatch(
    const RequestState& request,
    const std::vector<RequestState>& currentBatch,
    size_t currentBatchLength,
    size_t dynamicBatchSize
) {
    if (currentBatch.size() >= dynamicBatchSize) {
        return false;
    }
    
    size_t requestLength = request.getTotalLength();
    size_t totalLength = currentBatchLength + requestLength;
    
    return totalLength <= maxContextLength_;
}

BatchStats BatchManager::getStats() const {
    return stats_;
}

void BatchManager::resetStats() {
    stats_.reset();
}

size_t BatchManager::calculateRunningRequestsLength(
    const std::vector<RequestState>& runningRequests
) {
    size_t totalLength = 0;
    for (const auto& request : runningRequests) {
        totalLength += request.getTotalLength();
    }
    return totalLength;
}

size_t BatchManager::calculateAverageRequestLength(
    const std::vector<RequestState>& requests
) {
    if (requests.empty()) {
        return 0;
    }
    
    // ğŸ”¥ ä¼˜åŒ–: ä½¿ç”¨æ›´æ¿€è¿›çš„ä¼°ç®—ï¼Œé¿å…è¿‡åº¦é™åˆ¶æ‰¹å¤„ç†å¤§å°
    // å¯¹äºå°æ‰¹å¤„ç†ï¼Œä½¿ç”¨è¾ƒå°çš„å¹³å‡å€¼ï¼›å¯¹äºå¤§æ‰¹å¤„ç†ï¼Œä½¿ç”¨è¾ƒå¤§çš„å¹³å‡å€¼
    size_t totalLength = 0;
    size_t minLength = SIZE_MAX;
    size_t maxLength = 0;
    
    for (const auto& request : requests) {
        size_t len = request.getTotalLength();
        totalLength += len;
        if (len < minLength) minLength = len;
        if (len > maxLength) maxLength = len;
    }
    
    size_t avgLength = totalLength / requests.size();
    
    // ğŸ”¥ å…³é”®ä¼˜åŒ–: å¯¹äºå°æ‰¹å¤„ç†ï¼Œä½¿ç”¨æ›´å°çš„å¹³å‡å€¼ï¼ˆé¿å…è¿‡åº¦é™åˆ¶ï¼‰
    // å¯¹äºå¤§æ‰¹å¤„ç†ï¼Œä½¿ç”¨æ›´å¤§çš„å¹³å‡å€¼ï¼ˆå……åˆ†åˆ©ç”¨GPUï¼‰
    if (requests.size() <= 4) {
        // å°æ‰¹å¤„ç†ï¼šä½¿ç”¨æœ€å°å€¼å’Œå¹³å‡å€¼çš„ä¸­é—´å€¼
        avgLength = (minLength + avgLength) / 2;
    } else {
        // å¤§æ‰¹å¤„ç†ï¼šä½¿ç”¨å¹³å‡å€¼å’Œæœ€å¤§å€¼çš„ä¸­é—´å€¼ï¼ˆæ›´æ¿€è¿›ï¼‰
        avgLength = (avgLength + maxLength) / 2;
    }
    
    return avgLength;
}

void BatchManager::updateStats(const std::vector<RequestState>& batch) {
    if (batch.empty()) {
        return;
    }
    
    size_t batchLength = 0;
    for (const auto& request : batch) {
        batchLength += request.getTotalLength();
    }
    
    stats_.update(batch.size(), batchLength);
}

void BatchManager::checkStoppingConditions(RequestState& request, int nextToken) {
    if (request.generatedTokens.size() >= static_cast<size_t>(request.maxTokens)) {
        request.isCompleted = true;
        return;
    }

    // ä½¿ç”¨è¯·æ±‚æ³¨å…¥çš„ EOS token idï¼ˆé¿å…å†™æ­» 2/0 å¯¼è‡´ä¸åŒæ¨¡å‹è¡Œä¸ºé”™è¯¯ï¼‰
    if (request.eosTokenId >= 0 && nextToken == request.eosTokenId) {
        request.isCompleted = true;
        return;
    }
}



}
