#include "cllm/scheduler/scheduler.h"
#include "cllm/common/request_state.h"
#include "cllm/common/queue.h"
#include "cllm/batch/manager.h"
#include "cllm/model/executor.h"
#include "cllm/kv_cache/cache.h"
#include "cllm/memory/monitor.h"
#include "cllm/common/logger.h"
#include "cllm/inference/llama_cpp_backend.h"
#include "cllm/scheduler/dynamic_batch_tuner.h"
#include <chrono>
#include <stdexcept>
#include <queue>
#include <algorithm>

namespace cllm {

namespace {
size_t resolveMaxBatchSize(size_t overrideMax) {
    const size_t schedulerMax = static_cast<size_t>(Config::instance().schedulerMaxBatchSize());
    const size_t resourceMax = static_cast<size_t>(Config::instance().serverMaxBatchSize());
    size_t desired = overrideMax != 0 ? overrideMax : schedulerMax;
    if (desired == 0) {
        desired = resourceMax;
    }
    if (resourceMax > 0) {
        desired = std::min(desired, resourceMax);
    }
    return desired > 0 ? desired : 1;
}
}

Scheduler::Scheduler(
    ModelExecutor* modelExecutor,
    size_t maxBatchSize,
    size_t maxContextLength
) : batchManager_(
        (maxContextLength != 0) ? maxContextLength : Config::instance().serverMaxContextLength(),
        resolveMaxBatchSize(maxBatchSize)
    ),
    maxBatchSize_(resolveMaxBatchSize(maxBatchSize)),
    maxContextLength_((maxContextLength != 0) ? maxContextLength : Config::instance().serverMaxContextLength()),
    modelExecutor_(modelExecutor),
    ownsModelExecutor_(false) {
    
    config_.maxBatchSize = maxBatchSize_;
    config_.maxContextLength = maxContextLength_;
    config_.defaultTemperature = Config::instance().schedulerDefaultTemperature();
    config_.defaultTopP = Config::instance().schedulerDefaultTopP();
    config_.defaultTopK = Config::instance().schedulerDefaultTopK();
    config_.defaultMaxTokens = Config::instance().schedulerDefaultMaxTokens();
    config_.requestTimeout = Config::instance().schedulerRequestTimeout();
    config_.schedulerLoopInterval = Config::instance().schedulerLoopInterval();
    config_.idleLoopInterval = Config::instance().schedulerIdleLoopInterval();
    config_.contextUsageThreshold = Config::instance().schedulerContextUsageThreshold();
    // ğŸ”¥ ä¿®å¤ï¼šå¢åŠ æœ€å¤§å¹¶å‘è¯·æ±‚æ•°ï¼Œæ”¯æŒ32å¹¶å‘æµ‹è¯•
    config_.maxConcurrentRequests = 64;  // ä»é»˜è®¤8å¢åŠ åˆ°64ï¼Œæ”¯æŒé«˜å¹¶å‘åœºæ™¯
    
    kvCache_ = new KVCache(
        static_cast<size_t>(Config::instance().serverKvCacheMaxSize()),
        static_cast<size_t>(Config::instance().serverKvCacheMaxMemoryMb())
    );
    
    // éªŒè¯æ¨¡å‹å·²åŠ è½½
    if (!modelExecutor_->isLoaded()) {
        throw std::runtime_error("Model executor must be pre-loaded before creating Scheduler");
    }

    enforceBackendBatchConstraints();
}

Scheduler::Scheduler(
    const std::string& modelPath,
    const std::string& quantization,
    size_t maxBatchSize,
    size_t maxContextLength
) : batchManager_(
        (maxContextLength != 0) ? maxContextLength : Config::instance().serverMaxContextLength(),
        resolveMaxBatchSize(maxBatchSize)
    ),
    maxBatchSize_(resolveMaxBatchSize(maxBatchSize)),
    maxContextLength_((maxContextLength != 0) ? maxContextLength : Config::instance().serverMaxContextLength()),
    ownsModelExecutor_(true) {
    
    config_.maxBatchSize = maxBatchSize_;
    config_.maxContextLength = maxContextLength_;
    config_.defaultTemperature = Config::instance().schedulerDefaultTemperature();
    config_.defaultTopP = Config::instance().schedulerDefaultTopP();
    config_.defaultTopK = Config::instance().schedulerDefaultTopK();
    config_.defaultMaxTokens = Config::instance().schedulerDefaultMaxTokens();
    config_.requestTimeout = Config::instance().schedulerRequestTimeout();
    config_.schedulerLoopInterval = Config::instance().schedulerLoopInterval();
    config_.idleLoopInterval = Config::instance().schedulerIdleLoopInterval();
    config_.contextUsageThreshold = Config::instance().schedulerContextUsageThreshold();
    // ğŸ”¥ ä¿®å¤ï¼šå¢åŠ æœ€å¤§å¹¶å‘è¯·æ±‚æ•°ï¼Œæ”¯æŒ32å¹¶å‘æµ‹è¯•
    config_.maxConcurrentRequests = 64;  // ä»é»˜è®¤8å¢åŠ åˆ°64ï¼Œæ”¯æŒé«˜å¹¶å‘åœºæ™¯
    
    modelExecutor_ = new ModelExecutor(modelPath, quantization);
    kvCache_ = new KVCache(
        static_cast<size_t>(Config::instance().serverKvCacheMaxSize()),
        static_cast<size_t>(Config::instance().serverKvCacheMaxMemoryMb())
    );
    
    // åŠ è½½æ¨¡å‹
    modelExecutor_->loadModel();

    enforceBackendBatchConstraints();
}

Scheduler::~Scheduler() {
    stop();
    
    delete kvCache_;
    
    // Only delete modelExecutor_ if we own it (used by tests)
    if (ownsModelExecutor_ && modelExecutor_) {
        delete modelExecutor_;
    }
}

void Scheduler::enforceBackendBatchConstraints() {
    if (!modelExecutor_) {
        return;
    }

    const std::string backendName = modelExecutor_->getBackendName();
    if (backendName == "Kylin") {
        // Kylin åç«¯ç°åœ¨æ”¯æŒ per-request KV Cacheï¼Œå¯ä»¥çœŸæ­£å¹¶å‘
        // é™åˆ¶æœ€å¤§å¹¶å‘æ•°ä¸º 16ï¼ˆä¸ KVCachePool çš„ maxSlots ä¸€è‡´ï¼‰
        static constexpr size_t kKylinMaxConcurrent = 16;
        if (maxBatchSize_ > kKylinMaxConcurrent) {
            CLLM_WARN("[Scheduler] Kylin backend limits maxBatchSize from %zu to %zu (per-request KV cache slots)",
                      maxBatchSize_, kKylinMaxConcurrent);
            maxBatchSize_ = kKylinMaxConcurrent;
            tunedMaxBatchSize_.store(kKylinMaxConcurrent, std::memory_order_relaxed);
            config_.maxBatchSize = kKylinMaxConcurrent;
            batchManager_.setMaxBatchSize(kKylinMaxConcurrent);
        } else {
            CLLM_INFO("[Scheduler] Kylin backend with per-request KV cache, maxBatchSize=%zu", maxBatchSize_);
        }
    }
}

void Scheduler::start() {
    if (running_) {
        return;
    }
    
    running_ = true;

    const auto tunerConfig = Config::instance().dynamicBatchTunerConfig();
    if (tunerConfig.enabled && tunerConfig.strategy == "static") {
        if (tunerConfig.fixedBatchSize > 0) {
            applyTunedBatchSize(static_cast<size_t>(tunerConfig.fixedBatchSize));
            CLLM_INFO("[Scheduler] ä½¿ç”¨é™æ€ batch_size=%d", tunerConfig.fixedBatchSize);
        }
    } else if (tunerConfig.enabled && tunerConfig.strategy != "static") {
        DynamicBatchTuner::TunerConfig config;
        config.minBatchSize = std::max<size_t>(1, static_cast<size_t>(tunerConfig.minBatchSize));
        config.maxBatchSize = std::min<size_t>(maxBatchSize_, static_cast<size_t>(tunerConfig.maxBatchSize));
        config.initialBatchSize = static_cast<size_t>(tunerConfig.initialBatchSize);
        config.probingGrowthFactor = tunerConfig.probingGrowthFactor;
        config.maxProbingAttempts = static_cast<size_t>(tunerConfig.maxProbingAttempts);
        config.timeIncreaseThreshold = tunerConfig.timeIncreaseThreshold;
        config.adjustmentFactor = tunerConfig.adjustmentFactor;
        config.validationInterval = static_cast<size_t>(tunerConfig.validationInterval);
        config.explorationInterval = static_cast<size_t>(tunerConfig.explorationInterval);
        config.probeBatchCount = static_cast<size_t>(tunerConfig.probeBatchCount);
        config.validationBatchCount = static_cast<size_t>(tunerConfig.validationBatchCount);
        config.autoAdjustEnabled = (tunerConfig.strategy == "dynamic");
        config.maxConsecutiveTimeIncreases = static_cast<size_t>(tunerConfig.maxConsecutiveTimeIncreases);

        if (config.maxBatchSize == 0) {
            config.maxBatchSize = maxBatchSize_;
        }
        if (config.minBatchSize > config.maxBatchSize) {
            config.minBatchSize = config.maxBatchSize;
        }
        if (config.initialBatchSize == 0) {
            config.initialBatchSize = config.minBatchSize;
        }
        config.initialBatchSize = std::max(config.minBatchSize, std::min(config.initialBatchSize, config.maxBatchSize));

        batchTuner_ = std::make_unique<DynamicBatchTuner>(config);
        batchTuner_->startPassive();
        applyTunedBatchSize(batchTuner_->getCurrentBatchSize());
        CLLM_INFO("[Scheduler] åŠ¨æ€æ‰¹å¤„ç†è°ƒè°å™¨å·²å¯ç”¨: strategy=%s, batch_size=%zu",
                  tunerConfig.strategy.c_str(), batchTuner_->getCurrentBatchSize());
    }

    schedulerThread_ = std::thread(&Scheduler::schedulerLoop, this);
    cleanupThread_ = std::thread(&Scheduler::cleanupLoop, this);
}

void Scheduler::stop() {
    if (!running_) {
        return;
    }
    
    running_ = false;

    if (batchTuner_) {
        batchTuner_->stop();
    }
    
    // é€šçŸ¥æ¸…ç†çº¿ç¨‹é€€å‡º
    cleanupCondition_.notify_all();
    
    if (cleanupThread_.joinable()) {
        cleanupThread_.join();
    }
    
    if (schedulerThread_.joinable()) {
        schedulerThread_.join();
    }
}

size_t Scheduler::addRequest(const RequestState& request) {
    if (!running_) {
        throw SchedulerException(
            SchedulerError::SCHEDULER_NOT_RUNNING,
            "Scheduler is not running"
        );
    }
    
    RequestState req = request;
    
    if (req.requestId == 0) {
        req.requestId = requestTracker_.addRequest(req);
    }
    
    req.arrivalTime = getCurrentTime();
    
    if (req.temperature == 0.0f) {
        req.temperature = config_.defaultTemperature;
    }
    if (req.topP == 0.0f) {
        req.topP = config_.defaultTopP;
    }
    if (req.topK == 0) {
        req.topK = config_.defaultTopK;
    }
    if (req.maxTokens == 0) {
        req.maxTokens = config_.defaultMaxTokens;
    }
    
        {
            std::lock_guard<std::mutex> lock(queueMutex_);
            if (!requestQueue_.addRequest(req)) {
                throw SchedulerException(
                    SchedulerError::REQUEST_QUEUE_FULL,
                    "Request queue is full"
                );
            }
            // ğŸ”¥ ä¼˜åŒ–æ­¥éª¤1: æ›´æ–°åŸå­ç¼“å­˜
            cachedQueueSize_.store(requestQueue_.getQueueSize(), std::memory_order_relaxed);
        }
    
    {
        std::lock_guard<std::mutex> lock(statsMutex_);
        stats_.totalRequests++;
        
        size_t queueSize = requestQueue_.getQueueSize();
        if (queueSize > stats_.peakQueueSize.load()) {
            stats_.peakQueueSize.store(queueSize);
        }
    }
    
    queueCondition_.notify_one();
    return req.requestId;
}

bool Scheduler::removeRequest(size_t requestId) {
    if (!running_) {
        return false;
    }
    
    std::unique_lock<std::shared_mutex> lock(requestsMutex_);  // å†™æ“ä½œä½¿ç”¨ç‹¬å é”
    
    if (runningRequests_.erase(requestId) > 0) {
        requestTracker_.removeRequest(requestId);
        return true;
    }
    
    if (completedRequests_.erase(requestId) > 0) {
        requestTracker_.removeRequest(requestId);
        return true;
    }
    
    return requestQueue_.removeRequest(requestId);
}

RequestState Scheduler::getRequestResult(size_t requestId) {
    std::shared_lock<std::shared_mutex> lock(requestsMutex_);  // è¯»æ“ä½œä½¿ç”¨å…±äº«é”
    
    auto it = completedRequests_.find(requestId);
    if (it != completedRequests_.end()) {
        return it->second;
    }
    
    throw SchedulerException(
        SchedulerError::REQUEST_NOT_FOUND,
        "Request not found: " + std::to_string(requestId)
    );
}

bool Scheduler::waitForRequest(size_t requestId, float timeout) {
    auto startTime = std::chrono::steady_clock::now();
    auto timeoutDuration = std::chrono::duration<float>(timeout);
    
    // ğŸ”¥ ä¼˜åŒ–ï¼šä½¿ç”¨æ¡ä»¶å˜é‡æ›¿ä»£è½®è¯¢ï¼Œå‡å°‘ç­‰å¾…å»¶è¿Ÿ
    std::unique_lock<std::shared_mutex> lock(requestsMutex_);  // ç­‰å¾…éœ€è¦ç‹¬å é”
    
    // å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»å®Œæˆ
    if (completedRequests_.find(requestId) != completedRequests_.end()) {
        return true;
    }
    
    // ç­‰å¾…è¯·æ±‚å®Œæˆï¼Œä½¿ç”¨æ¡ä»¶å˜é‡é€šçŸ¥
    auto deadline = startTime + std::chrono::duration_cast<std::chrono::steady_clock::duration>(timeoutDuration);
    while (running_) {
        // ä½¿ç”¨æ¡ä»¶å˜é‡ç­‰å¾…ï¼Œè¶…æ—¶æ—¶é—´åŠ¨æ€è®¡ç®—
        auto remaining = std::chrono::duration_cast<std::chrono::milliseconds>(deadline - std::chrono::steady_clock::now()).count();
        if (remaining <= 0) {
            return false; // è¶…æ—¶
        }
        
        // ç­‰å¾…é€šçŸ¥ï¼Œæœ€å¤šç­‰å¾…remainingæ¯«ç§’
        if (resultCondition_.wait_for(lock, std::chrono::milliseconds(remaining), [this, requestId]() {
            return completedRequests_.find(requestId) != completedRequests_.end();
        })) {
            // è¯·æ±‚å·²å®Œæˆ
            return true;
        }
        
        // å†æ¬¡æ£€æŸ¥æ˜¯å¦å®Œæˆï¼ˆå¯èƒ½åœ¨wait_forè¿”å›falseæ—¶å·²ç»å®Œæˆï¼‰
        if (completedRequests_.find(requestId) != completedRequests_.end()) {
            return true;
        }
        
        // æ£€æŸ¥æ˜¯å¦è¶…æ—¶
        if (std::chrono::steady_clock::now() >= deadline) {
            return false;
        }
    }
    
    return false;
}

std::vector<RequestState> Scheduler::getRunningRequests() const {
    std::shared_lock<std::shared_mutex> lock(requestsMutex_);  // ä¼˜åŒ–ï¼šä½¿ç”¨å…±äº«é”ï¼Œå…è®¸å¤šä¸ªè¯»
    
    std::vector<RequestState> requests;
    requests.reserve(runningRequests_.size());
    
    // Phase 1: çŠ¶æ€æœºæ ¸å¿ƒå®ç° - åªè¿”å›æ´»è·ƒè¯·æ±‚ï¼ˆPENDINGæˆ–PROCESSINGï¼‰
    // è¿‡æ»¤æ‰å·²å®Œæˆçš„è¯·æ±‚ï¼ˆCOMPLETED/TIMEOUT/FAILEDï¼‰ï¼Œé¿å… formBatch è®¡ç®— runningLength æ—¶é«˜ä¼°
    for (const auto& pair : runningRequests_) {
        const RequestState& req = pair.second;
        // ä½¿ç”¨çŠ¶æ€åˆ¤æ–­è¾…åŠ©å‡½æ•°ï¼šåªè¿”å›æ´»è·ƒè¯·æ±‚ï¼ˆPENDINGæˆ–PROCESSINGï¼‰
        if (req.isActive()) {
            requests.push_back(req);
        }
    }
    
    return requests;
}

std::vector<RequestState> Scheduler::getCompletedRequests() const {
    std::shared_lock<std::shared_mutex> lock(requestsMutex_);  // ä¼˜åŒ–ï¼šä½¿ç”¨å…±äº«é”ï¼Œå…è®¸å¤šä¸ªè¯»
    
    std::vector<RequestState> requests;
    requests.reserve(completedRequests_.size());
    
    for (const auto& pair : completedRequests_) {
        requests.push_back(pair.second);
    }
    
    return requests;
}

size_t Scheduler::getQueueSize() const {
    std::lock_guard<std::mutex> lock(queueMutex_);
    return requestQueue_.getQueueSize();
}

SchedulerStats Scheduler::getStats() const {
    std::lock_guard<std::mutex> lock(statsMutex_);
    return stats_;
}

void Scheduler::resetStats() {
    std::lock_guard<std::mutex> lock(statsMutex_);
    stats_.reset();
}

size_t Scheduler::getRunningCount() const {
    std::shared_lock<std::shared_mutex> lock(requestsMutex_);  // è¯»æ“ä½œä½¿ç”¨å…±äº«é”
    return runningRequests_.size();
}

size_t Scheduler::getMaxConcurrentRequests() const {
    return config_.maxConcurrentRequests;
}

void Scheduler::setResponseCallback(ResponseCallback callback) {
    std::lock_guard<std::mutex> lock(callbackMutex_);
    responseCallback_ = callback;
}

void Scheduler::triggerResponseCallback(size_t requestId, const RequestState& state) {
    std::lock_guard<std::mutex> lock(callbackMutex_);
    if (responseCallback_) {
        try {
            responseCallback_(requestId, state);
        } catch (const std::exception& e) {
            CLLM_ERROR("Error in response callback for requestId=%zu: %s", requestId, e.what());
        } catch (...) {
            CLLM_ERROR("Unknown error in response callback for requestId=%zu", requestId);
        }
    }
}

void Scheduler::onBatchProcessed(size_t batchSize, double processingTimeMs) {
    if (!batchTuner_) {
        return;
    }

    batchTuner_->onBatchProcessed(batchSize, processingTimeMs);
    applyTunedBatchSize(batchTuner_->getCurrentBatchSize());
}

void Scheduler::applyTunedBatchSize(size_t tunedBatchSize) {
    if (tunedBatchSize == 0) {
        return;
    }

    size_t hardMax = resolveMaxBatchSize(0);
    size_t clamped = std::max<size_t>(1, std::min(tunedBatchSize, hardMax));
    size_t current = tunedMaxBatchSize_.load(std::memory_order_relaxed);
    if (current == clamped) {
        return;
    }

    tunedMaxBatchSize_.store(clamped, std::memory_order_relaxed);
    maxBatchSize_ = clamped;
    config_.maxBatchSize = clamped;
    batchManager_.setMaxBatchSize(clamped);
}

void Scheduler::schedulerLoop() {
    while (running_) {
        try {
            processRequests();
            
            checkRequestTimeout();
            
            checkKVCachEviction();
            
            std::unique_lock<std::mutex> lock(queueMutex_);
            
            // ğŸ”¥ ä¼˜åŒ–æ­¥éª¤2: ä½¿ç”¨åŸå­æ“ä½œå¿«é€Ÿæ£€æŸ¥ï¼ˆåªè¯»ï¼‰
            size_t queueSize = cachedQueueSize_.load(std::memory_order_relaxed);
            size_t runningCount = cachedRunningCount_.load(std::memory_order_relaxed);
            
            // å¦‚æœéœ€è¦ç²¾ç¡®å€¼æˆ–ç­‰å¾…æ¡ä»¶ï¼Œè·å–é”å¹¶åŒæ­¥
            if (queueSize == 0 && runningCount == 0) {
                // åŒé‡æ£€æŸ¥ï¼šè·å–ç²¾ç¡®å€¼
                queueSize = requestQueue_.getQueueSize();
                cachedQueueSize_.store(queueSize, std::memory_order_relaxed);
                
                {
                    std::shared_lock<std::shared_mutex> reqLock(requestsMutex_);  // è¯»æ“ä½œä½¿ç”¨å…±äº«é”
                    runningCount = runningRequests_.size();
                    cachedRunningCount_.store(runningCount, std::memory_order_relaxed);
                }
            }
            
            // å¦‚æœæ²¡æœ‰é˜Ÿåˆ—è¯·æ±‚ä¸”æ²¡æœ‰è¿è¡Œä¸­çš„è¯·æ±‚ï¼Œç­‰å¾…é€šçŸ¥
            // ä½¿ç”¨è¶…æ—¶é¿å…æ°¸ä¹…é˜»å¡ï¼ˆç”¨äºå¤„ç†è¿è¡Œä¸­è¯·æ±‚çš„ç»§ç»­å¤„ç†ï¼‰
            if (queueSize == 0 && runningCount == 0) {
                // ç©ºé—²æ—¶ç­‰å¾…æ–°è¯·æ±‚ï¼Œä½¿ç”¨è¶…æ—¶ä»¥å…è®¸å®šæœŸæ£€æŸ¥
                queueCondition_.wait_for(
                    lock,
                    std::chrono::microseconds(config_.idleLoopInterval),
                    [this]() {
                        return requestQueue_.getQueueSize() > 0 || !running_;
                    }
                );
            } else if (runningCount > 0) {
                // ğŸ”¥ ä¼˜åŒ–5: æœ‰è¿è¡Œä¸­è¯·æ±‚ï¼ŒæçŸ­é—´éš”ï¼ˆ1Î¼sï¼‰å¿«é€Ÿå¤„ç†ï¼Œæœ€å¤§åŒ–ååé‡
                lock.unlock();
                std::this_thread::sleep_for(
                    std::chrono::microseconds(1)  // ä¼˜åŒ–ï¼šå‡å°‘åˆ°1Î¼sï¼Œæœ€å¤§åŒ–ååé‡
                );
            } else {
                // ğŸ”¥ ä¼˜åŒ–5: æœ‰é˜Ÿåˆ—è¯·æ±‚ä½†æœªè¿è¡Œï¼ŒçŸ­é—´éš”ï¼ˆ10Î¼sï¼‰
                lock.unlock();
                std::this_thread::sleep_for(
                    std::chrono::microseconds(10)  // ä¼˜åŒ–ï¼šå‡å°‘åˆ°10Î¼sï¼Œæ›´å¿«å“åº”
                );
            }
            
        } catch (const std::exception& e) {
            CLLM_ERROR("Error in scheduler loop: %s", e.what());
            std::this_thread::sleep_for(std::chrono::seconds(1));
        }
    }
}

void Scheduler::processRequests() {
    // ğŸ”¥ ä¼˜åŒ–æ­¥éª¤1: ä½¿ç”¨åŸå­æ“ä½œå¿«é€Ÿæ£€æŸ¥ï¼ˆåªè¯»ï¼Œæ— å‰¯ä½œç”¨ï¼‰
    // å…ˆå¿«é€Ÿæ£€æŸ¥ç¼“å­˜å€¼ï¼Œå¦‚æœä¸º0åˆ™ç›´æ¥è¿”å›ï¼Œé¿å…ä¸å¿…è¦çš„é”ç«äº‰
    size_t queueSize = cachedQueueSize_.load(std::memory_order_relaxed);
    size_t runningCount = cachedRunningCount_.load(std::memory_order_relaxed);
    
    // å¦‚æœé˜Ÿåˆ—ä¸ºç©ºä¸”æ²¡æœ‰è¿è¡Œä¸­çš„è¯·æ±‚ï¼Œç›´æ¥è¿”å›
    if (queueSize == 0 && runningCount == 0) {
        return;
    }
    
    // éœ€è¦å®é™…å¤„ç†æ—¶ï¼Œè·å–ç²¾ç¡®å€¼ï¼ˆéœ€è¦é”ï¼‰
    {
        std::lock_guard<std::mutex> queueLock(queueMutex_);
        queueSize = requestQueue_.getQueueSize();
        cachedQueueSize_.store(queueSize, std::memory_order_relaxed);
    }
    
    {
        std::shared_lock<std::shared_mutex> reqLock(requestsMutex_);  // è¯»æ“ä½œä½¿ç”¨å…±äº«é”
        runningCount = runningRequests_.size();
        cachedRunningCount_.store(runningCount, std::memory_order_relaxed);
    }
    
    // å†æ¬¡æ£€æŸ¥ï¼ˆè·å–ç²¾ç¡®å€¼åï¼‰
    if (queueSize == 0 && runningCount == 0) {
        return;
    }
    
    // ğŸ”¥ å…³é”®ä¼˜åŒ–: æ‰¹å¤„ç†ç´¯ç§¯ç­–ç•¥
    // å¦‚æœé˜Ÿåˆ—è¯·æ±‚è¾ƒå°‘ä¸”æ²¡æœ‰è¿è¡Œä¸­çš„è¯·æ±‚ï¼Œç­‰å¾…æ›´å¤šè¯·æ±‚åˆ°è¾¾
    // è¿™æ ·å¯ä»¥å½¢æˆæ›´å¤§çš„æ‰¹å¤„ç†ï¼Œæé«˜ååé‡
    const size_t minBatchSize = std::min<size_t>(8, std::max<size_t>(1, maxBatchSize_));
    constexpr size_t MAX_WAIT_MS_FOR_BATCH = 50;  // æœ€å¤šç­‰å¾…50ms
    
    if (queueSize < minBatchSize && runningCount == 0) {
        CLLM_DEBUG("[Scheduler::processRequests] Queue size (%zu) < %zu, waiting for more requests (max %dms)",
                  queueSize, minBatchSize, MAX_WAIT_MS_FOR_BATCH);
        
        // ç­‰å¾…æ›´å¤šè¯·æ±‚åˆ°è¾¾
        std::unique_lock<std::mutex> lock(queueMutex_);
        auto waitStart = std::chrono::steady_clock::now();
        
        // ç­‰å¾…ç›´åˆ°é˜Ÿåˆ—è¶³å¤Ÿå¤§æˆ–è¶…æ—¶
        queueCondition_.wait_for(
            lock,
            std::chrono::milliseconds(MAX_WAIT_MS_FOR_BATCH),
            [this, minBatchSize]() {
                return requestQueue_.getQueueSize() >= minBatchSize || !running_;
            }
        );
        
        auto waitEnd = std::chrono::steady_clock::now();
        auto waitTime = std::chrono::duration_cast<std::chrono::milliseconds>(waitEnd - waitStart).count();
        CLLM_DEBUG("[Scheduler::processRequests] Waited %lldms, queue size now: %zu",
                  waitTime, requestQueue_.getQueueSize());
        
        // æ›´æ–°é˜Ÿåˆ—å¤§å°
        queueSize = requestQueue_.getQueueSize();
        cachedQueueSize_.store(queueSize, std::memory_order_relaxed);
        
        // å¦‚æœç­‰å¾…åé˜Ÿåˆ—ä»ç„¶ä¸ºç©ºï¼Œè¿”å›
        if (queueSize == 0) {
            return;
        }
    }
    
    // Phase 1: è¯·æ±‚æµè½¬é€»è¾‘ - RequestQueue â†’ runningRequests_ï¼ˆé€šè¿‡formBatché—´æ¥å®ç°ï¼‰
    // 1. ä» RequestQueue è·å–å¾…å¤„ç†è¯·æ±‚ï¼ˆPENDINGçŠ¶æ€ï¼‰
    std::vector<RequestState> running = getRunningRequests();  // è·å–å½“å‰è¿è¡Œä¸­çš„è¯·æ±‚ï¼ˆPENDINGæˆ–PROCESSINGï¼‰
    std::vector<RequestState> pending = requestQueue_.getPendingRequests();  // ä»é˜Ÿåˆ—è·å–å¾…å¤„ç†è¯·æ±‚
    
    // ğŸ”¥ ä¼˜åŒ–: å‡å°‘åºåˆ—IDæ£€æŸ¥é¢‘ç‡ï¼Œé¿å…é¢‘ç¹é”ç«äº‰
    // åªåœ¨é˜Ÿåˆ—å¤§å°è¾ƒå¤§æ—¶æ‰æ£€æŸ¥ï¼Œå°é˜Ÿåˆ—æ—¶å‡è®¾æœ‰è¶³å¤ŸID
    size_t availableSeqIds = 0;
    if (modelExecutor_ && queueSize > 4) {
        availableSeqIds = modelExecutor_->getAvailableSequenceIdCount();
        if (availableSeqIds > 0) {
            CLLM_DEBUG("[Scheduler::processRequests] Available sequence IDs: %zu", availableSeqIds);
        }
    } else if (modelExecutor_) {
        // å°é˜Ÿåˆ—æ—¶ï¼Œå‡è®¾æœ‰è¶³å¤ŸIDï¼ˆé¿å…é”ç«äº‰ï¼‰
        availableSeqIds = 64;  // å‡è®¾æœ‰è¶³å¤ŸID
    }
    
    // 3. formBatch å½¢æˆæ‰¹å¤„ç†ï¼ˆå¯èƒ½åŒ…å«æ¥è‡ª RequestQueue çš„æ–°è¯·æ±‚å’Œè¿è¡Œä¸­çš„è¯·æ±‚ï¼‰
    // formBatch ä¼šæ ¹æ® maxConcurrentRequestsã€èµ„æºé™åˆ¶å’Œå¯ç”¨åºåˆ—IDæ•°é‡å†³å®šå“ªäº›è¯·æ±‚å¯ä»¥åŠ å…¥æ‰¹å¤„ç†
    // ğŸ”¥ ä¼˜åŒ–æ­¥éª¤3: ä½¿ç”¨æ‰¹å¤„ç†æ± ï¼Œå‡å°‘å†…å­˜åˆ†é…
    auto& batch = batchPool_.acquire();
    batch = batchManager_.formBatch(pending, running, availableSeqIds);
    
    if (!batch.empty() && availableSeqIds > 0) {
        CLLM_DEBUG("[Scheduler::processRequests] Formed batch of %zu requests (availableSeqIds: %zu)", 
                  batch.size(), availableSeqIds);
    }
    
    // å¦‚æœ formBatch è¿”å›ç©ºï¼Œä½†é˜Ÿåˆ—ä¸­è¿˜æœ‰è¯·æ±‚ï¼Œå¯èƒ½æ˜¯å› ä¸ºèµ„æºé™åˆ¶
    // è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦ç»§ç»­å¤„ç†ï¼Œä½†éœ€è¦é€šçŸ¥è°ƒåº¦å™¨ç»§ç»­å°è¯•
    if (batch.empty() && queueSize > 0) {
        // é˜Ÿåˆ—ä¸­æœ‰è¯·æ±‚ä½†æ— æ³•å½¢æˆæ‰¹å¤„ç†ï¼Œå¯èƒ½æ˜¯èµ„æºé™åˆ¶
        // è¿”å›å¹¶è®©è°ƒåº¦å™¨ç¨åé‡è¯•
        batchPool_.release(batch);
        return;
    }
    
    if (batch.empty()) {
        batchPool_.release(batch);
        return;
    }
    
    // ğŸ”¥ ä¼˜åŒ–æ­¥éª¤3: æ‰¹é‡ç§»é™¤è¯·æ±‚å¹¶æ›´æ–°åŸå­ç¼“å­˜
    {
        std::lock_guard<std::mutex> queueLock(queueMutex_);
        for (const auto& req : batch) {
            requestQueue_.removeRequest(req.requestId);
        }
        cachedQueueSize_.store(requestQueue_.getQueueSize(), std::memory_order_relaxed);
    }
    
    processBatch(batch);
}

void Scheduler::processBatch(std::vector<RequestState>& batch) {
    // åœ¨å¼€å§‹å¤„ç†å‰ï¼Œæ£€æŸ¥å¹¶åˆå¹¶å·²å­˜åœ¨çš„è¯·æ±‚çŠ¶æ€
    // ğŸ”¥ ä¼˜åŒ–æ­¥éª¤2: å‡å°‘é”æŒæœ‰æ—¶é—´
    // æ­¥éª¤1: å¿«é€Ÿå¤åˆ¶éœ€è¦å¤„ç†çš„æ•°æ®ï¼ˆçŸ­æ—¶é—´æŒæœ‰é”ï¼‰
    struct RequestInfo {
        RequestState request;
        bool existsInRunning;
        bool isCompleted;
        bool isFailed;
        std::vector<int> existingTokens;
        bool isPending;
        bool isRunning;
        size_t startTime;
    };
    
    std::vector<RequestInfo> requestsToProcess;
    requestsToProcess.reserve(batch.size());
    
    {
        std::shared_lock<std::shared_mutex> lock(requestsMutex_);  // è¯»æ“ä½œä½¿ç”¨å…±äº«é”
        for (const auto& request : batch) {
            RequestInfo info;
            info.request = request;
            
            // æ£€æŸ¥è¯·æ±‚æ˜¯å¦å·²ç»å®Œæˆ
            auto completedIt = completedRequests_.find(request.requestId);
            if (completedIt != completedRequests_.end()) {
                CLLM_DEBUG("Request %llu already completed, filtering out (tokens: %zu)",
                         request.requestId, completedIt->second.generatedTokens.size());
                continue;  // å·²å®Œæˆçš„è¯·æ±‚ä¸å¤„ç†
            }
            
            // æ£€æŸ¥è¯·æ±‚æ˜¯å¦åœ¨è¿è¡Œä¸­
            auto it = runningRequests_.find(request.requestId);
            if (it != runningRequests_.end()) {
                info.existsInRunning = true;
                info.isCompleted = it->second.isCompleted;
                info.isFailed = it->second.isFailed;
                info.existingTokens = it->second.generatedTokens;
                info.isPending = it->second.isPending();
                info.isRunning = it->second.isRunning;
                info.startTime = it->second.startTime;
            } else {
                info.existsInRunning = false;
            }
            
            requestsToProcess.push_back(std::move(info));
        }
    }
    
    // æ­¥éª¤2: åœ¨é”å¤–å¤„ç†æ•°æ®ï¼ˆæ— é”ï¼‰
    std::vector<RequestState> activeBatch;
    activeBatch.reserve(requestsToProcess.size());
    
    for (auto& info : requestsToProcess) {
        auto& request = info.request;
        
        if (info.existsInRunning) {
            // è¯·æ±‚å·²å­˜åœ¨ï¼Œåˆå¹¶çŠ¶æ€
            CLLM_DEBUG("Request %llu already in runningRequests_, merging state (existing tokens: %zu, isCompleted: %d)",
                      request.requestId, info.existingTokens.size(), info.isCompleted ? 1 : 0);
            
            request.generatedTokens = std::move(info.existingTokens);
            request.isCompleted = info.isCompleted;
            request.isFailed = info.isFailed;

            // Phase 1: çŠ¶æ€è½¬æ¢ PENDING â†’ PROCESSING
            if (info.isPending) {
                CLLM_DEBUG("Request %llu: PENDING â†’ PROCESSING", request.requestId);
                if (info.startTime == 0) {
                    request.startTime = getCurrentTime();
                }
            }
            
            request.isRunning = true;
            request.startTime = info.startTime;
        } else {
            // æ–°è¯·æ±‚
            CLLM_DEBUG("Request %llu: NEW REQUEST (PENDING), will transition to PROCESSING", request.requestId);
            request.startTime = getCurrentTime();
            request.isRunning = false;
        }
        
        // ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥è¯·æ±‚æ˜¯å¦å·²ç»è¾¾åˆ°maxTokensé™åˆ¶
        // å¦‚æœå·²ç»è¾¾åˆ°ï¼Œæ ‡è®°ä¸ºå®Œæˆï¼Œé¿å…ç»§ç»­ç”Ÿæˆ
        if (!request.isCompleted && !request.isFailed && request.maxTokens > 0 &&
            request.generatedTokens.size() >= static_cast<size_t>(request.maxTokens)) {
            CLLM_DEBUG("Request %llu reached max tokens limit (%zu >= %d) before batch processing, marking as completed",
                      request.requestId, request.generatedTokens.size(), request.maxTokens);
            request.isCompleted = true;
        }
        
        // Phase 1: çŠ¶æ€è½¬æ¢ PENDING â†’ PROCESSING
        request.isRunning = true;
        requestTracker_.markAsRunning(request.requestId);
        if (modelExecutor_) {
            modelExecutor_->updateKVCacheRequestStatus(request.requestId, inference::RequestStatus::PROCESSING);
        }
        
        activeBatch.push_back(std::move(request));
    }
    
    // æ­¥éª¤3: æ‰¹é‡æ›´æ–°çŠ¶æ€ï¼ˆçŸ­æ—¶é—´æŒæœ‰é”ï¼‰
    {
        std::unique_lock<std::shared_mutex> lock(requestsMutex_);  // å†™æ“ä½œä½¿ç”¨ç‹¬å é”
        for (const auto& request : activeBatch) {
            auto it = runningRequests_.find(request.requestId);
            if (it != runningRequests_.end()) {
                it->second.isRunning = request.isRunning;
                if (it->second.startTime == 0) {
                    it->second.startTime = request.startTime;
                }
            } else {
                runningRequests_[request.requestId] = request;
            }
        }
    }
    
    // å¦‚æœæ‰€æœ‰è¯·æ±‚éƒ½å·²å®Œæˆï¼Œç›´æ¥è¿”å›ï¼Œä¸è°ƒç”¨ processor.processBatch
    if (activeBatch.empty()) {
        CLLM_DEBUG("All requests in batch are already completed, skipping processing");
        return;
    }
    
    CLLM_INFO("Starting batch processing for %zu requests (filtered from %zu total)",
              activeBatch.size(), batch.size());
    
    SchedulerBatchProcessor processor(this, modelExecutor_, kvCache_, &batchManager_);
    processor.processBatch(activeBatch);
    
    // ğŸ”¥ ä¼˜åŒ–: æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„è¯·æ±‚ï¼Œå°†å®ƒä»¬é‡æ–°åŠ å…¥é˜Ÿåˆ—ä»¥ä¾¿é‡ç»„
    std::vector<RequestState> incompleteRequests;
    for (auto& request : activeBatch) {
        if (!request.isCompleted && !request.isFailed) {
            incompleteRequests.push_back(request);
        }
    }
    
    // å¦‚æœæœ‰æœªå®Œæˆçš„è¯·æ±‚ï¼Œå°†å®ƒä»¬é‡æ–°åŠ å…¥é˜Ÿåˆ—ï¼Œä»¥ä¾¿ä¸å…¶ä»–è¯·æ±‚é‡ç»„
    if (!incompleteRequests.empty() && incompleteRequests.size() < activeBatch.size() * 0.5) {
        CLLM_DEBUG("[Scheduler] %zu incomplete requests from batch of %zu, re-queuing for regrouping",
                  incompleteRequests.size(), activeBatch.size());
        for (auto& request : incompleteRequests) {
            // é‡æ–°åŠ å…¥é˜Ÿåˆ—ï¼Œä»¥ä¾¿ä¸å…¶ä»–è¯·æ±‚é‡ç»„
            requestQueue_.addRequest(request);
        }
    }
    
    // æ›´æ–° batch å¼•ç”¨ï¼Œç”¨äºåç»­å¤„ç†
    batch = std::move(activeBatch);
    
    // ğŸ”¥ ä¼˜åŒ–: ç«‹å³é‡Šæ”¾å·²å®Œæˆè¯·æ±‚çš„åºåˆ—IDï¼Œé¿å…é˜»å¡åç»­æ‰¹å¤„ç†
    for (auto& request : batch) {
        request.completionTime = getCurrentTime();
        
        // ğŸ”¥ å…³é”®ä¼˜åŒ–: å¦‚æœè¯·æ±‚å·²å®Œæˆï¼Œç«‹å³é‡Šæ”¾åºåˆ—IDå’ŒKVç¼“å­˜
        if (request.isCompleted || request.isFailed) {
            if (modelExecutor_) {
                // ç«‹å³æ¸…ç†KVç¼“å­˜å’Œé‡Šæ”¾åºåˆ—IDï¼Œè€Œä¸æ˜¯ç­‰åˆ°å¼‚æ­¥æ¸…ç†
                modelExecutor_->cleanupKVCache(request.requestId);
                modelExecutor_->releaseSequenceId(request.requestId);
                CLLM_DEBUG("[Scheduler] Immediately released seq_id and KV cache for completed request %llu", 
                          request.requestId);
            }
        }
        
        CLLM_DEBUG("Request %llu generated tokens: %zu", request.requestId, request.generatedTokens.size());
        
        {
            std::unique_lock<std::shared_mutex> lock(requestsMutex_);  // å†™æ“ä½œä½¿ç”¨ç‹¬å é”
            if (request.isCompleted) {
                // Phase 1: çŠ¶æ€è½¬æ¢ PROCESSING â†’ COMPLETED
                // è¯·æ±‚å·²å®Œæˆï¼Œä» runningRequests_ ç§»é™¤ï¼Œæ·»åŠ åˆ° completedRequests_
                CLLM_DEBUG("Request %llu: PROCESSING â†’ COMPLETED (tokens: %zu)",
                          request.requestId, request.generatedTokens.size());
                
                if (modelExecutor_) {
                    modelExecutor_->updateKVCacheRequestStatus(request.requestId, inference::RequestStatus::COMPLETED);
                    // ğŸ”¥ ä¼˜åŒ–ï¼šç«‹å³åŒæ­¥æ¸…ç†èµ„æºï¼Œç«‹å³é‡Šæ”¾åºåˆ—IDï¼Œé¿å…é˜»å¡åç»­æ‰¹å¤„ç†
                    // ä¹‹å‰ä½¿ç”¨å¼‚æ­¥æ¸…ç†å¯¼è‡´åºåˆ—IDé‡Šæ”¾å»¶è¿Ÿï¼Œé™åˆ¶äº†æ‰¹å¤„ç†å¤§å°
                    modelExecutor_->cleanupKVCache(request.requestId);
                    modelExecutor_->releaseSequenceId(request.requestId);
                    CLLM_DEBUG("[Scheduler] Immediately released seq_id and KV cache for completed request %llu", 
                              request.requestId);
                }
                
                requestTracker_.markAsCompleted(request.requestId);
                stats_.update(request);
                runningRequests_.erase(request.requestId);
                completedRequests_[request.requestId] = request;
                
                // ğŸ”¥ ä¼˜åŒ–æ­¥éª¤3: æ›´æ–°åŸå­ç¼“å­˜
                cachedRunningCount_.store(runningRequests_.size(), std::memory_order_relaxed);
                
                // ğŸ”¥ ä¼˜åŒ–ï¼šé€šçŸ¥ç­‰å¾…è¯¥è¯·æ±‚çš„çº¿ç¨‹ï¼ˆä½¿ç”¨æ¡ä»¶å˜é‡ï¼‰
                resultCondition_.notify_all();
                
                // Phase 7: è§¦å‘å®Œæˆå›è°ƒ
                triggerResponseCallback(request.requestId, request);
            } else if (request.isFailed) {
                // Phase 1: çŠ¶æ€è½¬æ¢ PROCESSING â†’ FAILED
                // è¯·æ±‚å¤±è´¥ï¼Œä» runningRequests_ ç§»é™¤ï¼Œæ·»åŠ åˆ° completedRequests_
                CLLM_DEBUG("Request %llu: PROCESSING â†’ FAILED (error: %s)",
                          request.requestId, request.errorMessage.c_str());
                
                if (modelExecutor_) {
                    modelExecutor_->updateKVCacheRequestStatus(request.requestId, inference::RequestStatus::FAILED);
                    // ğŸ”¥ ä¼˜åŒ–ï¼šç«‹å³åŒæ­¥æ¸…ç†èµ„æºï¼Œç«‹å³é‡Šæ”¾åºåˆ—ID
                    modelExecutor_->cleanupKVCache(request.requestId);
                    modelExecutor_->releaseSequenceId(request.requestId);
                    CLLM_DEBUG("[Scheduler] Immediately released seq_id and KV cache for failed request %llu", 
                              request.requestId);
                }
                
                requestTracker_.markAsFailed(request.requestId, request.errorMessage);
                stats_.failedRequests++;
                runningRequests_.erase(request.requestId);
                completedRequests_[request.requestId] = request;
                
                // ğŸ”¥ ä¼˜åŒ–æ­¥éª¤3: æ›´æ–°åŸå­ç¼“å­˜
                cachedRunningCount_.store(runningRequests_.size(), std::memory_order_relaxed);
                
                // ğŸ”¥ ä¼˜åŒ–ï¼šé€šçŸ¥ç­‰å¾…è¯¥è¯·æ±‚çš„çº¿ç¨‹ï¼ˆä½¿ç”¨æ¡ä»¶å˜é‡ï¼‰
                resultCondition_.notify_all();
                
                // Phase 7: è§¦å‘å¤±è´¥å›è°ƒ
                triggerResponseCallback(request.requestId, request);
            } else {
                // Phase 1: çŠ¶æ€ä¿æŒ PROCESSING
                // è¯·æ±‚è¿˜åœ¨è¿è¡Œï¼ˆPROCESSINGçŠ¶æ€ï¼‰ï¼Œæ›´æ–° runningRequests_ ä¸­çš„çŠ¶æ€
                auto it = runningRequests_.find(request.requestId);
                if (it != runningRequests_.end()) {
                    // æ›´æ–°çŠ¶æ€ï¼Œä¿ç•™å·²æœ‰çš„ generatedTokens ç­‰
                    it->second = request;
                    // ğŸ”¥ ä¼˜åŒ–æ­¥éª¤3: æ›´æ–°åŸå­ç¼“å­˜ï¼ˆçŠ¶æ€æ›´æ–°ï¼‰
                    cachedRunningCount_.store(runningRequests_.size(), std::memory_order_relaxed);
                    // ç¡®ä¿ isRunning æ ‡å¿—æ­£ç¡®
                    it->second.isRunning = true;
                    CLLM_DEBUG("Request %llu: PROCESSING (continuing, tokens: %zu)",
                              request.requestId, request.generatedTokens.size());
                }
            }
        }
        
        resultCondition_.notify_all();
    }
    
    stats_.updateBatch(batch);
    
    // ğŸ”¥ ä¼˜åŒ–æ­¥éª¤3: é‡Šæ”¾æ‰¹å¤„ç†å¯¹è±¡å›æ± 
    batchPool_.release(batch);
    
    // Phase 1: è¯·æ±‚æµè½¬é€»è¾‘ - è¯·æ±‚å®Œæˆåè‡ªåŠ¨è§¦å‘ä¸‹ä¸€ä¸ªè¯·æ±‚çš„å¤„ç†
    // æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å¾…å¤„ç†çš„è¯·æ±‚ï¼Œå¦‚æœæœ‰ï¼Œé€šçŸ¥è°ƒåº¦å™¨ç»§ç»­å¤„ç†
    // è¿™æ ·å¯ä»¥é¿å…è°ƒåº¦å™¨åœ¨æœ‰ç©ºé—²èµ„æºæ—¶è¿˜åœ¨ç­‰å¾…
    {
        std::lock_guard<std::mutex> queueLock(queueMutex_);
        size_t remainingQueueSize = requestQueue_.getQueueSize();
        if (remainingQueueSize > 0) {
            // è¯·æ±‚å®Œæˆåï¼Œå¦‚æœ RequestQueue ä¸ä¸ºç©ºä¸” runningRequests_.size() < maxConcurrentRequestsï¼Œ
            // è‡ªåŠ¨è§¦å‘ä¸‹ä¸€ä¸ªè¯·æ±‚çš„å¤„ç†ï¼ˆé€šè¿‡ queueCondition_.notify_one()ï¼‰
            CLLM_DEBUG("Request completed, notifying scheduler to process next request (queue size: %zu)", remainingQueueSize);
            queueCondition_.notify_one();
        }
    }
}

void Scheduler::checkRequestTimeout() {
    std::unique_lock<std::shared_mutex> lock(requestsMutex_);  // å†™æ“ä½œä½¿ç”¨ç‹¬å é”
    
    size_t currentTimeMs = getCurrentTime();
    std::vector<size_t> timeoutRequests;
    
    for (auto& pair : runningRequests_) {
        size_t requestId = pair.first;
        RequestState& request = pair.second;
        
        if (request.isProcessing() && request.startTime > 0) {
            if (currentTimeMs < request.startTime) {
                continue;
            }
            float processingTimeSec = static_cast<float>(currentTimeMs - request.startTime) / 1000.0f;
            
            if (processingTimeSec > config_.requestTimeout) {
                CLLM_WARN("Request %zu: TIMEOUT (processing time: %.2fs, timeout: %.2fs)",
                          requestId, processingTimeSec, config_.requestTimeout);
                timeoutRequests.push_back(requestId);
            }
        }
    }
    
    for (size_t requestId : timeoutRequests) {
        auto it = runningRequests_.find(requestId);
        if (it != runningRequests_.end()) {
            RequestState request = it->second;
            
            CLLM_WARN("Request %zu: PROCESSING â†’ TIMEOUT", requestId);
            
            request.isTimeout = true;
            request.isFailed = true;
            request.errorMessage = "Request timeout";
            request.completionTime = currentTimeMs;
            
            CLLM_DEBUG("Request %zu: TIMEOUT (tokens: %zu)",
                      requestId, request.generatedTokens.size());
            
            if (modelExecutor_) {
                modelExecutor_->updateKVCacheRequestStatus(requestId, inference::RequestStatus::TIMEOUT);
                // ä¼˜åŒ–ï¼šå¼‚æ­¥æ¸…ç†èµ„æºï¼Œé¿å…é˜»å¡ä¸»æµç¨‹
                cleanupRequestAsync(requestId);
            }
            
            requestTracker_.markAsFailed(requestId, request.errorMessage);
            stats_.failedRequests++;
            runningRequests_.erase(requestId);
            completedRequests_[requestId] = request;
            
            // ğŸ”¥ ä¼˜åŒ–ï¼šé€šçŸ¥ç­‰å¾…è¯¥è¯·æ±‚çš„çº¿ç¨‹ï¼ˆä½¿ç”¨æ¡ä»¶å˜é‡ï¼‰
            resultCondition_.notify_all();
            
            // Phase 7: è§¦å‘è¶…æ—¶å›è°ƒ
            triggerResponseCallback(requestId, request);
            
            resultCondition_.notify_all();
        }
    }
}

void Scheduler::checkKVCachEviction() {
    // Phase 5: KVç¼“å­˜æ·˜æ±°
    if (!modelExecutor_) {
        return;
    }

    size_t evictedCount = modelExecutor_->evictKVCachesIfNeeded(config_.kvCacheEvictionThreshold);
    if (evictedCount > 0) {
        CLLM_INFO("[Scheduler] KV cache eviction completed: evicted %zu requests", evictedCount);
    }
}

size_t Scheduler::getCurrentTime() {
    auto now = std::chrono::steady_clock::now();
    auto duration = now.time_since_epoch();
    return static_cast<size_t>(
        std::chrono::duration_cast<std::chrono::milliseconds>(duration).count()
    );
}

void Scheduler::cleanupRequestAsync(size_t requestId) {
    std::lock_guard<std::mutex> lock(cleanupMutex_);
    cleanupQueue_.push(requestId);
    cleanupCondition_.notify_one();
}

void Scheduler::cleanupLoop() {
    size_t processedCount = 0;
    while (running_) {
        std::unique_lock<std::mutex> lock(cleanupMutex_);
        
        // ç­‰å¾…æ¸…ç†ä»»åŠ¡æˆ–åœæ­¢ä¿¡å·
        cleanupCondition_.wait_for(lock, std::chrono::milliseconds(100), [this]() {
            return !cleanupQueue_.empty() || !running_;
        });
        
        // å¤„ç†æ‰€æœ‰å¾…æ¸…ç†çš„è¯·æ±‚
        size_t batchSize = cleanupQueue_.size();
        while (!cleanupQueue_.empty()) {
            size_t requestId = cleanupQueue_.front();
            cleanupQueue_.pop();
            
            // é‡Šæ”¾é”ï¼Œæ‰§è¡Œæ¸…ç†æ“ä½œ
            lock.unlock();
            
            auto startTime = std::chrono::high_resolution_clock::now();
            if (modelExecutor_) {
                // Phase 4: æ¸…ç†KVç¼“å­˜
                modelExecutor_->cleanupKVCache(requestId);
                // Phase 2: é‡Šæ”¾åºåˆ—ID
                modelExecutor_->releaseSequenceId(requestId);
            }
            auto endTime = std::chrono::high_resolution_clock::now();
            auto duration = std::chrono::duration_cast<std::chrono::microseconds>(endTime - startTime).count();
            
            processedCount++;
            
            // æ¯å¤„ç†100ä¸ªè¯·æ±‚è®°å½•ä¸€æ¬¡ç»Ÿè®¡ä¿¡æ¯
            if (processedCount % 100 == 0) {
                CLLM_DEBUG("[Scheduler::cleanupLoop] Processed %zu cleanup tasks (avg time: %.2f us)", 
                          processedCount, static_cast<double>(duration));
            }
            
            // é‡æ–°è·å–é”ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªä»»åŠ¡
            lock.lock();
        }
        
        if (batchSize > 0) {
            CLLM_DEBUG("[Scheduler::cleanupLoop] Processed batch of %zu cleanup tasks", batchSize);
        }
    }
    CLLM_INFO("[Scheduler::cleanupLoop] Cleanup thread exiting (total processed: %zu)", processedCount);
}

RequestState Scheduler::generateStreaming(const RequestState& request, TokenCallback tokenCallback) {
    RequestState result = request;
    result.isRunning = true;
    result.startTime = getCurrentTime();
    
    CLLM_DEBUG("[generateStreaming] Starting streaming generation for %d tokens", request.maxTokens);
    
    // ä½¿ç”¨ Scheduler çš„ç°æœ‰æœºåˆ¶ï¼Œé€šè¿‡è½®è¯¢æ£€æŸ¥ç”Ÿæˆè¿›åº¦
    // è¿™æ ·å¯ä»¥æ­£ç¡®ä½¿ç”¨æ‰¹å¤„ç†å’Œ KV cache
    
    try {
        // æ·»åŠ è¯·æ±‚åˆ°é˜Ÿåˆ—
        size_t reqId = addRequest(request);
        
        size_t lastTokenCount = 0;
        const float timeoutSec = std::max(60.0f, static_cast<float>(request.maxTokens) * 0.5f);
        auto startTime = std::chrono::steady_clock::now();
        
        // è½®è¯¢æ£€æŸ¥ç”Ÿæˆè¿›åº¦ï¼Œæ¯å‘ç°æ–° token å°±è°ƒç”¨å›è°ƒ
        while (true) {
            // æ£€æŸ¥è¶…æ—¶
            auto now = std::chrono::steady_clock::now();
            float elapsed = std::chrono::duration<float>(now - startTime).count();
            if (elapsed > timeoutSec) {
                CLLM_WARN("[generateStreaming] Timeout after %.1fs", elapsed);
                result.isTimeout = true;
                break;
            }
            
            // è·å–å½“å‰çŠ¶æ€
            RequestState current;
            bool found = false;
            {
                std::shared_lock<std::shared_mutex> lock(requestsMutex_);
                auto it = runningRequests_.find(reqId);
                if (it != runningRequests_.end()) {
                    current = it->second;
                    found = true;
                }
                if (!found) {
                    auto cit = completedRequests_.find(reqId);
                    if (cit != completedRequests_.end()) {
                        current = cit->second;
                        found = true;
                    }
                }
            }
            
            if (!found) {
                // è¯·æ±‚è¿˜åœ¨é˜Ÿåˆ—ä¸­ï¼Œç­‰å¾…
                std::this_thread::sleep_for(std::chrono::milliseconds(5));
                continue;
            }
            
            // æ£€æŸ¥æ˜¯å¦æœ‰æ–° token
            size_t currentTokenCount = current.generatedTokens.size();
            if (currentTokenCount > lastTokenCount) {
                // æœ‰æ–° tokenï¼Œè°ƒç”¨å›è°ƒ
                for (size_t i = lastTokenCount; i < currentTokenCount; ++i) {
                    int token = current.generatedTokens[i];
                    if (tokenCallback) {
                        bool shouldContinue = tokenCallback(token);
                        if (!shouldContinue) {
                            CLLM_DEBUG("[generateStreaming] Callback requested stop");
                            // TODO: å¯ä»¥è€ƒè™‘å–æ¶ˆè¯·æ±‚
                            break;
                        }
                    }
                }
                lastTokenCount = currentTokenCount;
            }
            
            // æ£€æŸ¥æ˜¯å¦å®Œæˆ
            if (current.isCompleted || current.isFailed || current.isTimeout) {
                result = current;
                break;
            }
            
            // çŸ­æš‚ç­‰å¾…å†æ£€æŸ¥
            std::this_thread::sleep_for(std::chrono::milliseconds(10));
        }
        
    } catch (const std::exception& e) {
        CLLM_ERROR("[generateStreaming] Exception: %s", e.what());
        result.isFailed = true;
        result.errorMessage = e.what();
    }
    
    result.completionTime = getCurrentTime();
    CLLM_DEBUG("[generateStreaming] Completed, generated %zu tokens", result.generatedTokens.size());
    
    return result;
}

}
