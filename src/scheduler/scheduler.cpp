#include "cllm/scheduler/scheduler.h"
#include "cllm/common/request_state.h"
#include "cllm/common/queue.h"
#include "cllm/batch/manager.h"
#include "cllm/model/executor.h"
#include "cllm/kv_cache/cache.h"
#include "cllm/memory/monitor.h"
#include "cllm/common/logger.h"
#include "cllm/inference/llama_cpp_backend.h"
#include <chrono>
#include <stdexcept>
#include <queue>

namespace cllm {

Scheduler::Scheduler(
    ModelExecutor* modelExecutor,
    size_t maxBatchSize,
    size_t maxContextLength
) : batchManager_(
        (maxContextLength != 0) ? maxContextLength : Config::instance().serverMaxContextLength(),
        (maxBatchSize != 0) ? maxBatchSize : Config::instance().serverMaxBatchSize()
    ),
    maxBatchSize_((maxBatchSize != 0) ? maxBatchSize : Config::instance().serverMaxBatchSize()),
    maxContextLength_((maxContextLength != 0) ? maxContextLength : Config::instance().serverMaxContextLength()),
    modelExecutor_(modelExecutor),
    ownsModelExecutor_(false) {
    
    config_.maxBatchSize = maxBatchSize_;
    config_.maxContextLength = maxContextLength_;
    config_.defaultTemperature = Config::instance().schedulerDefaultTemperature();
    config_.defaultTopP = Config::instance().schedulerDefaultTopP();
    config_.defaultTopK = Config::instance().schedulerDefaultTopK();
    config_.defaultMaxTokens = Config::instance().schedulerDefaultMaxTokens();
    config_.requestTimeout = Config::instance().schedulerRequestTimeout();
    config_.schedulerLoopInterval = Config::instance().schedulerLoopInterval();
    config_.idleLoopInterval = Config::instance().schedulerIdleLoopInterval();
    config_.contextUsageThreshold = Config::instance().schedulerContextUsageThreshold();
    
    kvCache_ = new KVCache(
        static_cast<size_t>(Config::instance().serverKvCacheMaxSize()),
        static_cast<size_t>(Config::instance().serverKvCacheMaxMemoryMb())
    );
    
    // éªŒè¯æ¨¡å‹å·²åŠ è½½
    if (!modelExecutor_->isLoaded()) {
        throw std::runtime_error("Model executor must be pre-loaded before creating Scheduler");
    }
}

Scheduler::Scheduler(
    const std::string& modelPath,
    const std::string& quantization,
    size_t maxBatchSize,
    size_t maxContextLength
) : batchManager_(
        (maxContextLength != 0) ? maxContextLength : Config::instance().serverMaxContextLength(),
        (maxBatchSize != 0) ? maxBatchSize : Config::instance().serverMaxBatchSize()
    ),
    maxBatchSize_((maxBatchSize != 0) ? maxBatchSize : Config::instance().serverMaxBatchSize()),
    maxContextLength_((maxContextLength != 0) ? maxContextLength : Config::instance().serverMaxContextLength()),
    ownsModelExecutor_(true) {
    
    config_.maxBatchSize = maxBatchSize_;
    config_.maxContextLength = maxContextLength_;
    config_.defaultTemperature = Config::instance().schedulerDefaultTemperature();
    config_.defaultTopP = Config::instance().schedulerDefaultTopP();
    config_.defaultTopK = Config::instance().schedulerDefaultTopK();
    config_.defaultMaxTokens = Config::instance().schedulerDefaultMaxTokens();
    config_.requestTimeout = Config::instance().schedulerRequestTimeout();
    config_.schedulerLoopInterval = Config::instance().schedulerLoopInterval();
    config_.idleLoopInterval = Config::instance().schedulerIdleLoopInterval();
    config_.contextUsageThreshold = Config::instance().schedulerContextUsageThreshold();
    
    modelExecutor_ = new ModelExecutor(modelPath, quantization);
    kvCache_ = new KVCache(
        static_cast<size_t>(Config::instance().serverKvCacheMaxSize()),
        static_cast<size_t>(Config::instance().serverKvCacheMaxMemoryMb())
    );
    
    // åŠ è½½æ¨¡å‹
    modelExecutor_->loadModel();
}

Scheduler::~Scheduler() {
    stop();
    
    delete kvCache_;
    
    // Only delete modelExecutor_ if we own it (used by tests)
    if (ownsModelExecutor_ && modelExecutor_) {
        delete modelExecutor_;
    }
}

void Scheduler::start() {
    if (running_) {
        return;
    }
    
    running_ = true;
    schedulerThread_ = std::thread(&Scheduler::schedulerLoop, this);
    cleanupThread_ = std::thread(&Scheduler::cleanupLoop, this);
}

void Scheduler::stop() {
    if (!running_) {
        return;
    }
    
    running_ = false;
    
    // é€šçŸ¥æ¸…ç†çº¿ç¨‹é€€å‡º
    cleanupCondition_.notify_all();
    
    if (cleanupThread_.joinable()) {
        cleanupThread_.join();
    }
    
    if (schedulerThread_.joinable()) {
        schedulerThread_.join();
    }
}

size_t Scheduler::addRequest(const RequestState& request) {
    if (!running_) {
        throw SchedulerException(
            SchedulerError::SCHEDULER_NOT_RUNNING,
            "Scheduler is not running"
        );
    }
    
    RequestState req = request;
    
    if (req.requestId == 0) {
        req.requestId = requestTracker_.addRequest(req);
    }
    
    req.arrivalTime = getCurrentTime();
    
    if (req.temperature == 0.0f) {
        req.temperature = config_.defaultTemperature;
    }
    if (req.topP == 0.0f) {
        req.topP = config_.defaultTopP;
    }
    if (req.topK == 0) {
        req.topK = config_.defaultTopK;
    }
    if (req.maxTokens == 0) {
        req.maxTokens = config_.defaultMaxTokens;
    }
    
        {
            std::lock_guard<std::mutex> lock(queueMutex_);
            if (!requestQueue_.addRequest(req)) {
                throw SchedulerException(
                    SchedulerError::REQUEST_QUEUE_FULL,
                    "Request queue is full"
                );
            }
            // ğŸ”¥ ä¼˜åŒ–æ­¥éª¤1: æ›´æ–°åŸå­ç¼“å­˜
            cachedQueueSize_.store(requestQueue_.getQueueSize(), std::memory_order_relaxed);
        }
    
    {
        std::lock_guard<std::mutex> lock(statsMutex_);
        stats_.totalRequests++;
        
        size_t queueSize = requestQueue_.getQueueSize();
        if (queueSize > stats_.peakQueueSize.load()) {
            stats_.peakQueueSize.store(queueSize);
        }
    }
    
    queueCondition_.notify_one();
    return req.requestId;
}

bool Scheduler::removeRequest(size_t requestId) {
    if (!running_) {
        return false;
    }
    
    std::lock_guard<std::mutex> lock(requestsMutex_);
    
    if (runningRequests_.erase(requestId) > 0) {
        requestTracker_.removeRequest(requestId);
        return true;
    }
    
    if (completedRequests_.erase(requestId) > 0) {
        requestTracker_.removeRequest(requestId);
        return true;
    }
    
    return requestQueue_.removeRequest(requestId);
}

RequestState Scheduler::getRequestResult(size_t requestId) {
    std::lock_guard<std::mutex> lock(requestsMutex_);
    
    auto it = completedRequests_.find(requestId);
    if (it != completedRequests_.end()) {
        return it->second;
    }
    
    throw SchedulerException(
        SchedulerError::REQUEST_NOT_FOUND,
        "Request not found: " + std::to_string(requestId)
    );
}

bool Scheduler::waitForRequest(size_t requestId, float timeout) {
    auto startTime = std::chrono::steady_clock::now();
    auto timeoutDuration = std::chrono::duration<float>(timeout);
    
    // ğŸ”¥ ä¼˜åŒ–ï¼šä½¿ç”¨æ¡ä»¶å˜é‡æ›¿ä»£è½®è¯¢ï¼Œå‡å°‘ç­‰å¾…å»¶è¿Ÿ
    std::unique_lock<std::mutex> lock(requestsMutex_);
    
    // å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»å®Œæˆ
    if (completedRequests_.find(requestId) != completedRequests_.end()) {
        return true;
    }
    
    // ç­‰å¾…è¯·æ±‚å®Œæˆï¼Œä½¿ç”¨æ¡ä»¶å˜é‡é€šçŸ¥
    auto deadline = startTime + std::chrono::duration_cast<std::chrono::steady_clock::duration>(timeoutDuration);
    while (running_) {
        // ä½¿ç”¨æ¡ä»¶å˜é‡ç­‰å¾…ï¼Œè¶…æ—¶æ—¶é—´åŠ¨æ€è®¡ç®—
        auto remaining = std::chrono::duration_cast<std::chrono::milliseconds>(deadline - std::chrono::steady_clock::now()).count();
        if (remaining <= 0) {
            return false; // è¶…æ—¶
        }
        
        // ç­‰å¾…é€šçŸ¥ï¼Œæœ€å¤šç­‰å¾…remainingæ¯«ç§’
        if (resultCondition_.wait_for(lock, std::chrono::milliseconds(remaining), [this, requestId]() {
            return completedRequests_.find(requestId) != completedRequests_.end();
        })) {
            // è¯·æ±‚å·²å®Œæˆ
            return true;
        }
        
        // å†æ¬¡æ£€æŸ¥æ˜¯å¦å®Œæˆï¼ˆå¯èƒ½åœ¨wait_forè¿”å›falseæ—¶å·²ç»å®Œæˆï¼‰
        if (completedRequests_.find(requestId) != completedRequests_.end()) {
            return true;
        }
        
        // æ£€æŸ¥æ˜¯å¦è¶…æ—¶
        if (std::chrono::steady_clock::now() >= deadline) {
            return false;
        }
    }
    
    return false;
}

std::vector<RequestState> Scheduler::getRunningRequests() const {
    std::lock_guard<std::mutex> lock(requestsMutex_);
    
    std::vector<RequestState> requests;
    requests.reserve(runningRequests_.size());
    
    // Phase 1: çŠ¶æ€æœºæ ¸å¿ƒå®ç° - åªè¿”å›æ´»è·ƒè¯·æ±‚ï¼ˆPENDINGæˆ–PROCESSINGï¼‰
    // è¿‡æ»¤æ‰å·²å®Œæˆçš„è¯·æ±‚ï¼ˆCOMPLETED/TIMEOUT/FAILEDï¼‰ï¼Œé¿å… formBatch è®¡ç®— runningLength æ—¶é«˜ä¼°
    for (const auto& pair : runningRequests_) {
        const RequestState& req = pair.second;
        // ä½¿ç”¨çŠ¶æ€åˆ¤æ–­è¾…åŠ©å‡½æ•°ï¼šåªè¿”å›æ´»è·ƒè¯·æ±‚ï¼ˆPENDINGæˆ–PROCESSINGï¼‰
        if (req.isActive()) {
            requests.push_back(req);
        }
    }
    
    return requests;
}

std::vector<RequestState> Scheduler::getCompletedRequests() const {
    std::lock_guard<std::mutex> lock(requestsMutex_);
    
    std::vector<RequestState> requests;
    requests.reserve(completedRequests_.size());
    
    for (const auto& pair : completedRequests_) {
        requests.push_back(pair.second);
    }
    
    return requests;
}

size_t Scheduler::getQueueSize() const {
    std::lock_guard<std::mutex> lock(queueMutex_);
    return requestQueue_.getQueueSize();
}

SchedulerStats Scheduler::getStats() const {
    std::lock_guard<std::mutex> lock(statsMutex_);
    return stats_;
}

void Scheduler::resetStats() {
    std::lock_guard<std::mutex> lock(statsMutex_);
    stats_.reset();
}

size_t Scheduler::getRunningCount() const {
    std::lock_guard<std::mutex> lock(requestsMutex_);
    return runningRequests_.size();
}

size_t Scheduler::getMaxConcurrentRequests() const {
    return config_.maxConcurrentRequests;
}

void Scheduler::setResponseCallback(ResponseCallback callback) {
    std::lock_guard<std::mutex> lock(callbackMutex_);
    responseCallback_ = callback;
}

void Scheduler::triggerResponseCallback(size_t requestId, const RequestState& state) {
    std::lock_guard<std::mutex> lock(callbackMutex_);
    if (responseCallback_) {
        try {
            responseCallback_(requestId, state);
        } catch (const std::exception& e) {
            CLLM_ERROR("Error in response callback for requestId=%zu: %s", requestId, e.what());
        } catch (...) {
            CLLM_ERROR("Unknown error in response callback for requestId=%zu", requestId);
        }
    }
}

void Scheduler::schedulerLoop() {
    while (running_) {
        try {
            processRequests();
            
            checkRequestTimeout();
            
            checkKVCachEviction();
            
            std::unique_lock<std::mutex> lock(queueMutex_);
            
            // ğŸ”¥ ä¼˜åŒ–æ­¥éª¤2: ä½¿ç”¨åŸå­æ“ä½œå¿«é€Ÿæ£€æŸ¥ï¼ˆåªè¯»ï¼‰
            size_t queueSize = cachedQueueSize_.load(std::memory_order_relaxed);
            size_t runningCount = cachedRunningCount_.load(std::memory_order_relaxed);
            
            // å¦‚æœéœ€è¦ç²¾ç¡®å€¼æˆ–ç­‰å¾…æ¡ä»¶ï¼Œè·å–é”å¹¶åŒæ­¥
            if (queueSize == 0 && runningCount == 0) {
                // åŒé‡æ£€æŸ¥ï¼šè·å–ç²¾ç¡®å€¼
                queueSize = requestQueue_.getQueueSize();
                cachedQueueSize_.store(queueSize, std::memory_order_relaxed);
                
                {
                    std::lock_guard<std::mutex> reqLock(requestsMutex_);
                    runningCount = runningRequests_.size();
                    cachedRunningCount_.store(runningCount, std::memory_order_relaxed);
                }
            }
            
            // å¦‚æœæ²¡æœ‰é˜Ÿåˆ—è¯·æ±‚ä¸”æ²¡æœ‰è¿è¡Œä¸­çš„è¯·æ±‚ï¼Œç­‰å¾…é€šçŸ¥
            // ä½¿ç”¨è¶…æ—¶é¿å…æ°¸ä¹…é˜»å¡ï¼ˆç”¨äºå¤„ç†è¿è¡Œä¸­è¯·æ±‚çš„ç»§ç»­å¤„ç†ï¼‰
            if (queueSize == 0 && runningCount == 0) {
                // ç©ºé—²æ—¶ç­‰å¾…æ–°è¯·æ±‚ï¼Œä½¿ç”¨è¶…æ—¶ä»¥å…è®¸å®šæœŸæ£€æŸ¥
                queueCondition_.wait_for(
                    lock,
                    std::chrono::microseconds(config_.idleLoopInterval),
                    [this]() {
                        return requestQueue_.getQueueSize() > 0 || !running_;
                    }
                );
            } else if (runningCount > 0) {
                // ğŸ”¥ ä¼˜åŒ–5: æœ‰è¿è¡Œä¸­è¯·æ±‚ï¼ŒæçŸ­é—´éš”ï¼ˆ1Î¼sï¼‰å¿«é€Ÿå¤„ç†ï¼Œæœ€å¤§åŒ–ååé‡
                lock.unlock();
                std::this_thread::sleep_for(
                    std::chrono::microseconds(1)  // ä¼˜åŒ–ï¼šå‡å°‘åˆ°1Î¼sï¼Œæœ€å¤§åŒ–ååé‡
                );
            } else {
                // ğŸ”¥ ä¼˜åŒ–5: æœ‰é˜Ÿåˆ—è¯·æ±‚ä½†æœªè¿è¡Œï¼ŒçŸ­é—´éš”ï¼ˆ10Î¼sï¼‰
                lock.unlock();
                std::this_thread::sleep_for(
                    std::chrono::microseconds(10)  // ä¼˜åŒ–ï¼šå‡å°‘åˆ°10Î¼sï¼Œæ›´å¿«å“åº”
                );
            }
            
        } catch (const std::exception& e) {
            CLLM_ERROR("Error in scheduler loop: %s", e.what());
            std::this_thread::sleep_for(std::chrono::seconds(1));
        }
    }
}

void Scheduler::processRequests() {
    // ğŸ”¥ ä¼˜åŒ–æ­¥éª¤1: ä½¿ç”¨åŸå­æ“ä½œå¿«é€Ÿæ£€æŸ¥ï¼ˆåªè¯»ï¼Œæ— å‰¯ä½œç”¨ï¼‰
    // å…ˆå¿«é€Ÿæ£€æŸ¥ç¼“å­˜å€¼ï¼Œå¦‚æœä¸º0åˆ™ç›´æ¥è¿”å›ï¼Œé¿å…ä¸å¿…è¦çš„é”ç«äº‰
    size_t queueSize = cachedQueueSize_.load(std::memory_order_relaxed);
    size_t runningCount = cachedRunningCount_.load(std::memory_order_relaxed);
    
    // å¦‚æœé˜Ÿåˆ—ä¸ºç©ºä¸”æ²¡æœ‰è¿è¡Œä¸­çš„è¯·æ±‚ï¼Œç›´æ¥è¿”å›
    if (queueSize == 0 && runningCount == 0) {
        return;
    }
    
    // éœ€è¦å®é™…å¤„ç†æ—¶ï¼Œè·å–ç²¾ç¡®å€¼ï¼ˆéœ€è¦é”ï¼‰
    {
        std::lock_guard<std::mutex> queueLock(queueMutex_);
        queueSize = requestQueue_.getQueueSize();
        cachedQueueSize_.store(queueSize, std::memory_order_relaxed);
    }
    
    {
        std::lock_guard<std::mutex> reqLock(requestsMutex_);
        runningCount = runningRequests_.size();
        cachedRunningCount_.store(runningCount, std::memory_order_relaxed);
    }
    
    // å†æ¬¡æ£€æŸ¥ï¼ˆè·å–ç²¾ç¡®å€¼åï¼‰
    if (queueSize == 0 && runningCount == 0) {
        return;
    }
    
    // ğŸ”¥ ä¼˜åŒ–: æ‰¹å¤„ç†ç¼“å­˜ - æš‚æ—¶ç¦ç”¨ï¼Œé¿å…å»¶è¿Ÿå’Œæ­»é”é£é™©
    // åç»­å¯ä»¥ä¼˜åŒ–ä¸ºæ›´æ™ºèƒ½çš„ç´¯ç§¯ç­–ç•¥
    
    // Phase 1: è¯·æ±‚æµè½¬é€»è¾‘ - RequestQueue â†’ runningRequests_ï¼ˆé€šè¿‡formBatché—´æ¥å®ç°ï¼‰
    // 1. ä» RequestQueue è·å–å¾…å¤„ç†è¯·æ±‚ï¼ˆPENDINGçŠ¶æ€ï¼‰
    std::vector<RequestState> running = getRunningRequests();  // è·å–å½“å‰è¿è¡Œä¸­çš„è¯·æ±‚ï¼ˆPENDINGæˆ–PROCESSINGï¼‰
    std::vector<RequestState> pending = requestQueue_.getPendingRequests();  // ä»é˜Ÿåˆ—è·å–å¾…å¤„ç†è¯·æ±‚
    
    // ğŸ”¥ ä¼˜åŒ–: å‡å°‘åºåˆ—IDæ£€æŸ¥é¢‘ç‡ï¼Œé¿å…é¢‘ç¹é”ç«äº‰
    // åªåœ¨é˜Ÿåˆ—å¤§å°è¾ƒå¤§æ—¶æ‰æ£€æŸ¥ï¼Œå°é˜Ÿåˆ—æ—¶å‡è®¾æœ‰è¶³å¤ŸID
    size_t availableSeqIds = 0;
    if (modelExecutor_ && queueSize > 4) {
        availableSeqIds = modelExecutor_->getAvailableSequenceIdCount();
        if (availableSeqIds > 0) {
            CLLM_DEBUG("[Scheduler::processRequests] Available sequence IDs: %zu", availableSeqIds);
        }
    } else if (modelExecutor_) {
        // å°é˜Ÿåˆ—æ—¶ï¼Œå‡è®¾æœ‰è¶³å¤ŸIDï¼ˆé¿å…é”ç«äº‰ï¼‰
        availableSeqIds = 64;  // å‡è®¾æœ‰è¶³å¤ŸID
    }
    
    // 3. formBatch å½¢æˆæ‰¹å¤„ç†ï¼ˆå¯èƒ½åŒ…å«æ¥è‡ª RequestQueue çš„æ–°è¯·æ±‚å’Œè¿è¡Œä¸­çš„è¯·æ±‚ï¼‰
    // formBatch ä¼šæ ¹æ® maxConcurrentRequestsã€èµ„æºé™åˆ¶å’Œå¯ç”¨åºåˆ—IDæ•°é‡å†³å®šå“ªäº›è¯·æ±‚å¯ä»¥åŠ å…¥æ‰¹å¤„ç†
    std::vector<RequestState> batch = batchManager_.formBatch(pending, running, availableSeqIds);
    
    if (!batch.empty() && availableSeqIds > 0) {
        CLLM_DEBUG("[Scheduler::processRequests] Formed batch of %zu requests (availableSeqIds: %zu)", 
                  batch.size(), availableSeqIds);
    }
    
    // å¦‚æœ formBatch è¿”å›ç©ºï¼Œä½†é˜Ÿåˆ—ä¸­è¿˜æœ‰è¯·æ±‚ï¼Œå¯èƒ½æ˜¯å› ä¸ºèµ„æºé™åˆ¶
    // è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦ç»§ç»­å¤„ç†ï¼Œä½†éœ€è¦é€šçŸ¥è°ƒåº¦å™¨ç»§ç»­å°è¯•
    if (batch.empty() && queueSize > 0) {
        // é˜Ÿåˆ—ä¸­æœ‰è¯·æ±‚ä½†æ— æ³•å½¢æˆæ‰¹å¤„ç†ï¼Œå¯èƒ½æ˜¯èµ„æºé™åˆ¶
        // è¿”å›å¹¶è®©è°ƒåº¦å™¨ç¨åé‡è¯•
        return;
    }
    
    if (batch.empty()) {
        return;
    }
    
    // ğŸ”¥ ä¼˜åŒ–æ­¥éª¤3: æ‰¹é‡ç§»é™¤è¯·æ±‚å¹¶æ›´æ–°åŸå­ç¼“å­˜
    {
        std::lock_guard<std::mutex> queueLock(queueMutex_);
        for (const auto& req : batch) {
            requestQueue_.removeRequest(req.requestId);
        }
        cachedQueueSize_.store(requestQueue_.getQueueSize(), std::memory_order_relaxed);
    }
    
    processBatch(batch);
}

void Scheduler::processBatch(std::vector<RequestState>& batch) {
    // åœ¨å¼€å§‹å¤„ç†å‰ï¼Œæ£€æŸ¥å¹¶åˆå¹¶å·²å­˜åœ¨çš„è¯·æ±‚çŠ¶æ€
    // è¿‡æ»¤æ‰å·²å®Œæˆçš„è¯·æ±‚ï¼Œé¿å…é‡å¤å¤„ç†
    std::vector<RequestState> activeBatch;
    activeBatch.reserve(batch.size());
    
    {
        std::lock_guard<std::mutex> lock(requestsMutex_);
        for (auto& request : batch) {
            // æ£€æŸ¥è¯·æ±‚æ˜¯å¦å·²ç»å®Œæˆ
            auto completedIt = completedRequests_.find(request.requestId);
            if (completedIt != completedRequests_.end()) {
                CLLM_DEBUG("Request %llu already completed, filtering out (tokens: %zu)",
                         request.requestId, completedIt->second.generatedTokens.size());
                // å·²å®Œæˆçš„è¯·æ±‚ä¸åŠ å…¥ activeBatchï¼Œç›´æ¥è·³è¿‡
                continue;
            }
            
            // è¯·æ±‚æœªå®Œæˆï¼Œéœ€è¦å¤„ç†
            auto it = runningRequests_.find(request.requestId);
            if (it != runningRequests_.end()) {
                // è¯·æ±‚å·²å­˜åœ¨ï¼Œä» runningRequests_ è·å–å·²æœ‰çŠ¶æ€
                // ä¿ç•™å·²æœ‰çš„ generatedTokensã€isCompletedã€isFailed ç­‰çŠ¶æ€
                CLLM_DEBUG("Request %llu already in runningRequests_, merging state (existing tokens: %zu, isCompleted: %d)",
                          request.requestId, it->second.generatedTokens.size(), it->second.isCompleted ? 1 : 0);
                
                // ä¿å­˜å·²æœ‰çš„çŠ¶æ€
                std::vector<int> existingTokens = it->second.generatedTokens;
                bool existingCompleted = it->second.isCompleted;
                bool existingFailed = it->second.isFailed;
                
                // æ›´æ–° batch ä¸­çš„è¯·æ±‚å¯¹è±¡ï¼Œä¿ç•™å·²æœ‰çŠ¶æ€
                request.generatedTokens = std::move(existingTokens);
                request.isCompleted = existingCompleted;
                request.isFailed = existingFailed;
                
                // Phase 1: çŠ¶æ€è½¬æ¢ PENDING â†’ PROCESSING
                // å¦‚æœè¯·æ±‚æ˜¯PENDINGçŠ¶æ€ï¼Œè½¬æ¢ä¸ºPROCESSINGçŠ¶æ€
                if (it->second.isPending()) {
                    CLLM_DEBUG("Request %llu: PENDING â†’ PROCESSING", request.requestId);
                    it->second.isRunning = true;
                    if (it->second.startTime == 0) {
                        it->second.startTime = getCurrentTime();
                    }
                }
                
                // æ›´æ–°è¿è¡Œä¸­çš„è¯·æ±‚çŠ¶æ€
                request.isRunning = it->second.isRunning;
                request.startTime = it->second.startTime;
            } else {
                // æ–°è¯·æ±‚ï¼ŒçŠ¶æ€ä¸ºPENDINGï¼Œå‡†å¤‡è½¬æ¢ä¸ºPROCESSING
                CLLM_DEBUG("Request %llu: NEW REQUEST (PENDING), will transition to PROCESSING", request.requestId);
                request.startTime = getCurrentTime();
                request.isRunning = false;  // åˆå§‹çŠ¶æ€ä¸ºPENDINGï¼ˆisRunning=falseï¼‰
                runningRequests_[request.requestId] = request;
            }
            
            // Phase 1: çŠ¶æ€è½¬æ¢ PENDING â†’ PROCESSING
            // åœ¨æ‰¹å¤„ç†å¼€å§‹æ—¶ï¼Œæ˜ç¡®æ ‡è®°ä¸ºPROCESSINGçŠ¶æ€
            request.isRunning = true;
            requestTracker_.markAsRunning(request.requestId);
            if (modelExecutor_) {
                modelExecutor_->updateKVCacheRequestStatus(request.requestId, inference::RequestStatus::PROCESSING);
            }
            
            // æ›´æ–° runningRequests_ ä¸­çš„çŠ¶æ€
            {
                auto it = runningRequests_.find(request.requestId);
                if (it != runningRequests_.end()) {
                    it->second.isRunning = true;
                    if (it->second.startTime == 0) {
                        it->second.startTime = request.startTime;
                    }
                }
            }
            
            activeBatch.push_back(std::move(request));
        }
    }
    
    // å¦‚æœæ‰€æœ‰è¯·æ±‚éƒ½å·²å®Œæˆï¼Œç›´æ¥è¿”å›ï¼Œä¸è°ƒç”¨ processor.processBatch
    if (activeBatch.empty()) {
        CLLM_DEBUG("All requests in batch are already completed, skipping processing");
        return;
    }
    
    CLLM_INFO("Starting batch processing for %zu requests (filtered from %zu total)",
              activeBatch.size(), batch.size());
    
    SchedulerBatchProcessor processor(this, modelExecutor_, kvCache_, &batchManager_);
    processor.processBatch(activeBatch);
    
    // ğŸ”¥ ä¼˜åŒ–: æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„è¯·æ±‚ï¼Œå°†å®ƒä»¬é‡æ–°åŠ å…¥é˜Ÿåˆ—ä»¥ä¾¿é‡ç»„
    std::vector<RequestState> incompleteRequests;
    for (auto& request : activeBatch) {
        if (!request.isCompleted && !request.isFailed) {
            incompleteRequests.push_back(request);
        }
    }
    
    // å¦‚æœæœ‰æœªå®Œæˆçš„è¯·æ±‚ï¼Œå°†å®ƒä»¬é‡æ–°åŠ å…¥é˜Ÿåˆ—ï¼Œä»¥ä¾¿ä¸å…¶ä»–è¯·æ±‚é‡ç»„
    if (!incompleteRequests.empty() && incompleteRequests.size() < activeBatch.size() * 0.5) {
        CLLM_DEBUG("[Scheduler] %zu incomplete requests from batch of %zu, re-queuing for regrouping",
                  incompleteRequests.size(), activeBatch.size());
        for (auto& request : incompleteRequests) {
            // é‡æ–°åŠ å…¥é˜Ÿåˆ—ï¼Œä»¥ä¾¿ä¸å…¶ä»–è¯·æ±‚é‡ç»„
            requestQueue_.addRequest(request);
        }
    }
    
    // æ›´æ–° batch å¼•ç”¨ï¼Œç”¨äºåç»­å¤„ç†
    batch = std::move(activeBatch);
    
    // ğŸ”¥ ä¼˜åŒ–: ç«‹å³é‡Šæ”¾å·²å®Œæˆè¯·æ±‚çš„åºåˆ—IDï¼Œé¿å…é˜»å¡åç»­æ‰¹å¤„ç†
    for (auto& request : batch) {
        request.completionTime = getCurrentTime();
        
        // ğŸ”¥ å…³é”®ä¼˜åŒ–: å¦‚æœè¯·æ±‚å·²å®Œæˆï¼Œç«‹å³é‡Šæ”¾åºåˆ—IDå’ŒKVç¼“å­˜
        if (request.isCompleted || request.isFailed) {
            if (modelExecutor_) {
                // ç«‹å³æ¸…ç†KVç¼“å­˜å’Œé‡Šæ”¾åºåˆ—IDï¼Œè€Œä¸æ˜¯ç­‰åˆ°å¼‚æ­¥æ¸…ç†
                modelExecutor_->cleanupKVCache(request.requestId);
                modelExecutor_->releaseSequenceId(request.requestId);
                CLLM_DEBUG("[Scheduler] Immediately released seq_id and KV cache for completed request %llu", 
                          request.requestId);
            }
        }
        
        CLLM_DEBUG("Request %llu generated tokens: %zu", request.requestId, request.generatedTokens.size());
        
        {
            std::lock_guard<std::mutex> lock(requestsMutex_);
            if (request.isCompleted) {
                // Phase 1: çŠ¶æ€è½¬æ¢ PROCESSING â†’ COMPLETED
                // è¯·æ±‚å·²å®Œæˆï¼Œä» runningRequests_ ç§»é™¤ï¼Œæ·»åŠ åˆ° completedRequests_
                CLLM_DEBUG("Request %llu: PROCESSING â†’ COMPLETED (tokens: %zu)",
                          request.requestId, request.generatedTokens.size());
                
                if (modelExecutor_) {
                    modelExecutor_->updateKVCacheRequestStatus(request.requestId, inference::RequestStatus::COMPLETED);
                    // ğŸ”¥ ä¼˜åŒ–ï¼šç«‹å³åŒæ­¥æ¸…ç†èµ„æºï¼Œç«‹å³é‡Šæ”¾åºåˆ—IDï¼Œé¿å…é˜»å¡åç»­æ‰¹å¤„ç†
                    // ä¹‹å‰ä½¿ç”¨å¼‚æ­¥æ¸…ç†å¯¼è‡´åºåˆ—IDé‡Šæ”¾å»¶è¿Ÿï¼Œé™åˆ¶äº†æ‰¹å¤„ç†å¤§å°
                    modelExecutor_->cleanupKVCache(request.requestId);
                    modelExecutor_->releaseSequenceId(request.requestId);
                    CLLM_DEBUG("[Scheduler] Immediately released seq_id and KV cache for completed request %llu", 
                              request.requestId);
                }
                
                requestTracker_.markAsCompleted(request.requestId);
                stats_.update(request);
                runningRequests_.erase(request.requestId);
                completedRequests_[request.requestId] = request;
                
                // ğŸ”¥ ä¼˜åŒ–æ­¥éª¤3: æ›´æ–°åŸå­ç¼“å­˜
                cachedRunningCount_.store(runningRequests_.size(), std::memory_order_relaxed);
                
                // ğŸ”¥ ä¼˜åŒ–ï¼šé€šçŸ¥ç­‰å¾…è¯¥è¯·æ±‚çš„çº¿ç¨‹ï¼ˆä½¿ç”¨æ¡ä»¶å˜é‡ï¼‰
                resultCondition_.notify_all();
                
                // Phase 7: è§¦å‘å®Œæˆå›è°ƒ
                triggerResponseCallback(request.requestId, request);
            } else if (request.isFailed) {
                // Phase 1: çŠ¶æ€è½¬æ¢ PROCESSING â†’ FAILED
                // è¯·æ±‚å¤±è´¥ï¼Œä» runningRequests_ ç§»é™¤ï¼Œæ·»åŠ åˆ° completedRequests_
                CLLM_DEBUG("Request %llu: PROCESSING â†’ FAILED (error: %s)",
                          request.requestId, request.errorMessage.c_str());
                
                if (modelExecutor_) {
                    modelExecutor_->updateKVCacheRequestStatus(request.requestId, inference::RequestStatus::FAILED);
                    // ğŸ”¥ ä¼˜åŒ–ï¼šç«‹å³åŒæ­¥æ¸…ç†èµ„æºï¼Œç«‹å³é‡Šæ”¾åºåˆ—ID
                    modelExecutor_->cleanupKVCache(request.requestId);
                    modelExecutor_->releaseSequenceId(request.requestId);
                    CLLM_DEBUG("[Scheduler] Immediately released seq_id and KV cache for failed request %llu", 
                              request.requestId);
                }
                
                requestTracker_.markAsFailed(request.requestId, request.errorMessage);
                stats_.failedRequests++;
                runningRequests_.erase(request.requestId);
                completedRequests_[request.requestId] = request;
                
                // ğŸ”¥ ä¼˜åŒ–æ­¥éª¤3: æ›´æ–°åŸå­ç¼“å­˜
                cachedRunningCount_.store(runningRequests_.size(), std::memory_order_relaxed);
                
                // ğŸ”¥ ä¼˜åŒ–ï¼šé€šçŸ¥ç­‰å¾…è¯¥è¯·æ±‚çš„çº¿ç¨‹ï¼ˆä½¿ç”¨æ¡ä»¶å˜é‡ï¼‰
                resultCondition_.notify_all();
                
                // Phase 7: è§¦å‘å¤±è´¥å›è°ƒ
                triggerResponseCallback(request.requestId, request);
            } else {
                // Phase 1: çŠ¶æ€ä¿æŒ PROCESSING
                // è¯·æ±‚è¿˜åœ¨è¿è¡Œï¼ˆPROCESSINGçŠ¶æ€ï¼‰ï¼Œæ›´æ–° runningRequests_ ä¸­çš„çŠ¶æ€
                auto it = runningRequests_.find(request.requestId);
                if (it != runningRequests_.end()) {
                    // æ›´æ–°çŠ¶æ€ï¼Œä¿ç•™å·²æœ‰çš„ generatedTokens ç­‰
                    it->second = request;
                    // ğŸ”¥ ä¼˜åŒ–æ­¥éª¤3: æ›´æ–°åŸå­ç¼“å­˜ï¼ˆçŠ¶æ€æ›´æ–°ï¼‰
                    cachedRunningCount_.store(runningRequests_.size(), std::memory_order_relaxed);
                    // ç¡®ä¿ isRunning æ ‡å¿—æ­£ç¡®
                    it->second.isRunning = true;
                    CLLM_DEBUG("Request %llu: PROCESSING (continuing, tokens: %zu)",
                              request.requestId, request.generatedTokens.size());
                }
            }
        }
        
        resultCondition_.notify_all();
    }
    
    stats_.updateBatch(batch);
    
    // Phase 1: è¯·æ±‚æµè½¬é€»è¾‘ - è¯·æ±‚å®Œæˆåè‡ªåŠ¨è§¦å‘ä¸‹ä¸€ä¸ªè¯·æ±‚çš„å¤„ç†
    // æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å¾…å¤„ç†çš„è¯·æ±‚ï¼Œå¦‚æœæœ‰ï¼Œé€šçŸ¥è°ƒåº¦å™¨ç»§ç»­å¤„ç†
    // è¿™æ ·å¯ä»¥é¿å…è°ƒåº¦å™¨åœ¨æœ‰ç©ºé—²èµ„æºæ—¶è¿˜åœ¨ç­‰å¾…
    {
        std::lock_guard<std::mutex> queueLock(queueMutex_);
        size_t remainingQueueSize = requestQueue_.getQueueSize();
        if (remainingQueueSize > 0) {
            // è¯·æ±‚å®Œæˆåï¼Œå¦‚æœ RequestQueue ä¸ä¸ºç©ºä¸” runningRequests_.size() < maxConcurrentRequestsï¼Œ
            // è‡ªåŠ¨è§¦å‘ä¸‹ä¸€ä¸ªè¯·æ±‚çš„å¤„ç†ï¼ˆé€šè¿‡ queueCondition_.notify_one()ï¼‰
            CLLM_DEBUG("Request completed, notifying scheduler to process next request (queue size: %zu)", remainingQueueSize);
            queueCondition_.notify_one();
        }
    }
}

void Scheduler::checkRequestTimeout() {
    std::lock_guard<std::mutex> lock(requestsMutex_);
    
    size_t currentTimeMs = getCurrentTime();
    std::vector<size_t> timeoutRequests;
    
    for (auto& pair : runningRequests_) {
        size_t requestId = pair.first;
        RequestState& request = pair.second;
        
        if (request.isProcessing() && request.startTime > 0) {
            if (currentTimeMs < request.startTime) {
                continue;
            }
            float processingTimeSec = static_cast<float>(currentTimeMs - request.startTime) / 1000.0f;
            
            if (processingTimeSec > config_.requestTimeout) {
                CLLM_WARN("Request %zu: TIMEOUT (processing time: %.2fs, timeout: %.2fs)",
                          requestId, processingTimeSec, config_.requestTimeout);
                timeoutRequests.push_back(requestId);
            }
        }
    }
    
    for (size_t requestId : timeoutRequests) {
        auto it = runningRequests_.find(requestId);
        if (it != runningRequests_.end()) {
            RequestState request = it->second;
            
            CLLM_WARN("Request %zu: PROCESSING â†’ TIMEOUT", requestId);
            
            request.isTimeout = true;
            request.isFailed = true;
            request.errorMessage = "Request timeout";
            request.completionTime = currentTimeMs;
            
            CLLM_DEBUG("Request %zu: TIMEOUT (tokens: %zu)",
                      requestId, request.generatedTokens.size());
            
            if (modelExecutor_) {
                modelExecutor_->updateKVCacheRequestStatus(requestId, inference::RequestStatus::TIMEOUT);
                // ä¼˜åŒ–ï¼šå¼‚æ­¥æ¸…ç†èµ„æºï¼Œé¿å…é˜»å¡ä¸»æµç¨‹
                cleanupRequestAsync(requestId);
            }
            
            requestTracker_.markAsFailed(requestId, request.errorMessage);
            stats_.failedRequests++;
            runningRequests_.erase(requestId);
            completedRequests_[requestId] = request;
            
            // ğŸ”¥ ä¼˜åŒ–ï¼šé€šçŸ¥ç­‰å¾…è¯¥è¯·æ±‚çš„çº¿ç¨‹ï¼ˆä½¿ç”¨æ¡ä»¶å˜é‡ï¼‰
            resultCondition_.notify_all();
            
            // Phase 7: è§¦å‘è¶…æ—¶å›è°ƒ
            triggerResponseCallback(requestId, request);
            
            resultCondition_.notify_all();
        }
    }
}

void Scheduler::checkKVCachEviction() {
    // Phase 5: KVç¼“å­˜æ·˜æ±°
    if (!modelExecutor_) {
        return;
    }

    size_t evictedCount = modelExecutor_->evictKVCachesIfNeeded(config_.kvCacheEvictionThreshold);
    if (evictedCount > 0) {
        CLLM_INFO("[Scheduler] KV cache eviction completed: evicted %zu requests", evictedCount);
    }
}

size_t Scheduler::getCurrentTime() {
    auto now = std::chrono::steady_clock::now();
    auto duration = now.time_since_epoch();
    return static_cast<size_t>(
        std::chrono::duration_cast<std::chrono::milliseconds>(duration).count()
    );
}

void Scheduler::cleanupRequestAsync(size_t requestId) {
    std::lock_guard<std::mutex> lock(cleanupMutex_);
    cleanupQueue_.push(requestId);
    cleanupCondition_.notify_one();
}

void Scheduler::cleanupLoop() {
    size_t processedCount = 0;
    while (running_) {
        std::unique_lock<std::mutex> lock(cleanupMutex_);
        
        // ç­‰å¾…æ¸…ç†ä»»åŠ¡æˆ–åœæ­¢ä¿¡å·
        cleanupCondition_.wait_for(lock, std::chrono::milliseconds(100), [this]() {
            return !cleanupQueue_.empty() || !running_;
        });
        
        // å¤„ç†æ‰€æœ‰å¾…æ¸…ç†çš„è¯·æ±‚
        size_t batchSize = cleanupQueue_.size();
        while (!cleanupQueue_.empty()) {
            size_t requestId = cleanupQueue_.front();
            cleanupQueue_.pop();
            
            // é‡Šæ”¾é”ï¼Œæ‰§è¡Œæ¸…ç†æ“ä½œ
            lock.unlock();
            
            auto startTime = std::chrono::high_resolution_clock::now();
            if (modelExecutor_) {
                // Phase 4: æ¸…ç†KVç¼“å­˜
                modelExecutor_->cleanupKVCache(requestId);
                // Phase 2: é‡Šæ”¾åºåˆ—ID
                modelExecutor_->releaseSequenceId(requestId);
            }
            auto endTime = std::chrono::high_resolution_clock::now();
            auto duration = std::chrono::duration_cast<std::chrono::microseconds>(endTime - startTime).count();
            
            processedCount++;
            
            // æ¯å¤„ç†100ä¸ªè¯·æ±‚è®°å½•ä¸€æ¬¡ç»Ÿè®¡ä¿¡æ¯
            if (processedCount % 100 == 0) {
                CLLM_DEBUG("[Scheduler::cleanupLoop] Processed %zu cleanup tasks (avg time: %.2f us)", 
                          processedCount, static_cast<double>(duration));
            }
            
            // é‡æ–°è·å–é”ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªä»»åŠ¡
            lock.lock();
        }
        
        if (batchSize > 0) {
            CLLM_DEBUG("[Scheduler::cleanupLoop] Processed batch of %zu cleanup tasks", batchSize);
        }
    }
    CLLM_INFO("[Scheduler::cleanupLoop] Cleanup thread exiting (total processed: %zu)", processedCount);
}

}
