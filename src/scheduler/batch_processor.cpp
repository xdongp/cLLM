#include "cllm/scheduler/batch_processor.h"
#include <cstring>
#include "cllm/common/request_state.h"
#include "cllm/scheduler/scheduler.h"
#include "cllm/batch/manager.h"
#include "cllm/sampler.h"
#include "cllm/model/executor.h"
#include "cllm/common/logger.h"
#include "cllm/common/config.h"
#include <algorithm>
#include <chrono>

namespace cllm {

SchedulerBatchProcessor::SchedulerBatchProcessor(
    Scheduler* scheduler,
    ModelExecutor* executor,
    KVCache* cache,
    BatchManager* batchManager
) : scheduler_(scheduler), executor_(executor), cache_(cache), batchManager_(batchManager) {
}

SchedulerBatchProcessor::~SchedulerBatchProcessor() {
}

void SchedulerBatchProcessor::processBatch(std::vector<RequestState>& batch) {
    const int MAX_ITERATIONS = Config::instance().schedulerMaxIterations(); // é˜²æ­¢æ— é™å¾ªç¯
    int iterationCount = 0;
    
    auto batchStartTime = std::chrono::steady_clock::now();
    
    // ğŸ”¥ ä¼˜åŒ–ï¼šå‡å°‘æ—¥å¿—è¾“å‡ºï¼ˆåœ¨ç”Ÿäº§ç¯å¢ƒä¸­å…³é—­è¯¦ç»†æ—¥å¿—ï¼‰
    #ifdef CLLM_DEBUG_MODE
    CLLM_DEBUG("processBatch: Starting batch processing with %zu requests", batch.size());
    for (size_t i = 0; i < batch.size(); ++i) {
        CLLM_DEBUG("processBatch: Request %zu - ID=%llu, maxTokens=%d, generatedTokens=%zu, isCompleted=%d, isFailed=%d",
                  i, batch[i].requestId, batch[i].maxTokens, batch[i].generatedTokens.size(),
                  batch[i].isCompleted ? 1 : 0, batch[i].isFailed ? 1 : 0);
    }
    #endif
    
    // ğŸ”¥ ä¼˜åŒ–2: åˆå§‹åŒ–ç¼“å­˜ï¼ˆæ–°æ‰¹å¤„ç†å¼€å§‹æ—¶æ¸…ç©ºï¼‰
    cachedBatchInput_.clear();
    cachedTokenCounts_.clear();
    cachedRequestIds_.clear();
    
    // ğŸ”¥ ä¼˜åŒ–1: åŠ¨æ€æ‰¹å¤„ç†é‡ç»„é˜ˆå€¼ï¼ˆå½“æ´»è·ƒè¯·æ±‚æ•° < 30% æ—¶è€ƒè™‘é‡ç»„ï¼‰
    // ä¿®å¤ï¼šæ›´ç§¯æçš„é‡ç»„ç­–ç•¥ï¼ŒåŠæ—¶å°†æ…¢é€Ÿè¯·æ±‚ä¸æ–°è¯·æ±‚é‡ç»„ï¼Œå‡å°‘å“åº”æ—¶é—´é•¿å°¾
    constexpr double BATCH_REGROUP_THRESHOLD = 0.3;
    constexpr size_t MIN_EFFICIENT_BATCH_SIZE = 6;  // ä¿®å¤ï¼šå¢åŠ æœ€å°æ‰¹å¤„ç†å¤§å°ï¼Œé¿å…è¿‡åº¦é¢‘ç¹é‡ç»„
    
    while (!isBatchComplete(batch)) {
        auto activeRequests = getActiveRequests(batch);
        
        if (activeRequests.empty()) {
            #ifdef CLLM_DEBUG_MODE
            CLLM_DEBUG("processBatch: No active requests, breaking loop");
            #endif
            break;
        }
        
        // ğŸ”¥ ä¼˜åŒ–3: åŠ¨æ€æ‰¹å¤„ç†é‡ç»„ - å¦‚æœæ´»è·ƒè¯·æ±‚æ•° < æ‰¹å¤„ç†å¤§å°çš„30%ï¼Œæå‰ç»“æŸ
        // ä¿®å¤ï¼šæ›´ç§¯æçš„é‡ç»„ç­–ç•¥ï¼Œå½“æ‰¹å¤„ç†æ•ˆç‡ä¸‹é™æ—¶åŠæ—¶é‡ç»„ï¼Œé¿å…æ…¢é€Ÿè¯·æ±‚é˜»å¡æ•´ä¸ªæ‰¹å¤„ç†
        if (activeRequests.size() < batch.size() * BATCH_REGROUP_THRESHOLD) {
            CLLM_DEBUG("processBatch: Active requests (%zu) < 30%% of batch size (%zu), batch efficiency degraded", 
                      activeRequests.size(), batch.size());
            
            // ğŸ”¥ å…³é”®ä¿®å¤: å½“æ‰¹å¤„ç†æ•ˆç‡è¿‡ä½æ—¶ï¼Œæå‰ç»“æŸå½“å‰æ‰¹å¤„ç†
            // å°†å‰©ä½™çš„æ´»è·ƒè¯·æ±‚è¿”å›ç»™Schedulerï¼Œè®©å®ƒå¯ä»¥ä¸æ–°åˆ°è¾¾çš„è¯·æ±‚é‡ç»„
            // è¿™æ ·å¯ä»¥é¿å…æ…¢é€Ÿè¯·æ±‚ä¸€ç›´å ç”¨æ‰¹å¤„ç†èµ„æºï¼Œå¯¼è‡´å“åº”æ—¶é—´é•¿å°¾
            if (activeRequests.size() <= 3) {
                CLLM_INFO("processBatch: Batch efficiency too low (%zu/%zu), breaking to allow regrouping with new requests", 
                         activeRequests.size(), batch.size());
                
                // ğŸ”¥ å…³é”®ä¿®å¤ï¼šåœ¨æå‰ç»“æŸå‰ï¼Œç¡®ä¿æ‰€æœ‰è¯·æ±‚çš„çŠ¶æ€éƒ½è¢«æ­£ç¡®æ›´æ–°
                // æ£€æŸ¥æ‰€æœ‰è¯·æ±‚æ˜¯å¦å·²ç»è¾¾åˆ°maxTokensé™åˆ¶ï¼Œå¦‚æœæ˜¯åˆ™æ ‡è®°ä¸ºå®Œæˆ
                for (auto& req : batch) {
                    if (!req.isCompleted && !req.isFailed && 
                        req.generatedTokens.size() >= static_cast<size_t>(req.maxTokens)) {
                        CLLM_DEBUG("processBatch: Request %llu reached max tokens limit (%zu >= %d) before batch end, marking as completed",
                                  req.requestId, req.generatedTokens.size(), req.maxTokens);
                        req.isCompleted = true;
                        
                        // Phase 7: è§¦å‘å®Œæˆå›è°ƒ
                        if (scheduler_) {
                            scheduler_->triggerResponseCallback(req.requestId, req);
                        }
                    }
                }
                
                // æå‰ç»“æŸï¼Œå‰©ä½™çš„æ´»è·ƒè¯·æ±‚ä¼šåœ¨ä¸‹æ¬¡è°ƒåº¦æ—¶ä¸æ–°è¯·æ±‚é‡ç»„
                break;
            }
        }
        
        // è¶…æ—¶ä¿æŠ¤
        if (++iterationCount >= MAX_ITERATIONS) {
            CLLM_WARN("Reached max iterations (%d), marking all active requests as failed", MAX_ITERATIONS);
            for (auto& req : batch) {
                if (!req.isCompleted && !req.isFailed) {
                    req.isFailed = true;
                }
            }
            break;
        }
        
        CLLM_DEBUG("processBatch: Iteration %d, active requests: %zu (batch size: %zu)", 
                  iterationCount, activeRequests.size(), batch.size());
        
        // ğŸ”¥ ä¼˜åŒ–1: ä¼ é€’å·²è®¡ç®—çš„æ´»è·ƒè¯·æ±‚ï¼Œé¿å…åœ¨ processIteration ä¸­é‡å¤è®¡ç®—
        processIteration(batch, activeRequests);
    }
    
    #ifdef CLLM_DEBUG_MODE
    CLLM_DEBUG("Batch processing completed after %d iterations", iterationCount);
    
    // è°ƒè¯•ï¼šè®°å½•æœ€ç»ˆçŠ¶æ€
    for (size_t i = 0; i < batch.size(); ++i) {
        CLLM_DEBUG("processBatch: Final state - Request %zu - ID=%llu, generatedTokens=%zu, isCompleted=%d, isFailed=%d",
                  i, batch[i].requestId, batch[i].generatedTokens.size(),
                  batch[i].isCompleted ? 1 : 0, batch[i].isFailed ? 1 : 0);
    }
    #endif
    
    // ğŸ”¥ ä¼˜åŒ–: è®°å½•æ‰¹å¤„ç†æ—¶é—´å¹¶æ›´æ–°è‡ªé€‚åº”æ‰¹å¤„ç†å¤§å°
    auto batchEndTime = std::chrono::steady_clock::now();
    auto processingTimeMs = std::chrono::duration_cast<std::chrono::milliseconds>(
        batchEndTime - batchStartTime
    ).count();
    
    if (batchManager_) {
        batchManager_->updateBatchProcessingTime(processingTimeMs);
        CLLM_DEBUG("processBatch: Batch processing time: %zu ms, batch size: %zu", 
                  processingTimeMs, batch.size());
    }
}

bool SchedulerBatchProcessor::isBatchComplete(const std::vector<RequestState>& batch) const {
    for (size_t i = 0; i < batch.size(); ++i) {
        const auto& req = batch[i];
        bool completed = req.isCompleted || req.isFailed || 
                        req.generatedTokens.size() >= static_cast<size_t>(req.maxTokens);
        CLLM_DEBUG("isBatchComplete - Request %zu (ID=%llu): isCompleted=%d, isFailed=%d, generatedTokens=%zu, maxTokens=%d, completed=%d", 
                  i, req.requestId, req.isCompleted, req.isFailed, req.generatedTokens.size(), req.maxTokens, completed);
        if (!completed) {
            CLLM_DEBUG("isBatchComplete - Request %zu (ID=%llu) is NOT complete, batch continues", i, req.requestId);
            return false;
        }
    }
    CLLM_DEBUG("isBatchComplete - All requests are complete");
    return true;
}

std::vector<RequestState> SchedulerBatchProcessor::getActiveRequests(
    const std::vector<RequestState>& batch
) const {
    std::vector<RequestState> active;
    
    for (const auto& req : batch) {
        bool isActive = !req.isCompleted && !req.isFailed && 
                       req.generatedTokens.size() < static_cast<size_t>(req.maxTokens);
        if (isActive) {
            active.push_back(req);
            CLLM_DEBUG("getActiveRequests - Request ID=%llu is active (generatedTokens=%zu, maxTokens=%d)",
                      req.requestId, req.generatedTokens.size(), req.maxTokens);
        } else {
            CLLM_DEBUG("getActiveRequests - Request ID=%llu is NOT active (isCompleted=%d, isFailed=%d, generatedTokens=%zu, maxTokens=%d)",
                      req.requestId, req.isCompleted ? 1 : 0, req.isFailed ? 1 : 0,
                      req.generatedTokens.size(), req.maxTokens);
        }
    }
    
    CLLM_DEBUG("getActiveRequests - Found %zu active requests out of %zu total", active.size(), batch.size());
    return active;
}

void SchedulerBatchProcessor::processIteration(
    std::vector<RequestState>& batch,
    const std::vector<RequestState>& activeRequests
) {
    CLLM_DEBUG("processIteration called with batch size: %zu, active requests: %zu", 
              batch.size(), activeRequests.size());
    
    // ğŸ”¥ ä¼˜åŒ–1: ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„æ´»è·ƒè¯·æ±‚ï¼Œé¿å…é‡å¤è®¡ç®—
    if (activeRequests.empty()) {
        CLLM_DEBUG("No active requests, returning");
        return;
    }
    
    // ğŸ”¥ ä¼˜åŒ–: å‡å°‘è°ƒè¯•æ—¥å¿—è¾“å‡ºï¼ˆåœ¨ç”Ÿäº§ç¯å¢ƒä¸­å…³é—­è¯¦ç»†æ—¥å¿—ï¼‰
    // åªåœ¨DEBUGçº§åˆ«è¾“å‡ºå…³é”®ä¿¡æ¯
    CLLM_DEBUG("processIteration: %zu active requests", activeRequests.size());
    
    // ğŸ”¥ ä¼˜åŒ–2: å¢é‡å‡†å¤‡æ‰¹å¤„ç†è¾“å…¥ï¼ˆåªæ›´æ–°æ–°å¢çš„tokensï¼‰
    BatchInput input;
    
    // ğŸ”¥ å…³é”®ä¼˜åŒ–: å¯¹äºå•è¯·æ±‚ã€å•tokenåœºæ™¯ï¼Œç›´æ¥æ„å»ºBatchInputï¼Œè·³è¿‡BatchManagerçš„å¤æ‚é€»è¾‘
    if (activeRequests.size() == 1) {
        const auto& req = activeRequests[0];
        size_t currentTokenCount = req.getTotalLength();
        
        // æ£€æŸ¥æ˜¯å¦æ˜¯å¢é‡ç”Ÿæˆï¼ˆæœ‰å·²ç”Ÿæˆçš„tokensï¼‰
        if (!req.generatedTokens.empty()) {
            // ğŸ”¥ å•tokenå¢é‡ç”Ÿæˆï¼šç›´æ¥æ„å»ºåªåŒ…å«æ–°tokençš„BatchInputï¼ˆå®Œå…¨è·³è¿‡BatchManagerï¼‰
            // llama.cppæ”¯æŒå¢é‡æ¨ç†ï¼Œåªéœ€è¦ä¼ å…¥æ–°tokenå³å¯
            input.inputIds.clear();
            input.inputIds.push_back(req.generatedTokens.back());  // åªåŒ…å«æœ€åä¸€ä¸ªtokenï¼ˆæ–°tokenï¼‰
            input.requestPositions = {{0, 1}};
            input.sequenceIds = {req.requestId};
            input.batchSize = 1;
            
            // æ›´æ–°ç¼“å­˜ï¼ˆç”¨äºåç»­è¿­ä»£ï¼‰
            cachedTokenCounts_.clear();
            cachedTokenCounts_.push_back(currentTokenCount);
            cachedRequestIds_.clear();
            cachedRequestIds_.push_back(req.requestId);
            cachedBatchInput_ = input;  // ç¼“å­˜ç”¨äºä¸‹æ¬¡è¿­ä»£
            
            CLLM_DEBUG("Using direct batch input preparation (single request, single token, bypass BatchManager)");
        } else {
            // é¦–æ¬¡ç”Ÿæˆï¼šä½¿ç”¨BatchManagerå‡†å¤‡å®Œæ•´prompt
            input = batchManager_->prepareBatchInput(activeRequests);
            
            // åˆå§‹åŒ–ç¼“å­˜
            cachedBatchInput_ = input;
            cachedTokenCounts_.clear();
            cachedRequestIds_.clear();
            cachedTokenCounts_.push_back(currentTokenCount);
            cachedRequestIds_.push_back(req.requestId);
            
            CLLM_DEBUG("Using full batch input preparation (first iteration, single request)");
        }
    } else if (!cachedBatchInput_.empty() && cachedRequestIds_.size() == activeRequests.size()) {
        // å¤šè¯·æ±‚åœºæ™¯ï¼šæ£€æŸ¥è¯·æ±‚IDæ˜¯å¦åŒ¹é…ï¼ˆéªŒè¯æ˜¯å¦æ˜¯åŒä¸€ä¸ªæ‰¹å¤„ç†ï¼‰
        bool idsMatch = true;
        for (size_t i = 0; i < activeRequests.size() && i < cachedRequestIds_.size(); ++i) {
            if (activeRequests[i].requestId != cachedRequestIds_[i]) {
                idsMatch = false;
                break;
            }
        }
        
        if (idsMatch) {
            // è®¡ç®—å½“å‰æ¯ä¸ªè¯·æ±‚çš„tokenæ•°é‡
            std::vector<size_t> currentTokenCounts;
            currentTokenCounts.reserve(activeRequests.size());
            for (const auto& req : activeRequests) {
                currentTokenCounts.push_back(req.getTotalLength());
            }
            
            // ä½¿ç”¨å¢é‡å‡†å¤‡
            input = batchManager_->prepareBatchInputIncremental(
                activeRequests, 
                cachedBatchInput_, 
                cachedTokenCounts_
            );
            
            // æ›´æ–°ç¼“å­˜
            cachedTokenCounts_ = currentTokenCounts;
            cachedBatchInput_ = input;
            
            CLLM_DEBUG("Using incremental batch input preparation");
        } else {
            // è¯·æ±‚IDä¸åŒ¹é…ï¼Œå®Œæ•´é‡æ–°æ„å»º
            input = batchManager_->prepareBatchInput(activeRequests);
            
            // æ›´æ–°ç¼“å­˜
            cachedBatchInput_ = input;
            cachedTokenCounts_.clear();
            cachedRequestIds_.clear();
            for (const auto& req : activeRequests) {
                cachedTokenCounts_.push_back(req.getTotalLength());
                cachedRequestIds_.push_back(req.requestId);
            }
            
            CLLM_DEBUG("Using full batch input preparation (request IDs changed)");
        }
    } else {
        // é¦–æ¬¡è¿­ä»£ï¼Œå®Œæ•´æ„å»º
        input = batchManager_->prepareBatchInput(activeRequests);
        
        // åˆå§‹åŒ–ç¼“å­˜
        cachedBatchInput_ = input;
        cachedTokenCounts_.clear();
        cachedRequestIds_.clear();
        for (const auto& req : activeRequests) {
            cachedTokenCounts_.push_back(req.getTotalLength());
            cachedRequestIds_.push_back(req.requestId);
        }
        
        CLLM_DEBUG("Using full batch input preparation (first iteration)");
    }
    
    CLLM_DEBUG("BatchInput prepared:");
    CLLM_DEBUG("  Batch size: %zu", input.batchSize);
    CLLM_DEBUG("  Input IDs size: %zu", input.inputIds.size());
    CLLM_DEBUG("  Request positions size: %zu", input.requestPositions.size());
    CLLM_DEBUG("  Sequence IDs size: %zu", input.sequenceIds.size());
    
    std::string inputIdsStr;
    for (size_t i = 0; i < std::min(input.inputIds.size(), (size_t)20); ++i) {
        inputIdsStr += " " + std::to_string(input.inputIds[i]);
    }
    if (input.inputIds.size() > 20) {
        inputIdsStr += " ...";
    }
    CLLM_DEBUG("  Input IDs: [%s]", inputIdsStr.c_str());
    
    // ğŸ”¥ å…³é”®ä¿®å¤ï¼šåœ¨è°ƒç”¨executor->forward()ä¹‹å‰ï¼Œç¡®ä¿ä¸ºæ¯ä¸ªrequestIdåˆ†é…sequence ID
    // è¿™å¯¹äºæ–°è¯·æ±‚æ˜¯å¿…éœ€çš„ï¼Œå¯¹äºå·²å­˜åœ¨çš„è¯·æ±‚ï¼ŒgetSequenceIdä¼šè¿”å›å·²åˆ†é…çš„ID
    // æ³¨æ„ï¼šLlamaCppBackend::forwardBatch()å†…éƒ¨ä¼šè‡ªåŠ¨åˆ†é…sequence IDï¼Œä½†æˆ‘ä»¬éœ€è¦ç¡®ä¿
    // åœ¨é¦–æ¬¡è°ƒç”¨å‰å°±åˆ†é…å¥½ï¼Œä»¥ä¾¿æ­£ç¡®è·Ÿè¸ªä½ç½®
    // å®é™…ä¸Šï¼ŒforwardBatch()å†…éƒ¨å·²ç»å¤„ç†äº†sequence IDåˆ†é…ï¼Œæ‰€ä»¥è¿™é‡Œä¸éœ€è¦é¢å¤–å¤„ç†
    // é—®é¢˜å¯èƒ½åœ¨äºä½ç½®è®¡ç®—ï¼Œè®©æˆ‘ä»¬ç¡®ä¿BatchInputçš„requestPositionsæ­£ç¡®è®¾ç½®
    
    #ifdef CLLM_DEBUG_MODE
    CLLM_DEBUG("Calling executor_->forward(input)...");
    #endif
    BatchOutput output = executor_->forward(input);
    
    #ifdef CLLM_DEBUG_MODE
    CLLM_DEBUG("Model forward pass completed");
    #endif
    
    CLLM_DEBUG("Calling updateRequestStates...");
    updateRequestStates(batch, output);
    
    CLLM_DEBUG("processIteration completed");
}

void SchedulerBatchProcessor::updateRequestStates(
    std::vector<RequestState>& batch,
    const BatchOutput& output
) {
    CLLM_DEBUG("updateRequestStates called with batch size: %zu", batch.size());
    
    Sampler sampler;
    
    // åˆ›å»ºbatchç´¢å¼•åˆ°outputç´¢å¼•çš„æ˜ å°„
    std::vector<size_t> batchToOutputIndex;
    size_t outputIndex = 0;
    for (size_t i = 0; i < batch.size(); ++i) {
        if (!batch[i].isCompleted && !batch[i].isFailed && 
            batch[i].generatedTokens.size() < batch[i].maxTokens) {
            batchToOutputIndex.push_back(i);
        }
    }
    
    CLLM_DEBUG("Active requests in output: %zu", batchToOutputIndex.size());
    
    for (size_t activeIdx = 0; activeIdx < batchToOutputIndex.size(); ++activeIdx) {
        size_t i = batchToOutputIndex[activeIdx];
        CLLM_DEBUG("Processing request %zu (output index %zu)", i, activeIdx);
        
        if (batch[i].isCompleted || batch[i].isFailed) {
            CLLM_DEBUG("Request %zu is completed or failed, skipping", i);
            continue;
        }
        
        if (batch[i].generatedTokens.size() >= batch[i].maxTokens) {
            CLLM_DEBUG("Request %zu reached max tokens limit BEFORE generation (%zu >= %d), marking as completed", 
                      i, batch[i].generatedTokens.size(), batch[i].maxTokens);
            batch[i].isCompleted = true;
            
            // Phase 7: è§¦å‘å®Œæˆå›è°ƒ
            if (scheduler_) {
                scheduler_->triggerResponseCallback(batch[i].requestId, batch[i]);
            }
            continue;
        }
        
        CLLM_DEBUG("Request %zu - BEFORE generation: generatedTokens=%zu, maxTokens=%d", 
                  i, batch[i].generatedTokens.size(), batch[i].maxTokens);
        
        CLLM_DEBUG("Request %zu - Getting logits from output (using output index %zu)", i, activeIdx);
        
        // ä» ModelExecutor è·å–æ¨¡å‹çš„ vocab sizeï¼ˆç”¨äºæ­£ç¡®æå– logitsï¼‰
        size_t modelVocabSize = executor_ ? executor_->getConfig().vocabSize : 32000;
        FloatArray fullLogits = output.getLogitsForRequest(activeIdx, modelVocabSize);
        
        CLLM_DEBUG("Request %zu - Full logits size: %zu (model vocab_size)", i, fullLogits.size());
        
        if (fullLogits.empty()) {
            CLLM_ERROR("Request %zu got empty logits from model!", i);
            batch[i].isFailed = true;
            
            // Phase 7: è§¦å‘å¤±è´¥å›è°ƒ
            if (scheduler_) {
                scheduler_->triggerResponseCallback(batch[i].requestId, batch[i]);
            }
            continue;
        }
        
        // åœ¨é‡‡æ ·å‰å°† logits è£å‰ªåˆ° tokenizer çš„ vocab_size
        // è¿™æ˜¯æ ¹æœ¬ä¿®å¤ï¼šç¡®ä¿é‡‡æ ·åªä» tokenizer çš„æœ‰æ•ˆèŒƒå›´å†…é€‰æ‹© token
        size_t tokenizerVocabSize = executor_ ? executor_->getConfig().tokenizerVocabSize : fullLogits.size();
        if (tokenizerVocabSize == 0) {
            // å¦‚æœæœªè®¾ç½®ï¼Œé»˜è®¤ä½¿ç”¨æ¨¡å‹çš„ vocab_sizeï¼ˆå‘åå…¼å®¹ï¼‰
            tokenizerVocabSize = fullLogits.size();
        }
        FloatArray logits(std::min(fullLogits.size(), tokenizerVocabSize));
        // ğŸ”¥ ä¼˜åŒ–ï¼šä½¿ç”¨memcpyæ›¿ä»£å¾ªç¯æ‹·è´ï¼Œæå‡æ€§èƒ½
        std::memcpy(logits.data(), fullLogits.data(), logits.size() * sizeof(float));
        
        if (fullLogits.size() > tokenizerVocabSize) {
            CLLM_DEBUG("Request %zu - Clipped logits from %zu to %zu (tokenizer vocab_size)", 
                      i, fullLogits.size(), logits.size());
        }
        
        // ğŸ”¥ ä¼˜åŒ–ï¼šåªåœ¨DEBUGæ¨¡å¼ä¸‹ç»Ÿè®¡logitsä¿¡æ¯ï¼Œå‡å°‘ç”Ÿäº§ç¯å¢ƒå¼€é”€
        #ifdef CLLM_DEBUG_MODE
        // ç»Ÿè®¡ logits ä¿¡æ¯
        float maxLogit = logits.empty() ? 0.0f : logits[0];
        float minLogit = logits.empty() ? 0.0f : logits[0];
        float sumLogits = 0.0f;
        size_t nonZeroCount = 0;
        size_t zeroCount = 0;
        
        for (size_t j = 0; j < logits.size(); ++j) {
            float val = logits[j];
            if (val > maxLogit) maxLogit = val;
            if (val < minLogit) minLogit = val;
            sumLogits += val;
            if (val != 0.0f) {
                nonZeroCount++;
            } else {
                zeroCount++;
            }
        }
        
        float avgLogit = logits.empty() ? 0.0f : sumLogits / logits.size();
        
        CLLM_DEBUG("Request %zu - Logits statistics: size=%zu, max=%.6f, min=%.6f, avg=%.6f, non_zero=%zu, zero=%zu",
                   i, logits.size(), maxLogit, minLogit, avgLogit, nonZeroCount, zeroCount);
        
        // æ˜¾ç¤ºå‰ 50 ä¸ª logitsï¼ˆå¦‚æœ logits æ•°é‡å¤§äº 50ï¼‰
        std::string logitsStr;
        size_t showCount = std::min(logits.size(), (size_t)50);
        for (size_t j = 0; j < showCount; ++j) {
            if (j > 0 && j % 10 == 0) {
                logitsStr += "\n  ";
            }
            logitsStr += " " + std::to_string(logits[j]);
        }
        if (logits.size() > showCount) {
            logitsStr += " ...";
        }
        CLLM_DEBUG("Request %zu - First %zu logits: [%s]", i, showCount, logitsStr.c_str());
        
        // å¦‚æœ logits å…¨ä¸º 0ï¼Œæ˜¾ç¤ºè­¦å‘Š
        if (nonZeroCount == 0) {
            CLLM_WARN("Request %zu - WARNING: All logits are zero! This will cause uniform sampling.", i);
        }
        
        // æ˜¾ç¤ºæœ€å¤§å€¼å’Œæœ€å°å€¼çš„ä½ç½®
        if (nonZeroCount > 0) {
            size_t maxIdx = 0;
            size_t minIdx = 0;
            for (size_t j = 1; j < logits.size(); ++j) {
                if (logits[j] > logits[maxIdx]) maxIdx = j;
                if (logits[j] < logits[minIdx]) minIdx = j;
            }
            CLLM_DEBUG("Request %zu - Max logit at index %zu: %.6f, Min logit at index %zu: %.6f",
                       i, maxIdx, logits[maxIdx], minIdx, logits[minIdx]);
        }
        #endif
        
        // Get sampling parameters from request
        float temperature = batch[i].temperature;
        int topK = batch[i].topK;
        float topP = batch[i].topP;
        
        CLLM_DEBUG("Request %zu - Sampling with temp=%f, topK=%d, topP=%f", i, temperature, topK, topP);
        
        int nextToken = sampler.sample(logits, temperature, topK, topP);
        
        CLLM_DEBUG("Request %zu - Sampled token: %d", i, nextToken);
        
        if (nextToken == -1) {
            CLLM_ERROR("Sampler returned invalid token (-1) for request %zu", i);
            batch[i].isFailed = true;
            
            // Phase 7: è§¦å‘å¤±è´¥å›è°ƒ
            if (scheduler_) {
                scheduler_->triggerResponseCallback(batch[i].requestId, batch[i]);
            }
            continue;
        }
        
        // ğŸ”¥ å…³é”®ä¿®å¤ï¼šåœ¨ç”Ÿæˆtokenä¹‹å‰å†æ¬¡æ£€æŸ¥max_tokensï¼Œç¡®ä¿ä¸ä¼šè¶…å‡ºé™åˆ¶
        // è¿™æ˜¯ä¸ºäº†é˜²æ­¢åœ¨é«˜å¹¶å‘ä¸‹ï¼Œæ‰¹å¤„ç†æå‰ç»“æŸæ—¶ï¼Œæœªå®Œæˆçš„è¯·æ±‚å·²ç»ç”Ÿæˆäº†è¶…è¿‡maxTokensçš„tokens
        if (batch[i].generatedTokens.size() >= batch[i].maxTokens) {
            CLLM_DEBUG("Request %zu reached max tokens limit BEFORE adding token (%zu >= %d), marking as completed", 
                      i, batch[i].generatedTokens.size(), batch[i].maxTokens);
            batch[i].isCompleted = true;
            
            // Phase 7: è§¦å‘å®Œæˆå›è°ƒ
            if (scheduler_) {
                scheduler_->triggerResponseCallback(batch[i].requestId, batch[i]);
            }
            continue;
        }
        
        batch[i].generatedTokens.push_back(nextToken);
        CLLM_DEBUG("Request %zu - Generated tokens now: %zu", i, batch[i].generatedTokens.size());
        
        // ğŸ”’ å®‰å…¨å…œåº•ï¼šé˜²æ­¢ç”Ÿæˆæ•°é‡è¶…è¿‡ maxTokens
        if (batch[i].maxTokens > 0 &&
            batch[i].generatedTokens.size() > static_cast<size_t>(batch[i].maxTokens)) {
            CLLM_WARN("Request %zu - Generated tokens exceeded maxTokens (%zu > %d), truncating",
                      i, batch[i].generatedTokens.size(), batch[i].maxTokens);
            batch[i].generatedTokens.resize(static_cast<size_t>(batch[i].maxTokens));
        }
        
        // Check if we should complete the request
        const bool eosReached = (batch[i].eosTokenId >= 0 && nextToken == batch[i].eosTokenId);
        const bool maxTokensReached = (batch[i].generatedTokens.size() >= batch[i].maxTokens);

        if (eosReached) {
            CLLM_DEBUG("Request %zu - Reached EOS token (%d), completing", i, batch[i].eosTokenId);
            batch[i].isCompleted = true;
            
            // ğŸ”¥ ä¼˜åŒ–: ç«‹å³é‡Šæ”¾åºåˆ—IDå’ŒKVç¼“å­˜ï¼Œé¿å…é˜»å¡åç»­æ‰¹å¤„ç†
            // æ³¨æ„: è¿™é‡Œä¸èƒ½ç›´æ¥è°ƒç”¨modelExecutor_ï¼Œéœ€è¦é€šè¿‡scheduler_
            // Phase 7: è§¦å‘å®Œæˆå›è°ƒ
            if (scheduler_) {
                scheduler_->triggerResponseCallback(batch[i].requestId, batch[i]);
            }
        } else if (maxTokensReached) {
            CLLM_DEBUG("Request %zu - Reached max tokens (%zu), completing", i, batch[i].generatedTokens.size());
            batch[i].isCompleted = true;
            
            // ğŸ”¥ ä¼˜åŒ–: ç«‹å³é‡Šæ”¾åºåˆ—IDå’ŒKVç¼“å­˜ï¼Œé¿å…é˜»å¡åç»­æ‰¹å¤„ç†
            // Phase 7: è§¦å‘å®Œæˆå›è°ƒ
            if (scheduler_) {
                scheduler_->triggerResponseCallback(batch[i].requestId, batch[i]);
            }
        } else {
            CLLM_DEBUG("Request %zu - Continuing generation", i);
        }
    }
    
    CLLM_DEBUG("updateRequestStates completed");
}

}
