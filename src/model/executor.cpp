/**
 * @file executor.cpp
 * @brief æ¨¡å‹æ‰§è¡Œå™¨æ¨¡å—å®ç°
 * @author cLLM Team
 * @date 2026-01-08
 */
#include "cllm/model/executor.h"
#include "cllm/sampler.h"
#include "cllm/kv_cache/cache.h"
#include "cllm/common/logger.h"
#include <algorithm>
#include <chrono>
#include <cstring>
#include <errno.h>
#include <fstream>
#include <stdexcept>

namespace cllm {

ModelExecutor::ModelExecutor(
    const std::string& modelPath,
    const std::string& quantization,
    bool enableSIMD,
    bool useLibTorch,
    const std::string& backendType,
    const ModelConfig* initialConfig
)
    : modelPath_(modelPath)
    , quantization_(quantization)
    , enableSIMD_(enableSIMD)
    , useLibTorch_(useLibTorch)
    , backendType_(backendType)
    , modelHandle_(nullptr)
    , modelWeights_(nullptr)
    , modelSize_(0)
    , sampler_(nullptr)
    , kvCache_(nullptr)
    , quantizationManager_(nullptr)
    , inferenceOptimizer_(nullptr)
    , batchProcessor_(nullptr)
    , isModelLoaded_(false)
    , inferenceBuffer_()
    , inputBuffer_()
    , inferenceEngine_(nullptr) {
    
    if (!backendType_.empty()) {
        CLLM_INFO("[ModelExecutor] Initializing with backend: %s", backendType_.c_str());
    } else {
        CLLM_INFO("[ModelExecutor] Initializing with %s backend", 
                  (useLibTorch_ ? "LibTorch" : "Kylin"));
    }
    
    sampler_ = std::make_unique<Sampler>();

    if (initialConfig) {
        config_ = *initialConfig;
    }
    
    // Note: config_ is default-constructed, check useKVCache only if explicitly set
    // For now, skip KV cache initialization in constructor
    // kvCache_ will be initialized in loadModel() if needed
    
    if (!quantization.empty()) {
        quantizationManager_ = std::make_unique<QuantizationManager>(quantization);
    }
    
    inferenceOptimizer_ = std::make_unique<InferenceOptimizer>(enableSIMD);
    batchProcessor_ = std::make_unique<BatchProcessor>(this);
    
    // åˆå§‹åŒ– InferenceEngine
    if (!backendType_.empty()) {
        inferenceEngine_ = std::make_unique<inference::InferenceEngine>(config_, modelPath_, backendType_);
    } else {
        inferenceEngine_ = std::make_unique<inference::InferenceEngine>(config_, modelPath_, useLibTorch_);
    }
    if (!inferenceEngine_->initialize()) {
        throw std::runtime_error("ModelExecutor: failed to initialize inference engine");
    }
    
    // åŒæ­¥backendæ›´æ–°åçš„config(ä¾‹å¦‚LibTorchè‡ªåŠ¨æ£€æµ‹vocab_size)
    config_ = inferenceEngine_->getConfig();
    CLLM_INFO("[ModelExecutor] Config synchronized from InferenceEngine");
    CLLM_INFO("[ModelExecutor]   vocab_size: %u", config_.vocabSize);
    
    _optimizeMemoryUsage();
}

ModelExecutor::~ModelExecutor() {
    unloadModel();
}

void ModelExecutor::loadModel() {
    if (isModelLoaded_) {
        CLLM_WARN("Model already loaded");
        return;
    }

    // âœ… InferenceEngine ä¼šåœ¨æ„é€ å‡½æ•°é‡Œå®Œæˆ initialize()ï¼Œå¹¶ç”±åç«¯è´Ÿè´£æƒé‡åŠ è½½ï¼š
    // - KylinBackend: é€šè¿‡ ModelLoader è¯»å– .bin
    // - LibTorchBackend: åŠ è½½ TorchScript .pt
    // è¿™é‡Œä¸å†é‡å¤æŠŠæ•´ä¸ªæƒé‡æ–‡ä»¶è¯»å…¥å†…å­˜ï¼Œé¿å…åŒä»½åŠ è½½å¯¼è‡´ OOMã€‚
    if (inferenceEngine_ && inferenceEngine_->isInitialized()) {
        CLLM_INFO("[ModelExecutor] InferenceEngine already initialized, skipping raw weight loading");
        _applyInferenceOptimizations();
        _warmupModel();
        isModelLoaded_ = true;
        return;
    }

    // å›é€€è·¯å¾„ï¼šç†è®ºä¸Šä¸åº”å‘ç”Ÿ
    throw std::runtime_error("ModelExecutor: inference engine not initialized");
}

void ModelExecutor::unloadModel() {
    if (modelWeights_) {
        delete[] static_cast<float*>(modelWeights_);
        modelWeights_ = nullptr;
        modelHandle_ = nullptr;  // ç¡®ä¿handleä¹Ÿè¢«æ¸…ç©º
    } else if (modelHandle_) {
        // å¯¹äºé‡åŒ–æ¨¡å‹ï¼ŒmodelWeights_å¯èƒ½ä¸ºç©ºä½†modelHandle_ä¸ä¸ºç©º
        modelHandle_ = nullptr;
    }
    
    modelSize_ = 0;
    isModelLoaded_ = false;
}

void ModelExecutor::_loadFullPrecisionModel() {
    CLLM_INFO("Attempting to load full precision model from: %s", modelPath_.c_str());
    std::ifstream file(modelPath_, std::ios::binary | std::ios::ate);
    if (!file.is_open()) {
        CLLM_ERROR("Failed to open model file: %s, error: %s", modelPath_.c_str(), strerror(errno));
        throw std::runtime_error("Failed to open model file: " + modelPath_ + ", error: " + strerror(errno));
    }
    
    modelSize_ = file.tellg();
    CLLM_INFO("Model file size: %zu bytes", modelSize_);
    
    if (modelSize_ == 0) {
        file.close();
        CLLM_ERROR("Model file is empty: %s", modelPath_.c_str());
        throw std::runtime_error("Model file is empty: " + modelPath_);
    }
    
    size_t numFloats = modelSize_ / sizeof(float);
    CLLM_DEBUG("Number of floats: %zu", numFloats);
    
    modelWeights_ = new float[numFloats];
    if (!modelWeights_) {
        file.close();
        CLLM_ERROR("Failed to allocate memory for model weights");
        throw std::runtime_error("Failed to allocate memory for model weights");
    }
    
    file.seekg(0, std::ios::beg);
    file.read(static_cast<char*>(modelWeights_), modelSize_);
    
    if (!file) {
        CLLM_ERROR("Failed to read model file: %s, bytes read: %zu", modelPath_.c_str(), file.gcount());
        delete[] static_cast<float*>(modelWeights_);
        modelWeights_ = nullptr;
        file.close();
        throw std::runtime_error("Failed to read model file: " + modelPath_);
    }
    
    file.close();
    modelHandle_ = modelWeights_;
    
    CLLM_INFO("Model weights loaded successfully");
}

void ModelExecutor::_loadInt8QuantizedModel() {
    CLLM_INFO("Loading int8 quantized model from: %s", modelPath_.c_str());
    std::ifstream file(modelPath_, std::ios::binary | std::ios::ate);
    if (!file.is_open()) {
        CLLM_ERROR("Failed to open int8 quantized model file: %s, error: %s", 
                  modelPath_.c_str(), strerror(errno));
        throw std::runtime_error("Failed to open int8 quantized model file: " + modelPath_ + ", error: " + strerror(errno));
    }
    
    size_t fileSize = file.tellg();
    file.seekg(0, std::ios::beg);
    
    if (fileSize == 0) {
        file.close();
        CLLM_ERROR("Int8 quantized model file is empty: %s", modelPath_.c_str());
        throw std::runtime_error("Int8 quantized model file is empty: " + modelPath_);
    }
    
    int8_t* quantizedWeights = new (std::nothrow) int8_t[fileSize];
    if (!quantizedWeights) {
        file.close();
        CLLM_ERROR("Failed to allocate memory for int8 quantized weights: %zu bytes", fileSize);
        throw std::runtime_error("Failed to allocate memory for int8 quantized weights");
    }
    
    file.read(reinterpret_cast<char*>(quantizedWeights), fileSize);
    if (!file) {
        delete[] quantizedWeights;
        file.close();
        CLLM_ERROR("Failed to read int8 quantized model file: %s, bytes read: %zu", 
                  modelPath_.c_str(), file.gcount());
        throw std::runtime_error("Failed to read int8 quantized model file: " + modelPath_);
    }
    file.close();
    
    // ä½¿ç”¨é‡åŒ–ç®¡ç†å™¨è¿›è¡Œå¤„ç†
    if (quantizationManager_) {
        quantizationManager_->quantizeModel(quantizedWeights, fileSize);
    }
    
    // è½¬æ¢ä¸ºfloatç”¨äºæ¨ç†
    modelSize_ = fileSize * sizeof(float);
    modelWeights_ = new (std::nothrow) float[fileSize];
    if (!modelWeights_) {
        delete[] quantizedWeights;
        CLLM_ERROR("Failed to allocate memory for float weights: %zu bytes", modelSize_);
        throw std::runtime_error("Failed to allocate memory for float weights");
    }
    
    for (size_t i = 0; i < fileSize; ++i) {
        static_cast<float*>(modelWeights_)[i] = static_cast<float>(quantizedWeights[i]) / 127.0f;
    }
    
    delete[] quantizedWeights;
    modelHandle_ = modelWeights_;
    CLLM_INFO("Int8 quantized model loaded successfully, size: %zu bytes", modelSize_);
}

void ModelExecutor::_loadInt4QuantizedModel() {
    CLLM_INFO("Loading int4 quantized model from: %s", modelPath_.c_str());
    std::ifstream file(modelPath_, std::ios::binary | std::ios::ate);
    if (!file.is_open()) {
        CLLM_ERROR("Failed to open int4 quantized model file: %s, error: %s", 
                  modelPath_.c_str(), strerror(errno));
        throw std::runtime_error("Failed to open int4 quantized model file: " + modelPath_ + ", error: " + strerror(errno));
    }
    
    size_t fileSize = file.tellg();
    file.seekg(0, std::ios::beg);
    
    if (fileSize == 0) {
        file.close();
        CLLM_ERROR("Int4 quantized model file is empty: %s", modelPath_.c_str());
        throw std::runtime_error("Int4 quantized model file is empty: " + modelPath_);
    }
    
    uint8_t* quantizedWeights = new (std::nothrow) uint8_t[fileSize];
    if (!quantizedWeights) {
        file.close();
        CLLM_ERROR("Failed to allocate memory for int4 quantized weights: %zu bytes", fileSize);
        throw std::runtime_error("Failed to allocate memory for int4 quantized weights");
    }
    
    file.read(reinterpret_cast<char*>(quantizedWeights), fileSize);
    if (!file) {
        delete[] quantizedWeights;
        file.close();
        CLLM_ERROR("Failed to read int4 quantized model file: %s, bytes read: %zu", 
                  modelPath_.c_str(), file.gcount());
        throw std::runtime_error("Failed to read int4 quantized model file: " + modelPath_);
    }
    file.close();
    
    // ä½¿ç”¨é‡åŒ–ç®¡ç†å™¨è¿›è¡Œå¤„ç†
    if (quantizationManager_) {
        quantizationManager_->quantizeModel(quantizedWeights, fileSize);
    }
    
    // è½¬æ¢ä¸ºfloatç”¨äºæ¨ç†
    modelSize_ = fileSize * 2 * sizeof(float);
    modelWeights_ = new (std::nothrow) float[fileSize * 2];
    if (!modelWeights_) {
        delete[] quantizedWeights;
        CLLM_ERROR("Failed to allocate memory for float weights: %zu bytes", modelSize_);
        throw std::runtime_error("Failed to allocate memory for float weights");
    }
    
    for (size_t i = 0; i < fileSize; ++i) {
        uint8_t packed = quantizedWeights[i];
        static_cast<float*>(modelWeights_)[i * 2] = (static_cast<float>(packed & 0x0F) - 7.5f) / 7.5f;
        static_cast<float*>(modelWeights_)[i * 2 + 1] = (static_cast<float>((packed >> 4) & 0x0F) - 7.5f) / 7.5f;
    }
    
    delete[] quantizedWeights;
    modelHandle_ = modelWeights_;
    CLLM_INFO("Int4 quantized model loaded successfully, size: %zu bytes", modelSize_);
}

void ModelExecutor::_applyInferenceOptimizations() {
    if (inferenceOptimizer_) {
        inferenceOptimizer_->applyOptimizations();
    }
}

void ModelExecutor::_warmupModel() {
    std::vector<int> dummyInput(config_.vocabSize > 10 ? 10 : config_.vocabSize, 1);
    
    try {
        generate(dummyInput, 1, 1.0f);
    } catch (...) {
        
    }
}

FloatArray ModelExecutor::_prepareInput(const std::vector<int>& inputIds) {
    size_t inputSize = inputIds.size();
    FloatArray inputTensor(inputSize);
    
    for (size_t i = 0; i < inputSize; ++i) {
        inputTensor[i] = static_cast<float>(inputIds[i]);
    }
    
    return inputTensor;
}

void ModelExecutor::_processOutput(FloatArray& logits, size_t batchSize, size_t vocabSize) {
    if (kvCache_ && config_.useKVCache) {
        
    }
}

BatchOutput ModelExecutor::forward(const BatchInput& input) {
    // ğŸ”¥ ä¼˜åŒ–ï¼šç§»é™¤ä¸å¿…è¦çš„è®¡æ—¶å¼€é”€ï¼ˆåœ¨æ€§èƒ½æµ‹è¯•ä¸­å¯ä»¥è·³è¿‡ï¼‰
    // auto startTime = std::chrono::high_resolution_clock::now();
    
    if (!isModelLoaded_) {
        throw std::runtime_error("Model is not loaded");
    }
    
    #ifdef CLLM_DEBUG_MODE
    CLLM_DEBUG("ModelExecutor::forward() called");
    CLLM_DEBUG("  Input batch size: %zu", input.batchSize);
    CLLM_DEBUG("  Input IDs size: %zu", input.inputIds.size());
    CLLM_DEBUG("  Request positions count: %zu", input.requestPositions.size());
    #endif
    
    // ğŸ”¥ ä¼˜åŒ–ï¼šç§»é™¤modelMutex_é”ï¼Œå› ä¸ºï¼š
    // 1. InferenceEngineï¼ˆLlamaCppBackendï¼‰å†…éƒ¨å·²ç»æœ‰é”ä¿æŠ¤llama_decode
    // 2. å¤–å±‚è°ƒç”¨è€…ï¼ˆå¦‚Schedulerï¼‰ä¼šæä¾›å¿…è¦çš„åŒæ­¥
    // 3. å‡å°‘é”ç«äº‰ï¼Œæå‡å¹¶å‘æ€§èƒ½
    // std::lock_guard<std::mutex> lock(modelMutex_);  // å·²ç§»é™¤
    
    // ğŸ”¥ ä¼˜åŒ–ï¼šç§»é™¤_prepareInput()è°ƒç”¨ï¼Œå› ä¸ºInferenceEngine::forwardBatch()ç›´æ¥æ¥å—std::vector<int>
    // è¿™æ ·å¯ä»¥é¿å…ä¸å¿…è¦çš„intåˆ°floatè½¬æ¢å’ŒFloatArrayåˆ›å»º
    // FloatArray inputTensor = _prepareInput(input.inputIds);  // å·²ç§»é™¤
    
    size_t totalTokens = input.getTotalTokens();
    #ifdef CLLM_DEBUG_MODE
    CLLM_DEBUG("  Total tokens: %zu", totalTokens);
    #endif
    
    // ğŸ”¥ ä¼˜åŒ–ï¼šç›´æ¥è·å–Tensorï¼Œé¿å…æ•°æ®æ‹·è´
    if (!inferenceEngine_) {
        throw std::runtime_error("ModelExecutor::forward: inference engine not initialized");
    }
    
    inference::Tensor logitsTensor = inferenceEngine_->forwardBatch(
        input.inputIds,
        input.requestPositions,
        input.batchSize,
        input.sequenceIds
    );
    
    const auto& logitsShape = logitsTensor.shape();
    if (logitsShape.size() != 2 || logitsShape[0] != totalTokens || logitsShape[1] != config_.vocabSize) {
        throw std::runtime_error("ModelExecutor::forward: logits shape mismatch");
    }
    
    #ifdef CLLM_DEBUG_MODE
    CLLM_DEBUG("  Output tensor size: %zu", logitsTensor.size());
    CLLM_DEBUG("  Expected size: %zu * %zu = %zu", totalTokens, config_.vocabSize, totalTokens * config_.vocabSize);
    #endif
    
    // ğŸ”¥ ä¼˜åŒ–ï¼š_processOutput()ç›®å‰æ˜¯ç©ºå®ç°ï¼Œå¯ä»¥è·³è¿‡
    // _processOutput(outputTensor, input.batchSize, config_.vocabSize);
    
    BatchOutput output;
    // ğŸ”¥ ä¼˜åŒ–ï¼šç›´æ¥ä½¿ç”¨Tensorï¼Œå®Œå…¨é¿å…æ•°æ®æ‹·è´
    // inference::Tensorå°±æ˜¯kylin::Tensorï¼ˆé€šè¿‡using Tensor = kylin::Tensorï¼‰ï¼Œå¯ä»¥ç›´æ¥ç§»åŠ¨
    output.logitsTensor = std::make_unique<kylin::Tensor>(std::move(logitsTensor));
    // ä¸ºäº†å…¼å®¹æ€§ï¼Œä»ç„¶åˆ›å»ºç©ºçš„FloatArrayï¼ˆgetLogitsForRequestä¼šä¼˜å…ˆä½¿ç”¨logitsTensorï¼‰
    output.logits = FloatArray();  // ç©ºFloatArrayï¼ŒgetLogitsForRequestä¼šä½¿ç”¨logitsTensor
    output.requestPositions = input.requestPositions;
    output.sequenceIds = input.sequenceIds;
    
    #ifdef CLLM_DEBUG_MODE
    CLLM_DEBUG("  BatchOutput created with logits size: %zu", output.logits.size());
    #endif
    
    // ğŸ”¥ ä¼˜åŒ–ï¼šå‡å°‘ä¸å¿…è¦çš„ç»Ÿè®¡æ›´æ–°å¼€é”€ï¼ˆåœ¨æ€§èƒ½æµ‹è¯•ä¸­å¯ä»¥è·³è¿‡ï¼‰
    // auto endTime = std::chrono::high_resolution_clock::now();
    // float inferenceTime = std::chrono::duration<float>(endTime - startTime).count();
    // stats_.update(inferenceTime, totalTokens);  // æš‚æ—¶è·³è¿‡ï¼Œå‡å°‘å¼€é”€
    
    return output;
}

std::vector<int> ModelExecutor::generate(
    const std::vector<int>& inputIds,
    size_t maxNewTokens,
    float temperature
) {
    std::vector<int> generatedTokens;
    std::vector<int> currentInput = inputIds;
    
    for (size_t i = 0; i < maxNewTokens; ++i) {
        BatchInput batchInput;
        batchInput.inputIds = currentInput;
        batchInput.batchSize = 1;
        batchInput.requestPositions = {{0, currentInput.size()}};
        batchInput.sequenceIds = {0};
        
        BatchOutput output = forward(batchInput);
        
        // logits å­˜å‚¨å¸ƒå±€æ˜¯æŒ‰ model vocab å±•å¼€çš„ï¼Œä½†é‡‡æ ·åº”é™åˆ¶åœ¨ tokenizer vocab å†…
        // ï¼ˆä¸¤è€…å¯èƒ½ä¸åŒï¼Œä¾‹å¦‚ Qwen3: model vocab=151936, tokenizer vocab=151669ï¼‰
        FloatArray fullLogits = output.getLogitsForRequest(0, config_.vocabSize);
        size_t sampleVocabSize = std::min(config_.tokenizerVocabSize, config_.vocabSize);
        FloatArray logits(sampleVocabSize);
        for (size_t j = 0; j < sampleVocabSize; ++j) {
            logits[j] = fullLogits[j];
        }

        int nextToken = sampler_->sample(logits, temperature);
        
        generatedTokens.push_back(nextToken);
        currentInput.push_back(nextToken);
        
        if (nextToken == 0) {
            break;
        }
    }
    
    return generatedTokens;
}

ModelStats ModelExecutor::getStats() const {
    return stats_;
}

void ModelExecutor::resetStats() {
    stats_.reset();
}

Sampler* ModelExecutor::getSampler() const {
    return sampler_.get();
}

KVCache* ModelExecutor::getKVCache() const {
    return kvCache_.get();
}

const ModelConfig& ModelExecutor::getConfig() const {
    return config_;
}

std::string ModelExecutor::getBackendName() const {
    if (!inferenceEngine_) {
        return "None";
    }
    return inferenceEngine_->getBackendType();
}

void ModelExecutor::setConfig(const ModelConfig& config) {
    config_ = config;
    // å¦‚æœ InferenceEngine å·²åˆå§‹åŒ–ï¼Œä¹Ÿæ›´æ–°å®ƒçš„é…ç½®
    if (inferenceEngine_) {
        // InferenceEngine çš„é…ç½®æ˜¯åœ¨æ„é€ æ—¶ä¼ å…¥çš„ï¼Œéœ€è¦é‡æ–°åˆ›å»º
        if (!backendType_.empty()) {
            inferenceEngine_ = std::make_unique<inference::InferenceEngine>(config_, modelPath_, backendType_);
        } else {
            inferenceEngine_ = std::make_unique<inference::InferenceEngine>(config_, modelPath_, useLibTorch_);
        }
        if (!inferenceEngine_->initialize()) {
            throw std::runtime_error("ModelExecutor::setConfig: failed to reinitialize inference engine");
        }
    }
}

void ModelExecutor::setTokenizerVocabSize(size_t tokenizerVocabSize) {
    config_.tokenizerVocabSize = tokenizerVocabSize;
    // tokenizerVocabSize ä¸å½±å“ InferenceEngineï¼Œæ‰€ä»¥ä¸éœ€è¦é‡æ–°åˆå§‹åŒ–
}

bool ModelExecutor::releaseSequenceId(size_t requestId) const {
    if (!inferenceEngine_) {
        return false;
    }
    
    // Phase 2: å§”æ‰˜ç»™ InferenceEngine
    return inferenceEngine_->releaseSequenceId(requestId);
}

bool ModelExecutor::cleanupKVCache(size_t requestId) const {
    if (!inferenceEngine_) {
        return false;
    }
    
    // Phase 4: å§”æ‰˜ç»™ InferenceEngine
    return inferenceEngine_->cleanupKVCache(requestId);
}

bool ModelExecutor::updateKVCacheRequestStatus(size_t requestId, inference::RequestStatus status) const {
    if (!inferenceEngine_) {
        return false;
    }

    return inferenceEngine_->updateKVCacheRequestStatus(requestId, status);
}

size_t ModelExecutor::evictKVCachesIfNeeded(double evictionThreshold) const {
    if (!inferenceEngine_) {
        return 0;
    }

    return inferenceEngine_->evictKVCachesIfNeeded(evictionThreshold);
}

size_t ModelExecutor::getAvailableSequenceIdCount() const {
    if (!inferenceEngine_) {
        return 0;
    }

    return inferenceEngine_->getAvailableSequenceIdCount();
}

int ModelExecutor::sampleToken(const std::vector<int>& inputIds, float temperature) {
    if (!isModelLoaded_) {
        throw std::runtime_error("Model is not loaded");
    }
    
    // ğŸ”¥ ä¼˜åŒ–ï¼šç§»é™¤modelMutex_é”ï¼ˆåŸå› åŒforward()ï¼‰
    // std::lock_guard<std::mutex> lock(modelMutex_);  // å·²ç§»é™¤
    
    BatchInput batchInput;
    batchInput.inputIds = inputIds;
    batchInput.batchSize = 1;
    batchInput.requestPositions = {{0, inputIds.size()}};
    batchInput.sequenceIds = {0};
    
    BatchOutput output = forward(batchInput);
    
    // logits å­˜å‚¨å¸ƒå±€æ˜¯æŒ‰ model vocab å±•å¼€çš„ï¼Œä½†é‡‡æ ·åº”é™åˆ¶åœ¨ tokenizer vocab å†…
    FloatArray fullLogits = output.getLogitsForRequest(0, config_.vocabSize);
    size_t sampleVocabSize = std::min(config_.tokenizerVocabSize, config_.vocabSize);
    FloatArray logits(sampleVocabSize);
    for (size_t j = 0; j < sampleVocabSize; ++j) {
        logits[j] = fullLogits[j];
    }

    return sampler_->sample(logits, temperature);
}

FloatArray ModelExecutor::_executeModelInference(const BatchInput& input) {
    size_t totalTokens = input.getTotalTokens();
    size_t outputSize = totalTokens * config_.vocabSize;
    
    CLLM_DEBUG("[ModelExecutor::_executeModelInference] Total tokens: %zu", totalTokens);
    CLLM_DEBUG("[ModelExecutor::_executeModelInference] Vocab size: %zu", config_.vocabSize);
    CLLM_DEBUG("[ModelExecutor::_executeModelInference] Output size: %zu (%zu * %zu)", outputSize, totalTokens, config_.vocabSize);
    
    if (!inferenceEngine_) {
        throw std::runtime_error("ModelExecutor::_executeModelInference: inference engine not initialized");
    }
    
    // è°ƒç”¨è‡ªç ”æ¨ç†å¼•æ“ï¼Œè·å¾— [total_tokens, vocab_size] logits
    // Phase 2: ä¼ é€’ sequenceIdsï¼ˆrequestIdï¼‰ç”¨äºåºåˆ—IDç®¡ç†
    inference::Tensor logitsTensor = inferenceEngine_->forwardBatch(
        input.inputIds,
        input.requestPositions,
        input.batchSize,
        input.sequenceIds  // ä¼ é€’ requestId åˆ—è¡¨
    );
    
    const auto& logitsShape = logitsTensor.shape();
    if (logitsShape.size() != 2 || logitsShape[0] != totalTokens || logitsShape[1] != config_.vocabSize) {
        throw std::runtime_error("ModelExecutor::_executeModelInference: logits shape mismatch");
    }
    
    // ğŸ”¥ ä¼˜åŒ–ï¼šç›´æ¥ä½¿ç”¨Tensorï¼Œé¿å…æ•°æ®æ‹·è´
    // æ³¨æ„ï¼šç”±äºBatchOutputç°åœ¨æ”¯æŒç›´æ¥ä½¿ç”¨Tensorï¼Œæˆ‘ä»¬å¯ä»¥é¿å…ä»Tensoråˆ°FloatArrayçš„æ‹·è´
    // ä½†æ˜¯ï¼Œç”±äºforward()è¿”å›BatchOutputï¼Œè€ŒBatchOutputéœ€è¦FloatArrayæˆ–Tensorï¼Œ
    // æˆ‘ä»¬ä»ç„¶éœ€è¦åˆ›å»ºä¸€ä¸ªFloatArrayï¼ˆä¸ºäº†å…¼å®¹æ€§ï¼‰ï¼Œæˆ–è€…ç›´æ¥ä½¿ç”¨Tensor
    // 
    // ä¸ºäº†æœ€å¤§åŒ–æ€§èƒ½ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨Tensorï¼Œé¿å…æ‹·è´
    // ä½†æ˜¯ï¼Œç”±äºforward()çš„è¿”å›ç±»å‹æ˜¯BatchOutputï¼Œè€ŒBatchOutput.logitsæ˜¯FloatArrayï¼Œ
    // æˆ‘ä»¬éœ€è¦åœ¨forward()ä¸­è®¾ç½®logitsTensorè€Œä¸æ˜¯logits
    
    // æš‚æ—¶ä»ç„¶ä½¿ç”¨FloatArrayï¼ˆä¸ºäº†å…¼å®¹æ€§ï¼‰ï¼Œä½†å¯ä»¥è€ƒè™‘åç»­ä¼˜åŒ–ä¸ºç›´æ¥ä½¿ç”¨Tensor
    FloatArray outputTensor(outputSize);
    const float* src = logitsTensor.data();
    std::memcpy(outputTensor.data(), src, outputSize * sizeof(float));
    
    CLLM_DEBUG("[ModelExecutor::_executeModelInference] Generated logits for %zu token positions", totalTokens);
    return outputTensor;
}

void ModelExecutor::_optimizeMemoryUsage() {
    // ä¼˜åŒ–å†…å­˜ä½¿ç”¨
    _enableMemoryCompression();
    
    // æ ¹æ®æ¨¡å‹å¤§å°åˆ†é…æ¨ç†ç¼“å†²åŒº
    size_t bufferSize = config_.vocabSize * config_.maxSequenceLength * sizeof(float);
    inferenceBuffer_ = FloatArray(bufferSize);
    inputBuffer_ = FloatArray(bufferSize);
}

void ModelExecutor::_enableMemoryCompression() {
    // å¯ç”¨å†…å­˜å‹ç¼©
    if (config_.useMemoryCompression) {
        // å®ç°å†…å­˜å‹ç¼©é€»è¾‘
    }
}



}  // namespace cllm
