/**
 * @file main.cpp
 * @brief cLLM æœåŠ¡å™¨ä¸»å…¥å£
 * @author cLLM Team
 * @date 2026-01-10
 */

#include "cllm/http/http_server.h"

#include <signal.h>
#include <getopt.h>

#include <memory>
#include <string>
#include <filesystem>
#include <optional>
#include <vector>
#include <thread>
#include <chrono>

#include "cllm/http/handler.h"
#include "cllm/http/health_endpoint.h"
#include "cllm/http/generate_endpoint.h"
#include "cllm/http/encode_endpoint.h"
#include "cllm/http/benchmark_endpoint.h"
#include "cllm/scheduler/scheduler.h"
#include "cllm/model/executor.h"
#include "cllm/tokenizer/manager.h"
#include "cllm/common/config.h"
#include "cllm/common/logger.h"
#include "cllm/common/asio_handler.h"

// å…¨å±€å˜é‡ç”¨äºä¿¡å·å¤„ç†
static std::unique_ptr<cllm::Scheduler> g_scheduler;
static std::unique_ptr<cllm::ModelExecutor> g_modelExecutor;
static std::unique_ptr<cllm::TokenizerManager> g_tokenizerManager;

/**
 * @brief ä¿¡å·å¤„ç†å‡½æ•°
 * @param signal ä¿¡å·ç¼–å·
 */
void signalHandler(int signal) {
    CLLM_INFO("Received signal %d, shutting down gracefully...", signal);
    
    // åœæ­¢è°ƒåº¦å™¨
    if (g_scheduler) {
        CLLM_INFO("Stopping scheduler...");
        g_scheduler->stop();
    }
    
    // åœæ­¢æœåŠ¡å™¨
    CLLM_INFO("Stopping HTTP server...");
    cllm::HttpServer::stop();
    
    CLLM_INFO("Shutdown complete");
    cllm::Logger::instance().flush();
    exit(0);
}

/**
 * @brief æ‰“å°ä½¿ç”¨è¯´æ˜
 * @param programName ç¨‹åºåç§°
 */
void printUsage(const char* programName) {
    CLLM_INFO("Usage: %s [options]", programName);
    CLLM_INFO("");
    CLLM_INFO("Options:");
    CLLM_INFO("  --model-path PATH        Path to model directory (required)");
    CLLM_INFO("  --port PORT              Server port (default: 8080)");
    CLLM_INFO("  --host HOST              Server host (default: 0.0.0.0)");
    CLLM_INFO("  --quantization TYPE      Quantization type: fp16, int8, int4 (default: fp16)");
    CLLM_INFO("  --max-batch-size SIZE    Maximum batch size (default: 8)");
    CLLM_INFO("  --max-context-length LEN Maximum context length (default: 2048)");
    CLLM_INFO("  --use-libtorch           Use LibTorch backend (default: false, use Kylin)");
    CLLM_INFO("  --config PATH            Path to config file (optional)");
    CLLM_INFO("  --log-level LEVEL        Log level: trace, debug, info, warn, error (default: info)");
    CLLM_INFO("  --log-file PATH          Log file path (optional)");
    CLLM_INFO("  --help                   Show this help message");
    CLLM_INFO("");
    CLLM_INFO("Examples:");
    CLLM_INFO("  %s --model-path /path/to/model", programName);
    CLLM_INFO("  %s --model-path /path/to/model --port 9000 --use-libtorch", programName);
}

/**
 * @brief æ‰“å°æœåŠ¡å™¨æ¨ªå¹…
 */
void printBanner() {
    CLLM_INFO("");
    CLLM_INFO("  _____ _      _      __  __ ");
    CLLM_INFO(" / ____| |    | |    |  \\/  |");
    CLLM_INFO("| |    | |    | |    | \\  / |");
    CLLM_INFO("| |    | |    | |    | |\\/| |");
    CLLM_INFO("| |____| |____| |____| |  | |");
    CLLM_INFO(" \\_____|______|______|_|  |_|");
    CLLM_INFO("");
}

/**
 * @brief ä¸»å‡½æ•°
 * @param argc å‚æ•°ä¸ªæ•°
 * @param argv å‚æ•°æ•°ç»„
 * @return ç¨‹åºé€€å‡ºç 
 */
int main(int argc, char* argv[]) {
    // åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
    cllm::Logger::instance().setLevel(spdlog::level::info);
    
    // é…ç½®å‚æ•°ï¼ˆå…ˆæ”¶é›† CLI è¦†ç›–é¡¹ï¼›Config æ–‡ä»¶åŠ è½½åå†è®¡ç®—æœ€ç»ˆå€¼ï¼‰
    std::optional<std::string> modelPathOpt;
    std::optional<std::string> hostOpt;
    std::optional<int> portOpt;
    std::optional<std::string> quantizationOpt;
    std::optional<size_t> maxBatchSizeOpt;
    std::optional<size_t> maxContextLengthOpt;
    bool useLibTorchOpt = false;
    bool useLibTorchOptSet = false;
    std::optional<std::string> logLevelOpt;
    std::optional<std::string> logFileOpt;
    std::optional<std::string> backendTypeOpt;

    std::string configPath;
    
    // è§£æå‘½ä»¤è¡Œå‚æ•°
    static struct option long_options[] = {
        {"model-path", required_argument, 0, 'm'},
        {"port", required_argument, 0, 'p'},
        {"host", required_argument, 0, 'h'},
        {"quantization", required_argument, 0, 'q'},
        {"max-batch-size", required_argument, 0, 'b'},
        {"max-context-length", required_argument, 0, 'c'},
        {"use-libtorch", no_argument, 0, 'l'},
        {"config", required_argument, 0, 'f'},
        {"log-level", required_argument, 0, 'L'},
        {"log-file", required_argument, 0, 'F'},
        {"help", no_argument, 0, '?'},
        {0, 0, 0, 0}
    };
    
    int opt;
    int option_index = 0;
    while ((opt = getopt_long(argc, argv, "m:p:h:q:b:c:lf:L:F:?", 
                              long_options, &option_index)) != -1) {
        switch (opt) {
            case 'm':
                modelPathOpt = optarg;
                break;
            case 'p':
                portOpt = std::atoi(optarg);
                break;
            case 'h':
                hostOpt = optarg;
                break;
            case 'q':
                quantizationOpt = optarg;
                break;
            case 'b':
                maxBatchSizeOpt = static_cast<size_t>(std::atoi(optarg));
                break;
            case 'c':
                maxContextLengthOpt = static_cast<size_t>(std::atoi(optarg));
                break;
            case 'l':
                useLibTorchOpt = true;
                useLibTorchOptSet = true;
                break;
            case 'f':
                configPath = optarg;
                break;
            case 'L':
                logLevelOpt = optarg;
                break;
            case 'F':
                logFileOpt = optarg;
                break;
            case '?':
                printUsage(argv[0]);
                return 0;
            default:
                printUsage(argv[0]);
                return 1;
        }
    }
    
    printBanner();

    try {
        // åŠ è½½é…ç½®æ–‡ä»¶ï¼šä¼˜å…ˆä½¿ç”¨ --configï¼Œå¦åˆ™è‡ªåŠ¨åŠ è½½ config/config.yaml
        {
            namespace fs = std::filesystem;
            std::string selectedConfigPath;

            if (!configPath.empty()) {
                selectedConfigPath = configPath;
            } else {
                const fs::path candidates[] = {
                    fs::path("config") / "config.yaml",
                    fs::path("../config") / "config.yaml",
                    fs::path("../../config") / "config.yaml"
                };

                for (const auto& p : candidates) {
                    if (fs::exists(p)) {
                        selectedConfigPath = p.string();
                        break;
                    }
                }
            }

            if (selectedConfigPath.empty() || !fs::exists(selectedConfigPath.c_str())) {
                throw std::runtime_error("Config file not found. Please pass --config or ensure config/config.yaml exists.");
            }

            CLLM_INFO("Loading config from: %s", selectedConfigPath.c_str());
            cllm::Config::instance().load(selectedConfigPath.c_str());
        }

        // è®¡ç®—æœ€ç»ˆé…ç½®ï¼ˆConfig + CLI è¦†ç›–ï¼‰
        std::string modelPath = modelPathOpt.value_or(cllm::Config::instance().serverModelPath());
        std::string host = hostOpt.value_or(cllm::Config::instance().serverHost());
        int port = portOpt.value_or(cllm::Config::instance().serverPort());
        std::string quantization = quantizationOpt.value_or(cllm::Config::instance().serverQuantization());
        size_t maxBatchSize = maxBatchSizeOpt.value_or(static_cast<size_t>(cllm::Config::instance().serverMaxBatchSize()));
        size_t maxContextLength = maxContextLengthOpt.value_or(static_cast<size_t>(cllm::Config::instance().serverMaxContextLength()));
        std::string backendType = backendTypeOpt.value_or(cllm::Config::instance().backendType());
        bool useLibTorch = useLibTorchOptSet ? useLibTorchOpt : (backendType == "libtorch" || backendType == "LibTorch" || cllm::Config::instance().serverUseLibTorch());
        if (useLibTorch) {
            backendType = "libtorch";
        }
        std::string logLevel = logLevelOpt.value_or(cllm::Config::instance().loggingLevel());
        std::string logFile = logFileOpt.value_or(cllm::Config::instance().loggingFile());

        // è®¾ç½®æ—¥å¿—çº§åˆ«ï¼ˆä½¿ç”¨æœ€ç»ˆå€¼ï¼‰
        if (logLevel == "trace") {
            cllm::Logger::instance().setLevel(spdlog::level::trace);
        } else if (logLevel == "debug") {
            cllm::Logger::instance().setLevel(spdlog::level::debug);
        } else if (logLevel == "info") {
            cllm::Logger::instance().setLevel(spdlog::level::info);
        } else if (logLevel == "warn") {
            cllm::Logger::instance().setLevel(spdlog::level::warn);
        } else if (logLevel == "error") {
            cllm::Logger::instance().setLevel(spdlog::level::err);
        }

        // è®¾ç½®æ—¥å¿—æ–‡ä»¶
        if (!logFile.empty()) {
            cllm::Logger::instance().addFileSink(logFile.c_str());
            CLLM_INFO("Logging to file: %s", logFile.c_str());
        }

        // æ£€æŸ¥å¿…éœ€å‚æ•°ï¼ˆå…è®¸ä»é…ç½®æä¾› model.pathï¼‰
        // å…è®¸ç©ºæ¨¡å‹è·¯å¾„ç”¨äºæµ‹è¯•ç›®çš„ï¼ˆå°†ä½¿ç”¨å ä½æƒé‡ï¼‰
        if (modelPath == "/path/to/model") {
            CLLM_ERROR("Model path is required. Use --model-path or set model.path in config");
            printUsage(argv[0]);
            return 1;
        }
        
        if (modelPath.empty()) {
            CLLM_INFO("Using empty model path - will use placeholder weights for testing");
        }

        CLLM_INFO("========================================");
        CLLM_INFO("cLLM - High-Performance LLM Inference Engine");
        CLLM_INFO("========================================");
        CLLM_INFO("Starting cLLM Server...");
        CLLM_INFO("Configuration:");
        CLLM_INFO("  - Model Path: %s", modelPath.c_str());
        CLLM_INFO("  - Host: %s", host.c_str());
        CLLM_INFO("  - Port: %d", port);
        CLLM_INFO("  - Quantization: %s", quantization.c_str());
        CLLM_INFO("  - Max Batch Size: %zu", maxBatchSize);
        CLLM_INFO("  - Max Context Length: %zu", maxContextLength);
        CLLM_INFO("  - Backend: %s", (backendType.empty() ? (useLibTorch ? "LibTorch" : "Auto") : backendType.c_str()));
        CLLM_INFO("  - Log Level: %s", logLevel.c_str());

        // æ³¨å†Œä¿¡å·å¤„ç†å™¨
        signal(SIGINT, signalHandler);
        signal(SIGTERM, signalHandler);
        
        // åˆå§‹åŒ– Asio å¤„ç†å™¨ï¼ˆæ»¡è¶³æŠ€æœ¯æ ˆè¦æ±‚ï¼‰
        CLLM_INFO("Initializing Asio handler...");
        cllm::AsioHandler asioHandler;
        CLLM_INFO("Asio thread pool size: %zu", asioHandler.getThreadPoolSize());
        
        // æäº¤ä¸€ä¸ªå¼‚æ­¥ä»»åŠ¡ä½œä¸ºç¤ºä¾‹
        asioHandler.postTask([]() {
            CLLM_DEBUG("Asio async task executed successfully");
        });
        
        // åˆå§‹åŒ–æ¨¡å‹æ‰§è¡Œå™¨
        CLLM_INFO("Initializing model executor...");

        namespace fs = std::filesystem;

        std::string tokenizerModelDir;
        std::string backendModelPath;

        if (modelPath.empty()) {
            // Empty model path case - use placeholder weights and default tokenizer
            CLLM_INFO("Empty model path provided - using placeholder weights and default tokenizer");
            tokenizerModelDir = "";
            backendModelPath = "";
        } else {
            // 1) Check if model path is a directory
            const bool isDir = fs::is_directory(modelPath);

            // 2) è§£æ tokenizer ç›®å½•ï¼ˆä¼˜å…ˆ HuggingFace tokenizer.jsonï¼‰
            auto resolveTokenizerDir = [](const fs::path& baseDir) -> std::string {
                if (fs::exists(baseDir / "tokenizer.json")) {
                    return baseDir.string();
                }

                std::vector<fs::path> matches;
                for (const auto& ent : fs::directory_iterator(baseDir)) {
                    if (!ent.is_directory()) {
                        continue;
                    }
                    const fs::path cand = ent.path() / "tokenizer.json";
                    if (fs::exists(cand)) {
                        matches.push_back(ent.path());
                    }
                }

                if (matches.size() == 1) {
                    return matches.front().string();
                }

                // å›é€€ï¼šç”¨ baseDirï¼ˆTokenizerManager ä¼šå†å°è¯•å…¶ä»–æ–¹å¼/æˆ–ç»™å‡ºé”™è¯¯ï¼‰
                return baseDir.string();
            };

            if (isDir) {
                tokenizerModelDir = resolveTokenizerDir(fs::path(modelPath));
            } else {
                fs::path file(modelPath);
                tokenizerModelDir = resolveTokenizerDir(file.parent_path());
            }

            // 3) è§£æåç«¯æƒé‡æ–‡ä»¶è·¯å¾„
            auto listFilesWithExt = [](const fs::path& dir, const std::string& ext) -> std::vector<fs::path> {
                std::vector<fs::path> out;
                for (const auto& ent : fs::directory_iterator(dir)) {
                    if (!ent.is_regular_file()) {
                        continue;
                    }
                    const fs::path p = ent.path();
                    if (p.extension() == ext) {
                        out.push_back(p);
                    }
                }
                return out;
            };

            if (isDir) {
                fs::path dir(modelPath);

                if (!useLibTorch) {
                    auto ggufs = listFilesWithExt(dir, ".gguf");
                    if (ggufs.size() == 1) {
                        backendModelPath = ggufs.front().string();
                    } else if (ggufs.size() > 1) {
                        std::string msg = "Multiple .gguf files found. Pass a .gguf path directly via --model-path. Candidates: ";
                        for (const auto& p : ggufs) {
                            msg += p.filename().string();
                            msg += " ";
                        }
                        throw std::runtime_error(msg);
                    }
                }

                if (useLibTorch) {
                    // LibTorch éœ€è¦ TorchScript .pt
                    auto pts = listFilesWithExt(dir, ".pt");
                    if (pts.size() == 1) {
                        backendModelPath = pts.front().string();
                    } else {
                        std::string msg = "LibTorch backend requires exactly one .pt in directory (or pass a .pt path). Found: ";
                        msg += std::to_string(pts.size());
                        if (!pts.empty()) {
                            msg += " (";
                            for (const auto& p : pts) {
                                msg += p.filename().string();
                                msg += " ";
                            }
                            msg += ")";
                        }
                        throw std::runtime_error(msg);
                    }
                } else if (!backendModelPath.empty()) {
                    // å·²é€‰æ‹© .gguf
                } else {
                    // Kylin éœ€è¦ .bin
                    auto bins = listFilesWithExt(dir, ".bin");
                    if (bins.empty()) {
                        throw std::runtime_error("Kylin backend requires .bin model file. No .bin found in: " + dir.string());
                    }

                    // è‹¥ç›®å½•ä¸‹åªæœ‰ä¸€ä¸ª .binï¼Œç›´æ¥ç”¨ï¼›å¦åˆ™æŒ‰ quantization å…³é”®è¯ç­›é€‰
                    std::vector<fs::path> filtered;
                    if (bins.size() == 1) {
                        filtered = bins;
                    } else {
                        for (const auto& p : bins) {
                            const std::string name = p.filename().string();
                            if (quantization == "fp16" && name.find("fp16") != std::string::npos) {
                                filtered.push_back(p);
                            } else if (quantization == "fp32" && name.find("fp32") != std::string::npos) {
                                filtered.push_back(p);
                            } else if (quantization == "int8" && name.find("int8") != std::string::npos) {
                                filtered.push_back(p);
                            } else if (quantization == "int4" && name.find("int4") != std::string::npos) {
                                filtered.push_back(p);
                            }
                        }
                    }

                    if (filtered.size() == 1) {
                        backendModelPath = filtered.front().string();
                    } else {
                        std::string msg = "Could not uniquely select .bin for quantization=" + quantization +
                                          ". Pass a .bin path directly via --model-path. Candidates: ";
                        for (const auto& p : bins) {
                            msg += p.filename().string();
                            msg += " ";
                        }
                        throw std::runtime_error(msg);
                    }
                }
            } else {
                backendModelPath = modelPath;
            }
        }

        if (!backendModelPath.empty()) {
            namespace fs = std::filesystem;
            if (fs::path(backendModelPath).extension() == ".gguf") {
                tokenizerModelDir = backendModelPath;
                if (backendType.empty()) {
                    backendType = "llama_cpp";
                }
            }
        }

        CLLM_INFO("Resolved paths:");
        CLLM_INFO("  - Backend model file: %s", backendModelPath.c_str());
        CLLM_INFO("  - Tokenizer dir: %s", tokenizerModelDir.c_str());
        CLLM_INFO("  - Backend type: %s", backendType.empty() ? "auto" : backendType.c_str());

        cllm::ModelConfig initialConfig;
        initialConfig.maxSequenceLength = maxContextLength;
        initialConfig.ropeNctxOrig = maxContextLength;
        initialConfig.llamaBatchSize = static_cast<size_t>(cllm::Config::instance().backendLlamaCppBatchSize());
        initialConfig.llamaNumThreads = cllm::Config::instance().backendLlamaCppNumThreads();
        initialConfig.llamaGpuLayers = cllm::Config::instance().backendLlamaCppGpuLayers();
        initialConfig.llamaUseMmap = cllm::Config::instance().backendLlamaCppUseMmap();
        initialConfig.llamaUseMlock = cllm::Config::instance().backendLlamaCppUseMlock();

        g_modelExecutor = std::make_unique<cllm::ModelExecutor>(
            backendModelPath,
            quantization,
            true,  // enableSIMD
            useLibTorch,
            backendType,
            &initialConfig
        );

        // åŠ è½½æ¨¡å‹ï¼ˆå®é™…æƒé‡åŠ è½½ç”± InferenceEngine åç«¯è´Ÿè´£ï¼Œè¿™é‡Œä¸»è¦åš warmup / æ ‡è®°ï¼‰
        CLLM_INFO("Loading model...");
        g_modelExecutor->loadModel();
        CLLM_INFO("Model loaded successfully");

        // åˆå§‹åŒ–åˆ†è¯å™¨ï¼ˆTokenizerManager ä¼šè‡ªåŠ¨é€‰æ‹© HFTokenizer æˆ– NativeTokenizerï¼‰
        CLLM_INFO("Initializing tokenizer...");
        g_tokenizerManager = std::make_unique<cllm::TokenizerManager>(tokenizerModelDir, g_modelExecutor.get());
        cllm::ITokenizer* tokenizer = g_tokenizerManager->getTokenizer();
        CLLM_INFO("Tokenizer initialized");
        CLLM_INFO("  - Vocab size: %zu", tokenizer->getVocabSize());
        
        // å°† tokenizer çš„ vocab_size è®¾ç½®åˆ° ModelExecutor çš„ config ä¸­
        // è¿™æ ·åœ¨é‡‡æ ·æ—¶å¯ä»¥ä½¿ç”¨æ­£ç¡®çš„ tokenizer vocab_size æ¥é™åˆ¶ logits èŒƒå›´
        size_t tokenizerVocabSize = tokenizer->getVocabSize();
        g_modelExecutor->setTokenizerVocabSize(tokenizerVocabSize);
        CLLM_INFO("Model config updated with tokenizer vocab_size: %zu", tokenizerVocabSize);
        
        // åˆå§‹åŒ–è°ƒåº¦å™¨
        CLLM_INFO("Initializing scheduler...");
        g_scheduler = std::make_unique<cllm::Scheduler>(
            g_modelExecutor.get(),
            maxBatchSize,
            maxContextLength
        );
        
        // å¯åŠ¨è°ƒåº¦å™¨
        CLLM_INFO("Starting scheduler...");
        g_scheduler->start();
        CLLM_INFO("Scheduler started");
        
        // åˆ›å»º HTTP å¤„ç†å™¨
        CLLM_INFO("Setting up HTTP endpoints...");
        auto httpHandler = std::make_unique<cllm::HttpHandler>();
        
        // æ³¨å†Œç«¯ç‚¹
        auto healthEndpoint = std::make_unique<cllm::HealthEndpoint>();
        httpHandler->get(cllm::Config::instance().apiEndpointHealthPath(), [endpoint = healthEndpoint.get()](const cllm::HttpRequest& req) {
            return endpoint->handle(req);
        });
        
        auto generateEndpoint = std::make_unique<cllm::GenerateEndpoint>(
            g_scheduler.get(),
            tokenizer
        );
        httpHandler->post(cllm::Config::instance().apiEndpointGeneratePath(), [endpoint = generateEndpoint.get()](const cllm::HttpRequest& req) {
            return endpoint->handle(req);
        });
        
        httpHandler->post(cllm::Config::instance().apiEndpointGenerateStreamPath(), [endpoint = generateEndpoint.get()](const cllm::HttpRequest& req) {
            return endpoint->handle(req);
        });
        
        auto encodeEndpoint = std::make_unique<cllm::EncodeEndpoint>(tokenizer);
        httpHandler->post(cllm::Config::instance().apiEndpointEncodePath(), [endpoint = encodeEndpoint.get()](const cllm::HttpRequest& req) {
            return endpoint->handle(req);
        });
        
        // ğŸ”¥ ä¼˜åŒ–ï¼šä½¿ç”¨ç‹¬ç«‹Scheduleræ¨¡å¼ï¼ˆå‚è€ƒStage 15ï¼‰ï¼Œé¿å…å…±äº«Schedulerçš„ç«äº‰
        // ä½¿ç”¨ä¸Stage 15ç›¸åŒçš„é…ç½®ï¼šmaxBatchSize=8, maxContextLength=2048
        auto benchmarkEndpoint = std::make_unique<cllm::BenchmarkEndpoint>(
            g_modelExecutor.get(), tokenizer, 8, 2048);
        httpHandler->post("/benchmark", [endpoint = benchmarkEndpoint.get()](const cllm::HttpRequest& req) {
            return endpoint->handle(req);
        });
        
        CLLM_INFO("Registered endpoints:");
        CLLM_INFO("  - GET  %s", cllm::Config::instance().apiEndpointHealthPath().c_str());
        CLLM_INFO("  - POST %s", cllm::Config::instance().apiEndpointGeneratePath().c_str());
        CLLM_INFO("  - POST %s", cllm::Config::instance().apiEndpointGenerateStreamPath().c_str());
        CLLM_INFO("  - POST %s", cllm::Config::instance().apiEndpointEncodePath().c_str());
        CLLM_INFO("  - POST /benchmark");
        
        // åˆå§‹åŒ–å¹¶å¯åŠ¨ HTTP æœåŠ¡å™¨ï¼ˆè‡ªç ”é«˜æ€§èƒ½æœåŠ¡å™¨ï¼‰
        CLLM_INFO("Initializing HTTP server...");
        cllm::HttpServer::init(host, port, httpHandler.get());
        
        CLLM_INFO("========================================");
        CLLM_INFO("âœ“ cLLM Server is ready!");
        CLLM_INFO("Listening on http://%s:%d", host.c_str(), port);
        CLLM_INFO("Press Ctrl+C to stop the server");
        CLLM_INFO("========================================");
        
        // Start server (å¯åŠ¨åå°çº¿ç¨‹)
        cllm::HttpServer::start();
        
        // ä¿æŒæœåŠ¡å™¨è¿è¡Œï¼ˆä¸»çº¿ç¨‹ç­‰å¾…ï¼‰
        while (cllm::HttpServer::isRunning()) {
            std::this_thread::sleep_for(std::chrono::seconds(1));
        }
        
        // Keep endpoint objects alive
        healthEndpoint.reset();
        generateEndpoint.reset();
        encodeEndpoint.reset();
        benchmarkEndpoint.reset();
    } catch (const std::exception& e) {
        CLLM_ERROR("Failed to start server: %s", e.what());
        cllm::Logger::instance().flush();
        return 1;
    }
    
    cllm::Logger::instance().flush();
    return 0;
}