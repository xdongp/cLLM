#include "cllm/http/benchmark_endpoint.h"
#include "cllm/http/generate_endpoint.h"
#include "cllm/http/json_request_parser.h"
#include "cllm/http/response_builder.h"
#include "cllm/scheduler/scheduler.h"
#include "cllm/tokenizer/i_tokenizer.h"
#include "cllm/model/executor.h"
#include "cllm/common/request_state.h"
#include "cllm/common/logger.h"
#include "cllm/common/config.h"
#include <nlohmann/json.hpp>
#include <thread>
#include <mutex>
#include <vector>
#include <atomic>
#include <chrono>
#include <algorithm>
#include <numeric>
#include <limits>
#include <memory>

namespace cllm {

BenchmarkEndpoint::BenchmarkEndpoint(Scheduler* scheduler, ITokenizer* tokenizer)
    : ApiEndpoint("benchmark", "/benchmark", "POST"),
      useDirectMode_(true),
      useIndependentScheduler_(false),
      generateEndpoint_(nullptr),
      scheduler_(scheduler),
      tokenizer_(tokenizer),
      maxBatchSize_(8),
      maxContextLength_(2048) {
}

BenchmarkEndpoint::BenchmarkEndpoint(ModelExecutor* modelExecutor, ITokenizer* tokenizer, 
                                     size_t maxBatchSize, size_t maxContextLength)
    : ApiEndpoint("benchmark", "/benchmark", "POST"),
      useDirectMode_(true),
      useIndependentScheduler_(true),
      generateEndpoint_(nullptr),
      scheduler_(nullptr),
      independentScheduler_(std::make_unique<Scheduler>(modelExecutor, maxBatchSize, maxContextLength)),
      tokenizer_(tokenizer),
      maxBatchSize_(maxBatchSize),
      maxContextLength_(maxContextLength) {
    // ğŸ”¥ ä¼˜åŒ–ï¼šå¯åŠ¨ç‹¬ç«‹çš„Schedulerï¼Œç¡®ä¿ç‹¬ç«‹è¿è¡Œï¼ˆä¸Stage 15ä¸€è‡´ï¼‰
    independentScheduler_->start();
    std::this_thread::sleep_for(std::chrono::milliseconds(100)); // ç­‰å¾…Schedulerå¯åŠ¨ï¼ˆä¸Stage 15ä¸€è‡´ï¼‰
}

BenchmarkEndpoint::BenchmarkEndpoint(GenerateEndpoint* generateEndpoint)
    : ApiEndpoint("benchmark", "/benchmark", "POST"),
      useDirectMode_(false),
      useIndependentScheduler_(false),
      generateEndpoint_(generateEndpoint),
      scheduler_(nullptr),
      tokenizer_(nullptr),
      maxBatchSize_(8),
      maxContextLength_(2048) {
}

BenchmarkEndpoint::~BenchmarkEndpoint() {
    // ğŸ”¥ ä¼˜åŒ–ï¼šåœæ­¢ç‹¬ç«‹çš„Schedulerï¼ˆå¦‚æœä½¿ç”¨ï¼‰
    if (useIndependentScheduler_ && independentScheduler_) {
        independentScheduler_->stop();
    }
}

void BenchmarkEndpoint::setGenerateEndpoint(GenerateEndpoint* generateEndpoint) {
    useDirectMode_ = false;
    generateEndpoint_ = generateEndpoint;
    scheduler_ = nullptr;
    tokenizer_ = nullptr;
}

void BenchmarkEndpoint::setSchedulerAndTokenizer(Scheduler* scheduler, ITokenizer* tokenizer) {
    useDirectMode_ = true;
    scheduler_ = scheduler;
    tokenizer_ = tokenizer;
    generateEndpoint_ = nullptr;
}

BenchmarkEndpoint::BenchmarkRequest BenchmarkEndpoint::parseRequest(const HttpRequest& request) {
    BenchmarkRequest req;
    
    nlohmann::json jsonBody;
    
    if (!JsonRequestParser::validateJson(request.getBody(), jsonBody)) {
        CLLM_WARN("Failed to parse JSON request body: %s, using default values", JsonRequestParser::getLastError().c_str());
        return req;
    }
    
    int defaultRequests = 40;
    int defaultConcurrency = 8;
    int defaultMaxTokens = 50;
    std::string defaultPrompt = "Hello, world! How are you today?";
    float defaultTemperature = 0.7f;
    
    JsonRequestParser::getFieldWithDefault(jsonBody, "requests", req.requests, defaultRequests);
    JsonRequestParser::getFieldWithDefault(jsonBody, "concurrency", req.concurrency, defaultConcurrency);
    JsonRequestParser::getFieldWithDefault(jsonBody, "max_tokens", req.maxTokens, defaultMaxTokens);
    JsonRequestParser::getFieldWithDefault(jsonBody, "prompt", req.prompt, defaultPrompt);
    JsonRequestParser::getFieldWithDefault(jsonBody, "temperature", req.temperature, defaultTemperature);
    
    // å‚æ•°éªŒè¯
    if (req.requests <= 0) {
        req.requests = 40;
        CLLM_WARN("Invalid requests parameter, using default: 40");
    }
    if (req.concurrency <= 0) {
        req.concurrency = 8;
        CLLM_WARN("Invalid concurrency parameter, using default: 8");
    }
    if (req.maxTokens <= 0) {
        req.maxTokens = 50;
        CLLM_WARN("Invalid max_tokens parameter, using default: 50");
    }
    if (req.concurrency > req.requests) {
        req.concurrency = req.requests;
        CLLM_WARN("Concurrency exceeds requests, setting concurrency to requests: %d", req.concurrency);
    }
    
    return req;
}

BenchmarkEndpoint::RequestResult BenchmarkEndpoint::executeSingleRequest(
    const BenchmarkRequest& params, 
    int requestIndex
) {
    RequestResult result;
    
    if (!generateEndpoint_) {
        result.success = false;
        result.errorMessage = "GenerateEndpoint not initialized";
        return result;
    }
    
    auto startTime = std::chrono::high_resolution_clock::now();
    
    try {
        // æ„å»ºHttpRequestå¯¹è±¡ï¼Œæ¨¡æ‹ŸHTTPè¯·æ±‚
        HttpRequest httpRequest;
        httpRequest.setMethod("POST");
        httpRequest.setPath("/generate");
        httpRequest.setHeader("Content-Type", "application/json");
        
        // æ„å»ºJSONè¯·æ±‚ä½“
        nlohmann::json requestJson;
        requestJson["prompt"] = params.prompt;
        requestJson["max_tokens"] = params.maxTokens;
        requestJson["temperature"] = params.temperature;
        requestJson["stream"] = false;
        httpRequest.setBody(requestJson.dump());
        
        // ç›´æ¥è°ƒç”¨GenerateEndpoint::handle()ï¼Œé¿å…HTTPå¼€é”€
        HttpResponse httpResponse = generateEndpoint_->handle(httpRequest);
        
        auto endTime = std::chrono::high_resolution_clock::now();
        result.responseTime = std::chrono::duration<double>(endTime - startTime).count();
        
        // è§£æå“åº”
        if (httpResponse.getStatusCode() == 200) {
            try {
                nlohmann::json responseJson = nlohmann::json::parse(httpResponse.getBody());
                if (responseJson.contains("success") && responseJson["success"] == true) {
                    if (responseJson.contains("data")) {
                        auto data = responseJson["data"];
                        
                        // æå–tokens_per_second
                        if (data.contains("tokens_per_second")) {
                            result.tokensPerSecond = data["tokens_per_second"].get<float>();
                        }
                        
                        // æå–ç”Ÿæˆçš„æ–‡æœ¬å’Œtokenæ•°
                        if (data.contains("text")) {
                            std::string text = data["text"].get<std::string>();
                            // ç®€å•ä¼°ç®—tokenæ•°ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨tokenizerï¼‰
                            if (result.tokensPerSecond > 0 && result.responseTime > 0) {
                                result.generatedTokens = static_cast<size_t>(result.tokensPerSecond * result.responseTime);
                            } else {
                                // å›é€€æ–¹æ¡ˆï¼šæ ¹æ®æ–‡æœ¬é•¿åº¦ä¼°ç®—
                                result.generatedTokens = text.length() / 4; // ç²—ç•¥ä¼°ç®—
                            }
                        }
                        
                        // æå–response_timeï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        if (data.contains("response_time")) {
                            double responseTimeFromData = data["response_time"].get<double>();
                            if (responseTimeFromData > 0) {
                                result.responseTime = responseTimeFromData;
                            }
                        }
                        
                        result.totalTokens = params.prompt.length() / 4 + result.generatedTokens; // ç²—ç•¥ä¼°ç®—
                        result.success = true;
                    } else {
                        result.success = false;
                        result.errorMessage = "Response data field missing";
                    }
                } else {
                    result.success = false;
                    if (responseJson.contains("error")) {
                        result.errorMessage = responseJson["error"].get<std::string>();
                    } else {
                        result.errorMessage = "Request failed";
                    }
                }
            } catch (const std::exception& e) {
                result.success = false;
                result.errorMessage = std::string("Failed to parse response: ") + e.what();
            }
        } else {
            result.success = false;
            result.errorMessage = "HTTP " + std::to_string(httpResponse.getStatusCode());
        }
    } catch (const std::exception& e) {
        auto endTime = std::chrono::high_resolution_clock::now();
        result.responseTime = std::chrono::duration<double>(endTime - startTime).count();
        result.success = false;
        result.errorMessage = std::string("Exception: ") + e.what();
    }
    
    return result;
}

BenchmarkEndpoint::Statistics BenchmarkEndpoint::calculateStatistics(
    const std::vector<RequestResult>& results,
    double totalTime
) {
    Statistics stats;
    
    stats.totalRequests = static_cast<int>(results.size());
    stats.totalTime = totalTime;
    
    // ğŸ”¥ ä¼˜åŒ–ï¼šå•æ¬¡éå†ï¼Œé¿å…å¤šæ¬¡æ‹·è´å’Œåˆ›å»ºä¸´æ—¶vector
    size_t successfulCount = 0;
    double totalResponseTime = 0.0;
    double minResponseTime = std::numeric_limits<double>::max();
    double maxResponseTime = 0.0;
    size_t totalGeneratedTokens = 0;
    size_t totalTokens = 0;
    double totalTokensPerSecond = 0.0;
    
    for (const auto& result : results) {
        if (result.success) {
            successfulCount++;
            totalResponseTime += result.responseTime;
            if (result.responseTime < minResponseTime) {
                minResponseTime = result.responseTime;
            }
            if (result.responseTime > maxResponseTime) {
                maxResponseTime = result.responseTime;
            }
            totalGeneratedTokens += result.generatedTokens;
            totalTokens += result.totalTokens;
            totalTokensPerSecond += result.tokensPerSecond;
        }
    }
    
    stats.successfulRequests = static_cast<int>(successfulCount);
    stats.failedRequests = stats.totalRequests - stats.successfulRequests;
    
    if (successfulCount == 0) {
        return stats;
    }
    
    // è®¡ç®—ç»Ÿè®¡
    stats.avgResponseTime = totalResponseTime / successfulCount;
    stats.minResponseTime = minResponseTime == std::numeric_limits<double>::max() ? 0.0 : minResponseTime;
    stats.maxResponseTime = maxResponseTime;
    
    stats.totalTokensProcessed = totalTokens;
    stats.avgGeneratedTokens = static_cast<double>(totalGeneratedTokens) / successfulCount;
    stats.avgTokensPerSecond = totalTokensPerSecond / successfulCount;
    
    // è®¡ç®—å¹³å‡ååé‡ï¼ˆæ€»ç”Ÿæˆtokenæ•° / æ€»æ—¶é—´ï¼‰
    if (totalTime > 0) {
        stats.avgThroughput = static_cast<double>(totalGeneratedTokens) / totalTime;
    }
    
    return stats;
}

HttpResponse BenchmarkEndpoint::buildResponse(const Statistics& stats) {
    nlohmann::json responseJson;
    responseJson["success"] = true;
    
    nlohmann::json dataJson;
    dataJson["total_requests"] = stats.totalRequests;
    dataJson["successful_requests"] = stats.successfulRequests;
    dataJson["failed_requests"] = stats.failedRequests;
    dataJson["avg_response_time"] = stats.avgResponseTime;
    dataJson["min_response_time"] = stats.minResponseTime;
    dataJson["max_response_time"] = stats.maxResponseTime;
    dataJson["avg_throughput"] = stats.avgThroughput;
    dataJson["avg_tokens_per_second"] = stats.avgTokensPerSecond;
    dataJson["total_tokens_processed"] = stats.totalTokensProcessed;
    dataJson["avg_generated_tokens"] = stats.avgGeneratedTokens;
    dataJson["total_time"] = stats.totalTime;
    
    responseJson["data"] = dataJson;
    
    return ResponseBuilder::json(responseJson, 200);
}

HttpResponse BenchmarkEndpoint::handle(const HttpRequest& request) {
    try {
        BenchmarkRequest params = parseRequest(request);
        
        // ğŸ”¥ ä¼˜åŒ–ï¼šç§»é™¤å¯åŠ¨æ—¥å¿—ï¼Œå‡å°‘å¼€é”€
        // CLLM_INFO("Starting benchmark: requests=%d, concurrency=%d, max_tokens=%d",
        //           params.requests, params.concurrency, params.maxTokens);
        
        auto totalStartTime = std::chrono::high_resolution_clock::now();
        
        // ğŸ”¥ ä¼˜åŒ–ï¼šå‚è€ƒStage 15çš„å®ç°ï¼Œä½¿ç”¨åŸå­æ“ä½œæ”¶é›†æœ€å°å¿…è¦ç»Ÿè®¡
        // å®Œå…¨ç§»é™¤responseTimesæ”¶é›†ï¼Œå‡å°‘é”ç«äº‰
        std::atomic<size_t> completedRequests{0};
        std::atomic<size_t> totalGeneratedTokens{0};
        
        // å·¥ä½œçº¿ç¨‹å‡½æ•°
        auto worker = [&](int startIndex, int count) {
            for (int i = 0; i < count; ++i) {
                int requestIndex = startIndex + i;
                if (requestIndex >= params.requests) {
                    break;
                }
                
                // ğŸ”¥ ä¼˜åŒ–ï¼šç›´æ¥è°ƒç”¨Schedulerï¼Œä¸åˆ›å»ºRequestResultå¯¹è±¡
                // ä¼˜å…ˆä½¿ç”¨ç‹¬ç«‹çš„Schedulerå®ä¾‹ï¼ˆæœ€ä¼˜æ¨¡å¼ï¼‰
                Scheduler* activeScheduler = useIndependentScheduler_ ? independentScheduler_.get() : scheduler_;
                if (useDirectMode_ && activeScheduler && tokenizer_) {
                    try {
                        // ç›´æ¥åˆ›å»ºRequestState
                        RequestState requestState;
                        requestState.requestId = 0;
                        requestState.maxTokens = params.maxTokens;
                        requestState.temperature = params.temperature;
                        requestState.topP = 0.9f;
                        requestState.topK = 0;
                        requestState.repetitionPenalty = 1.1f; // é»˜è®¤è½»å¾®æƒ©ç½šé‡å¤
                        requestState.eosTokenId = tokenizer_->getEosId();
                        requestState.priority = 0;
                        requestState.arrivalTime = 0;
                        requestState.startTime = 0;
                        requestState.completionTime = 0;
                        requestState.isCompleted = false;
                        requestState.isRunning = false;
                        requestState.isFailed = false;
                        requestState.samplingStrategy = "";
                        requestState.errorMessage = "";
                        requestState.tokenizedPrompt = tokenizer_->encode(params.prompt, false);
                        
                        // æ§åˆ¶è¾“å…¥é•¿åº¦
                        const int maxInputTokens = cllm::Config::instance().httpMaxInputTokens();
                        if (maxInputTokens > 0 && requestState.tokenizedPrompt.size() > static_cast<size_t>(maxInputTokens)) {
                            requestState.tokenizedPrompt.resize(maxInputTokens);
                        }
                        
                        // ç›´æ¥è°ƒç”¨Schedulerï¼ˆä½¿ç”¨ç‹¬ç«‹çš„æˆ–å…±äº«çš„ï¼‰
                        size_t reqId = activeScheduler->addRequest(requestState);
                        const float timeoutMin = cllm::Config::instance().apiTimeoutMin();
                        const float timeoutMax = cllm::Config::instance().apiTimeoutMax();
                        const float tokenFactor = cllm::Config::instance().apiTimeoutTokenFactor();
                        const float timeoutSec = std::max(timeoutMin, std::min(timeoutMax, static_cast<float>(params.maxTokens) * tokenFactor));
                        
                        if (activeScheduler->waitForRequest(reqId, timeoutSec)) {
                            RequestState resultState = activeScheduler->getRequestResult(reqId);
                            if (!resultState.isTimeout && !resultState.isFailed && !resultState.generatedTokens.empty()) {
                                // ğŸ”¥ ä¼˜åŒ–ï¼šç›´æ¥æ›´æ–°åŸå­å˜é‡ï¼Œä¸åˆ›å»ºRequestResultå¯¹è±¡
                                completedRequests++;
                                totalGeneratedTokens += resultState.generatedTokens.size();
                            }
                        }
                    } catch (...) {
                        // å¿½ç•¥é”™è¯¯ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªè¯·æ±‚
                    }
                } else {
                    // å›é€€åˆ°åŸæœ‰æ–¹å¼
                    RequestResult result = useDirectMode_ ? 
                        executeSingleRequestDirect(params, requestIndex) : 
                        executeSingleRequest(params, requestIndex);
                    if (result.success) {
                        completedRequests++;
                        totalGeneratedTokens += result.generatedTokens;
                    }
                }
            }
        };
        
        // åˆ›å»ºçº¿ç¨‹æ± 
        std::vector<std::thread> threads;
        int requestsPerThread = params.requests / params.concurrency;
        int remainder = params.requests % params.concurrency;
        
        int currentIndex = 0;
        for (int i = 0; i < params.concurrency; ++i) {
            int threadRequests = requestsPerThread + (i < remainder ? 1 : 0);
            if (threadRequests > 0) {
                threads.emplace_back(worker, currentIndex, threadRequests);
                currentIndex += threadRequests;
            }
        }
        
        // ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for (auto& thread : threads) {
            thread.join();
        }
        
        auto totalEndTime = std::chrono::high_resolution_clock::now();
        double totalTime = std::chrono::duration<double>(totalEndTime - totalStartTime).count();
        
        // ğŸ”¥ ä¼˜åŒ–ï¼šç›´æ¥ä»åŸå­å˜é‡è®¡ç®—ç»Ÿè®¡ï¼Œå®Œå…¨æ— é”
        Statistics stats;
        stats.totalRequests = params.requests;
        stats.totalTime = totalTime;
        stats.successfulRequests = static_cast<int>(completedRequests.load());
        stats.failedRequests = stats.totalRequests - stats.successfulRequests;
        
        if (stats.successfulRequests > 0) {
            // è®¡ç®—tokenç»Ÿè®¡
            size_t totalGenTokens = totalGeneratedTokens.load();
            stats.totalTokensProcessed = totalGenTokens; // ç®€åŒ–ï¼šåªè®¡ç®—ç”Ÿæˆçš„token
            stats.avgGeneratedTokens = static_cast<double>(totalGenTokens) / stats.successfulRequests;
            
            // è®¡ç®—å¹³å‡ååé‡ï¼ˆä¸»è¦æŒ‡æ ‡ï¼‰
            if (totalTime > 0) {
                stats.avgThroughput = static_cast<double>(totalGenTokens) / totalTime;
            }
            
            // ç®€åŒ–ç»Ÿè®¡ï¼šä½¿ç”¨æ€»æ—¶é—´å’Œæ€»tokenæ•°ä¼°ç®—
            // å‡è®¾æ¯ä¸ªè¯·æ±‚çš„å¹³å‡å“åº”æ—¶é—´ = æ€»æ—¶é—´ / æˆåŠŸè¯·æ±‚æ•°
            stats.avgResponseTime = totalTime / stats.successfulRequests;
            stats.minResponseTime = 0.0;  // ç®€åŒ–ï¼šä¸æ”¶é›†
            stats.maxResponseTime = 0.0;  // ç®€åŒ–ï¼šä¸æ”¶é›†
            
            // ç®€åŒ–ï¼šä½¿ç”¨å¹³å‡ååé‡ä½œä¸ºavg_tokens_per_second
            stats.avgTokensPerSecond = stats.avgThroughput;
        }
        
        // ğŸ”¥ ä¼˜åŒ–ï¼šç§»é™¤å®Œæˆæ—¥å¿—ï¼Œå‡å°‘å¼€é”€ï¼ˆä»…åœ¨DEBUGæ¨¡å¼ä¸‹è¾“å‡ºï¼‰
        #ifdef CLLM_DEBUG_MODE
        CLLM_INFO("Benchmark completed: throughput=%.2f t/s, successful=%d/%d, total_time=%.2fs",
                  stats.avgThroughput, stats.successfulRequests, stats.totalRequests, stats.totalTime);
        #endif
        
        return buildResponse(stats);
        
    } catch (const std::exception& e) {
        CLLM_ERROR("Benchmark failed: %s", e.what());
        return ResponseBuilder::internalError(std::string("Benchmark error: ") + e.what());
    }
}

BenchmarkEndpoint::RequestResult BenchmarkEndpoint::executeSingleRequestDirect(
    const BenchmarkRequest& params, 
    int requestIndex
) {
    // ğŸ”¥ ä¼˜åŒ–2: ä½¿ç”¨è¿”å›å€¼ä¼˜åŒ–ï¼ˆRVOï¼‰ï¼Œé¿å…ä¸å¿…è¦çš„æ‹·è´
    RequestResult result;
    
    if (!scheduler_ || !tokenizer_) {
        result.success = false;
        result.errorMessage = "Scheduler or Tokenizer not initialized";
        return result;  // RVOä¼˜åŒ–
    }
    
    auto startTime = std::chrono::high_resolution_clock::now();
    
    try {
        // ğŸ”¥ ä¼˜åŒ–ï¼šç›´æ¥åˆ›å»ºRequestStateï¼Œæ— éœ€JSONè§£æ
        RequestState requestState;
        requestState.requestId = 0; // ç”±scheduleråˆ†é…
        requestState.maxTokens = params.maxTokens;
        requestState.temperature = params.temperature;
        requestState.topP = 0.9f; // ä½¿ç”¨é»˜è®¤å€¼
        requestState.topK = 0; // ä½¿ç”¨é»˜è®¤å€¼
        requestState.repetitionPenalty = 1.1f; // é»˜è®¤è½»å¾®æƒ©ç½šé‡å¤
        requestState.eosTokenId = tokenizer_->getEosId();
        requestState.priority = 0;
        requestState.arrivalTime = 0;
        requestState.startTime = 0;
        requestState.completionTime = 0;
        requestState.isCompleted = false;
        requestState.isRunning = false;
        requestState.isFailed = false;
        requestState.samplingStrategy = "";
        requestState.errorMessage = "";
        
        // ğŸ”¥ ä¼˜åŒ–ï¼šç›´æ¥è°ƒç”¨Tokenizer::encode()ï¼Œæ— éœ€JSON
        requestState.tokenizedPrompt = tokenizer_->encode(params.prompt, false);
        
        // æ§åˆ¶è¾“å…¥é•¿åº¦
        const int maxInputTokens = cllm::Config::instance().httpMaxInputTokens();
        if (maxInputTokens > 0) {
            const size_t MAX_INPUT_TOKENS = static_cast<size_t>(maxInputTokens);
            if (requestState.tokenizedPrompt.size() > MAX_INPUT_TOKENS) {
                CLLM_WARN("Input tokens (%zu) exceeds limit (%zu), truncating",
                          requestState.tokenizedPrompt.size(), MAX_INPUT_TOKENS);
                requestState.tokenizedPrompt.resize(MAX_INPUT_TOKENS);
            }
        }
        
        // ğŸ”¥ ä¼˜åŒ–ï¼šç›´æ¥è°ƒç”¨Scheduler::addRequest()ï¼Œæ— éœ€GenerateEndpoint
        size_t reqId = scheduler_->addRequest(requestState);
        
        // ğŸ”¥ ä¼˜åŒ–ï¼šç›´æ¥è°ƒç”¨Scheduler::waitForRequest()ï¼Œæ— éœ€HTTPå±‚
        const float timeoutMin = cllm::Config::instance().apiTimeoutMin();
        const float timeoutMax = cllm::Config::instance().apiTimeoutMax();
        const float tokenFactor = cllm::Config::instance().apiTimeoutTokenFactor();
        const float timeoutSec = std::max(timeoutMin, std::min(timeoutMax, static_cast<float>(params.maxTokens) * tokenFactor));
        
        if (scheduler_->waitForRequest(reqId, timeoutSec)) {
            // ğŸ”¥ ä¼˜åŒ–ï¼šç›´æ¥è°ƒç”¨Scheduler::getRequestResult()ï¼Œæ— éœ€JSONè§£æ
            RequestState resultState = scheduler_->getRequestResult(reqId);
            
            if (resultState.isTimeout) {
                result.success = false;
                result.errorMessage = "Request timeout";
            } else if (resultState.isFailed) {
                result.success = false;
                result.errorMessage = resultState.errorMessage.empty() ? "Request failed" : resultState.errorMessage;
            } else {
                // ğŸ”¥ ä¼˜åŒ–ï¼šç›´æ¥ä½¿ç”¨RequestStateä¸­çš„tokenæ•°ï¼Œæ— éœ€JSONè§£æ
                result.generatedTokens = resultState.generatedTokens.size();
                result.totalTokens = requestState.tokenizedPrompt.size() + result.generatedTokens;
                
                // è®¡ç®—tokens per second
                auto endTime = std::chrono::high_resolution_clock::now();
                result.responseTime = std::chrono::duration<double>(endTime - startTime).count();
                if (result.responseTime > 0) {
                    result.tokensPerSecond = static_cast<double>(result.generatedTokens) / result.responseTime;
                }
                
                result.success = true;
            }
        } else {
            result.success = false;
            result.errorMessage = "Request timeout (scheduler timeout)";
            auto endTime = std::chrono::high_resolution_clock::now();
            result.responseTime = std::chrono::duration<double>(endTime - startTime).count();
        }
    } catch (const SchedulerException& e) {
        auto endTime = std::chrono::high_resolution_clock::now();
        result.responseTime = std::chrono::duration<double>(endTime - startTime).count();
        result.success = false;
        result.errorMessage = std::string("Scheduler error: ") + e.what();
    } catch (const std::exception& e) {
        auto endTime = std::chrono::high_resolution_clock::now();
        result.responseTime = std::chrono::duration<double>(endTime - startTime).count();
        result.success = false;
        result.errorMessage = std::string("Exception: ") + e.what();
    }
    
    return result;
}

} // namespace cllm
