#include "cllm/http/generate_endpoint.h"
#include "cllm/http/json_request_parser.h"
#include "cllm/http/response_builder.h"
#include "cllm/scheduler/scheduler.h"
#include "cllm/tokenizer/i_tokenizer.h"
#include "cllm/tokenizer/unicode_utils.h"
#include "cllm/common/logger.h"
#include "cllm/common/config.h"
#include <nlohmann/json.hpp>
#include <sstream>
#include <chrono>
#include <random>
#include <algorithm>

namespace cllm {

namespace {

std::string sanitizeUtf8(const std::string& input) {
    std::string output;
    output.reserve(input.size());

    auto isContinuation = [](unsigned char c) {
        return (c & 0xC0) == 0x80;
    };

    for (size_t i = 0; i < input.size();) {
        unsigned char c = static_cast<unsigned char>(input[i]);
        size_t len = 0;

        if ((c & 0x80) == 0x00) {
            len = 1;
        } else if ((c & 0xE0) == 0xC0) {
            len = 2;
        } else if ((c & 0xF0) == 0xE0) {
            len = 3;
        } else if ((c & 0xF8) == 0xF0) {
            len = 4;
        } else {
            output.push_back('?');
            i += 1;
            continue;
        }

        if (i + len > input.size()) {
            output.push_back('?');
            break;
        }

        bool valid = true;
        for (size_t j = 1; j < len; ++j) {
            if (!isContinuation(static_cast<unsigned char>(input[i + j]))) {
                valid = false;
                break;
            }
        }

        if (valid) {
            output.append(input, i, len);
            i += len;
        } else {
            output.push_back('?');
            i += 1;
        }
    }

    return output;
}

} // namespace

GenerateEndpoint::GenerateEndpoint(Scheduler* scheduler, ITokenizer* tokenizer)
    : ApiEndpoint(cllm::Config::instance().apiEndpointGenerateName(), cllm::Config::instance().apiEndpointGeneratePath(), cllm::Config::instance().apiEndpointGenerateMethod()),
      scheduler_(scheduler),
      tokenizer_(tokenizer) {
}

GenerateEndpoint::~GenerateEndpoint() {
}

void GenerateEndpoint::setScheduler(Scheduler* scheduler) {
    scheduler_ = scheduler;
}

void GenerateEndpoint::setTokenizer(ITokenizer* tokenizer) {
    tokenizer_ = tokenizer;
}

GenerateEndpoint::GenerateRequest GenerateEndpoint::parseRequest(const HttpRequest& request) {
    GenerateRequest req;
    
    nlohmann::json jsonBody;
    
    if (!JsonRequestParser::validateJson(request.getBody(), jsonBody)) {
        CLLM_WARN("Failed to parse JSON request body: %s, using default values", JsonRequestParser::getLastError().c_str());
    }
    
    JsonRequestParser::getFieldWithDefault(jsonBody, "prompt", req.prompt, cllm::Config::instance().apiDefaultPrompt());
    JsonRequestParser::getFieldWithDefault(jsonBody, "max_tokens", req.maxTokens, cllm::Config::instance().apiDefaultMaxTokens());
    JsonRequestParser::getFieldWithDefault(jsonBody, "temperature", req.temperature, cllm::Config::instance().apiDefaultTemperature());
    JsonRequestParser::getFieldWithDefault(jsonBody, "top_p", req.topP, cllm::Config::instance().apiDefaultTopP());
    JsonRequestParser::getFieldWithDefault(jsonBody, "repetition_penalty", req.repetitionPenalty, cllm::Config::instance().apiDefaultRepetitionPenalty());
    JsonRequestParser::getFieldWithDefault(jsonBody, "stream", req.stream, false);
    
    // è°ƒè¯•ï¼šæ‰“å°è§£æåçš„å‚æ•°ï¼ˆä»…åœ¨ DEBUG æ¨¡å¼ä¸‹ï¼‰
    #ifdef CLLM_DEBUG_MODE
    CLLM_DEBUG("[GenerateEndpoint] Parsed request: prompt='%s', max_tokens=%d, temperature=%.4f, top_p=%.4f",
              req.prompt.c_str(), req.maxTokens, req.temperature, req.topP);
    #endif
    
    return req;
}

HttpResponse GenerateEndpoint::handle(const HttpRequest& request) {
    try {
        GenerateRequest req = parseRequest(request);
        
        if (req.stream) {
            return handleStreaming(req);
        } else {
            return handleNonStreaming(req);
        }
    } catch (const std::exception& e) {
        return ResponseBuilder::internalError(std::string("Error handling request: ") + e.what());
    }
}

HttpResponse GenerateEndpoint::handleNonStreaming(const GenerateRequest& req) {
    // ğŸ”¥ ä¼˜åŒ–ï¼šå»¶è¿Ÿå¼€å§‹æ—¶é—´æµ‹é‡ï¼Œæ’é™¤JSONè§£æç­‰éæ ¸å¿ƒå¼€é”€
    // åœ¨çœŸæ­£å¼€å§‹å¤„ç†è¯·æ±‚æ—¶æ‰å¼€å§‹è®¡æ—¶ï¼ˆä¸Stage 15å¯¹é½ï¼‰
    std::string requestId = generateRequestId();
    std::string generatedText = "";
    size_t generatedTokenCount = 0;
    
    // ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šåœ¨tokenizationä¹‹å‰å¼€å§‹è®¡æ—¶ï¼ˆä¸Stage 15å¯¹é½ï¼‰
    auto startTime = std::chrono::high_resolution_clock::now();
    
    if (scheduler_ != nullptr && tokenizer_ != nullptr) {
        try {
            #ifdef CLLM_DEBUG_MODE
            CLLM_DEBUG("Starting non-streaming request processing");
            CLLM_DEBUG("Prompt: %s", req.prompt.c_str());
            CLLM_DEBUG("Max tokens: %d", req.maxTokens);
            CLLM_DEBUG("Temperature: %f", req.temperature);
            #endif
            
            // åˆ›å»ºè¯·æ±‚çŠ¶æ€
            RequestState requestState;
            requestState.requestId = 0; // ç”±scheduleråˆ†é…
            requestState.maxTokens = req.maxTokens;
            requestState.temperature = req.temperature;
            requestState.topP = req.topP;
            requestState.topK = 0; // ä½¿ç”¨é»˜è®¤å€¼
            requestState.repetitionPenalty = req.repetitionPenalty;

            // ä» tokenizer æ³¨å…¥ EOSï¼Œç¡®ä¿è°ƒåº¦/æ‰¹å¤„ç†èƒ½æ­£ç¡®åœæ­¢
            requestState.eosTokenId = tokenizer_->getEosId();

            requestState.priority = 0;
            requestState.arrivalTime = 0;
            requestState.startTime = 0;
            requestState.completionTime = 0;
            requestState.isCompleted = false; // æ˜ç¡®åˆå§‹åŒ–
            requestState.isRunning = false;
            requestState.isFailed = false;
            requestState.samplingStrategy = "";
            requestState.errorMessage = "";
            
            // ç¼–ç prompt
            #ifdef CLLM_DEBUG_MODE
            CLLM_DEBUG("Starting tokenization...");
            #endif
            requestState.tokenizedPrompt = tokenizer_->encode(req.prompt, false);
            #ifdef CLLM_DEBUG_MODE
            CLLM_DEBUG("Tokenization completed, got %zu tokens", requestState.tokenizedPrompt.size());
            #endif
            
            // æ§åˆ¶è¾“å…¥é•¿åº¦ï¼šTorchScript trace å¯èƒ½å›ºåŒ– seq_lenï¼ˆå½“å‰æ¨¡å‹ä¸º 128ï¼‰ï¼Œè¿‡é•¿è¾“å…¥ä¼šå¯¼è‡´æ¨ç†å¼€é”€å˜å¤§
            // è¿™é‡Œåšä¸€ä¸ªæ¸©å’Œçš„ä¸Šé™ï¼Œé¿å…è¶…é•¿ prompt æŠŠ CPU æ¨ç†æ‹–å®ï¼›çœŸæ­£çš„è£å‰ª/å¡«å……ç”±åç«¯æŒ‰ traced seq_len å¤„ç†
            const int maxInputTokens = cllm::Config::instance().httpMaxInputTokens();
            if (maxInputTokens > 0) {
                const size_t MAX_INPUT_TOKENS = static_cast<size_t>(maxInputTokens);
                if (requestState.tokenizedPrompt.size() > MAX_INPUT_TOKENS) {
                    CLLM_WARN("Input tokens (%zu) exceeds limit (%zu), truncating",
                              requestState.tokenizedPrompt.size(), MAX_INPUT_TOKENS);
                    requestState.tokenizedPrompt.resize(MAX_INPUT_TOKENS);
                }
            }
            
            #ifdef CLLM_DEBUG_MODE
            if (!requestState.tokenizedPrompt.empty()) {
                CLLM_DEBUG("Token IDs: [");
                size_t showCount = std::min(requestState.tokenizedPrompt.size(), (size_t)10);
                std::stringstream tokenIds;
                for (size_t i = 0; i < showCount; ++i) {
                    tokenIds << " " << requestState.tokenizedPrompt[i];
                }
                if (requestState.tokenizedPrompt.size() > showCount) {
                    tokenIds << " ...";
                }
                tokenIds << " ]";
                CLLM_DEBUG("%s", tokenIds.str().c_str());
            }
            #endif
            
            // Phase 6: æ£€æŸ¥å¹¶å‘é™åˆ¶
            size_t runningCount = scheduler_->getRunningCount();
            size_t maxConcurrent = scheduler_->getMaxConcurrentRequests();
            if (runningCount >= maxConcurrent) {
                #ifdef CLLM_DEBUG_MODE
                CLLM_WARN("Concurrent request limit reached: %zu/%zu, returning HTTP 429", runningCount, maxConcurrent);
                #endif
                nlohmann::json errorResp;
                errorResp["success"] = false;
                errorResp["error"] = "Too many concurrent requests";
                errorResp["message"] = "Server is currently at maximum capacity. Please try again later.";
                errorResp["retry_after"] = 5;  // å»ºè®®é‡è¯•æ—¶é—´ï¼ˆç§’ï¼‰
                HttpResponse response = ResponseBuilder::json(errorResp, 429);
                response.setHeader("Retry-After", "5");
                return response;
            }
            
            // æ·»åŠ è¯·æ±‚åˆ°è°ƒåº¦å™¨
            #ifdef CLLM_DEBUG_MODE
            CLLM_DEBUG("Adding request to scheduler...");
            #endif
            size_t reqId = scheduler_->addRequest(requestState);
            #ifdef CLLM_DEBUG_MODE
            CLLM_DEBUG("Request added with ID: %zu", reqId);
            #endif
            
            // ç­‰å¾…è¯·æ±‚å®Œæˆ
            const float timeoutMin = cllm::Config::instance().apiTimeoutMin();
            const float timeoutMax = cllm::Config::instance().apiTimeoutMax();
            const float tokenFactor = cllm::Config::instance().apiTimeoutTokenFactor();
            const float timeoutSec = std::max(timeoutMin, std::min(timeoutMax, static_cast<float>(req.maxTokens) * tokenFactor));
            #ifdef CLLM_DEBUG_MODE
            CLLM_DEBUG("Waiting for request completion (timeout=%.1fs)...", timeoutSec);
            #endif
            if (scheduler_->waitForRequest(reqId, timeoutSec)) {
                #ifdef CLLM_DEBUG_MODE
                CLLM_DEBUG("Request completed, retrieving result...");
                #endif
                RequestState result = scheduler_->getRequestResult(reqId);
                
                if (result.isTimeout) {
                    #ifdef CLLM_DEBUG_MODE
                    CLLM_WARN("Request timed out (scheduler timeout)");
                    #endif
                    nlohmann::json errorResp;
                    errorResp["success"] = false;
                    errorResp["error"] = "Request timeout";
                    errorResp["message"] = "Request timed out";
                    return ResponseBuilder::json(errorResp, 408);
                }
                
                #ifdef CLLM_DEBUG_MODE
                CLLM_DEBUG("Tokenized prompt in result: %zu tokens", result.tokenizedPrompt.size());
                CLLM_DEBUG("Generated tokens count: %zu", result.generatedTokens.size());
                CLLM_DEBUG("Request ID: %llu, isCompleted: %d, isFailed: %d, isTimeout: %d", 
                          result.requestId, result.isCompleted ? 1 : 0, result.isFailed ? 1 : 0, result.isTimeout ? 1 : 0);
                #endif
                
                if (!result.generatedTokens.empty()) {
                    // æ‰“å°ç”Ÿæˆçš„ tokensï¼ˆé DEBUG æ¨¡å¼ä¹Ÿæ‰“å°ï¼Œç”¨äºè¯Šæ–­ï¼‰
                    std::stringstream tokenStr;
                    tokenStr << "Generated tokens: [";
                    size_t showCount = std::min(result.generatedTokens.size(), (size_t)10);
                    for (size_t i = 0; i < showCount; ++i) {
                        if (i > 0) tokenStr << ", ";
                        tokenStr << result.generatedTokens[i];
                    }
                    if (result.generatedTokens.size() > showCount) {
                        tokenStr << " ...";
                    }
                    tokenStr << "]";
                    CLLM_INFO("%s", tokenStr.str().c_str());

                    // è¯¦ç»†æ‰“å°å‰10ä¸ªtokenåŠå…¶åç§°
                    CLLM_INFO("=== Token Detail Analysis ===");
                    for (size_t i = 0; i < std::min(result.generatedTokens.size(), (size_t)10); ++i) {
                        int tokenId = result.generatedTokens[i];
                        std::string tokenName = tokenizer_->idToToken(tokenId);
                        bool isSpecial = tokenizer_->isSpecialToken(tokenId);
                        CLLM_INFO("  Token[%zu]: ID=%d, Name='%s', Special=%s",
                                 i, tokenId, tokenName.c_str(), isSpecial ? "YES" : "NO");
                    }
                    if (result.generatedTokens.size() > 10) {
                        CLLM_INFO("  ... and %zu more tokens", result.generatedTokens.size() - 10);
                    }
                    CLLM_INFO("=== End Token Analysis ===");

                    // è§£ç å‰ï¼šæŒ‰ EOS æˆªæ–­ï¼Œé¿å… EOS åç»§ç»­é‡‡æ ·å¯¼è‡´"ä¹±ç "
                    std::vector<int> toDecode = result.generatedTokens;
                    const int eosId = tokenizer_->getEosId();
                    CLLM_INFO("EOS token ID: %d", eosId);
                    if (eosId >= 0) {
                        for (size_t k = 0; k < toDecode.size(); ++k) {
                            if (toDecode[k] == eosId) {
                                toDecode.resize(k);
                                break;
                            }
                        }
                    }

                    generatedTokenCount = toDecode.size();
                    CLLM_INFO("Tokens to decode: %zu", generatedTokenCount);

                    try {
                        generatedText = tokenizer_->decode(toDecode, true);
                        CLLM_INFO("Decoded text length: %zu, content: [%s]", generatedText.length(), generatedText.c_str());
                        if (!UnicodeUtils::isValidUtf8(generatedText)) {
                            CLLM_WARN("Decoded text contains invalid UTF-8, sanitizing");
                            generatedText = sanitizeUtf8(generatedText);
                        }
                    } catch (const std::exception& e) {
                        CLLM_ERROR("Exception during tokenizer decode: %s", e.what());
                        generatedText = "[Decode Error: " + std::string(e.what()) + "]";
                    }
                } else {
                    #ifdef CLLM_DEBUG_MODE
                    CLLM_WARN("No tokens generated!");
                    #endif
                    generatedText = "No tokens generated";
                }
            } else {
                CLLM_ERROR("Request timed out");
                nlohmann::json errorResp;
                errorResp["success"] = false;
                errorResp["error"] = "Request timeout";
                errorResp["message"] = "Request timed out";
                return ResponseBuilder::json(errorResp, 408);
            }
        } catch (const SchedulerException& e) {
            if (e.getError() == SchedulerError::REQUEST_QUEUE_FULL) {
                #ifdef CLLM_DEBUG_MODE
                CLLM_WARN("Request rejected: queue full");
                #endif
                nlohmann::json errorResp;
                errorResp["success"] = false;
                errorResp["error"] = "Request queue is full";
                errorResp["message"] = "Server is currently at maximum capacity. Please try again later.";
                errorResp["retry_after"] = 5;
                HttpResponse response = ResponseBuilder::json(errorResp, 429);
                response.setHeader("Retry-After", "5");
                return response;
            }
            CLLM_ERROR("Scheduler error: %s", e.what());
            generatedText = std::string("Error: ") + e.what();
        } catch (const std::exception& e) {
            CLLM_ERROR("Error processing request: %s", e.what());
            generatedText = std::string("Error: ") + e.what();
        }
    } else {
        CLLM_ERROR("Scheduler or tokenizer not initialized");
        generatedText = "Server not ready";
    }
    
    auto endTime = std::chrono::high_resolution_clock::now();
    float responseTime = std::chrono::duration<float>(endTime - startTime).count();
    
    // TPS ä½¿ç”¨å®é™…ç”Ÿæˆ token æ•°é‡ï¼ˆreq.maxTokens åªæ˜¯ä¸Šé™ï¼‰
    float tokensPerSecond = 0.0f;
    if (responseTime > 0.0f) {
        tokensPerSecond = static_cast<float>(generatedTokenCount) / responseTime;
    }
    
    // ç”¨ JSON åº“æ„é€ å“åº”ï¼Œç¡®ä¿ text ç­‰å­—æ®µæ­£ç¡®è½¬ä¹‰ï¼ˆé¿å…å‡ºç°åŒå¼•å·å¯¼è‡´ JSON æ–­è£‚ï¼‰
    nlohmann::json resp;
    resp["id"] = requestId;
    resp["text"] = generatedText;
    resp["response_time"] = responseTime;
    resp["tokens_per_second"] = tokensPerSecond;
    resp["generated_tokens"] = generatedTokenCount;
    resp["max_tokens"] = req.maxTokens;

    return ResponseBuilder::success(resp);
}

HttpResponse GenerateEndpoint::handleStreaming(const GenerateRequest& req) {
    std::string requestId = generateRequestId();
    
    HttpResponse response;
    response.setStatusCode(200);
    response.enableStreaming();
    response.setContentType(cllm::Config::instance().apiResponseContentTypeStream());
    response.setHeader("Cache-Control", cllm::Config::instance().apiResponseHeaderCacheControl());
    response.setHeader("Connection", cllm::Config::instance().apiResponseHeaderConnection());
    
    if (scheduler_ == nullptr || tokenizer_ == nullptr) {
        nlohmann::json errorChunk;
        errorChunk["id"] = requestId;
        errorChunk["error"] = "Server not ready";
        errorChunk["done"] = true;
        std::ostringstream oss;
        oss << "data: " << errorChunk.dump() << "\n\n";
        response.addChunk(oss.str());
        return response;
    }
    
    try {
        // åˆ›å»ºè¯·æ±‚çŠ¶æ€
        RequestState requestState;
        requestState.requestId = 0;
        requestState.maxTokens = req.maxTokens;
        requestState.temperature = req.temperature;
        requestState.topP = req.topP;
        requestState.topK = 0;
        requestState.repetitionPenalty = req.repetitionPenalty;
        requestState.eosTokenId = tokenizer_->getEosId();
        requestState.priority = 0;
        requestState.arrivalTime = 0;
        requestState.startTime = 0;
        requestState.completionTime = 0;
        requestState.isCompleted = false;
        requestState.isRunning = false;
        requestState.isFailed = false;
        requestState.samplingStrategy = "";
        requestState.errorMessage = "";
        
        // ç¼–ç prompt
        requestState.tokenizedPrompt = tokenizer_->encode(req.prompt, false);
        
        // æ§åˆ¶è¾“å…¥é•¿åº¦
        const int maxInputTokens = cllm::Config::instance().httpMaxInputTokens();
        if (maxInputTokens > 0) {
            const size_t MAX_INPUT_TOKENS = static_cast<size_t>(maxInputTokens);
            if (requestState.tokenizedPrompt.size() > MAX_INPUT_TOKENS) {
                requestState.tokenizedPrompt.resize(MAX_INPUT_TOKENS);
            }
        }
        
        // æ£€æŸ¥å¹¶å‘é™åˆ¶
        size_t runningCount = scheduler_->getRunningCount();
        size_t maxConcurrent = scheduler_->getMaxConcurrentRequests();
        if (runningCount >= maxConcurrent) {
            nlohmann::json errorChunk;
            errorChunk["id"] = requestId;
            errorChunk["error"] = "Too many concurrent requests";
            errorChunk["done"] = true;
            std::ostringstream oss;
            oss << "data: " << errorChunk.dump() << "\n\n";
            response.addChunk(oss.str());
            return response;
        }
        
        // æ·»åŠ è¯·æ±‚åˆ°è°ƒåº¦å™¨
        size_t reqId = scheduler_->addRequest(requestState);
        
        // ç­‰å¾…è¯·æ±‚å®Œæˆï¼ˆæµå¼åœºæ™¯ä¸‹ï¼Œæ¯ä¸ª token éƒ½éœ€è¦ä» scheduler æ‹‰å–ï¼‰
        // è¿™é‡Œå…ˆå®ç°ç®€åŒ–ç‰ˆï¼šç­‰å¾…å®Œæˆåé€ token è¿”å›ï¼ˆéçœŸæ­£å®æ—¶æµå¼ï¼‰
        const float timeoutMin = cllm::Config::instance().apiTimeoutMin();
        const float timeoutMax = cllm::Config::instance().apiTimeoutMax();
        const float tokenFactor = cllm::Config::instance().apiTimeoutTokenFactor();
        const float timeoutSec = std::max(timeoutMin, std::min(timeoutMax, static_cast<float>(req.maxTokens) * tokenFactor));
        
        if (scheduler_->waitForRequest(reqId, timeoutSec)) {
            RequestState result = scheduler_->getRequestResult(reqId);
            
            if (result.isTimeout) {
                nlohmann::json errorChunk;
                errorChunk["id"] = requestId;
                errorChunk["error"] = "Request timeout";
                errorChunk["done"] = true;
                std::ostringstream oss;
                oss << "data: " << errorChunk.dump() << "\n\n";
                response.addChunk(oss.str());
                return response;
            }
            
            if (!result.generatedTokens.empty()) {
                // æŒ‰ EOS æˆªæ–­
                std::vector<int> toDecode = result.generatedTokens;
                const int eosId = tokenizer_->getEosId();
                if (eosId >= 0) {
                    for (size_t k = 0; k < toDecode.size(); ++k) {
                        if (toDecode[k] == eosId) {
                            toDecode.resize(k);
                            break;
                        }
                    }
                }
                
                // é€ token è§£ç å¹¶è¿”å›ï¼ˆæ¨¡æ‹Ÿæµå¼è¾“å‡ºï¼‰
                for (size_t i = 0; i < toDecode.size(); ++i) {
                    std::string tokenText;
                    try {
                        tokenText = tokenizer_->decode({toDecode[i]}, false);
                    } catch (...) {
                        continue;
                    }
                    
                    nlohmann::json chunk;
                    chunk["id"] = requestId;
                    chunk["token"] = tokenText;
                    chunk["done"] = false;
                    
                    std::ostringstream oss;
                    oss << "data: " << chunk.dump() << "\n\n";
                    response.addChunk(oss.str());
                }
            }
        } else {
            nlohmann::json errorChunk;
            errorChunk["id"] = requestId;
            errorChunk["error"] = "Request timeout";
            errorChunk["done"] = true;
            std::ostringstream oss;
            oss << "data: " << errorChunk.dump() << "\n\n";
            response.addChunk(oss.str());
            return response;
        }
        
        // å‘é€å®Œæˆæ¶ˆæ¯
        nlohmann::json finalChunk;
        finalChunk["id"] = requestId;
        finalChunk["token"] = "";
        finalChunk["done"] = true;
        
        std::ostringstream finalOss;
        finalOss << "data: " << finalChunk.dump() << "\n\n";
        response.addChunk(finalOss.str());
        
    } catch (const SchedulerException& e) {
        if (e.getError() == SchedulerError::REQUEST_QUEUE_FULL) {
            nlohmann::json errorChunk;
            errorChunk["id"] = requestId;
            errorChunk["error"] = "Request queue is full";
            errorChunk["done"] = true;
            std::ostringstream oss;
            oss << "data: " << errorChunk.dump() << "\n\n";
            response.addChunk(oss.str());
            return response;
        }
        nlohmann::json errorChunk;
        errorChunk["id"] = requestId;
        errorChunk["error"] = std::string("Scheduler error: ") + e.what();
        errorChunk["done"] = true;
        std::ostringstream oss;
        oss << "data: " << errorChunk.dump() << "\n\n";
        response.addChunk(oss.str());
    } catch (const std::exception& e) {
        nlohmann::json errorChunk;
        errorChunk["id"] = requestId;
        errorChunk["error"] = std::string("Error: ") + e.what();
        errorChunk["done"] = true;
        std::ostringstream oss;
        oss << "data: " << errorChunk.dump() << "\n\n";
        response.addChunk(oss.str());
    }
    
    return response;
}

std::string GenerateEndpoint::generateRequestId() {
    static std::random_device rd;
    static std::mt19937 gen(rd());
    static std::uniform_int_distribution<> dis(0, 15);
    
    const char hexChars[] = "0123456789abcdef";
    std::string id;
    
    for (int i = 0; i < 32; ++i) {
        id += hexChars[dis(gen)];
    }
    
    return id;
}

void GenerateEndpoint::handleStreamingCallback(const HttpRequest& request, StreamingWriteCallback writeCallback) {
    std::string requestId = generateRequestId();
    
    // è§£æè¯·æ±‚
    GenerateRequest req = parseRequest(request);
    
    // æ£€æŸ¥ä¾èµ–
    if (scheduler_ == nullptr || tokenizer_ == nullptr) {
        nlohmann::json errorChunk;
        errorChunk["id"] = requestId;
        errorChunk["error"] = "Server not ready";
        errorChunk["done"] = true;
        std::string errorData = "data: " + errorChunk.dump() + "\n\n";
        writeCallback(errorData);
        return;
    }
    
    // åˆ›å»ºè¯·æ±‚çŠ¶æ€
    RequestState requestState;
    requestState.requestId = 0;
    requestState.maxTokens = req.maxTokens;
    requestState.temperature = req.temperature;
    requestState.topP = req.topP;
    requestState.topK = 0;
    requestState.repetitionPenalty = req.repetitionPenalty;
    requestState.eosTokenId = tokenizer_->getEosId();
    requestState.priority = 0;
    requestState.isCompleted = false;
    requestState.isRunning = false;
    requestState.isFailed = false;
    
    // ç¼–ç  prompt
    try {
        requestState.tokenizedPrompt = tokenizer_->encode(req.prompt, false);
    } catch (const std::exception& e) {
        nlohmann::json errorChunk;
        errorChunk["id"] = requestId;
        errorChunk["error"] = std::string("Tokenization failed: ") + e.what();
        errorChunk["done"] = true;
        std::string errorData = "data: " + errorChunk.dump() + "\n\n";
        writeCallback(errorData);
        return;
    }
    
    // æ§åˆ¶è¾“å…¥é•¿åº¦
    const int maxInputTokens = cllm::Config::instance().httpMaxInputTokens();
    if (maxInputTokens > 0) {
        const size_t MAX_INPUT_TOKENS = static_cast<size_t>(maxInputTokens);
        if (requestState.tokenizedPrompt.size() > MAX_INPUT_TOKENS) {
            requestState.tokenizedPrompt.resize(MAX_INPUT_TOKENS);
        }
    }
    
    CLLM_INFO("[generateStreaming] Starting real streaming for request %s, prompt tokens: %zu, max_tokens: %d",
              requestId.c_str(), requestState.tokenizedPrompt.size(), req.maxTokens);
    
    // ä½¿ç”¨ Scheduler::generateStreaming è¿›è¡ŒçœŸæµå¼ç”Ÿæˆ
    // æ¯ç”Ÿæˆä¸€ä¸ª token ç«‹å³é€šè¿‡å›è°ƒå‘é€
    
    // ç´¯ç§¯ token IDsï¼Œç”¨äºæ‰¹é‡è§£ç ï¼ˆå¤„ç†ä¸å®Œæ•´ UTF-8ï¼‰
    std::vector<int> pendingTokens;
    
    auto tokenCallback = [this, &requestId, &writeCallback, &pendingTokens](int tokenId) -> bool {
        // ç´¯ç§¯ token
        pendingTokens.push_back(tokenId);
        
        // å°è¯•è§£ç æ‰€æœ‰ç´¯ç§¯çš„ tokens
        std::string tokenText;
        try {
            tokenText = tokenizer_->decode(pendingTokens, false);
        } catch (...) {
            // è§£ç å¤±è´¥ï¼ˆå¯èƒ½æ˜¯ä¸å®Œæ•´çš„ UTF-8ï¼‰ï¼Œç»§ç»­ç´¯ç§¯
            return true;
        }
        
        // æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ UTF-8ï¼ˆä¸ä»¥ä¸å®Œæ•´çš„å¤šå­—èŠ‚åºåˆ—ç»“å°¾ï¼‰
        if (!tokenText.empty()) {
            unsigned char lastByte = static_cast<unsigned char>(tokenText.back());
            // å¦‚æœæœ€åä¸€ä¸ªå­—èŠ‚æ˜¯å¤šå­—èŠ‚åºåˆ—çš„å¼€å§‹ä½†ä¸å®Œæ•´ï¼Œç»§ç»­ç´¯ç§¯
            if ((lastByte & 0xC0) == 0xC0) {  // å¤šå­—èŠ‚åºåˆ—å¼€å§‹
                // æ£€æŸ¥åºåˆ—æ˜¯å¦å®Œæ•´
                size_t expectedLen = 0;
                if ((lastByte & 0xE0) == 0xC0) expectedLen = 2;
                else if ((lastByte & 0xF0) == 0xE0) expectedLen = 3;
                else if ((lastByte & 0xF8) == 0xF0) expectedLen = 4;
                
                // ä»æœ€åä¸€ä¸ªå¤šå­—èŠ‚åºåˆ—å¼€å§‹æ£€æŸ¥
                size_t pos = tokenText.size() - 1;
                while (pos > 0 && (static_cast<unsigned char>(tokenText[pos]) & 0xC0) == 0x80) {
                    pos--;
                }
                size_t actualLen = tokenText.size() - pos;
                if (actualLen < expectedLen) {
                    return true;  // ä¸å®Œæ•´ï¼Œç»§ç»­ç´¯ç§¯
                }
            }
        }
        
        // æ¸…ç©ºç´¯ç§¯çš„ tokens
        pendingTokens.clear();
        
        // éªŒè¯ UTF-8ï¼Œå¦‚æœæ— æ•ˆåˆ™æ¸…ç†
        if (!UnicodeUtils::isValidUtf8(tokenText)) {
            tokenText = sanitizeUtf8(tokenText);
        }
        
        // æ„å»º SSE chunk
        try {
            nlohmann::json chunk;
            chunk["id"] = requestId;
            chunk["token"] = tokenText;
            chunk["done"] = false;
            
            std::string sseData = "data: " + chunk.dump() + "\n\n";
            
            // å‘é€ï¼ˆè¿”å› false è¡¨ç¤ºå®¢æˆ·ç«¯æ–­å¼€ï¼‰
            return writeCallback(sseData);
        } catch (const std::exception& e) {
            CLLM_WARN("[streaming] JSON error, skipping chunk: %s", e.what());
            return true;  // ç»§ç»­ç”Ÿæˆ
        }
    };
    
    // æ‰§è¡Œæµå¼ç”Ÿæˆ
    RequestState result = scheduler_->generateStreaming(requestState, tokenCallback);
    
    // å‘é€å‰©ä½™çš„ pending tokensï¼ˆå¦‚æœæœ‰ï¼‰
    if (!pendingTokens.empty()) {
        try {
            std::string remainingText = tokenizer_->decode(pendingTokens, false);
            if (!remainingText.empty()) {
                // éªŒè¯ UTF-8
                if (!UnicodeUtils::isValidUtf8(remainingText)) {
                    remainingText = sanitizeUtf8(remainingText);
                }
                nlohmann::json chunk;
                chunk["id"] = requestId;
                chunk["token"] = remainingText;
                chunk["done"] = false;
                std::string sseData = "data: " + chunk.dump() + "\n\n";
                writeCallback(sseData);
            }
        } catch (const std::exception& e) {
            CLLM_WARN("[streaming] Failed to send remaining tokens: %s", e.what());
        }
    }
    
    // å‘é€å®Œæˆæ¶ˆæ¯
    nlohmann::json finalChunk;
    finalChunk["id"] = requestId;
    finalChunk["token"] = "";
    finalChunk["done"] = true;
    finalChunk["generated_tokens"] = result.generatedTokens.size();
    
    std::string finalData = "data: " + finalChunk.dump() + "\n\n";
    writeCallback(finalData);
    
    CLLM_INFO("[generateStreaming] Completed request %s, generated %zu tokens",
              requestId.c_str(), result.generatedTokens.size());
}

} // namespace cllm