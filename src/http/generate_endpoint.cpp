#include "cllm/http/generate_endpoint.h"
#include "cllm/http/json_request_parser.h"
#include "cllm/http/response_builder.h"
#include "cllm/scheduler/scheduler.h"
#include "cllm/tokenizer/i_tokenizer.h"
#include "cllm/common/logger.h"
#include "cllm/common/config.h"
#include <nlohmann/json.hpp>
#include <sstream>
#include <chrono>
#include <random>
#include <algorithm>

namespace cllm {

GenerateEndpoint::GenerateEndpoint(Scheduler* scheduler, ITokenizer* tokenizer)
    : ApiEndpoint(cllm::Config::instance().apiEndpointGenerateName(), cllm::Config::instance().apiEndpointGeneratePath(), cllm::Config::instance().apiEndpointGenerateMethod()),
      scheduler_(scheduler),
      tokenizer_(tokenizer) {
}

GenerateEndpoint::~GenerateEndpoint() {
}

void GenerateEndpoint::setScheduler(Scheduler* scheduler) {
    scheduler_ = scheduler;
}

void GenerateEndpoint::setTokenizer(ITokenizer* tokenizer) {
    tokenizer_ = tokenizer;
}

GenerateEndpoint::GenerateRequest GenerateEndpoint::parseRequest(const HttpRequest& request) {
    GenerateRequest req;
    
    nlohmann::json jsonBody;
    
    if (!JsonRequestParser::validateJson(request.getBody(), jsonBody)) {
        CLLM_WARN("Failed to parse JSON request body: %s, using default values", JsonRequestParser::getLastError().c_str());
    }
    
    JsonRequestParser::getFieldWithDefault(jsonBody, "prompt", req.prompt, cllm::Config::instance().apiDefaultPrompt());
    JsonRequestParser::getFieldWithDefault(jsonBody, "max_tokens", req.maxTokens, cllm::Config::instance().apiDefaultMaxTokens());
    JsonRequestParser::getFieldWithDefault(jsonBody, "temperature", req.temperature, cllm::Config::instance().apiDefaultTemperature());
    JsonRequestParser::getFieldWithDefault(jsonBody, "top_p", req.topP, cllm::Config::instance().apiDefaultTopP());
    JsonRequestParser::getFieldWithDefault(jsonBody, "stream", req.stream, false);
    
    // è°ƒè¯•ï¼šæ‰“å°è§£æåçš„å‚æ•°
    CLLM_INFO("[GenerateEndpoint] Parsed request: prompt='%s', max_tokens=%d, temperature=%.4f, top_p=%.4f",
              req.prompt.c_str(), req.maxTokens, req.temperature, req.topP);
    
    return req;
}

HttpResponse GenerateEndpoint::handle(const HttpRequest& request) {
    try {
        GenerateRequest req = parseRequest(request);
        
        if (req.stream) {
            return handleStreaming(req);
        } else {
            return handleNonStreaming(req);
        }
    } catch (const std::exception& e) {
        return ResponseBuilder::internalError(std::string("Error handling request: ") + e.what());
    }
}

HttpResponse GenerateEndpoint::handleNonStreaming(const GenerateRequest& req) {
    // ğŸ”¥ ä¼˜åŒ–ï¼šå»¶è¿Ÿå¼€å§‹æ—¶é—´æµ‹é‡ï¼Œæ’é™¤JSONè§£æç­‰éæ ¸å¿ƒå¼€é”€
    // åœ¨çœŸæ­£å¼€å§‹å¤„ç†è¯·æ±‚æ—¶æ‰å¼€å§‹è®¡æ—¶ï¼ˆä¸Stage 15å¯¹é½ï¼‰
    std::string requestId = generateRequestId();
    std::string generatedText = "";
    size_t generatedTokenCount = 0;
    
    // ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šåœ¨tokenizationä¹‹å‰å¼€å§‹è®¡æ—¶ï¼ˆä¸Stage 15å¯¹é½ï¼‰
    auto startTime = std::chrono::high_resolution_clock::now();
    
    if (scheduler_ != nullptr && tokenizer_ != nullptr) {
        try {
            #ifdef CLLM_DEBUG_MODE
            CLLM_DEBUG("Starting non-streaming request processing");
            CLLM_DEBUG("Prompt: %s", req.prompt.c_str());
            CLLM_DEBUG("Max tokens: %d", req.maxTokens);
            CLLM_DEBUG("Temperature: %f", req.temperature);
            #endif
            
            // åˆ›å»ºè¯·æ±‚çŠ¶æ€
            RequestState requestState;
            requestState.requestId = 0; // ç”±scheduleråˆ†é…
            requestState.maxTokens = req.maxTokens;
            requestState.temperature = req.temperature;
            requestState.topP = req.topP;
            requestState.topK = 0; // ä½¿ç”¨é»˜è®¤å€¼

            // ä» tokenizer æ³¨å…¥ EOSï¼Œç¡®ä¿è°ƒåº¦/æ‰¹å¤„ç†èƒ½æ­£ç¡®åœæ­¢
            requestState.eosTokenId = tokenizer_->getEosId();

            requestState.priority = 0;
            requestState.arrivalTime = 0;
            requestState.startTime = 0;
            requestState.completionTime = 0;
            requestState.isCompleted = false; // æ˜ç¡®åˆå§‹åŒ–
            requestState.isRunning = false;
            requestState.isFailed = false;
            requestState.samplingStrategy = "";
            requestState.errorMessage = "";
            
            // ç¼–ç prompt
            #ifdef CLLM_DEBUG_MODE
            CLLM_DEBUG("Starting tokenization...");
            #endif
            requestState.tokenizedPrompt = tokenizer_->encode(req.prompt, false);
            #ifdef CLLM_DEBUG_MODE
            CLLM_DEBUG("Tokenization completed, got %zu tokens", requestState.tokenizedPrompt.size());
            #endif
            
            // æ§åˆ¶è¾“å…¥é•¿åº¦ï¼šTorchScript trace å¯èƒ½å›ºåŒ– seq_lenï¼ˆå½“å‰æ¨¡å‹ä¸º 128ï¼‰ï¼Œè¿‡é•¿è¾“å…¥ä¼šå¯¼è‡´æ¨ç†å¼€é”€å˜å¤§
            // è¿™é‡Œåšä¸€ä¸ªæ¸©å’Œçš„ä¸Šé™ï¼Œé¿å…è¶…é•¿ prompt æŠŠ CPU æ¨ç†æ‹–å®ï¼›çœŸæ­£çš„è£å‰ª/å¡«å……ç”±åç«¯æŒ‰ traced seq_len å¤„ç†
            const int maxInputTokens = cllm::Config::instance().httpMaxInputTokens();
            if (maxInputTokens > 0) {
                const size_t MAX_INPUT_TOKENS = static_cast<size_t>(maxInputTokens);
                if (requestState.tokenizedPrompt.size() > MAX_INPUT_TOKENS) {
                    CLLM_WARN("Input tokens (%zu) exceeds limit (%zu), truncating",
                              requestState.tokenizedPrompt.size(), MAX_INPUT_TOKENS);
                    requestState.tokenizedPrompt.resize(MAX_INPUT_TOKENS);
                }
            }
            
            #ifdef CLLM_DEBUG_MODE
            if (!requestState.tokenizedPrompt.empty()) {
                CLLM_DEBUG("Token IDs: [");
                size_t showCount = std::min(requestState.tokenizedPrompt.size(), (size_t)10);
                std::stringstream tokenIds;
                for (size_t i = 0; i < showCount; ++i) {
                    tokenIds << " " << requestState.tokenizedPrompt[i];
                }
                if (requestState.tokenizedPrompt.size() > showCount) {
                    tokenIds << " ...";
                }
                tokenIds << " ]";
                CLLM_DEBUG("%s", tokenIds.str().c_str());
            }
            #endif
            
            // Phase 6: æ£€æŸ¥å¹¶å‘é™åˆ¶
            size_t runningCount = scheduler_->getRunningCount();
            size_t maxConcurrent = scheduler_->getMaxConcurrentRequests();
            if (runningCount >= maxConcurrent) {
                #ifdef CLLM_DEBUG_MODE
                CLLM_WARN("Concurrent request limit reached: %zu/%zu, returning HTTP 429", runningCount, maxConcurrent);
                #endif
                nlohmann::json errorResp;
                errorResp["success"] = false;
                errorResp["error"] = "Too many concurrent requests";
                errorResp["message"] = "Server is currently at maximum capacity. Please try again later.";
                errorResp["retry_after"] = 5;  // å»ºè®®é‡è¯•æ—¶é—´ï¼ˆç§’ï¼‰
                HttpResponse response = ResponseBuilder::json(errorResp, 429);
                response.setHeader("Retry-After", "5");
                return response;
            }
            
            // æ·»åŠ è¯·æ±‚åˆ°è°ƒåº¦å™¨
            #ifdef CLLM_DEBUG_MODE
            CLLM_DEBUG("Adding request to scheduler...");
            #endif
            size_t reqId = scheduler_->addRequest(requestState);
            #ifdef CLLM_DEBUG_MODE
            CLLM_DEBUG("Request added with ID: %zu", reqId);
            #endif
            
            // ç­‰å¾…è¯·æ±‚å®Œæˆ
            const float timeoutMin = cllm::Config::instance().apiTimeoutMin();
            const float timeoutMax = cllm::Config::instance().apiTimeoutMax();
            const float tokenFactor = cllm::Config::instance().apiTimeoutTokenFactor();
            const float timeoutSec = std::max(timeoutMin, std::min(timeoutMax, static_cast<float>(req.maxTokens) * tokenFactor));
            #ifdef CLLM_DEBUG_MODE
            CLLM_DEBUG("Waiting for request completion (timeout=%.1fs)...", timeoutSec);
            #endif
            if (scheduler_->waitForRequest(reqId, timeoutSec)) {
                #ifdef CLLM_DEBUG_MODE
                CLLM_DEBUG("Request completed, retrieving result...");
                #endif
                RequestState result = scheduler_->getRequestResult(reqId);
                
                if (result.isTimeout) {
                    #ifdef CLLM_DEBUG_MODE
                    CLLM_WARN("Request timed out (scheduler timeout)");
                    #endif
                    nlohmann::json errorResp;
                    errorResp["success"] = false;
                    errorResp["error"] = "Request timeout";
                    errorResp["message"] = "Request timed out";
                    return ResponseBuilder::json(errorResp, 408);
                }
                
                #ifdef CLLM_DEBUG_MODE
                CLLM_DEBUG("Tokenized prompt in result: %zu tokens", result.tokenizedPrompt.size());
                CLLM_DEBUG("Generated tokens count: %zu", result.generatedTokens.size());
                CLLM_DEBUG("Request ID: %llu, isCompleted: %d, isFailed: %d, isTimeout: %d", 
                          result.requestId, result.isCompleted ? 1 : 0, result.isFailed ? 1 : 0, result.isTimeout ? 1 : 0);
                #endif
                
                if (!result.generatedTokens.empty()) {
                    #ifdef CLLM_DEBUG_MODE
                    CLLM_DEBUG("Generated tokens: [");
                    size_t showCount = std::min(result.generatedTokens.size(), (size_t)10);
                    std::stringstream generatedTokens;
                    for (size_t i = 0; i < showCount; ++i) {
                        generatedTokens << " " << result.generatedTokens[i];
                    }
                    if (result.generatedTokens.size() > showCount) {
                        generatedTokens << " ...";
                    }
                    generatedTokens << " ]";
                    CLLM_DEBUG("%s", generatedTokens.str().c_str());
                    #endif

                    // è§£ç å‰ï¼šæŒ‰ EOS æˆªæ–­ï¼Œé¿å… EOS åç»§ç»­é‡‡æ ·å¯¼è‡´"ä¹±ç "
                    std::vector<int> toDecode = result.generatedTokens;
                    const int eosId = tokenizer_->getEosId();
                    if (eosId >= 0) {
                        for (size_t k = 0; k < toDecode.size(); ++k) {
                            if (toDecode[k] == eosId) {
                                toDecode.resize(k);
                                break;
                            }
                        }
                    }

                    generatedTokenCount = toDecode.size();

                    try {
                        generatedText = tokenizer_->decode(toDecode, true);
                        #ifdef CLLM_DEBUG_MODE
                        CLLM_DEBUG("Decoded text: [%s]", generatedText.c_str());
                        CLLM_DEBUG("Decoded text length: %zu", generatedText.length());
                        #endif
                    } catch (const std::exception& e) {
                        CLLM_ERROR("Exception during tokenizer decode: %s", e.what());
                        generatedText = "[Decode Error: " + std::string(e.what()) + "]";
                    }
                } else {
                    #ifdef CLLM_DEBUG_MODE
                    CLLM_WARN("No tokens generated!");
                    #endif
                    generatedText = "No tokens generated";
                }
            } else {
                CLLM_ERROR("Request timed out");
                nlohmann::json errorResp;
                errorResp["success"] = false;
                errorResp["error"] = "Request timeout";
                errorResp["message"] = "Request timed out";
                return ResponseBuilder::json(errorResp, 408);
            }
        } catch (const SchedulerException& e) {
            if (e.getError() == SchedulerError::REQUEST_QUEUE_FULL) {
                #ifdef CLLM_DEBUG_MODE
                CLLM_WARN("Request rejected: queue full");
                #endif
                nlohmann::json errorResp;
                errorResp["success"] = false;
                errorResp["error"] = "Request queue is full";
                errorResp["message"] = "Server is currently at maximum capacity. Please try again later.";
                errorResp["retry_after"] = 5;
                HttpResponse response = ResponseBuilder::json(errorResp, 429);
                response.setHeader("Retry-After", "5");
                return response;
            }
            CLLM_ERROR("Scheduler error: %s", e.what());
            generatedText = std::string("Error: ") + e.what();
        } catch (const std::exception& e) {
            CLLM_ERROR("Error processing request: %s", e.what());
            generatedText = std::string("Error: ") + e.what();
        }
    } else {
        CLLM_ERROR("Scheduler or tokenizer not initialized");
        generatedText = "Server not ready";
    }
    
    auto endTime = std::chrono::high_resolution_clock::now();
    float responseTime = std::chrono::duration<float>(endTime - startTime).count();
    
    // TPS ä½¿ç”¨å®é™…ç”Ÿæˆ token æ•°é‡ï¼ˆreq.maxTokens åªæ˜¯ä¸Šé™ï¼‰
    float tokensPerSecond = 0.0f;
    if (responseTime > 0.0f) {
        tokensPerSecond = static_cast<float>(generatedTokenCount) / responseTime;
    }
    
    // ç”¨ JSON åº“æ„é€ å“åº”ï¼Œç¡®ä¿ text ç­‰å­—æ®µæ­£ç¡®è½¬ä¹‰ï¼ˆé¿å…å‡ºç°åŒå¼•å·å¯¼è‡´ JSON æ–­è£‚ï¼‰
    nlohmann::json resp;
    resp["id"] = requestId;
    resp["text"] = generatedText;
    resp["response_time"] = responseTime;
    resp["tokens_per_second"] = tokensPerSecond;

    return ResponseBuilder::success(resp);
}

HttpResponse GenerateEndpoint::handleStreaming(const GenerateRequest& req) {
    std::string requestId = generateRequestId();
    
    HttpResponse response;
    response.setStatusCode(200);
    response.enableStreaming();
    response.setContentType(cllm::Config::instance().apiResponseContentTypeStream());
    response.setHeader("Cache-Control", cllm::Config::instance().apiResponseHeaderCacheControl());
    response.setHeader("Connection", cllm::Config::instance().apiResponseHeaderConnection());
    
    if (scheduler_ == nullptr || tokenizer_ == nullptr) {
        nlohmann::json errorChunk;
        errorChunk["id"] = requestId;
        errorChunk["error"] = "Server not ready";
        errorChunk["done"] = true;
        std::ostringstream oss;
        oss << "data: " << errorChunk.dump() << "\n\n";
        response.addChunk(oss.str());
        return response;
    }
    
    try {
        // åˆ›å»ºè¯·æ±‚çŠ¶æ€
        RequestState requestState;
        requestState.requestId = 0;
        requestState.maxTokens = req.maxTokens;
        requestState.temperature = req.temperature;
        requestState.topP = req.topP;
        requestState.topK = 0;
        requestState.eosTokenId = tokenizer_->getEosId();
        requestState.priority = 0;
        requestState.arrivalTime = 0;
        requestState.startTime = 0;
        requestState.completionTime = 0;
        requestState.isCompleted = false;
        requestState.isRunning = false;
        requestState.isFailed = false;
        requestState.samplingStrategy = "";
        requestState.errorMessage = "";
        
        // ç¼–ç prompt
        requestState.tokenizedPrompt = tokenizer_->encode(req.prompt, false);
        
        // æ§åˆ¶è¾“å…¥é•¿åº¦
        const int maxInputTokens = cllm::Config::instance().httpMaxInputTokens();
        if (maxInputTokens > 0) {
            const size_t MAX_INPUT_TOKENS = static_cast<size_t>(maxInputTokens);
            if (requestState.tokenizedPrompt.size() > MAX_INPUT_TOKENS) {
                requestState.tokenizedPrompt.resize(MAX_INPUT_TOKENS);
            }
        }
        
        // æ£€æŸ¥å¹¶å‘é™åˆ¶
        size_t runningCount = scheduler_->getRunningCount();
        size_t maxConcurrent = scheduler_->getMaxConcurrentRequests();
        if (runningCount >= maxConcurrent) {
            nlohmann::json errorChunk;
            errorChunk["id"] = requestId;
            errorChunk["error"] = "Too many concurrent requests";
            errorChunk["done"] = true;
            std::ostringstream oss;
            oss << "data: " << errorChunk.dump() << "\n\n";
            response.addChunk(oss.str());
            return response;
        }
        
        // æ·»åŠ è¯·æ±‚åˆ°è°ƒåº¦å™¨
        size_t reqId = scheduler_->addRequest(requestState);
        
        // ç­‰å¾…è¯·æ±‚å®Œæˆï¼ˆæµå¼åœºæ™¯ä¸‹ï¼Œæ¯ä¸ª token éƒ½éœ€è¦ä» scheduler æ‹‰å–ï¼‰
        // è¿™é‡Œå…ˆå®ç°ç®€åŒ–ç‰ˆï¼šç­‰å¾…å®Œæˆåé€ token è¿”å›ï¼ˆéçœŸæ­£å®æ—¶æµå¼ï¼‰
        const float timeoutMin = cllm::Config::instance().apiTimeoutMin();
        const float timeoutMax = cllm::Config::instance().apiTimeoutMax();
        const float tokenFactor = cllm::Config::instance().apiTimeoutTokenFactor();
        const float timeoutSec = std::max(timeoutMin, std::min(timeoutMax, static_cast<float>(req.maxTokens) * tokenFactor));
        
        if (scheduler_->waitForRequest(reqId, timeoutSec)) {
            RequestState result = scheduler_->getRequestResult(reqId);
            
            if (result.isTimeout) {
                nlohmann::json errorChunk;
                errorChunk["id"] = requestId;
                errorChunk["error"] = "Request timeout";
                errorChunk["done"] = true;
                std::ostringstream oss;
                oss << "data: " << errorChunk.dump() << "\n\n";
                response.addChunk(oss.str());
                return response;
            }
            
            if (!result.generatedTokens.empty()) {
                // æŒ‰ EOS æˆªæ–­
                std::vector<int> toDecode = result.generatedTokens;
                const int eosId = tokenizer_->getEosId();
                if (eosId >= 0) {
                    for (size_t k = 0; k < toDecode.size(); ++k) {
                        if (toDecode[k] == eosId) {
                            toDecode.resize(k);
                            break;
                        }
                    }
                }
                
                // é€ token è§£ç å¹¶è¿”å›ï¼ˆæ¨¡æ‹Ÿæµå¼è¾“å‡ºï¼‰
                for (size_t i = 0; i < toDecode.size(); ++i) {
                    std::string tokenText;
                    try {
                        tokenText = tokenizer_->decode({toDecode[i]}, false);
                    } catch (...) {
                        continue;
                    }
                    
                    nlohmann::json chunk;
                    chunk["id"] = requestId;
                    chunk["token"] = tokenText;
                    chunk["done"] = false;
                    
                    std::ostringstream oss;
                    oss << "data: " << chunk.dump() << "\n\n";
                    response.addChunk(oss.str());
                }
            }
        } else {
            nlohmann::json errorChunk;
            errorChunk["id"] = requestId;
            errorChunk["error"] = "Request timeout";
            errorChunk["done"] = true;
            std::ostringstream oss;
            oss << "data: " << errorChunk.dump() << "\n\n";
            response.addChunk(oss.str());
            return response;
        }
        
        // å‘é€å®Œæˆæ¶ˆæ¯
        nlohmann::json finalChunk;
        finalChunk["id"] = requestId;
        finalChunk["token"] = "";
        finalChunk["done"] = true;
        
        std::ostringstream finalOss;
        finalOss << "data: " << finalChunk.dump() << "\n\n";
        response.addChunk(finalOss.str());
        
    } catch (const SchedulerException& e) {
        if (e.getError() == SchedulerError::REQUEST_QUEUE_FULL) {
            nlohmann::json errorChunk;
            errorChunk["id"] = requestId;
            errorChunk["error"] = "Request queue is full";
            errorChunk["done"] = true;
            std::ostringstream oss;
            oss << "data: " << errorChunk.dump() << "\n\n";
            response.addChunk(oss.str());
            return response;
        }
        nlohmann::json errorChunk;
        errorChunk["id"] = requestId;
        errorChunk["error"] = std::string("Scheduler error: ") + e.what();
        errorChunk["done"] = true;
        std::ostringstream oss;
        oss << "data: " << errorChunk.dump() << "\n\n";
        response.addChunk(oss.str());
    } catch (const std::exception& e) {
        nlohmann::json errorChunk;
        errorChunk["id"] = requestId;
        errorChunk["error"] = std::string("Error: ") + e.what();
        errorChunk["done"] = true;
        std::ostringstream oss;
        oss << "data: " << errorChunk.dump() << "\n\n";
        response.addChunk(oss.str());
    }
    
    return response;
}

std::string GenerateEndpoint::generateRequestId() {
    static std::random_device rd;
    static std::mt19937 gen(rd());
    static std::uniform_int_distribution<> dis(0, 15);
    
    const char hexChars[] = "0123456789abcdef";
    std::string id;
    
    for (int i = 0; i < 32; ++i) {
        id += hexChars[dis(gen)];
    }
    
    return id;
}

} // namespace cllm