/**
 * @file llama_cpp_backend.cpp
 * @brief llama.cpp æ¨ç†åç«¯å®ç°
 * 
 * å‚è€ƒæ–‡æ¡£ï¼šllama.cppåç«¯é›†æˆè®¾è®¡.md
 */

#include "cllm/inference/llama_cpp_backend.h"
#include "cllm/common/logger.h"
#include "cllm/common/config.h"

#include "llama.h"

#include <algorithm>
#include <stdexcept>
#include <cstring>
#include <cstdint>
#include <atomic>  // ğŸ”¥ ä¼˜åŒ–: ç”¨äºåŸå­æ“ä½œ

namespace cllm {
namespace inference {

LlamaCppBackend::LlamaCppBackend(const ModelConfig &config, const std::string &modelPath)
    : modelPath_(modelPath)
    , config_(config)
    , model_(nullptr)
    , ctx_(nullptr)
    , initialized_(false)
    , modelParams_(nullptr)
    , contextParams_(nullptr)
    , numThreads_(config.llamaNumThreads)  // 0 è¡¨ç¤ºä½¿ç”¨é»˜è®¤å€¼
    , nGpuLayers_(config.llamaGpuLayers)  // 0 è¡¨ç¤ºä»…ä½¿ç”¨ CPU
    , currentPosition_(0)
    , nSeqMax_(0)  // Phase 2: å°†åœ¨ initialize() ä¸­ä»é…ç½®è¯»å–
{
    CLLM_INFO("[LlamaCppBackend] Constructing llama.cpp backend");
    CLLM_INFO("[LlamaCppBackend] Model path: %s", modelPath_.c_str());
    CLLM_INFO("[LlamaCppBackend] Config vocab_size: %zu", config_.vocabSize);
    CLLM_INFO("[LlamaCppBackend] Config max_seq_len: %zu", config_.maxSequenceLength);
}

LlamaCppBackend::~LlamaCppBackend() {
    if (ctx_) {
        llama_free(ctx_);
        ctx_ = nullptr;
    }
    
    if (model_) {
        llama_model_free(model_);
        model_ = nullptr;
    }
    
    if (modelParams_) {
        delete modelParams_;
        modelParams_ = nullptr;
    }
    
    if (contextParams_) {
        delete contextParams_;
        contextParams_ = nullptr;
    }
    
    CLLM_INFO("[LlamaCppBackend] Destructed");
}

void LlamaCppBackend::createModelParams() {
    modelParams_ = new llama_model_params();
    *modelParams_ = llama_model_default_params();
    
    // è®¾ç½® GPU å±‚æ•°
    modelParams_->n_gpu_layers = nGpuLayers_;
    
    // ä½¿ç”¨ mmap / mlockï¼ˆä»é…ç½®è¯»å–ï¼‰
    modelParams_->use_mmap = config_.llamaUseMmap;
    modelParams_->use_mlock = config_.llamaUseMlock;
    
    CLLM_INFO("[LlamaCppBackend] Model params: n_gpu_layers=%d, use_mmap=%s, use_mlock=%s",
              modelParams_->n_gpu_layers,
              modelParams_->use_mmap ? "true" : "false",
              modelParams_->use_mlock ? "true" : "false");
}

void LlamaCppBackend::createContextParams() {
    contextParams_ = new llama_context_params();
    *contextParams_ = llama_context_default_params();
    
    // è®¾ç½®ä¸Šä¸‹æ–‡é•¿åº¦
    contextParams_->n_ctx = static_cast<uint32_t>(config_.maxSequenceLength);
    
    // è®¾ç½®æ‰¹å¤„ç†å¤§å°
    contextParams_->n_batch = config_.llamaBatchSize > 0 ? static_cast<uint32_t>(config_.llamaBatchSize) : 512;
    
    // è®¾ç½®çº¿ç¨‹æ•°
    if (numThreads_ > 0) {
        contextParams_->n_threads = numThreads_;
        contextParams_->n_threads_batch = numThreads_;
    } else {
        // ä½¿ç”¨é»˜è®¤å€¼ï¼ˆllama.cpp ä¼šè‡ªåŠ¨é€‰æ‹©ï¼‰
        contextParams_->n_threads = 0;
        contextParams_->n_threads_batch = 0;
    }
    
    // Phase 2: ä»é…ç½®è¯»å–å¹¶è®¾ç½® n_seq_maxï¼ˆå¿…é¡»åœ¨åˆ›å»ºcontextä¹‹å‰è®¾ç½®ï¼‰
    nSeqMax_ = Config::instance().backendLlamaCppNSeqMax();
    contextParams_->n_seq_max = nSeqMax_;
    
    CLLM_INFO("[LlamaCppBackend] Context params: n_ctx=%u, n_batch=%u, n_threads=%d, n_seq_max=%d",
              contextParams_->n_ctx, contextParams_->n_batch, contextParams_->n_threads, contextParams_->n_seq_max);
}

bool LlamaCppBackend::validateVocabSize() {
    if (!model_) {
        return false;
    }
    
    // è·å–æ¨¡å‹çš„ vocab size
    const struct llama_vocab* vocab = llama_model_get_vocab(model_);
    if (!vocab) {
        CLLM_ERROR("[LlamaCppBackend] Failed to get vocab from model");
        return false;
    }
    
    int32_t modelVocabSize = llama_vocab_n_tokens(vocab);
    CLLM_INFO("[LlamaCppBackend] Model vocab_size: %d", modelVocabSize);
    
    // æ›´æ–° config ä¸­çš„ vocab_size
    if (config_.vocabSize != static_cast<size_t>(modelVocabSize)) {
        CLLM_INFO("[LlamaCppBackend] Updating config vocab_size from %zu to %d",
                  config_.vocabSize, modelVocabSize);
        config_.vocabSize = static_cast<size_t>(modelVocabSize);
    }
    
    return true;
}

bool LlamaCppBackend::initialize() {
    if (initialized_) {
        CLLM_INFO("[LlamaCppBackend] Already initialized, skipping");
        return true;
    }
    
    try {
        CLLM_INFO("[LlamaCppBackend] ========== Initializing llama.cpp Backend ==========");
        
        // æ£€æŸ¥æ¨¡å‹è·¯å¾„
        if (modelPath_.empty()) {
            CLLM_ERROR("[LlamaCppBackend] Model path is empty");
            return false;
        }
        
        CLLM_INFO("[LlamaCppBackend] Loading GGUF model from: %s", modelPath_.c_str());
        
        // 1. åˆ›å»ºæ¨¡å‹å‚æ•°
        createModelParams();
        
        // 2. åŠ è½½æ¨¡å‹
        model_ = llama_model_load_from_file(modelPath_.c_str(), *modelParams_);
        if (!model_) {
            CLLM_ERROR("[LlamaCppBackend] Failed to load model from: %s", modelPath_.c_str());
            return false;
        }
        CLLM_INFO("[LlamaCppBackend] Model loaded successfully");
        
        // 3. åˆ›å»ºä¸Šä¸‹æ–‡å‚æ•°
        createContextParams();
        
        // 4. åˆ›å»ºä¸Šä¸‹æ–‡
        ctx_ = llama_init_from_model(model_, *contextParams_);
        if (!ctx_) {
            CLLM_ERROR("[LlamaCppBackend] Failed to create context");
            llama_model_free(model_);
            model_ = nullptr;
            return false;
        }
        CLLM_INFO("[LlamaCppBackend] Context created successfully");
        
        // 5. è·³è¿‡ GGUFTokenizer åŠ è½½ - ç›´æ¥ä½¿ç”¨ llama.cpp å†…ç½® tokenizer
        CLLM_INFO("[LlamaCppBackend] Skipping GGUFTokenizer, using llama.cpp native tokenizer");
        
        // ä» llama.cpp è·å– vocab size å¹¶æ›´æ–°é…ç½®
        const struct llama_vocab* vocab = llama_model_get_vocab(model_);
        if (vocab) {
            int32_t modelVocabSize = llama_vocab_n_tokens(vocab);
            CLLM_INFO("[LlamaCppBackend] llama.cpp vocab_size: %d", modelVocabSize);
            config_.vocabSize = static_cast<size_t>(modelVocabSize);
        } else {
            CLLM_ERROR("[LlamaCppBackend] Failed to get vocab from model");
            llama_free(ctx_);
            ctx_ = nullptr;
            llama_model_free(model_);
            model_ = nullptr;
            return false;
        }
        
        // 7. ä»æ¨¡å‹æ›´æ–°é…ç½®ï¼ˆå¦‚æœå¯èƒ½ï¼‰
        // è·å–å®é™…çš„ä¸Šä¸‹æ–‡é•¿åº¦
        uint32_t actualNctx = llama_n_ctx(ctx_);
        if (actualNctx > 0 && actualNctx != config_.maxSequenceLength) {
            CLLM_INFO("[LlamaCppBackend] Updating maxSequenceLength from %zu to %u",
                      config_.maxSequenceLength, actualNctx);
            config_.maxSequenceLength = actualNctx;
        }
        
        // 8. Phase 2: åˆå§‹åŒ–åºåˆ—IDæ± 
        initializeSequenceIdPool();
        
        // 9. Phase 4: åˆå§‹åŒ–KVç¼“å­˜ç®¡ç†å™¨
        // ä»é…ç½®è¯»å–å‚æ•°ï¼ˆå¦‚æœé…ç½®ä¸­æœ‰ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼
        size_t maxItems = 4 * 1024 * 1024;  // é»˜è®¤å€¼ï¼š4Mæ¡ç›®
        size_t maxMemoryMb = 1024;  // é»˜è®¤å€¼ï¼š1024MB
        
        // TODO: ä»é…ç½®è¯»å– maxKVCachesItems å’Œ kvCacheMaxMemoryMbï¼ˆå¦‚æœé…ç½®ä¸­å·²æ·»åŠ ï¼‰
        // å½“å‰å…ˆä½¿ç”¨é»˜è®¤å€¼ï¼Œåç»­å¯ä»¥ä» Config è¯»å–
        kvCacheManager_ = std::make_unique<KVCacheManager>(maxItems, maxMemoryMb);
        CLLM_INFO("[LlamaCppBackend] KV cache manager initialized: maxItems=%zu, maxMemoryMb=%zu", 
                  maxItems, maxMemoryMb);
        
        CLLM_INFO("[LlamaCppBackend] ========== Initialization Complete ==========");
        CLLM_INFO("[LlamaCppBackend] Model vocab_size: %zu", config_.vocabSize);
        CLLM_INFO("[LlamaCppBackend] Context size: %zu", config_.maxSequenceLength);
        
        initialized_ = true;
        return true;
        
    } catch (const std::exception &e) {
        CLLM_ERROR("[LlamaCppBackend] Exception during initialization: %s", e.what());
        return false;
    }
}

std::vector<int32_t> LlamaCppBackend::convertToLlamaTokens(const std::vector<int> &inputIds) {
    std::vector<int32_t> tokens;
    tokens.reserve(inputIds.size());
    for (int id : inputIds) {
        tokens.push_back(static_cast<int32_t>(id));
    }
    return tokens;
}

// extractLogits æ–¹æ³•å·²ç§»é™¤ï¼Œç›´æ¥åœ¨ forward ä¸­å¤„ç†

Tensor LlamaCppBackend::forward(const std::vector<int> &inputIds) {
    if (!initialized_) {
        throw std::runtime_error("LlamaCppBackend::forward: backend not initialized");
    }
    
    if (inputIds.empty()) {
        throw std::invalid_argument("LlamaCppBackend::forward: inputIds cannot be empty");
    }
    
    // åˆ¤æ–­æ˜¯å¦æ˜¯ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆå®Œæ•´åºåˆ—ï¼‰è¿˜æ˜¯å¢é‡æ¨ç†
    bool isFirstCall = (inputIds.size() > 1);
    if (isFirstCall) {
        currentPosition_ = 0;
    }
    
    // 1. è½¬æ¢ä¸º llama_token
    std::vector<int32_t> tokens = convertToLlamaTokens(inputIds);
    
    // 2. åˆ›å»º batch
    struct llama_batch batch = llama_batch_init(static_cast<int32_t>(tokens.size()), 0, 1);
    
    try {
        // å¡«å…… batch
        for (size_t i = 0; i < tokens.size(); ++i) {
            batch.token[i] = tokens[i];
            batch.pos[i] = static_cast<llama_pos>(currentPosition_ + i);
            batch.n_seq_id[i] = 1;
            batch.seq_id[i] = new llama_seq_id[1];
            batch.seq_id[i][0] = 0;
            batch.logits[i] = (i == tokens.size() - 1);
        }
        batch.n_tokens = static_cast<int32_t>(tokens.size());
        
        // 3. æ¨ç†
        int decodeResult = llama_decode(ctx_, batch);
        
        if (decodeResult != 0) {
            for (int32_t i = 0; i < batch.n_tokens; ++i) {
                if (batch.seq_id[i]) {
                    delete[] batch.seq_id[i];
                    batch.seq_id[i] = nullptr;
                }
            }
            llama_batch_free(batch);
            
            if (decodeResult < 0) {
                throw std::runtime_error("LlamaCppBackend::forward: llama_decode failed with error code " + std::to_string(decodeResult));
            } else {
                CLLM_WARN("[LlamaCppBackend] llama_decode returned warning code: %d", decodeResult);
            }
        }
        
        // 4. æ›´æ–°å½“å‰ä½ç½®
        currentPosition_ += tokens.size();
        
        // 5. æå– logits
        size_t seqLen = inputIds.size();
        Tensor result({seqLen, config_.vocabSize});
        
        float* logitsPtr = llama_get_logits(ctx_);
        if (logitsPtr) {
            // åªå¤åˆ¶æœ€åä¸€ä¸ª token çš„ logits
            // å‰é¢çš„ä½ç½®å¯ä»¥è®¾ç½®ä¸º 0 æˆ–é‡å¤æœ€åä¸€ä¸ª
            std::memcpy(
                result.data() + (seqLen - 1) * config_.vocabSize,
                logitsPtr,
                config_.vocabSize * sizeof(float)
            );
            
            // å‰é¢çš„ä½ç½®è®¾ç½®ä¸º 0ï¼ˆæˆ–å¤åˆ¶æœ€åä¸€ä¸ªï¼‰
            for (size_t i = 0; i < seqLen - 1; ++i) {
                std::memset(
                    result.data() + i * config_.vocabSize,
                    0,
                    config_.vocabSize * sizeof(float)
                );
            }
        } else {
            CLLM_WARN("[LlamaCppBackend] Failed to get logits, returning zero tensor");
        }
        
        // 5. æ¸…ç† batchï¼ˆåŒ…æ‹¬ seq_id æ•°ç»„ï¼‰
        for (int32_t i = 0; i < batch.n_tokens; ++i) {
            if (batch.seq_id[i]) {
                delete[] batch.seq_id[i];
                batch.seq_id[i] = nullptr;
            }
        }
        llama_batch_free(batch);
        
        return result;
        
    } catch (...) {
        // ç¡®ä¿æ¸…ç† batchï¼ˆåŒ…æ‹¬ seq_id æ•°ç»„ï¼‰
        for (int32_t i = 0; i < batch.n_tokens; ++i) {
            if (batch.seq_id[i]) {
                delete[] batch.seq_id[i];
                batch.seq_id[i] = nullptr;
            }
        }
        llama_batch_free(batch);
        throw;
    }
}

Tensor LlamaCppBackend::forwardBatch(
    const std::vector<int> &flatInputIds,
    const std::vector<std::pair<size_t, size_t>> &requestPositions,
    size_t batchSize,
    const std::vector<size_t> &sequenceIds
) {
    if (!initialized_) {
        throw std::runtime_error("LlamaCppBackend::forwardBatch: backend not initialized");
    }
    
    if (requestPositions.size() != batchSize) {
        throw std::invalid_argument("LlamaCppBackend::forwardBatch: requestPositions size mismatch");
    }
    
    // Phase 2: éªŒè¯ sequenceIds å‚æ•°ï¼ˆå¿…é¡»æä¾› requestId åˆ—è¡¨ï¼‰
    if (sequenceIds.empty()) {
        throw std::invalid_argument("LlamaCppBackend::forwardBatch: sequenceIds is required (requestId list)");
    }
    if (sequenceIds.size() != batchSize) {
        throw std::invalid_argument("LlamaCppBackend::forwardBatch: sequenceIds size mismatch");
    }
    
    // ç»Ÿè®¡æ€» token æ•°ï¼ˆç”¨äºè¾“å‡ºå¼ é‡å¤§å°ï¼‰
    size_t totalTokens = 0;
    std::vector<size_t> seqLengths(batchSize, 0);
    for (size_t i = 0; i < batchSize; ++i) {
        size_t seqLength = requestPositions[i].second - requestPositions[i].first;
        seqLengths[i] = seqLength;
        totalTokens += seqLength;
    }
    
    if (totalTokens == 0) {
        throw std::invalid_argument("LlamaCppBackend::forwardBatch: total tokens cannot be zero");
    }
    
    // è®¡ç®—å®é™…éœ€è¦é€å…¥ llama_decode çš„ token æ•°ï¼š
    // æ–°è¯·æ±‚é€å…¨é‡åºåˆ—ï¼›å·²æœ‰è¯·æ±‚ä»…é€æœ€åä¸€ä¸ª token
    std::vector<bool> isNewRequest(batchSize, false);
    std::vector<int32_t> cachedSeqIds(batchSize, -1);
    size_t actualTokenCount = 0;
    for (size_t i = 0; i < batchSize; ++i) {
        size_t requestId = sequenceIds[i];
        int32_t seqId = getSequenceId(requestId);
        cachedSeqIds[i] = seqId;
        if (seqId == -1) {
            isNewRequest[i] = true;
            actualTokenCount += seqLengths[i];
        } else {
            actualTokenCount += 1;
        }
    }
    
    if (actualTokenCount == 0) {
        throw std::invalid_argument("LlamaCppBackend::forwardBatch: actual tokens cannot be zero");
    }
    
    // è½¬æ¢ä¸º llama_token
    std::vector<int32_t> allTokens;
    allTokens.reserve(totalTokens);
    for (int id : flatInputIds) {
        allTokens.push_back(static_cast<int32_t>(id));
    }
    
    // åˆ›å»º batch
    struct llama_batch batch = llama_batch_init(static_cast<int32_t>(actualTokenCount), 0, static_cast<int32_t>(batchSize));
    
    try {
        size_t tokenIdx = 0;
        for (size_t batchIdx = 0; batchIdx < batchSize; ++batchIdx) {
            const auto& pos = requestPositions[batchIdx];
            size_t seqStart = pos.first;
            size_t seqEnd = pos.second;
            
            // Phase 2: è·å– requestId å¹¶åˆ†é…/æŸ¥è¯¢ seq_id
            size_t requestId = sequenceIds[batchIdx];
            int32_t seqId = cachedSeqIds[batchIdx];
            
            // ğŸ”¥ ä¼˜åŒ–: å¦‚æœæ˜¯æ–°è¯·æ±‚ï¼ˆé¦–æ¬¡åˆ†é…ï¼‰ï¼Œæ‰¹é‡åˆ†é… seq_idï¼ˆå‡å°‘é”ç«äº‰ï¼‰
            if (seqId == -1) {
                seqId = allocateSequenceId(requestId);
                if (seqId == -1) {
                    // å¦‚æœåˆ†é…å¤±è´¥ï¼Œå°è¯•ç­‰å¾…å¹¶é‡è¯•ä¸€æ¬¡ï¼ˆå¯èƒ½åˆšæœ‰IDè¢«é‡Šæ”¾ï¼‰
                    std::this_thread::sleep_for(std::chrono::microseconds(10));
                    seqId = allocateSequenceId(requestId);
                    if (seqId == -1) {
                        throw std::runtime_error(
                            "LlamaCppBackend::forwardBatch: failed to allocate seq_id for request " + std::to_string(requestId) + " (pool exhausted)"
                        );
                    }
                }
                cachedSeqIds[batchIdx] = seqId;
            }
            
            // ğŸ”¥ è·å–åºåˆ—çš„å½“å‰æ€»é•¿åº¦ï¼ˆç´¯è®¡ä½ç½®ï¼‰
            size_t currentSeqPosition = 0;
            {
                std::lock_guard<std::mutex> lock(sequenceIdMutex_);
                auto posIt = seqIdToPosition_.find(seqId);
                if (posIt != seqIdToPosition_.end()) {
                    currentSeqPosition = posIt->second;
                }
            }
            
            size_t actualSeqStart = seqStart;
            size_t actualSeqEnd = seqEnd;
            if (!isNewRequest[batchIdx]) {
                if (seqEnd == seqStart) {
                    continue;
                }
                actualSeqStart = seqEnd - 1;
            }
            
            // ğŸ”¥ ä¼˜åŒ–: æ‰¹é‡è®¾ç½®batchå­—æ®µï¼Œå‡å°‘å¾ªç¯å¼€é”€
            // ğŸ”¥ ä¿®å¤ä½ç½®è®¡ç®—ï¼šä½¿ç”¨ç»å¯¹ä½ç½®ï¼ˆç´¯è®¡ä½ç½®ï¼‰ï¼Œè€Œä¸æ˜¯ç›¸å¯¹ä½ç½®
            for (size_t i = actualSeqStart; i < actualSeqEnd; ++i) {
                batch.token[tokenIdx] = allTokens[i];
                
                // è®¡ç®—ç»å¯¹ä½ç½®ï¼šæ–°è¯·æ±‚ä»0å¼€å§‹ï¼Œå·²å­˜åœ¨è¯·æ±‚ä»å½“å‰æ€»é•¿åº¦å¼€å§‹
                size_t posInSeq;
                if (isNewRequest[batchIdx]) {
                    // æ–°è¯·æ±‚ï¼šä½ç½®ä»0å¼€å§‹ï¼Œå³ (i - seqStart)
                    posInSeq = i - seqStart;
                } else {
                    // å·²å­˜åœ¨è¯·æ±‚ï¼šä½ç½®ä»å½“å‰æ€»é•¿åº¦å¼€å§‹ï¼Œå³ currentSeqPosition + (i - actualSeqStart)
                    posInSeq = currentSeqPosition + (i - actualSeqStart);
                }
                
                batch.pos[tokenIdx] = static_cast<llama_pos>(posInSeq);
                batch.n_seq_id[tokenIdx] = 1;
                // seq_id å¿…é¡»æŒ‡å‘æœ‰æ•ˆçš„æ•°ç»„ï¼Œä¸èƒ½ä¸º nullptr
                batch.seq_id[tokenIdx] = new llama_seq_id[1];
                // Phase 2: ä½¿ç”¨åŸºäº requestId çš„ seq_idï¼ˆè€Œä¸æ˜¯æ‰¹å¤„ç†ç´¢å¼•ï¼‰
                batch.seq_id[tokenIdx][0] = static_cast<llama_seq_id>(seqId);
                // åªè®¡ç®—æœ€åä¸€ä¸ªä½ç½®çš„ logitsï¼ˆå‚è€ƒllama-benchçš„æ–¹å¼ï¼‰
                batch.logits[tokenIdx] = (i == actualSeqEnd - 1);
                ++tokenIdx;
            }
            
            // ğŸ”¥ æ›´æ–°åºåˆ—çš„æ€»é•¿åº¦ï¼ˆç´¯è®¡ä½ç½®ï¼‰
            {
                std::lock_guard<std::mutex> lock(sequenceIdMutex_);
                size_t tokensProcessed = actualSeqEnd - actualSeqStart;
                seqIdToPosition_[seqId] = currentSeqPosition + tokensProcessed;
            }
        }
        batch.n_tokens = static_cast<int32_t>(actualTokenCount);
        
        CLLM_DEBUG("[LlamaCppBackend::forwardBatch] Calling llama_decode with %d tokens (totalTokens=%zu)...", 
                  batch.n_tokens, totalTokens);
        
        // æ¨ç†
        int decodeResult = llama_decode(ctx_, batch);
        if (decodeResult != 0) {
            // æ¸…ç† batchï¼ˆåŒ…æ‹¬ seq_id æ•°ç»„ï¼‰
            for (int32_t i = 0; i < batch.n_tokens; ++i) {
                if (batch.seq_id[i]) {
                    delete[] batch.seq_id[i];
                    batch.seq_id[i] = nullptr;
                }
            }
            llama_batch_free(batch);
            
            if (decodeResult < 0) {
                throw std::runtime_error("LlamaCppBackend::forwardBatch: llama_decode failed with error code " + std::to_string(decodeResult));
            } else {
                CLLM_WARN("[LlamaCppBackend::forwardBatch] llama_decode returned warning code: %d", decodeResult);
            }
        }
        
        CLLM_DEBUG("[LlamaCppBackend::forwardBatch] llama_decode completed successfully");
        
        // Phase 4: æ›´æ–°KVç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        // requestPositions å¯¹åº”æ¯ä¸ªè¯·æ±‚çš„å®Œæ•´è¾“å…¥åºåˆ—ï¼ˆprompt + å·²ç”Ÿæˆ tokensï¼‰
        if (kvCacheManager_) {
            for (size_t batchIdx = 0; batchIdx < batchSize; ++batchIdx) {
                size_t requestId = sequenceIds[batchIdx];
                const auto& pos = requestPositions[batchIdx];
                size_t sequenceLength = pos.second - pos.first;  // å½“å‰åºåˆ—é•¿åº¦ï¼ˆtokensï¼‰
                
                // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯ï¼ˆåºåˆ—é•¿åº¦ = tokenæ•°é‡ï¼‰
                kvCacheManager_->updateKVCacheStats(requestId, sequenceLength);
            }
        }
        
        // æå– logits
        // llama.cpp çš„ logits æŒ‰ logits[i] != 0 çš„é¡ºåºè¿ç»­å­˜å‚¨
        // ç”±äºæˆ‘ä»¬åªè®¾ç½®äº†æœ€åä¸€ä¸ªä½ç½®çš„ logits[i] = 1ï¼Œæ‰€ä»¥åªæœ‰æœ€åä¸€ä¸ªä½ç½®çš„ logits å·²è®¡ç®—
        
        // è¿”å›çš„ tensor å¤§å°å¿…é¡»æ˜¯ [totalTokens, vocabSize]ï¼ˆModelExecutor çš„è¦æ±‚ï¼‰
        // æˆ‘ä»¬åªå¡«å……æœ€åä¸€ä¸ªä½ç½®çš„ logitsï¼ˆç”¨äºé‡‡æ ·ï¼‰ï¼Œå…¶ä»–ä½ç½®ä¿æŒé›¶
        Tensor result({totalTokens, config_.vocabSize});
        float* logitsPtr = llama_get_logits(ctx_);
        
        if (logitsPtr) {
            // è·Ÿè¸ªå“ªäº› token ä½ç½®éœ€è¦ logitsï¼Œä»¥åŠå®ƒä»¬åœ¨ batch ä¸­çš„ç´¢å¼•
            // llama_get_logits(ctx_) è¿”å›çš„ logits æ˜¯æŒ‰ batch.logits[i] = 1 çš„é¡ºåºè¿ç»­å­˜å‚¨çš„
            size_t logitsOffset = 0;
            size_t tokenIdx = 0;
            
            for (size_t batchIdx = 0; batchIdx < batchSize; ++batchIdx) {
                const auto& pos = requestPositions[batchIdx];
                size_t seqStart = pos.first;
                size_t seqEnd = pos.second;
                
                size_t actualSeqStart = seqStart;
                size_t actualSeqEnd = seqEnd;
                if (!isNewRequest[batchIdx]) {
                    if (seqEnd == seqStart) {
                        continue;
                    }
                    actualSeqStart = seqEnd - 1;
                }
                
                // éå†è¯¥è¯·æ±‚çš„æ‰€æœ‰ token
                for (size_t i = actualSeqStart; i < actualSeqEnd; ++i) {
                    if (batch.logits[tokenIdx]) {
                        // è¿™ä¸ª token éœ€è¦ logitsï¼Œä» logitsPtr ä¸­æå–
                        size_t resultPos = i;  // åœ¨ result ä¸­çš„ä½ç½®
                        std::memcpy(
                            result.data() + resultPos * config_.vocabSize,
                            logitsPtr + logitsOffset * config_.vocabSize,
                            config_.vocabSize * sizeof(float)
                        );
                        CLLM_DEBUG("[LlamaCppBackend::forwardBatch] Copied logits for request %zu, token position %zu (batch tokenIdx=%zu, logitsOffset=%zu)", 
                                  batchIdx, resultPos, tokenIdx, logitsOffset);
                        logitsOffset++;
                    }
                    tokenIdx++;
                }
            }
            
            CLLM_DEBUG("[LlamaCppBackend::forwardBatch] Extracted logits for %zu positions", logitsOffset);
        } else {
            CLLM_WARN("[LlamaCppBackend::forwardBatch] Failed to get logits, returning zero tensor");
        }
        
        // æ¸…ç† batchï¼ˆåŒ…æ‹¬ seq_id æ•°ç»„ï¼‰
        for (int32_t i = 0; i < batch.n_tokens; ++i) {
            if (batch.seq_id[i]) {
                delete[] batch.seq_id[i];
                batch.seq_id[i] = nullptr;
            }
        }
        llama_batch_free(batch);
        
        return result;
        
    } catch (...) {
        // ç¡®ä¿æ¸…ç† batchï¼ˆåŒ…æ‹¬ seq_id æ•°ç»„ï¼‰
        for (int32_t i = 0; i < batch.n_tokens; ++i) {
            if (batch.seq_id[i]) {
                delete[] batch.seq_id[i];
                batch.seq_id[i] = nullptr;
            }
        }
        llama_batch_free(batch);
        throw;
    }
}

void LlamaCppBackend::setNumThreads(int numThreads) {
    numThreads_ = numThreads;
    if (ctx_ && numThreads > 0) {
        llama_set_n_threads(ctx_, numThreads, numThreads);
        CLLM_INFO("[LlamaCppBackend] Set threads to %d", numThreads);
    }
}

void LlamaCppBackend::setNGpuLayers(int nGpuLayers) {
    nGpuLayers_ = nGpuLayers;
    // æ³¨æ„ï¼šGPU å±‚æ•°åªèƒ½åœ¨æ¨¡å‹åŠ è½½å‰è®¾ç½®
    if (initialized_) {
        CLLM_WARN("[LlamaCppBackend] setNGpuLayers called after initialization, will take effect on next load");
    }
}

void LlamaCppBackend::initializeSequenceIdPool() {
    // Phase 2: n_seq_max å·²åœ¨ createContextParams() ä¸­ä»é…ç½®è¯»å–å¹¶è®¾ç½®
    // è¿™é‡Œåªéœ€è¦åˆå§‹åŒ–åºåˆ—IDæ± 
    CLLM_INFO("[LlamaCppBackend] Initializing sequence ID pool with n_seq_max=%d", nSeqMax_);
    
    // ğŸ”¥ ä¼˜åŒ–: åˆå§‹åŒ–æ—¶é¢„åˆ†é…æ‰€æœ‰IDåˆ°å¯ç”¨æ± 
    availableSeqIds_.clear();
    availableSeqIds_.reserve(nSeqMax_);
    for (int32_t i = 0; i < nSeqMax_; ++i) {
        availableSeqIds_.push_back(i);
    }
    
    // æ¸…ç©ºæ˜ å°„
    requestIdToSeqId_.clear();
    
    // ğŸ”¥ æ¸…ç©ºåºåˆ—ä½ç½®è·Ÿè¸ª
    seqIdToPosition_.clear();
    
    // ğŸ”¥ ä¼˜åŒ–: é‡ç½®åŸå­è®¡æ•°å™¨
    nextSeqId_.store(0, std::memory_order_relaxed);
    
    CLLM_INFO("[LlamaCppBackend] Sequence ID pool initialized: %d available IDs", nSeqMax_);
}

int32_t LlamaCppBackend::allocateSequenceId(size_t requestId) {
    // ğŸ”¥ ä¼˜åŒ–: å…ˆå¿«é€Ÿæ£€æŸ¥æ˜¯å¦å·²åˆ†é…ï¼ˆæ— é”è¯»å–ï¼‰
    {
        std::lock_guard<std::mutex> lock(sequenceIdMutex_);
        auto it = requestIdToSeqId_.find(requestId);
        if (it != requestIdToSeqId_.end()) {
            CLLM_DEBUG("[LlamaCppBackend] Request %zu already has seq_id %d", requestId, it->second);
            return it->second;
        }
    }
    
    // ğŸ”¥ ä¼˜åŒ–: ä¼˜å…ˆä»å¯ç”¨æ± åˆ†é…ï¼ˆå¤ç”¨å·²é‡Šæ”¾çš„IDï¼‰
    int32_t seqId = -1;
    {
        std::lock_guard<std::mutex> lock(sequenceIdMutex_);
        
        // å†æ¬¡æ£€æŸ¥ï¼ˆåŒé‡æ£€æŸ¥ï¼Œé¿å…ç«æ€æ¡ä»¶ï¼‰
        auto it = requestIdToSeqId_.find(requestId);
        if (it != requestIdToSeqId_.end()) {
            return it->second;
        }
        
        // ä¼˜å…ˆä»å¯ç”¨æ± åˆ†é…
        if (!availableSeqIds_.empty()) {
            seqId = availableSeqIds_.back();
            availableSeqIds_.pop_back();
        } else {
            // å¯ç”¨æ± ä¸ºç©ºï¼Œä½¿ç”¨åŸå­è®¡æ•°å™¨åˆ†é…æ–°ID
            size_t nextId = nextSeqId_.fetch_add(1, std::memory_order_relaxed);
            if (nextId >= static_cast<size_t>(nSeqMax_)) {
                // è¶…å‡ºé™åˆ¶ï¼Œå°è¯•ä»å·²é‡Šæ”¾çš„IDä¸­å›æ”¶
                CLLM_WARN("[LlamaCppBackend] Sequence ID pool exhausted for request %zu", requestId);
                return -1;
            }
            seqId = static_cast<int32_t>(nextId);
        }
        
        // å»ºç«‹æ˜ å°„
        requestIdToSeqId_[requestId] = seqId;
        
        // ğŸ”¥ åˆå§‹åŒ–åºåˆ—ä½ç½®è·Ÿè¸ªï¼ˆæ–°åºåˆ—ä»ä½ç½®0å¼€å§‹ï¼‰
        seqIdToPosition_[seqId] = 0;
    }
    
    CLLM_DEBUG("[LlamaCppBackend] Allocated seq_id %d for request %zu", seqId, requestId);
    return seqId;
}

bool LlamaCppBackend::releaseSequenceId(size_t requestId) {
    // ğŸ”¥ ä¼˜åŒ–: å¿«é€Ÿé‡Šæ”¾ï¼Œå‡å°‘é”æŒæœ‰æ—¶é—´
    int32_t seqId = -1;
    {
        std::lock_guard<std::mutex> lock(sequenceIdMutex_);
        
        // æŸ¥æ‰¾ requestId å¯¹åº”çš„ seqId
        auto it = requestIdToSeqId_.find(requestId);
        if (it == requestIdToSeqId_.end()) {
            CLLM_DEBUG("[LlamaCppBackend] Request %zu not found in sequence ID mapping (may have been released)", requestId);
            return false;
        }
        
        seqId = it->second;
        
        // åˆ é™¤æ˜ å°„
        requestIdToSeqId_.erase(it);
        
        // ğŸ”¥ æ¸…é™¤åºåˆ—ä½ç½®è·Ÿè¸ªï¼ˆé‡Šæ”¾åºåˆ—æ—¶æ¸…é™¤ä½ç½®ä¿¡æ¯ï¼‰
        seqIdToPosition_.erase(seqId);
        
        // å°† seqId è¿”å›å¯ç”¨æ± ï¼ˆä¼˜å…ˆå¤ç”¨ï¼‰
        availableSeqIds_.push_back(seqId);
    }
    
    // ğŸ”¥ æ¸…ç†KVç¼“å­˜ï¼ˆåœ¨é‡Šæ”¾åºåˆ—IDä¹‹å‰ï¼Œç¡®ä¿KVç¼“å­˜è¢«æ¸…ç†ï¼Œé¿å…åºåˆ—IDå¤ç”¨æ—¶çš„ä½ç½®ä¸ä¸€è‡´ï¼‰
    if (kvCacheManager_ && ctx_) {
        kvCacheManager_->removeKVCache(ctx_, requestId, seqId);
    }
    
    CLLM_DEBUG("[LlamaCppBackend] Released seq_id %d for request %zu", seqId, requestId);
    return true;
}

int32_t LlamaCppBackend::getSequenceId(size_t requestId) const {
    // ğŸ”¥ ä¼˜åŒ–: å¿«é€Ÿè¯»å–ï¼ˆè™½ç„¶ä»éœ€è¦é”ï¼Œä½†æ“ä½œç®€å•ï¼‰
    std::lock_guard<std::mutex> lock(sequenceIdMutex_);
    
    auto it = requestIdToSeqId_.find(requestId);
    if (it == requestIdToSeqId_.end()) {
        return -1;
    }
    
    return it->second;
}

double LlamaCppBackend::getSequenceIdPoolUsage() const {
    std::lock_guard<std::mutex> lock(sequenceIdMutex_);
    
    if (nSeqMax_ == 0) {
        return 0.0;
    }
    
    size_t used = requestIdToSeqId_.size();
    double usage = static_cast<double>(used) / static_cast<double>(nSeqMax_);
    
    // å½“ä½¿ç”¨ç‡è¶…è¿‡80%æ—¶è®°å½•è­¦å‘Š
    if (usage > 0.8) {
        CLLM_WARN("[LlamaCppBackend] Sequence ID pool usage high: %zu/%d (%.1f%%)",
                  used, nSeqMax_, usage * 100);
    }
    
    return usage;
}

size_t LlamaCppBackend::getAvailableSequenceIdCount() const {
    // ğŸ”¥ ä¼˜åŒ–: å¿«é€Ÿä¼°ç®—å¯ç”¨æ•°é‡ï¼ˆä½¿ç”¨åŸå­æ“ä½œå’Œé”çš„ç»„åˆï¼‰
    std::lock_guard<std::mutex> lock(sequenceIdMutex_);
    size_t available = availableSeqIds_.size();
    
    // å¦‚æœå¯ç”¨æ± ä¸ºç©ºï¼Œä½†nextSeqId_è¿˜æ²¡è¾¾åˆ°ä¸Šé™ï¼Œè¯´æ˜è¿˜æœ‰å¯ç”¨ID
    if (available == 0) {
        size_t nextId = nextSeqId_.load(std::memory_order_relaxed);
        if (nextId < static_cast<size_t>(nSeqMax_)) {
            available = nSeqMax_ - static_cast<int32_t>(nextId);
        }
    }
    
    return available;
}

bool LlamaCppBackend::cleanupKVCache(size_t requestId) {
    if (!kvCacheManager_ || !ctx_) {
        CLLM_WARN("[LlamaCppBackend] Cannot clean KV cache: kvCacheManager_ or ctx_ is nullptr");
        return false;
    }
    
    // è·å– requestId å¯¹åº”çš„ seqId
    int32_t seqId = getSequenceId(requestId);
    if (seqId < 0) {
        CLLM_WARN("[LlamaCppBackend] Cannot clean KV cache: seqId not found for requestId=%zu", requestId);
        return false;
    }
    
    // è°ƒç”¨ KVCacheManager æ¸…ç†KVç¼“å­˜
    bool success = kvCacheManager_->removeKVCache(ctx_, requestId, seqId);
    
    if (success) {
        CLLM_DEBUG("[LlamaCppBackend] Cleaned KV cache for requestId=%zu, seqId=%d", requestId, seqId);
    } else {
        CLLM_WARN("[LlamaCppBackend] Failed to clean KV cache for requestId=%zu", requestId);
    }
    
    return success;
}

} // namespace inference
} // namespace cllm
