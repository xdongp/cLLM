#!/usr/bin/env python3
"""
åˆ†æç¨³å®šæ€§æµ‹è¯•ç»“æœï¼Œè¯†åˆ«å“åº”æ—¶é—´æ–¹å·®å’Œæœ€å¤§å“åº”æ—¶é—´çš„æ ¹æœ¬åŸå› 
"""

import json
import statistics
from datetime import datetime
from typing import List, Dict


def load_results(file_path: str) -> Dict:
    """åŠ è½½æµ‹è¯•ç»“æœ"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def analyze_response_time_distribution(response_times: List[float]) -> Dict:
    """åˆ†æå“åº”æ—¶é—´åˆ†å¸ƒ"""
    if not response_times:
        return {}
    
    sorted_times = sorted(response_times)
    mean = statistics.mean(response_times)
    std_dev = statistics.stdev(response_times) if len(response_times) > 1 else 0
    
    # è¯†åˆ«å¼‚å¸¸å€¼ï¼ˆè¶…è¿‡3ä¸ªæ ‡å‡†å·®ï¼‰
    outliers = [rt for rt in response_times if rt > mean + 3 * std_dev]
    
    # åˆ†ææ—¶é—´é—´éš”æ¨¡å¼
    intervals = []
    for i in range(1, len(sorted_times)):
        intervals.append(sorted_times[i] - sorted_times[i-1])
    
    # è¯†åˆ«æ‰¹æ¬¡æ¨¡å¼ï¼ˆå“åº”æ—¶é—´é›†ä¸­çš„åŒºé—´ï¼‰
    batch_patterns = {}
    for rt in response_times:
        key = round(rt, 1)  # å››èˆäº”å…¥åˆ°å°æ•°ç‚¹å1ä½
        batch_patterns[key] = batch_patterns.get(key, 0) + 1
    
    return {
        'mean': mean,
        'std_dev': std_dev,
        'variance': std_dev ** 2,
        'cv': std_dev / mean if mean > 0 else 0,
        'outlier_count': len(outliers),
        'outlier_percentage': (len(outliers) / len(response_times)) * 100,
        'outliers': outliers[:10],  # åªä¿ç•™å‰10ä¸ª
        'interval_analysis': {
            'min_interval': min(intervals) if intervals else 0,
            'max_interval': max(intervals) if intervals else 0,
            'mean_interval': statistics.mean(intervals) if intervals else 0,
            'std_interval': statistics.stdev(intervals) if len(intervals) > 1 else 0
        },
        'batch_patterns': dict(sorted(batch_patterns.items(), key=lambda x: x[1], reverse=True)[:10])
    }


def analyze_max_response_time_causes(test_results: Dict) -> Dict:
    """åˆ†ææœ€å¤§å“åº”æ—¶é—´çš„æ ¹æœ¬åŸå› """
    raw_data = test_results.get('raw_data', {})
    response_times = raw_data.get('response_times', [])
    timestamps = raw_data.get('timestamps', [])
    
    if not response_times:
        return {}
    
    max_rt = max(response_times)
    max_indices = [i for i, rt in enumerate(response_times) if rt == max_rt]
    
    # åˆ†ææœ€å¤§å“åº”æ—¶é—´å‘ç”Ÿçš„æ—¶é—´ç‚¹
    max_timestamps = [timestamps[i] for i in max_indices if i < len(timestamps)]
    
    # åˆ†æå‰åçš„å“åº”æ—¶é—´
    context_analysis = []
    for idx in max_indices:
        before = response_times[max(0, idx-5):idx]
        after = response_times[idx+1:min(len(response_times), idx+6)]
        
        context_analysis.append({
            'position': idx,
            'max_rt': response_times[idx],
            'before_mean': statistics.mean(before) if before else 0,
            'after_mean': statistics.mean(after) if after else 0,
            'before_count': len(before),
            'after_count': len(after)
        })
    
    # è¯†åˆ«å¯èƒ½çš„åŸå› 
    possible_causes = []
    
    # åŸå› 1: ç³»ç»Ÿåˆå§‹åŒ–/é¢„çƒ­
    if max_indices and max_indices[0] < 10:
        possible_causes.append({
            'cause': 'ç³»ç»Ÿåˆå§‹åŒ–/é¢„çƒ­',
            'evidence': f"æœ€å¤§å“åº”æ—¶é—´å‘ç”Ÿåœ¨ç¬¬{max_indices[0]+1}ä¸ªè¯·æ±‚ï¼ˆå‰10ä¸ªï¼‰",
            'confidence': 'é«˜'
        })
    
    # åŸå› 2: æ‰¹å¤„ç†é‡ç»„
    if len(max_indices) > 1:
        interval_between = max_indices[1] - max_indices[0]
        if interval_between < 20:
            possible_causes.append({
                'cause': 'æ‰¹å¤„ç†é‡ç»„å¼€é”€',
                'evidence': f"å¤šä¸ªæœ€å¤§å“åº”æ—¶é—´é›†ä¸­å‡ºç°ï¼Œé—´éš”{interval_between}ä¸ªè¯·æ±‚",
                'confidence': 'é«˜'
            })
    
    # åŸå› 3: èµ„æºç«äº‰
    if max_rt > statistics.mean(response_times) * 2:
        possible_causes.append({
            'cause': 'èµ„æºç«äº‰/é”ç«äº‰',
            'evidence': f"æœ€å¤§å“åº”æ—¶é—´({max_rt:.2f}s)æ˜¯å¹³å‡å€¼({statistics.mean(response_times):.2f}s)çš„{max_rt/statistics.mean(response_times):.1f}å€",
            'confidence': 'ä¸­'
        })
    
    # åŸå› 4: åºåˆ—IDæ± è€—å°½
    if test_results.get('test_name', '').startswith('é«˜å¹¶å‘'):
        possible_causes.append({
            'cause': 'åºåˆ—IDæ± å¯èƒ½è€—å°½',
            'evidence': f"åœ¨é«˜å¹¶å‘åœºæ™¯ä¸‹({test_results.get('total_requests', 0)}ä¸ªè¯·æ±‚)ï¼Œåºåˆ—IDæ± å¯èƒ½æˆä¸ºç“¶é¢ˆ",
            'confidence': 'ä¸­'
        })
    
    # åŸå› 5: GPU/CPUèµ„æºé™åˆ¶
    if max_rt > 10:
        possible_causes.append({
            'cause': 'GPU/CPUèµ„æºé™åˆ¶',
            'evidence': f"å“åº”æ—¶é—´è¶…è¿‡10ç§’ï¼Œå¯èƒ½æ˜¯ç¡¬ä»¶èµ„æºç“¶é¢ˆ",
            'confidence': 'ä¸­'
        })
    
    return {
        'max_response_time': max_rt,
        'occurrences': len(max_indices),
        'timestamps': max_timestamps,
        'context_analysis': context_analysis[:5],  # åªä¿ç•™å‰5ä¸ª
        'possible_causes': possible_causes
    }


def generate_analysis_report(results: Dict) -> str:
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    report = ["="*80]
    report.append("CLLMç³»ç»Ÿç¨³å®šæ€§æµ‹è¯• - æ ¹æœ¬åŸå› åˆ†ææŠ¥å‘Š")
    report.append("="*80)
    report.append(f"\næµ‹è¯•æ—¶é—´: {results.get('test_time', 'N/A')}")
    report.append(f"æµ‹è¯•æ•°é‡: {results.get('summary', {}).get('total_tests', 0)}")
    report.append(f"å¹³å‡ç¨³å®šæ€§åˆ†æ•°: {results.get('summary', {}).get('overall_stability_score', 0) * 100:.2f}%")
    report.append(f"å¹³å‡æ–¹å·®: {results.get('summary', {}).get('overall_variance', 0):.2f}")
    report.append(f"æœ€å¤§å“åº”æ—¶é—´: {results.get('summary', {}).get('overall_max_rt', 0):.2f}ç§’")
    
    report.append("\n" + "="*80)
    report.append("å„æµ‹è¯•è¯¦ç»†åˆ†æ")
    report.append("="*80)
    
    for test in results.get('tests', []):
        report.append(f"\n{'â”€'*80}")
        report.append(f"æµ‹è¯•åç§°: {test.get('test_name', 'N/A')}")
        report.append(f"{'â”€'*80}")
        
        # åŸºæœ¬ç»Ÿè®¡
        rt_stats = test.get('response_time_stats', {})
        report.append(f"\nğŸ“Š å“åº”æ—¶é—´ç»Ÿè®¡:")
        report.append(f"  å¹³å‡å€¼: {rt_stats.get('mean', 0):.2f}ç§’")
        report.append(f"  ä¸­ä½æ•°: {rt_stats.get('median', 0):.2f}ç§’")
        report.append(f"  æœ€å¤§å€¼: {rt_stats.get('max', 0):.2f}ç§’")
        report.append(f"  æ ‡å‡†å·®: {rt_stats.get('std_dev', 0):.2f}ç§’")
        report.append(f"  æ–¹å·®: {rt_stats.get('variance', 0):.2f}")
        report.append(f"  å˜å¼‚ç³»æ•°(CV): {rt_stats.get('cv', 0) * 100:.2f}%")
        report.append(f"  ç¨³å®šæ€§åˆ†æ•°: {rt_stats.get('stability_score', 0) * 100:.2f}%")
        
        # ç™¾åˆ†ä½æ•°
        percentiles = test.get('percentiles', {})
        report.append(f"\nğŸ“ˆ ç™¾åˆ†ä½æ•°åˆ†æ:")
        report.append(f"  P50: {percentiles.get('p50', 0):.2f}ç§’")
        report.append(f"  P90: {percentiles.get('p90', 0):.2f}ç§’")
        report.append(f"  P95: {percentiles.get('p95', 0):.2f}ç§’")
        report.append(f"  P99: {percentiles.get('p99', 0):.2f}ç§’")
        
        # å“åº”æ—¶é—´åˆ†å¸ƒåˆ†æ
        raw_data = test.get('raw_data', {})
        response_times = raw_data.get('response_times', [])
        if response_times:
            dist_analysis = analyze_response_time_distribution(response_times)
            report.append(f"\nğŸ“Š åˆ†å¸ƒåˆ†æ:")
            report.append(f"  å¼‚å¸¸å€¼æ•°é‡: {dist_analysis.get('outlier_count', 0)}ä¸ª ({dist_analysis.get('outlier_percentage', 0):.2f}%)")
            report.append(f"  æ—¶é—´é—´éš”åˆ†æ:")
            report.append(f"    æœ€å°é—´éš”: {dist_analysis.get('interval_analysis', {}).get('min_interval', 0):.3f}ç§’")
            report.append(f"    æœ€å¤§é—´éš”: {dist_analysis.get('interval_analysis', {}).get('max_interval', 0):.3f}ç§’")
            report.append(f"    å¹³å‡é—´éš”: {dist_analysis.get('interval_analysis', {}).get('mean_interval', 0):.3f}ç§’")
            report.append(f"    é—´éš”æ ‡å‡†å·®: {dist_analysis.get('interval_analysis', {}).get('std_interval', 0):.3f}ç§’")
            
            # æ‰¹æ¬¡æ¨¡å¼
            batch_patterns = dist_analysis.get('batch_patterns', {})
            if batch_patterns:
                report.append(f"\nğŸ“¦ å“åº”æ—¶é—´é›†ä¸­æ¨¡å¼:")
                for rt, count in list(batch_patterns.items())[:5]:
                    report.append(f"    {rt:.1f}ç§’: {count}æ¬¡")
        
        # æœ€å¤§å“åº”æ—¶é—´åŸå› åˆ†æ
        max_analysis = analyze_max_response_time_causes(test)
        report.append(f"\nğŸ” æœ€å¤§å“åº”æ—¶é—´æ ¹æœ¬åŸå› åˆ†æ:")
        report.append(f"  æœ€å¤§å“åº”æ—¶é—´: {max_analysis.get('max_response_time', 0):.2f}ç§’")
        report.append(f"  å‡ºç°æ¬¡æ•°: {max_analysis.get('occurrences', 0)}æ¬¡")
        
        # ä¸Šä¸‹æ–‡åˆ†æ
        context = max_analysis.get('context_analysis', [])
        if context:
            report.append(f"\n  ä¸Šä¸‹æ–‡åˆ†æ:")
            for ctx in context[:3]:
                report.append(f"    ä½ç½®#{ctx['position']}: {ctx['max_rt']:.2f}ç§’")
                report.append(f"      å‰5ä¸ªå¹³å‡: {ctx['before_mean']:.2f}ç§’")
                report.append(f"      å5ä¸ªå¹³å‡: {ctx['after_mean']:.2f}ç§’")
        
        # å¯èƒ½çš„åŸå› 
        causes = max_analysis.get('possible_causes', [])
        if causes:
            report.append(f"\n  å¯èƒ½çš„æ ¹æœ¬åŸå› :")
            for cause in causes:
                report.append(f"    â€¢ {cause.get('cause', 'N/A')}")
                report.append(f"      è¯æ®: {cause.get('evidence', 'N/A')}")
                report.append(f"      ç½®ä¿¡åº¦: {cause.get('confidence', 'N/A')}")
        
        # é”™è¯¯åˆ†æ
        error_analysis = test.get('error_analysis', {})
        if error_analysis.get('error_types', {}):
            report.append(f"\nâŒ é”™è¯¯åˆ†æ:")
            for error_type, count in error_analysis.get('error_types', {}).items():
                report.append(f"  {error_type}: {count}æ¬¡")
    
    report.append("\n" + "="*80)
    report.append("ç»¼åˆä¼˜åŒ–å»ºè®®")
    report.append("="*80)
    
    # ç”Ÿæˆä¼˜åŒ–å»ºè®®
    recommendations = []
    
    # æ ¹æ®æ–¹å·®åˆ†æå»ºè®®
    overall_variance = results.get('summary', {}).get('overall_variance', 0)
    if overall_variance > 5:
        recommendations.append({
            'priority': 'é«˜',
            'title': 'é™ä½æ‰¹å¤„ç†é‡ç»„é¢‘ç‡',
            'description': 'å½“å‰æ–¹å·®è¾ƒé«˜({:.2f})ï¼Œè¡¨æ˜æ‰¹å¤„ç†é‡ç»„è¿‡äºé¢‘ç¹ã€‚å»ºè®®è°ƒæ•´BATCH_REGROUP_THRESHOLDå‚æ•°ã€‚'.format(overall_variance),
            'action': 'å‡å°BATCH_REGROUP_THRESHOLDå€¼ï¼Œæˆ–å¢åŠ MIN_EFFICIENT_BATCH_SIZE'
        })
    
    # æ ¹æ®æœ€å¤§å“åº”æ—¶é—´åˆ†æå»ºè®®
    overall_max_rt = results.get('summary', {}).get('overall_max_rt', 0)
    if overall_max_rt > 10:
        recommendations.append({
            'priority': 'é«˜',
            'title': 'ä¼˜åŒ–èµ„æºåˆ†é…',
            'description': 'æœ€å¤§å“åº”æ—¶é—´è¿‡é•¿({:.2f}ç§’)ï¼Œå¯èƒ½å­˜åœ¨èµ„æºç«äº‰æˆ–åºåˆ—IDæ± ç“¶é¢ˆã€‚'.format(overall_max_rt),
            'action': 'æ£€æŸ¥n_seq_maxé…ç½®ï¼Œè€ƒè™‘å¢åŠ åºåˆ—IDæ± å¤§å°ï¼›ä¼˜åŒ–çº¿ç¨‹æ± é…ç½®'
        })
    
    # æ ¹æ®ç¨³å®šæ€§åˆ†æ•°åˆ†æå»ºè®®
    overall_stability = results.get('summary', {}).get('overall_stability_score', 0)
    if overall_stability < 0.85:
        recommendations.append({
            'priority': 'ä¸­',
            'title': 'æé«˜ç³»ç»Ÿç¨³å®šæ€§',
            'description': 'ç¨³å®šæ€§åˆ†æ•°è¾ƒä½({:.2f}%)ï¼Œéœ€è¦ä¼˜åŒ–è¯·æ±‚è°ƒåº¦å’Œæ‰¹å¤„ç†ç­–ç•¥ã€‚'.format(overall_stability * 100),
            'action': 'å®ç°è‡ªé€‚åº”æ‰¹å¤„ç†å¤§å°ï¼›ä¼˜åŒ–è¯·æ±‚é˜Ÿåˆ—ç®¡ç†ï¼›æ·»åŠ è¯·æ±‚ä¼˜å…ˆçº§æœºåˆ¶'
        })
    
    # é€šç”¨å»ºè®®
    recommendations.append({
        'priority': 'ä¸­',
        'title': 'æ·»åŠ é¢„çƒ­æœºåˆ¶',
        'description': 'å‰å‡ ä¸ªè¯·æ±‚å“åº”æ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®æ·»åŠ ç³»ç»Ÿé¢„çƒ­æœºåˆ¶ã€‚',
        'action': 'åœ¨æœåŠ¡å™¨å¯åŠ¨æ—¶å‘é€å‡ ä¸ªé¢„çƒ­è¯·æ±‚ï¼›ç¼“å­˜åˆå§‹è®¡ç®—ç»“æœ'
    })
    
    recommendations.append({
        'priority': 'ä½',
        'title': 'å®ç°è¯·æ±‚ä¼˜å…ˆçº§',
        'description': 'é¿å…é•¿è¯·æ±‚é˜»å¡çŸ­è¯·æ±‚ï¼Œæé«˜æ•´ä½“å“åº”æ—¶é—´ä¸€è‡´æ€§ã€‚',
        'action': 'æ·»åŠ è¯·æ±‚ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼›å®ç°åŠ¨æ€è¶…æ—¶è°ƒæ•´'
    })
    
    for rec in recommendations:
        report.append(f"\n{'ğŸ”´' if rec['priority'] == 'é«˜' else 'ğŸŸ¡' if rec['priority'] == 'ä¸­' else 'ğŸŸ¢'} {rec['title']} ({rec['priority']}ä¼˜å…ˆçº§)")
        report.append(f"  æè¿°: {rec['description']}")
        report.append(f"  è¡ŒåŠ¨: {rec['action']}")
    
    report.append("\n" + "="*80)
    
    return "\n".join(report)


def main():
    """ä¸»å‡½æ•°"""
    result_file = "/tmp/stability_test_comprehensive_20260121_223609.json"
    
    try:
        results = load_results(result_file)
        
        report = generate_analysis_report(results)
        
        print(report)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = f"/tmp/stability_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\n\næŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
    except FileNotFoundError:
        print(f"é”™è¯¯: æœªæ‰¾åˆ°æ–‡ä»¶ {result_file}")
    except Exception as e:
        print(f"é”™è¯¯: {str(e)}")


if __name__ == "__main__":
    main()
