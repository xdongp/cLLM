/**
 * @file direct_benchmark.cpp
 * @brief ç›´æ¥æ€§èƒ½æµ‹è¯•ç¨‹åº - å‚è€ƒ llama-bench å®ç°
 * 
 * ç›®æ ‡ï¼šç»•è¿‡ Schedulerã€BatchManager ç­‰ä¸­é—´å±‚ï¼Œç›´æ¥æµ‹è¯•åº•å±‚ API æ€§èƒ½
 * å‚è€ƒï¼šllama-bench çš„ç®€å•ç›´æ¥å®ç°æ–¹å¼
 */

#include <iostream>
#include <vector>
#include <chrono>
#include <thread>
#include <cstdlib>
#include <cstring>
#include <atomic>
#include <mutex>
#include <condition_variable>
#include <queue>
#include <algorithm>

#include "cllm/common/config.h"
#include "cllm/inference/llama_cpp_backend.h"
#include "cllm/model/config.h"
#include "cllm/common/logger.h"
#include <numeric>

using namespace cllm;
using namespace cllm::inference;

// å·¥å…·å‡½æ•°
static uint64_t get_time_ns() {
    using clock = std::chrono::high_resolution_clock;
    return std::chrono::nanoseconds(clock::now().time_since_epoch()).count();
}

static double get_time_sec() {
    using clock = std::chrono::high_resolution_clock;
    return std::chrono::duration<double>(clock::now().time_since_epoch()).count();
}

// æµ‹è¯•å‚æ•°
struct BenchParams {
    std::string model_path;
    int n_prompt = 32;      // prompt tokens
    int n_gen = 50;         // generation tokens per request
    int n_requests = 40;    // total requests
    int n_concurrent = 5;   // concurrent requests
    int n_reps = 1;         // repetitions
    int n_batch = 512;      // llama.cpp batch size
    int n_ubatch = 512;     // llama.cpp ubatch size
    int n_seq_max = 64;     // max sequences
    int n_gpu_layers = 99;  // GPU layers
    bool verbose = false;
};

// å‚è€ƒ llama-bench çš„ test_gen å‡½æ•°
static bool test_gen_direct(
    LlamaCppBackend& backend,
    std::mutex& backendMutex,  // ğŸ”¥ ä¿æŠ¤ llama_decode è°ƒç”¨
    int n_gen,
    size_t requestId,
    int32_t /*seqId*/,  // æœªä½¿ç”¨ï¼ŒforwardBatch ä¼šè‡ªåŠ¨åˆ†é…
    std::vector<int>& promptTokens,
    std::vector<int>& generatedTokens
) {
    generatedTokens.clear();
    generatedTokens.reserve(n_gen);
    
    // ğŸ”¥ ä¸è¦é¢„å…ˆåˆ†é…åºåˆ—IDï¼Œè®© forwardBatch åœ¨é¦–æ¬¡è°ƒç”¨æ—¶è‡ªåŠ¨åˆ†é…
    // è¿™æ ·å¯ä»¥ç¡®ä¿æ–°è¯·æ±‚ä»ä½ç½®0å¼€å§‹ï¼Œå·²å­˜åœ¨çš„è¯·æ±‚ä»æ­£ç¡®çš„ä½ç½®ç»§ç»­
    
    // åˆå§‹åŒ–ï¼šå¦‚æœæœ‰ promptï¼Œå…ˆå¤„ç† prompt
    if (!promptTokens.empty()) {
        std::vector<int> flatInputIds = promptTokens;
        std::vector<std::pair<size_t, size_t>> requestPositions = {{0, promptTokens.size()}};
        std::vector<size_t> sequenceIds = {requestId};
        
        Tensor logits;
        {
            std::lock_guard<std::mutex> lock(backendMutex);
            logits = backend.forwardBatch(flatInputIds, requestPositions, 1, sequenceIds);
        }
        
        // é‡‡æ ·ç¬¬ä¸€ä¸ª tokenï¼ˆç®€å•éšæœºé‡‡æ ·ï¼‰
        const float* logitsPtr = logits.data() + (promptTokens.size() - 1) * logits.shape()[1];
        size_t vocabSize = logits.shape()[1];
        int nextToken = std::rand() % vocabSize;
        generatedTokens.push_back(nextToken);
    } else {
        // å¦‚æœæ²¡æœ‰ promptï¼Œç”Ÿæˆä¸€ä¸ªåˆå§‹ token
        int32_t bosToken = 151643;  // Qwen3 BOS token
        std::vector<int> flatInputIds = {bosToken};
        std::vector<std::pair<size_t, size_t>> requestPositions = {{0, 1}};
        std::vector<size_t> sequenceIds = {requestId};
        
        Tensor logits;
        {
            std::lock_guard<std::mutex> lock(backendMutex);
            logits = backend.forwardBatch(flatInputIds, requestPositions, 1, sequenceIds);
        }
        
        const float* logitsPtr = logits.data();
        size_t vocabSize = logits.shape()[1];
        int nextToken = std::rand() % vocabSize;
        generatedTokens.push_back(nextToken);
    }
    
    // ç”Ÿæˆ tokens
    for (int i = generatedTokens.size(); i < n_gen; ++i) {
        // ğŸ”¥ å‚è€ƒ llama-bench: åªé€æœ€åä¸€ä¸ª tokenï¼ˆå¢é‡ç”Ÿæˆï¼‰
        std::vector<int> flatInputIds = {generatedTokens.back()};
        std::vector<std::pair<size_t, size_t>> requestPositions = {{0, 1}};
        std::vector<size_t> sequenceIds = {requestId};
        
        Tensor logits;
        {
            std::lock_guard<std::mutex> lock(backendMutex);
            logits = backend.forwardBatch(flatInputIds, requestPositions, 1, sequenceIds);
        }
        
        // é‡‡æ ·ä¸‹ä¸€ä¸ª tokenï¼ˆç®€å•éšæœºé‡‡æ ·ï¼‰
        const float* logitsPtr = logits.data();
        size_t vocabSize = logits.shape()[1];
        int nextToken = std::rand() % vocabSize;
        generatedTokens.push_back(nextToken);
    }
    
    // ğŸ”¥ é‡Šæ”¾åºåˆ—ID
    try {
        backend.releaseSequenceId(requestId);
    } catch (...) {
        // å¿½ç•¥é‡Šæ”¾é”™è¯¯
    }
    
    return true;
}

// å¹¶å‘æµ‹è¯•ï¼šå‚è€ƒ llama-bench çš„ç®€å•ç›´æ¥æ–¹å¼
static void run_concurrent_test(const BenchParams& params) {
    std::cout << "=== Direct Benchmark Test (å‚è€ƒ llama-bench) ===" << std::endl;
    std::cout << "Model: " << params.model_path << std::endl;
    std::cout << "Requests: " << params.n_requests << std::endl;
    std::cout << "Concurrent: " << params.n_concurrent << std::endl;
    std::cout << "Prompt tokens: " << params.n_prompt << std::endl;
    std::cout << "Gen tokens per request: " << params.n_gen << std::endl;
    std::cout << std::endl;
    
    // ğŸ”¥ åˆå§‹åŒ– Configï¼ˆLlamaCppBackend éœ€è¦ä» Config è¯»å– n_seq_max å’Œ n_ubatchï¼‰
    try {
        Config::instance().load("config/config.yaml");
    } catch (...) {
        // å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        std::cerr << "Warning: Failed to load config file, using defaults" << std::endl;
    }
    
    // åˆ›å»ºåç«¯é…ç½®
    ModelConfig modelConfig;
    modelConfig.vocabSize = 151936;
    modelConfig.maxSequenceLength = 2048;
    modelConfig.llamaBatchSize = params.n_batch;
    modelConfig.llamaGpuLayers = params.n_gpu_layers;
    
    LlamaCppBackend backend(modelConfig, params.model_path);
    
    if (!backend.initialize()) {
        std::cerr << "Failed to initialize backend" << std::endl;
        return;
    }
    
    std::cout << "Backend initialized successfully" << std::endl;
    
    // å‡†å¤‡ prompt tokensï¼ˆéšæœºï¼‰
    std::vector<int> promptTokens;
    promptTokens.reserve(params.n_prompt);
    for (int i = 0; i < params.n_prompt; ++i) {
        promptTokens.push_back(std::rand() % 1000);  // ç®€å•éšæœº tokens
    }
    
    // å¹¶å‘è¯·æ±‚é˜Ÿåˆ—
    std::queue<size_t> requestQueue;
    std::mutex queueMutex;
    std::condition_variable queueCondition;
    std::atomic<size_t> completedRequests{0};
    std::atomic<size_t> totalTokens{0};
    std::vector<double> requestTimes;
    std::mutex timesMutex;
    
    // ğŸ”¥ llama_decode ä¸æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œéœ€è¦äº’æ–¥é”ä¿æŠ¤
    std::mutex backendMutex;
    
    // åˆå§‹åŒ–è¯·æ±‚é˜Ÿåˆ—
    for (size_t i = 0; i < params.n_requests; ++i) {
        requestQueue.push(i);
    }
    
    // å·¥ä½œçº¿ç¨‹å‡½æ•°
    auto worker = [&](int workerId) {
        while (true) {
            size_t requestId;
            {
                std::unique_lock<std::mutex> lock(queueMutex);
                if (requestQueue.empty()) {
                    break;
                }
                requestId = requestQueue.front();
                requestQueue.pop();
            }
            
            // æµ‹è¯•å•ä¸ªè¯·æ±‚
            double startTime = get_time_sec();
            std::vector<int> generatedTokens;
            
            bool success = test_gen_direct(
                backend,
                backendMutex,
                params.n_gen,
                requestId,
                0,  // seqId ä¼šåœ¨ forwardBatch ä¸­è‡ªåŠ¨åˆ†é…
                promptTokens,
                generatedTokens
            );
            
            double endTime = get_time_sec();
            double elapsed = endTime - startTime;
            
            if (success && !generatedTokens.empty()) {
                size_t tokens = generatedTokens.size();
                totalTokens += tokens;
                completedRequests++;
                
                {
                    std::lock_guard<std::mutex> lock(timesMutex);
                    requestTimes.push_back(elapsed);
                }
                
                if (params.verbose) {
                    std::cout << "Worker " << workerId << ": Request " << requestId 
                              << " completed in " << elapsed << "s, generated " << tokens << " tokens" << std::endl;
                }
            } else {
                std::cerr << "Worker " << workerId << ": Request " << requestId << " failed!" << std::endl;
                // test_gen_direct å·²ç»åœ¨å‡½æ•°å†…éƒ¨é‡Šæ”¾äº†åºåˆ—IDï¼ˆæ— è®ºæˆåŠŸæˆ–å¤±è´¥ï¼‰
            }
        }
    };
    
    // å¯åŠ¨å¹¶å‘æµ‹è¯•
    double testStart = get_time_sec();
    
    std::vector<std::thread> workers;
    for (int i = 0; i < params.n_concurrent; ++i) {
        workers.emplace_back(worker, i);
    }
    
    // ç­‰å¾…æ‰€æœ‰å·¥ä½œçº¿ç¨‹å®Œæˆ
    for (auto& w : workers) {
        w.join();
    }
    
    double testEnd = get_time_sec();
    double totalTime = testEnd - testStart;
    
    // è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    size_t successful = completedRequests.load();
    size_t totalGenTokens = totalTokens.load();
    
    std::sort(requestTimes.begin(), requestTimes.end());
    double avgTime = requestTimes.empty() ? 0.0 : 
                     std::accumulate(requestTimes.begin(), requestTimes.end(), 0.0) / requestTimes.size();
    double p50Time = requestTimes.empty() ? 0.0 : requestTimes[requestTimes.size() / 2];
    double p99Time = requestTimes.empty() ? 0.0 : requestTimes[static_cast<size_t>(requestTimes.size() * 0.99)];
    double minTime = requestTimes.empty() ? 0.0 : requestTimes.front();
    double maxTime = requestTimes.empty() ? 0.0 : requestTimes.back();
    
    double throughput = totalTime > 0 ? (totalGenTokens / totalTime) : 0.0;
    
    // è¾“å‡ºç»“æœ
    std::cout << std::endl;
    std::cout << "=== Test Results ===" << std::endl;
    std::cout << "Total time: " << totalTime << "s" << std::endl;
    std::cout << "Successful requests: " << successful << "/" << params.n_requests << std::endl;
    std::cout << "Total generated tokens: " << totalGenTokens << std::endl;
    std::cout << "Average throughput: " << throughput << " tokens/sec" << std::endl;
    std::cout << std::endl;
    std::cout << "Response time stats:" << std::endl;
    std::cout << "  Min: " << minTime << "s" << std::endl;
    std::cout << "  Max: " << maxTime << "s" << std::endl;
    std::cout << "  Avg: " << avgTime << "s" << std::endl;
    std::cout << "  P50: " << p50Time << "s" << std::endl;
    std::cout << "  P99: " << p99Time << "s" << std::endl;
    std::cout << std::endl;
    
    // ç›®æ ‡æ£€æŸ¥
    double target = 80.0;
    if (throughput >= target) {
        std::cout << "âœ… å·²è¾¾åˆ°ç¬¬ä¸€é˜¶æ®µç›®æ ‡: " << throughput << " >= " << target << " tokens/sec" << std::endl;
    } else {
        std::cout << "âŒ æœªè¾¾åˆ°ç¬¬ä¸€é˜¶æ®µç›®æ ‡: " << throughput << " < " << target << " tokens/sec" << std::endl;
        std::cout << "   å·®è·: " << (target - throughput) << " tokens/sec (" 
                  << ((target - throughput) / target * 100) << "%)" << std::endl;
    }
}

// è§£æå‘½ä»¤è¡Œå‚æ•°
static BenchParams parse_args(int argc, char** argv) {
    BenchParams params;
    
    // é»˜è®¤å€¼
    params.model_path = "/Users/dannypan/PycharmProjects/xllm/cpp/cLLM/model/Qwen/qwen3-0.6b-q8_0.gguf";
    
    for (int i = 1; i < argc; i++) {
        if (strcmp(argv[i], "--model") == 0 && i + 1 < argc) {
            params.model_path = argv[++i];
        } else if (strcmp(argv[i], "--n-prompt") == 0 && i + 1 < argc) {
            params.n_prompt = std::stoi(argv[++i]);
        } else if (strcmp(argv[i], "--n-gen") == 0 && i + 1 < argc) {
            params.n_gen = std::stoi(argv[++i]);
        } else if (strcmp(argv[i], "--n-requests") == 0 && i + 1 < argc) {
            params.n_requests = std::stoi(argv[++i]);
        } else if (strcmp(argv[i], "--n-concurrent") == 0 && i + 1 < argc) {
            params.n_concurrent = std::stoi(argv[++i]);
        } else if (strcmp(argv[i], "--n-batch") == 0 && i + 1 < argc) {
            params.n_batch = std::stoi(argv[++i]);
        } else if (strcmp(argv[i], "--n-ubatch") == 0 && i + 1 < argc) {
            params.n_ubatch = std::stoi(argv[++i]);
        } else if (strcmp(argv[i], "--n-seq-max") == 0 && i + 1 < argc) {
            params.n_seq_max = std::stoi(argv[++i]);
        } else if (strcmp(argv[i], "--n-gpu-layers") == 0 && i + 1 < argc) {
            params.n_gpu_layers = std::stoi(argv[++i]);
        } else if (strcmp(argv[i], "--verbose") == 0 || strcmp(argv[i], "-v") == 0) {
            params.verbose = true;
        } else if (strcmp(argv[i], "--help") == 0 || strcmp(argv[i], "-h") == 0) {
            std::cout << "Usage: " << argv[0] << " [options]" << std::endl;
            std::cout << "Options:" << std::endl;
            std::cout << "  --model <path>           Model path (default: " << params.model_path << ")" << std::endl;
            std::cout << "  --n-prompt <n>           Prompt tokens (default: " << params.n_prompt << ")" << std::endl;
            std::cout << "  --n-gen <n>              Generation tokens per request (default: " << params.n_gen << ")" << std::endl;
            std::cout << "  --n-requests <n>         Total requests (default: " << params.n_requests << ")" << std::endl;
            std::cout << "  --n-concurrent <n>       Concurrent requests (default: " << params.n_concurrent << ")" << std::endl;
            std::cout << "  --n-batch <n>            Batch size (default: " << params.n_batch << ")" << std::endl;
            std::cout << "  --n-ubatch <n>           Ubatch size (default: " << params.n_ubatch << ")" << std::endl;
            std::cout << "  --n-seq-max <n>          Max sequences (default: " << params.n_seq_max << ")" << std::endl;
            std::cout << "  --n-gpu-layers <n>       GPU layers (default: " << params.n_gpu_layers << ")" << std::endl;
            std::cout << "  --verbose, -v            Verbose output" << std::endl;
            std::cout << "  --help, -h               Show this help" << std::endl;
            exit(0);
        }
    }
    
    return params;
}

int main(int argc, char** argv) {
    BenchParams params = parse_args(argc, argv);
    
    // è¿è¡Œæµ‹è¯•
    run_concurrent_test(params);
    
    return 0;
}
