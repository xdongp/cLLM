#!/usr/bin/env python3
"""
CLLMç³»ç»Ÿç¨³å®šæ€§æµ‹è¯•æ¡†æ¶
ç”¨äºæµ‹é‡å’Œåˆ†æè¯·æ±‚å“åº”æ—¶é—´çš„ç¨³å®šæ€§å’Œä¸€è‡´æ€§
"""

import time
import json
import statistics
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests
import threading


class StabilityTestFramework:
    def __init__(self, base_url: str = "http://localhost:8080"):
        self.base_url = base_url
        self.results = []
        self.lock = threading.Lock()
        
    def send_request(self, prompt: str, max_tokens: int = 50, timeout: int = 300) -> Dict:
        """å‘é€å•ä¸ªè¯·æ±‚å¹¶è®°å½•å“åº”æ—¶é—´"""
        url = f"{self.base_url}/generate"
        headers = {"Content-Type": "application/json"}
        data = {
            "prompt": prompt,
            "max_tokens": max_tokens,
            "temperature": 0.7
        }
        
        start_time = time.time()
        try:
            response = requests.post(url, headers=headers, json=data, timeout=timeout)
            response_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                return {
                    "success": True,
                    "response_time": response_time,
                    "status_code": response.status_code,
                    "tokens_generated": len(result.get("text", "").split()),
                    "timestamp": datetime.now().isoformat()
                }
            else:
                return {
                    "success": False,
                    "response_time": response_time,
                    "status_code": response.status_code,
                    "error": f"HTTP {response.status_code}",
                    "timestamp": datetime.now().isoformat()
                }
        except requests.exceptions.RequestException as e:
            response_time = time.time() - start_time
            return {
                "success": False,
                "response_time": response_time,
                "status_code": 0,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    def run_concurrent_test(self, num_requests: int, concurrency: int, 
                           max_tokens: int = 50, prompt: str = "Hello") -> List[Dict]:
        """è¿è¡Œå¹¶å‘æµ‹è¯•"""
        print(f"\n{'='*60}")
        print(f"è¿è¡Œå¹¶å‘æµ‹è¯•: {num_requests}ä¸ªè¯·æ±‚, å¹¶å‘åº¦{concurrency}")
        print(f"{'='*60}")
        
        responses = []
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=concurrency) as executor:
            futures = [executor.submit(self.send_request, prompt, max_tokens) 
                      for _ in range(num_requests)]
            
            for i, future in enumerate(as_completed(futures)):
                try:
                    result = future.result()
                    responses.append(result)
                    if (i + 1) % 10 == 0 or (i + 1) == num_requests:
                        print(f"  å·²å®Œæˆ: {i + 1}/{num_requests} è¯·æ±‚")
                except Exception as e:
                    responses.append({
                        "success": False,
                        "response_time": 0,
                        "status_code": 0,
                        "error": f"Future exception: {str(e)}",
                        "timestamp": datetime.now().isoformat()
                    })
        
        total_time = time.time() - start_time
        print(f"æµ‹è¯•å®Œæˆ, æ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        return responses
    
    def analyze_results(self, responses: List[Dict], test_name: str) -> Dict:
        """åˆ†ææµ‹è¯•ç»“æœ,è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡"""
        if not responses:
            return {}
        
        success_responses = [r for r in responses if r["success"]]
        failed_responses = [r for r in responses if not r["success"]]
        
        response_times = [r["response_time"] for r in success_responses]
        
        if not response_times:
            return {
                "test_name": test_name,
                "total_requests": len(responses),
                "success_count": 0,
                "failed_count": len(responses),
                "success_rate": 0,
                "error": "æ‰€æœ‰è¯·æ±‚å¤±è´¥",
                "response_time_stats": {
                    "mean": 0,
                    "median": 0,
                    "min": 0,
                    "max": 0,
                    "std_dev": 0,
                    "variance": 0,
                    "cv": 0,
                    "stability_score": 0
                },
                "percentiles": {
                    "p50": 0,
                    "p90": 0,
                    "p95": 0,
                    "p99": 0
                },
                "throughput": {
                    "requests_per_second": 0,
                    "tokens_per_second": 0
                },
                "max_response_time_analysis": {
                    "max_rt": 0,
                    "count": 0,
                    "samples": []
                },
                "error_analysis": {
                    "error_types": {},
                    "failed_responses": failed_responses[:5]
                },
                "raw_data": {
                    "response_times": [],
                    "timestamps": []
                }
            }
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        mean_rt = statistics.mean(response_times)
        median_rt = statistics.median(response_times)
        min_rt = min(response_times)
        max_rt = max(response_times)
        std_dev = statistics.stdev(response_times) if len(response_times) > 1 else 0
        variance = statistics.variance(response_times) if len(response_times) > 1 else 0
        
        # è®¡ç®—ç™¾åˆ†ä½æ•°ï¼ˆçº¯Pythonå®ç°ï¼‰
        sorted_times = sorted(response_times)
        n = len(sorted_times)
        
        def percentile(p):
            idx = (p / 100) * (n - 1)
            if idx.is_integer():
                return sorted_times[int(idx)]
            else:
                lower = sorted_times[int(idx)]
                upper = sorted_times[int(idx) + 1]
                return lower + (upper - lower) * (idx - int(idx))
        
        p50 = percentile(50)
        p90 = percentile(90)
        p95 = percentile(95)
        p99 = percentile(99)
        
        # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡
        cv = std_dev / mean_rt  # å˜å¼‚ç³»æ•°
        stability_score = 1 / (1 + cv)  # ç¨³å®šæ€§åˆ†æ•°(0-1)
        
        # åˆ†ææœ€å¤§å“åº”æ—¶é—´çš„åŸå› 
        max_rt_indices = [i for i, rt in enumerate(response_times) if rt == max_rt]
        max_rt_samples = [success_responses[i] for i in max_rt_indices[:3]]
        
        # åˆ†æå¤±è´¥è¯·æ±‚
        error_types = {}
        for r in failed_responses:
            error_msg = r.get("error", "Unknown")
            error_types[error_msg] = error_types.get(error_msg, 0) + 1
        
        analysis = {
            "test_name": test_name,
            "total_requests": len(responses),
            "success_count": len(success_responses),
            "failed_count": len(failed_responses),
            "success_rate": len(success_responses) / len(responses),
            
            # å“åº”æ—¶é—´ç»Ÿè®¡
            "response_time_stats": {
                "mean": mean_rt,
                "median": median_rt,
                "min": min_rt,
                "max": max_rt,
                "std_dev": std_dev,
                "variance": variance,
                "cv": cv,
                "stability_score": stability_score
            },
            
            # ç™¾åˆ†ä½æ•°
            "percentiles": {
                "p50": p50,
                "p90": p90,
                "p95": p95,
                "p99": p99
            },
            
            # ååé‡
            "throughput": {
                "requests_per_second": len(success_responses) / sum(response_times) if sum(response_times) > 0 else 0,
                "tokens_per_second": sum(r.get("tokens_generated", 0) for r in success_responses) / sum(response_times) if sum(response_times) > 0 else 0
            },
            
            # æœ€å¤§å“åº”æ—¶é—´åˆ†æ
            "max_response_time_analysis": {
                "max_rt": max_rt,
                "count": len(max_rt_indices),
                "samples": max_rt_samples
            },
            
            # é”™è¯¯åˆ†æ
            "error_analysis": {
                "error_types": error_types,
                "failed_responses": failed_responses[:5]  # åªä¿ç•™å‰5ä¸ªå¤±è´¥ç¤ºä¾‹
            },
            
            "raw_data": {
                "response_times": response_times,
                "timestamps": [r["timestamp"] for r in success_responses]
            }
        }
        
        return analysis
    
    def print_analysis_summary(self, analysis: Dict):
        """æ‰“å°åˆ†ææ‘˜è¦"""
        print(f"\n{'='*60}")
        print(f"æµ‹è¯•ç»“æœæ‘˜è¦: {analysis['test_name']}")
        print(f"{'='*60}")
        
        print(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
        print(f"  æ€»è¯·æ±‚æ•°: {analysis['total_requests']}")
        print(f"  æˆåŠŸæ•°: {analysis['success_count']}")
        print(f"  å¤±è´¥æ•°: {analysis['failed_count']}")
        print(f"  æˆåŠŸç‡: {analysis['success_rate']*100:.2f}%")
        
        print(f"\nâ±ï¸  å“åº”æ—¶é—´ç»Ÿè®¡ (ç§’):")
        rt_stats = analysis['response_time_stats']
        print(f"  å¹³å‡å€¼: {rt_stats['mean']:.2f}")
        print(f"  ä¸­ä½æ•°: {rt_stats['median']:.2f}")
        print(f"  æœ€å°å€¼: {rt_stats['min']:.2f}")
        print(f"  æœ€å¤§å€¼: {rt_stats['max']:.2f}")
        print(f"  æ ‡å‡†å·®: {rt_stats['std_dev']:.2f}")
        print(f"  æ–¹å·®: {rt_stats['variance']:.2f}")
        print(f"  å˜å¼‚ç³»æ•°(CV): {rt_stats['cv']*100:.2f}%")
        print(f"  ç¨³å®šæ€§åˆ†æ•°: {rt_stats['stability_score']*100:.2f}%")
        
        print(f"\nğŸ“ˆ ç™¾åˆ†ä½æ•° (ç§’):")
        percentiles = analysis['percentiles']
        print(f"  P50: {percentiles['p50']:.2f}")
        print(f"  P90: {percentiles['p90']:.2f}")
        print(f"  P95: {percentiles['p95']:.2f}")
        print(f"  P99: {percentiles['p99']:.2f}")
        
        print(f"\nâš¡ ååé‡:")
        throughput = analysis['throughput']
        print(f"  è¯·æ±‚/ç§’: {throughput['requests_per_second']:.2f}")
        print(f"  Token/ç§’: {throughput['tokens_per_second']:.2f}")
        
        if analysis['failed_count'] > 0:
            print(f"\nâŒ é”™è¯¯åˆ†æ:")
            for error_type, count in analysis['error_analysis']['error_types'].items():
                print(f"  {error_type}: {count}æ¬¡")
        
        print(f"\n{'='*60}")
    
    def save_results(self, analysis: Dict, output_file: str):
        """ä¿å­˜åˆ†æç»“æœåˆ°JSONæ–‡ä»¶"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    def compare_benchmarks(self, baseline: Dict, optimized: Dict) -> Dict:
        """æ¯”è¾ƒåŸºå‡†æµ‹è¯•å’Œä¼˜åŒ–åçš„æµ‹è¯•ç»“æœ"""
        baseline_rt = baseline['response_time_stats']
        optimized_rt = optimized['response_time_stats']
        
        improvements = {
            'variance_improvement': ((baseline_rt['variance'] - optimized_rt['variance']) / baseline_rt['variance']) * 100 if baseline_rt['variance'] > 0 else 0,
            'max_rt_improvement': ((baseline_rt['max'] - optimized_rt['max']) / baseline_rt['max']) * 100 if baseline_rt['max'] > 0 else 0,
            'stability_improvement': ((optimized_rt['stability_score'] - baseline_rt['stability_score']) / baseline_rt['stability_score']) * 100 if baseline_rt['stability_score'] > 0 else 0,
            'cv_improvement': ((baseline_rt['cv'] - optimized_rt['cv']) / baseline_rt['cv']) * 100 if baseline_rt['cv'] > 0 else 0,
        }
        
        return {
            'baseline': baseline_rt,
            'optimized': optimized_rt,
            'improvements': improvements,
            'target_achieved': improvements['stability_improvement'] >= 20
        }


def main():
    """ä¸»å‡½æ•°: è¿è¡Œç¨³å®šæ€§æµ‹è¯•"""
    framework = StabilityTestFramework()
    
    # æµ‹è¯•é…ç½®
    test_configs = [
        {"name": "ä½å¹¶å‘ç¨³å®šæ€§æµ‹è¯•", "requests": 100, "concurrency": 8, "max_tokens": 50},
        {"name": "ä¸­å¹¶å‘ç¨³å®šæ€§æµ‹è¯•", "requests": 150, "concurrency": 16, "max_tokens": 50},
        {"name": "é«˜å¹¶å‘ç¨³å®šæ€§æµ‹è¯•", "requests": 200, "concurrency": 24, "max_tokens": 50},
    ]
    
    prompt = "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²å’Œæœªæ¥è¶‹åŠ¿"
    
    all_analyses = []
    
    for config in test_configs:
        print(f"\n{'#'*60}")
        print(f"å¼€å§‹: {config['name']}")
        print(f"{'#'*60}")
        
        responses = framework.run_concurrent_test(
            num_requests=config['requests'],
            concurrency=config['concurrency'],
            max_tokens=config['max_tokens'],
            prompt=prompt
        )
        
        analysis = framework.analyze_results(responses, config['name'])
        framework.print_analysis_summary(analysis)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"/tmp/stability_test_{config['name']}_{timestamp}.json"
        framework.save_results(analysis, output_file)
        
        all_analyses.append(analysis)
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    comprehensive_report = {
        "test_time": datetime.now().isoformat(),
        "tests": all_analyses,
        "summary": {
            "total_tests": len(all_analyses),
            "overall_stability_score": statistics.mean([a['response_time_stats']['stability_score'] for a in all_analyses]),
            "overall_variance": statistics.mean([a['response_time_stats']['variance'] for a in all_analyses]),
            "overall_max_rt": max([a['response_time_stats']['max'] for a in all_analyses])
        }
    }
    
    report_file = f"/tmp/stability_test_comprehensive_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(comprehensive_report, f, indent=2, ensure_ascii=False)
    
    print(f"\n{'='*60}")
    print(f"ç»¼åˆæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    print(f"{'='*60}")
    
    print(f"\nğŸ“Š ç»¼åˆç»Ÿè®¡:")
    print(f"  å¹³å‡ç¨³å®šæ€§åˆ†æ•°: {comprehensive_report['summary']['overall_stability_score']*100:.2f}%")
    print(f"  å¹³å‡æ–¹å·®: {comprehensive_report['summary']['overall_variance']:.2f}")
    print(f"  æœ€å¤§å“åº”æ—¶é—´: {comprehensive_report['summary']['overall_max_rt']:.2f}ç§’")


if __name__ == "__main__":
    main()
