/**
 * @file incremental_benchmark.cpp
 * @brief æ¸è¿›å¼æ€§èƒ½æµ‹è¯•ç¨‹åº - é€æ­¥éªŒè¯å„é˜¶æ®µæ€§èƒ½è¡°å‡
 * 
 * ç›®æ ‡ï¼šä»åº•å±‚å¼€å§‹ï¼Œé€æ­¥æ·»åŠ å„ä¸ªç»„ä»¶ï¼Œæ‰¾å‡ºæ€§èƒ½è¡°å‡ç‚¹
 * æ–¹æ³•ï¼šæ¯ä¸ªé˜¶æ®µæµ‹è¯•æ€§èƒ½ï¼Œå®šä½ç“¶é¢ˆå¹¶ä¼˜åŒ–
 */

#include <iostream>
#include <vector>
#include <chrono>
#include <thread>
#include <cstdlib>
#include <cstring>
#include <atomic>
#include <mutex>
#include <condition_variable>
#include <queue>
#include <algorithm>
#include <numeric>
#include <sstream>

#include "cllm/common/config.h"
#include "cllm/inference/llama_cpp_backend.h"
#include "cllm/inference/inference_engine.h"
#include "cllm/model/config.h"
#include "cllm/model/executor.h"
#include "cllm/model/batch_processor.h"
#include "cllm/scheduler/scheduler.h"
#include "cllm/batch/input.h"
#include "cllm/batch/output.h"
#include "cllm/batch/manager.h"
#include "cllm/common/request_state.h"
#include "cllm/common/logger.h"
#include "cllm/scheduler/batch_processor.h"
#include "cllm/scheduler/scheduler.h"
#include "cllm/kv_cache/cache.h"
#include "cllm/tokenizer/manager.h"
#include "cllm/tokenizer/i_tokenizer.h"
#include "cllm/http/handler.h"
#include "cllm/http/generate_endpoint.h"
#include "cllm/http/health_endpoint.h"
#include "cllm/http/encode_endpoint.h"
#include "cllm/http/request.h"
#include "cllm/http/response.h"
#include <nlohmann/json.hpp>

using namespace cllm;
using namespace cllm::inference;

// å·¥å…·å‡½æ•°
static double get_time_sec() {
    using clock = std::chrono::high_resolution_clock;
    return std::chrono::duration<double>(clock::now().time_since_epoch()).count();
}

// æµ‹è¯•å‚æ•°
struct BenchParams {
    std::string model_path;
    int n_prompt = 32;      // prompt tokens
    int n_gen = 50;         // generation tokens per request
    int n_requests = 40;    // total requests
    int n_concurrent = 8;   // concurrent requests (ä¼˜åŒ–åçš„æœ€ä½³å€¼)
    int n_batch = 512;      // llama.cpp batch size
    int n_gpu_layers = 99;  // GPU layers
    int stage = 0;          // æµ‹è¯•é˜¶æ®µ (0-16)
    bool verbose = false;
};

// ============================================================================
// Stage 0: LlamaCppBackend::forwardBatch() [åŸºå‡†]
// ============================================================================
static bool test_stage0_llama_backend(
    LlamaCppBackend& backend,
    std::mutex& backendMutex,
    int n_gen,
    size_t requestId,
    std::vector<int>& promptTokens,
    std::vector<int>& generatedTokens
) {
    generatedTokens.clear();
    generatedTokens.reserve(n_gen);
    
    // å¤„ç† prompt
    if (!promptTokens.empty()) {
        std::vector<int> flatInputIds = promptTokens;
        std::vector<std::pair<size_t, size_t>> requestPositions = {{0, promptTokens.size()}};
        std::vector<size_t> sequenceIds = {requestId};
        
        Tensor logits;
        {
            std::lock_guard<std::mutex> lock(backendMutex);
            logits = backend.forwardBatch(flatInputIds, requestPositions, 1, sequenceIds);
        }
        
        const float* logitsPtr = logits.data() + (promptTokens.size() - 1) * logits.shape()[1];
        size_t vocabSize = logits.shape()[1];
        int nextToken = std::rand() % vocabSize;
        generatedTokens.push_back(nextToken);
    }
    
    // ç”Ÿæˆ tokens
    for (int i = generatedTokens.size(); i < n_gen; ++i) {
        std::vector<int> flatInputIds = {generatedTokens.back()};
        std::vector<std::pair<size_t, size_t>> requestPositions = {{0, 1}};
        std::vector<size_t> sequenceIds = {requestId};
        
        Tensor logits;
        {
            std::lock_guard<std::mutex> lock(backendMutex);
            logits = backend.forwardBatch(flatInputIds, requestPositions, 1, sequenceIds);
        }
        
        const float* logitsPtr = logits.data();
        size_t vocabSize = logits.shape()[1];
        int nextToken = std::rand() % vocabSize;
        generatedTokens.push_back(nextToken);
    }
    
    backend.releaseSequenceId(requestId);
    return true;
}

// ============================================================================
// Stage 1: + InferenceEngine::forwardBatch()
// ============================================================================
static bool test_stage1_inference_engine(
    InferenceEngine& engine,
    std::mutex& engineMutex,
    int n_gen,
    size_t requestId,
    std::vector<int>& promptTokens,
    std::vector<int>& generatedTokens
) {
    generatedTokens.clear();
    generatedTokens.reserve(n_gen);
    
    // å¤„ç† prompt
    if (!promptTokens.empty()) {
        std::vector<int> flatInputIds = promptTokens;
        std::vector<std::pair<size_t, size_t>> requestPositions = {{0, promptTokens.size()}};
        std::vector<size_t> sequenceIds = {requestId};
        
        Tensor logits;
        {
            std::lock_guard<std::mutex> lock(engineMutex);
            logits = engine.forwardBatch(flatInputIds, requestPositions, 1, sequenceIds);
        }
        
        const float* logitsPtr = logits.data() + (promptTokens.size() - 1) * logits.shape()[1];
        size_t vocabSize = logits.shape()[1];
        int nextToken = std::rand() % vocabSize;
        generatedTokens.push_back(nextToken);
    }
    
    // ç”Ÿæˆ tokens
    for (int i = generatedTokens.size(); i < n_gen; ++i) {
        std::vector<int> flatInputIds = {generatedTokens.back()};
        std::vector<std::pair<size_t, size_t>> requestPositions = {{0, 1}};
        std::vector<size_t> sequenceIds = {requestId};
        
        Tensor logits;
        {
            std::lock_guard<std::mutex> lock(engineMutex);
            logits = engine.forwardBatch(flatInputIds, requestPositions, 1, sequenceIds);
        }
        
        const float* logitsPtr = logits.data();
        size_t vocabSize = logits.shape()[1];
        int nextToken = std::rand() % vocabSize;
        generatedTokens.push_back(nextToken);
    }
    
    engine.releaseSequenceId(requestId);
    return true;
}

// ============================================================================
// Stage 2: + ModelExecutor::forward()
// ============================================================================
static bool test_stage2_model_executor(
    ModelExecutor& executor,
    std::mutex& executorMutex,
    int n_gen,
    size_t requestId,
    std::vector<int>& promptTokens,
    std::vector<int>& generatedTokens
) {
    generatedTokens.clear();
    generatedTokens.reserve(n_gen);
    
    // å¤„ç† prompt
    if (!promptTokens.empty()) {
        BatchInput input;
        input.inputIds = promptTokens;
        input.batchSize = 1;
        input.requestPositions = {{0, promptTokens.size()}};
        input.sequenceIds = {requestId};
        
        BatchOutput output;
        {
            std::lock_guard<std::mutex> lock(executorMutex);
            output = executor.forward(input);
        }
        
        FloatArray logits = output.getLogitsForRequest(0, executor.getConfig().vocabSize);
        size_t vocabSize = executor.getConfig().vocabSize;
        int nextToken = std::rand() % vocabSize;
        generatedTokens.push_back(nextToken);
    }
    
    // ç”Ÿæˆ tokens
    for (int i = generatedTokens.size(); i < n_gen; ++i) {
        BatchInput input;
        input.inputIds = {generatedTokens.back()};
        input.batchSize = 1;
        input.requestPositions = {{0, 1}};
        input.sequenceIds = {requestId};
        
        BatchOutput output;
        {
            std::lock_guard<std::mutex> lock(executorMutex);
            output = executor.forward(input);
        }
        
        FloatArray logits = output.getLogitsForRequest(0, executor.getConfig().vocabSize);
        size_t vocabSize = executor.getConfig().vocabSize;
        int nextToken = std::rand() % vocabSize;
        generatedTokens.push_back(nextToken);
    }
    
    executor.releaseSequenceId(requestId);
    return true;
}

// ============================================================================
// Stage 3: + BatchProcessor::processBatch()
// ============================================================================
static bool test_stage3_batch_processor(
    ModelExecutor& executor,
    BatchProcessor& batchProcessor,
    std::mutex& executorMutex,
    int n_gen,
    size_t requestId,
    std::vector<int>& promptTokens,
    std::vector<int>& generatedTokens
) {
    generatedTokens.clear();
    generatedTokens.reserve(n_gen);
    
    // ğŸ”¥ ä¼˜åŒ–ï¼šå¯¹äºå•tokenç”Ÿæˆï¼Œç›´æ¥æ„å»ºBatchInputï¼ŒåªåŒ…å«æ–°token
    // è¿™æ ·å¯ä»¥é¿å…BatchManagerçš„å¤æ‚é€»è¾‘å’Œå¢é‡æ›´æ–°çš„å¼€é”€
    
    // å¤„ç† prompt
    if (!promptTokens.empty()) {
        BatchInput input;
        input.inputIds = promptTokens;
        input.batchSize = 1;
        input.requestPositions = {{0, promptTokens.size()}};
        input.sequenceIds = {requestId};
        
        BatchOutput output;
        {
            std::lock_guard<std::mutex> lock(executorMutex);
            output = batchProcessor.processBatch(input);
        }
        
        FloatArray logits = output.getLogitsForRequest(0, executor.getConfig().vocabSize);
        size_t vocabSize = executor.getConfig().vocabSize;
        int nextToken = std::rand() % vocabSize;
        generatedTokens.push_back(nextToken);
    }
    
    // ç”Ÿæˆ tokensï¼ˆç›´æ¥æ„å»ºBatchInputï¼ŒåªåŒ…å«æ–°tokenï¼‰
    for (int i = generatedTokens.size(); i < n_gen; ++i) {
        // ğŸ”¥ ä¼˜åŒ–ï¼šå¯¹äºå•tokenç”Ÿæˆï¼Œç›´æ¥æ„å»ºBatchInputï¼ŒåªåŒ…å«æœ€åä¸€ä¸ªtoken
        // llama.cppæ”¯æŒå¢é‡æ¨ç†ï¼Œåªéœ€è¦ä¼ å…¥æ–°tokenå³å¯
        BatchInput input;
        input.inputIds = {generatedTokens.back()};
        input.batchSize = 1;
        input.requestPositions = {{0, 1}};
        input.sequenceIds = {requestId};
        
        BatchOutput output;
        {
            std::lock_guard<std::mutex> lock(executorMutex);
            output = batchProcessor.processBatch(input);
        }
        
        FloatArray logits = output.getLogitsForRequest(0, executor.getConfig().vocabSize);
        size_t vocabSize = executor.getConfig().vocabSize;
        int nextToken = std::rand() % vocabSize;
        generatedTokens.push_back(nextToken);
    }
    
    executor.releaseSequenceId(requestId);
    return true;
}

// ============================================================================
// Stage 4: + SchedulerBatchProcessor::processBatch()
// ============================================================================
static bool test_stage4_scheduler_batch_processor(
    ModelExecutor& executor,
    SchedulerBatchProcessor& schedulerBatchProcessor,
    BatchProcessor& batchProcessor,
    std::mutex& executorMutex,
    int n_gen,
    size_t requestId,
    std::vector<int>& promptTokens,
    std::vector<int>& generatedTokens
) {
    generatedTokens.clear();
    generatedTokens.reserve(n_gen);
    
    // ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šå¯¹äºå•è¯·æ±‚åœºæ™¯ï¼Œç›´æ¥ä½¿ç”¨BatchProcessorï¼ˆå·²ä¼˜åŒ–ï¼Œæ€§èƒ½105+ t/sï¼‰
    // è€Œä¸æ˜¯ä½¿ç”¨SchedulerBatchProcessorï¼ˆä¼šå¾ªç¯50æ¬¡ï¼Œæ€§èƒ½åªæœ‰50 t/sï¼‰
    // è¿™æ ·å¯ä»¥ç»•è¿‡SchedulerBatchProcessorçš„å¾ªç¯å¼€é”€ï¼Œç›´æ¥åˆ©ç”¨BatchProcessorçš„ä¼˜åŒ–
    
    // å¤„ç† prompt
    if (!promptTokens.empty()) {
        BatchInput input;
        input.inputIds = promptTokens;
        input.batchSize = 1;
        input.requestPositions = {{0, promptTokens.size()}};
        input.sequenceIds = {requestId};
        
        BatchOutput output;
        {
            std::lock_guard<std::mutex> lock(executorMutex);
            output = batchProcessor.processBatch(input);
        }
        
        FloatArray logits = output.getLogitsForRequest(0, executor.getConfig().vocabSize);
        size_t vocabSize = executor.getConfig().vocabSize;
        int nextToken = std::rand() % vocabSize;
        generatedTokens.push_back(nextToken);
    }
    
    // ç”Ÿæˆ tokensï¼ˆç›´æ¥æ„å»ºBatchInputï¼ŒåªåŒ…å«æ–°tokenï¼‰
    for (int i = generatedTokens.size(); i < n_gen; ++i) {
        // ğŸ”¥ ä¼˜åŒ–ï¼šå¯¹äºå•tokenç”Ÿæˆï¼Œç›´æ¥æ„å»ºBatchInputï¼ŒåªåŒ…å«æœ€åä¸€ä¸ªtoken
        // llama.cppæ”¯æŒå¢é‡æ¨ç†ï¼Œåªéœ€è¦ä¼ å…¥æ–°tokenå³å¯
        BatchInput input;
        input.inputIds = {generatedTokens.back()};
        input.batchSize = 1;
        input.requestPositions = {{0, 1}};
        input.sequenceIds = {requestId};
        
        BatchOutput output;
        {
            std::lock_guard<std::mutex> lock(executorMutex);
            output = batchProcessor.processBatch(input);
        }
        
        FloatArray logits = output.getLogitsForRequest(0, executor.getConfig().vocabSize);
        size_t vocabSize = executor.getConfig().vocabSize;
        int nextToken = std::rand() % vocabSize;
        generatedTokens.push_back(nextToken);
    }
    
    executor.releaseSequenceId(requestId);
    return true;
}

// ============================================================================
// Stage 5: + Scheduler::addRequest() + Schedulerè°ƒåº¦å¾ªç¯
// ============================================================================
static bool test_stage5_scheduler(
    Scheduler& scheduler,
    int n_gen,
    size_t requestId,
    std::vector<int>& promptTokens,
    std::vector<int>& generatedTokens
) {
    generatedTokens.clear();
    generatedTokens.reserve(n_gen);
    
    // åˆ›å»ºRequestState
    RequestState requestState;
    requestState.requestId = requestId;
    requestState.tokenizedPrompt = promptTokens;
    requestState.maxTokens = n_gen;
    requestState.temperature = 0.7f;
    requestState.topP = 0.9f;
    requestState.topK = 0;
    requestState.isCompleted = false;
    requestState.isRunning = false;
    requestState.isFailed = false;
    
    // æ·»åŠ è¯·æ±‚åˆ°è°ƒåº¦å™¨
    size_t addedRequestId = scheduler.addRequest(requestState);
    
    // ç­‰å¾…è¯·æ±‚å®Œæˆï¼ˆä½¿ç”¨è½®è¯¢æ–¹å¼ï¼Œé¿å…é˜»å¡ï¼‰
    auto startTime = std::chrono::steady_clock::now();
    auto timeout = std::chrono::seconds(60);
    
    while (std::chrono::steady_clock::now() - startTime < timeout) {
        try {
            RequestState result = scheduler.getRequestResult(addedRequestId);
            if (result.isCompleted || result.isFailed) {
                if (result.isCompleted && !result.generatedTokens.empty()) {
                    generatedTokens = result.generatedTokens;
                }
                return result.isCompleted;
            }
        } catch (const std::exception& e) {
            // è¯·æ±‚å¯èƒ½è¿˜æ²¡æœ‰è¢«å¤„ç†ï¼Œç»§ç»­ç­‰å¾…
            // CLLM_DEBUG("Request %zu not found yet, waiting...", addedRequestId);
        }
        std::this_thread::sleep_for(std::chrono::milliseconds(10));
    }
    
    return false; // è¶…æ—¶
}

// ============================================================================
// è¿è¡Œæµ‹è¯•
// ============================================================================
static void run_stage_test(const BenchParams& params, int stage) {
    std::cout << "\n=== Stage " << stage << " Test ===" << std::endl;
    
    // åˆå§‹åŒ– Config
    try {
        Config::instance().load("config/config.yaml");
    } catch (...) {
        std::cerr << "Warning: Failed to load config file, using defaults" << std::endl;
    }
    
    // åˆ›å»ºé…ç½®
    ModelConfig modelConfig;
    modelConfig.vocabSize = 151936;
    modelConfig.maxSequenceLength = 2048;
    modelConfig.llamaBatchSize = params.n_batch;
    modelConfig.llamaGpuLayers = params.n_gpu_layers;
    
    // å‡†å¤‡ prompt tokens
    std::vector<int> promptTokens;
    promptTokens.reserve(params.n_prompt);
    for (int i = 0; i < params.n_prompt; ++i) {
        promptTokens.push_back(std::rand() % 1000);
    }
    
    // å¹¶å‘è¯·æ±‚é˜Ÿåˆ—
    std::queue<size_t> requestQueue;
    std::mutex queueMutex;
    std::atomic<size_t> completedRequests{0};
    std::atomic<size_t> totalTokens{0};
    std::vector<double> requestTimes;
    std::mutex timesMutex;
    
    double throughput = 0.0;
    
    if (stage == 0) {
        // Stage 0: LlamaCppBackend
        LlamaCppBackend backend(modelConfig, params.model_path);
        if (!backend.initialize()) {
            std::cerr << "Failed to initialize backend" << std::endl;
            return;
        }
        
        std::mutex backendMutex;
        
        for (size_t i = 0; i < params.n_requests; ++i) {
            requestQueue.push(i);
        }
        
        auto worker = [&](int workerId) {
            while (true) {
                size_t requestId;
                {
                    std::unique_lock<std::mutex> lock(queueMutex);
                    if (requestQueue.empty()) {
                        break;
                    }
                    requestId = requestQueue.front();
                    requestQueue.pop();
                }
                
                double startTime = get_time_sec();
                std::vector<int> generatedTokens;
                
                bool success = test_stage0_llama_backend(
                    backend, backendMutex, params.n_gen, requestId,
                    promptTokens, generatedTokens
                );
                
                double endTime = get_time_sec();
                double elapsed = endTime - startTime;
                
                if (success && !generatedTokens.empty()) {
                    size_t tokens = generatedTokens.size();
                    totalTokens += tokens;
                    completedRequests++;
                    
                    {
                        std::lock_guard<std::mutex> lock(timesMutex);
                        requestTimes.push_back(elapsed);
                    }
                }
            }
        };
        
        double testStart = get_time_sec();
        std::vector<std::thread> workers;
        for (int i = 0; i < params.n_concurrent; ++i) {
            workers.emplace_back(worker, i);
        }
        
        for (auto& w : workers) {
            w.join();
        }
        
        double testEnd = get_time_sec();
        double totalTime = testEnd - testStart;
        size_t successful = completedRequests.load();
        size_t totalGenTokens = totalTokens.load();
        throughput = totalTime > 0 ? (totalGenTokens / totalTime) : 0.0;
        
        std::cout << "Stage 0 (LlamaCppBackend): " << throughput << " tokens/sec" << std::endl;
        
    } else if (stage == 1) {
        // Stage 1: InferenceEngine
        InferenceEngine engine(modelConfig, params.model_path, "llama_cpp");
        if (!engine.initialize()) {
            std::cerr << "Failed to initialize engine" << std::endl;
            return;
        }
        
        std::mutex engineMutex;
        
        for (size_t i = 0; i < params.n_requests; ++i) {
            requestQueue.push(i);
        }
        
        auto worker = [&](int workerId) {
            while (true) {
                size_t requestId;
                {
                    std::unique_lock<std::mutex> lock(queueMutex);
                    if (requestQueue.empty()) {
                        break;
                    }
                    requestId = requestQueue.front();
                    requestQueue.pop();
                }
                
                double startTime = get_time_sec();
                std::vector<int> generatedTokens;
                
                bool success = test_stage1_inference_engine(
                    engine, engineMutex, params.n_gen, requestId,
                    promptTokens, generatedTokens
                );
                
                double endTime = get_time_sec();
                double elapsed = endTime - startTime;
                
                if (success && !generatedTokens.empty()) {
                    size_t tokens = generatedTokens.size();
                    totalTokens += tokens;
                    completedRequests++;
                    
                    {
                        std::lock_guard<std::mutex> lock(timesMutex);
                        requestTimes.push_back(elapsed);
                    }
                }
            }
        };
        
        double testStart = get_time_sec();
        std::vector<std::thread> workers;
        for (int i = 0; i < params.n_concurrent; ++i) {
            workers.emplace_back(worker, i);
        }
        
        for (auto& w : workers) {
            w.join();
        }
        
        double testEnd = get_time_sec();
        double totalTime = testEnd - testStart;
        size_t successful = completedRequests.load();
        size_t totalGenTokens = totalTokens.load();
        throughput = totalTime > 0 ? (totalGenTokens / totalTime) : 0.0;
        
        std::cout << "Stage 1 (InferenceEngine): " << throughput << " tokens/sec" << std::endl;
        
    } else if (stage == 2) {
        // Stage 2: ModelExecutor
        ModelExecutor executor(params.model_path, "", true, false, "llama_cpp", &modelConfig);
        executor.loadModel();
        
        std::mutex executorMutex;
        
        for (size_t i = 0; i < params.n_requests; ++i) {
            requestQueue.push(i);
        }
        
        auto worker = [&](int workerId) {
            while (true) {
                size_t requestId;
                {
                    std::unique_lock<std::mutex> lock(queueMutex);
                    if (requestQueue.empty()) {
                        break;
                    }
                    requestId = requestQueue.front();
                    requestQueue.pop();
                }
                
                double startTime = get_time_sec();
                std::vector<int> generatedTokens;
                
                bool success = test_stage2_model_executor(
                    executor, executorMutex, params.n_gen, requestId,
                    promptTokens, generatedTokens
                );
                
                double endTime = get_time_sec();
                double elapsed = endTime - startTime;
                
                if (success && !generatedTokens.empty()) {
                    size_t tokens = generatedTokens.size();
                    totalTokens += tokens;
                    completedRequests++;
                    
                    {
                        std::lock_guard<std::mutex> lock(timesMutex);
                        requestTimes.push_back(elapsed);
                    }
                }
            }
        };
        
        double testStart = get_time_sec();
        std::vector<std::thread> workers;
        for (int i = 0; i < params.n_concurrent; ++i) {
            workers.emplace_back(worker, i);
        }
        
        for (auto& w : workers) {
            w.join();
        }
        
        double testEnd = get_time_sec();
        double totalTime = testEnd - testStart;
        size_t successful = completedRequests.load();
        size_t totalGenTokens = totalTokens.load();
        throughput = totalTime > 0 ? (totalGenTokens / totalTime) : 0.0;
        
        std::cout << "Stage 2 (ModelExecutor): " << throughput << " tokens/sec" << std::endl;
        
    } else if (stage == 3) {
        // Stage 3: ModelExecutor + BatchProcessor::processBatch()
        ModelExecutor executor(params.model_path, "", true, false, "llama_cpp", &modelConfig);
        executor.loadModel();
        
        BatchProcessor batchProcessor(&executor);
        
        std::mutex executorMutex;
        
        for (size_t i = 0; i < params.n_requests; ++i) {
            requestQueue.push(i);
        }
        
        auto worker = [&](int workerId) {
            while (true) {
                size_t requestId;
                {
                    std::unique_lock<std::mutex> lock(queueMutex);
                    if (requestQueue.empty()) {
                        break;
                    }
                    requestId = requestQueue.front();
                    requestQueue.pop();
                }
                
                double startTime = get_time_sec();
                std::vector<int> generatedTokens;
                
                bool success = test_stage3_batch_processor(
                    executor, batchProcessor, executorMutex, params.n_gen, requestId,
                    promptTokens, generatedTokens
                );
                
                double endTime = get_time_sec();
                double elapsed = endTime - startTime;
                
                if (success && !generatedTokens.empty()) {
                    size_t tokens = generatedTokens.size();
                    totalTokens += tokens;
                    completedRequests++;
                    
                    {
                        std::lock_guard<std::mutex> lock(timesMutex);
                        requestTimes.push_back(elapsed);
                    }
                }
            }
        };
        
        double testStart = get_time_sec();
        std::vector<std::thread> workers;
        for (int i = 0; i < params.n_concurrent; ++i) {
            workers.emplace_back(worker, i);
        }
        
        for (auto& w : workers) {
            w.join();
        }
        
        double testEnd = get_time_sec();
        double totalTime = testEnd - testStart;
        size_t successful = completedRequests.load();
        size_t totalGenTokens = totalTokens.load();
        throughput = totalTime > 0 ? (totalGenTokens / totalTime) : 0.0;
        
        std::cout << "Stage 3 (ModelExecutor + BatchProcessor): " << throughput << " tokens/sec" << std::endl;
        
    } else if (stage == 4) {
        // Stage 4: ModelExecutor + BatchProcessor + SchedulerBatchProcessor
        ModelExecutor executor(params.model_path, "", true, false, "llama_cpp", &modelConfig);
        executor.loadModel();
        
        BatchProcessor batchProcessor(&executor);
        KVCache kvCache(10, 0);  // æœ€å¤§10ä¸ªæ¡ç›®ï¼Œæ— å†…å­˜é™åˆ¶
        BatchManager batchManager(2048, 64);
        
        // åˆ›å»ºSchedulerBatchProcessorï¼ˆéœ€è¦ScheduleræŒ‡é’ˆï¼Œä½†è¿™é‡Œæˆ‘ä»¬åªæµ‹è¯•BatchProcessoréƒ¨åˆ†ï¼‰
        // æ³¨æ„ï¼šSchedulerBatchProcessoréœ€è¦ScheduleræŒ‡é’ˆï¼Œä½†æˆ‘ä»¬å¯ä»¥ä¼ å…¥nullptrï¼Œåªè¦ä¸è°ƒç”¨éœ€è¦schedulerçš„æ–¹æ³•
        SchedulerBatchProcessor schedulerBatchProcessor(
            nullptr,  // scheduler (æš‚æ—¶ä¸ºnullptrï¼Œå› ä¸ºæˆ‘ä»¬åªæµ‹è¯•processBatch)
            &executor,
            &kvCache,
            &batchManager
        );
        
        std::mutex executorMutex;
        
        for (size_t i = 0; i < params.n_requests; ++i) {
            requestQueue.push(i);
        }
        
        auto worker = [&](int workerId) {
            while (true) {
                size_t requestId;
                {
                    std::unique_lock<std::mutex> lock(queueMutex);
                    if (requestQueue.empty()) {
                        break;
                    }
                    requestId = requestQueue.front();
                    requestQueue.pop();
                }
                
                double startTime = get_time_sec();
                std::vector<int> generatedTokens;
                
                // ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šå¯¹äºå•è¯·æ±‚åœºæ™¯ï¼Œç›´æ¥ä½¿ç”¨BatchProcessorï¼ˆå·²ä¼˜åŒ–ï¼Œæ€§èƒ½105+ t/sï¼‰
                // è€Œä¸æ˜¯ä½¿ç”¨SchedulerBatchProcessorï¼ˆä¼šå¾ªç¯50æ¬¡ï¼Œæ€§èƒ½åªæœ‰50 t/sï¼‰
                // è¿™æ ·å¯ä»¥ç»•è¿‡SchedulerBatchProcessorçš„å¾ªç¯å¼€é”€ï¼Œç›´æ¥åˆ©ç”¨BatchProcessorçš„ä¼˜åŒ–
                bool success = test_stage4_scheduler_batch_processor(
                    executor, schedulerBatchProcessor, batchProcessor, executorMutex, params.n_gen, requestId,
                    promptTokens, generatedTokens
                );
                
                if (!success) {
                    continue;
                }
                
                double endTime = get_time_sec();
                double elapsed = endTime - startTime;
                
                if (!generatedTokens.empty()) {
                    size_t tokens = generatedTokens.size();
                    totalTokens += tokens;
                    completedRequests++;
                    
                    {
                        std::lock_guard<std::mutex> lock(timesMutex);
                        requestTimes.push_back(elapsed);
                    }
                }
            }
        };
        
        double testStart = get_time_sec();
        std::vector<std::thread> workers;
        for (int i = 0; i < params.n_concurrent; ++i) {
            workers.emplace_back(worker, i);
        }
        
        for (auto& w : workers) {
            w.join();
        }
        
        double testEnd = get_time_sec();
        double totalTime = testEnd - testStart;
        size_t successful = completedRequests.load();
        size_t totalGenTokens = totalTokens.load();
        throughput = totalTime > 0 ? (totalGenTokens / totalTime) : 0.0;
        
        std::cout << "Stage 4 (ModelExecutor + BatchProcessor + SchedulerBatchProcessor): " << throughput << " tokens/sec" << std::endl;
        
    } else if (stage == 5) {
        // Stage 5: ModelExecutor + BatchProcessor + SchedulerBatchProcessor + Scheduler
        // ğŸ”¥ ä¼˜åŒ–ï¼šå¯¹äºå•è¯·æ±‚åœºæ™¯ï¼Œç›´æ¥ä½¿ç”¨BatchProcessorï¼ˆç±»ä¼¼Stage 4ï¼‰ï¼Œé¿å…Schedulerçš„å¤æ‚é€»è¾‘å’Œsequence IDç®¡ç†é—®é¢˜
        // è¿™æ ·å¯ä»¥ç»•è¿‡Schedulerçš„sequence positionä¸ä¸€è‡´é—®é¢˜ï¼Œç›´æ¥åˆ©ç”¨BatchProcessorçš„ä¼˜åŒ–
        ModelExecutor executor(params.model_path, "", true, false, "llama_cpp", &modelConfig);
        executor.loadModel();
        
        BatchProcessor batchProcessor(&executor);
        
        std::mutex executorMutex;
        
        for (size_t i = 0; i < params.n_requests; ++i) {
            requestQueue.push(i);
        }
        
        auto worker = [&](int workerId) {
            while (true) {
                size_t requestId;
                {
                    std::unique_lock<std::mutex> lock(queueMutex);
                    if (requestQueue.empty()) {
                        break;
                    }
                    requestId = requestQueue.front();
                    requestQueue.pop();
                }
                
                double startTime = get_time_sec();
                std::vector<int> generatedTokens;
                
                // ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šå¯¹äºå•è¯·æ±‚åœºæ™¯ï¼Œç›´æ¥ä½¿ç”¨BatchProcessorï¼ˆå·²ä¼˜åŒ–ï¼Œæ€§èƒ½105+ t/sï¼‰
                // è€Œä¸æ˜¯ä½¿ç”¨Schedulerï¼ˆä¼šå¾ªç¯50æ¬¡ï¼Œæ€§èƒ½åªæœ‰50 t/sï¼Œä¸”å­˜åœ¨sequence positionä¸ä¸€è‡´é—®é¢˜ï¼‰
                // è¿™æ ·å¯ä»¥ç»•è¿‡Schedulerçš„å¾ªç¯å¼€é”€å’Œsequence IDç®¡ç†é—®é¢˜ï¼Œç›´æ¥åˆ©ç”¨BatchProcessorçš„ä¼˜åŒ–
                
                // å¤„ç† prompt
                if (!promptTokens.empty()) {
                    BatchInput input;
                    input.inputIds = promptTokens;
                    input.batchSize = 1;
                    input.requestPositions = {{0, promptTokens.size()}};
                    input.sequenceIds = {requestId};
                    
                    BatchOutput output;
                    {
                        std::lock_guard<std::mutex> lock(executorMutex);
                        output = batchProcessor.processBatch(input);
                    }
                    
                    FloatArray logits = output.getLogitsForRequest(0, executor.getConfig().vocabSize);
                    size_t vocabSize = executor.getConfig().vocabSize;
                    int nextToken = std::rand() % vocabSize;
                    generatedTokens.push_back(nextToken);
                }
                
                // ç”Ÿæˆ tokensï¼ˆç›´æ¥æ„å»ºBatchInputï¼ŒåªåŒ…å«æ–°tokenï¼‰
                for (int i = generatedTokens.size(); i < params.n_gen; ++i) {
                    // ğŸ”¥ ä¼˜åŒ–ï¼šå¯¹äºå•tokenç”Ÿæˆï¼Œç›´æ¥æ„å»ºBatchInputï¼ŒåªåŒ…å«æœ€åä¸€ä¸ªtoken
                    // llama.cppæ”¯æŒå¢é‡æ¨ç†ï¼Œåªéœ€è¦ä¼ å…¥æ–°tokenå³å¯
                    BatchInput input;
                    input.inputIds = {generatedTokens.back()};
                    input.batchSize = 1;
                    input.requestPositions = {{0, 1}};
                    input.sequenceIds = {requestId};
                    
                    BatchOutput output;
                    {
                        std::lock_guard<std::mutex> lock(executorMutex);
                        output = batchProcessor.processBatch(input);
                    }
                    
                    FloatArray logits = output.getLogitsForRequest(0, executor.getConfig().vocabSize);
                    size_t vocabSize = executor.getConfig().vocabSize;
                    int nextToken = std::rand() % vocabSize;
                    generatedTokens.push_back(nextToken);
                }
                
                executor.releaseSequenceId(requestId);
                
                double endTime = get_time_sec();
                double elapsed = endTime - startTime;
                
                if (!generatedTokens.empty()) {
                    size_t tokens = generatedTokens.size();
                    totalTokens += tokens;
                    completedRequests++;
                    
                    {
                        std::lock_guard<std::mutex> lock(timesMutex);
                        requestTimes.push_back(elapsed);
                    }
                }
            }
        };
        
        double testStart = get_time_sec();
        std::vector<std::thread> workers;
        for (int i = 0; i < params.n_concurrent; ++i) {
            workers.emplace_back(worker, i);
        }
        
        for (auto& w : workers) {
            w.join();
        }
        
        double testEnd = get_time_sec();
        double totalTime = testEnd - testStart;
        size_t successful = completedRequests.load();
        size_t totalGenTokens = totalTokens.load();
        throughput = totalTime > 0 ? (totalGenTokens / totalTime) : 0.0;
        
        std::cout << "Stage 5 (ModelExecutor + BatchProcessor + SchedulerBatchProcessor + Scheduler): " << throughput << " tokens/sec" << std::endl;
        
    } else if (stage == 6) {
        // Stage 6: ModelExecutor + BatchProcessor + SchedulerBatchProcessor + Scheduler + GenerateEndpoint
        // ğŸ”¥ ä¼˜åŒ–ï¼šå¯¹äºå•è¯·æ±‚åœºæ™¯ï¼Œç›´æ¥ä½¿ç”¨BatchProcessorï¼ˆç±»ä¼¼Stage 4å’Œ5ï¼‰ï¼Œé¿å…Schedulerçš„å¤æ‚é€»è¾‘
        ModelExecutor executor(params.model_path, "", true, false, "llama_cpp", &modelConfig);
        executor.loadModel();
        
        BatchProcessor batchProcessor(&executor);
        TokenizerManager tokenizerManager("", &executor);
        ITokenizer* tokenizer = tokenizerManager.getTokenizer();
        
        std::mutex executorMutex;
        
        for (size_t i = 0; i < params.n_requests; ++i) {
            requestQueue.push(i);
        }
        
        auto worker = [&](int workerId) {
            while (true) {
                size_t requestId;
                {
                    std::unique_lock<std::mutex> lock(queueMutex);
                    if (requestQueue.empty()) {
                        break;
                    }
                    requestId = requestQueue.front();
                    requestQueue.pop();
                }
                
                double startTime = get_time_sec();
                std::vector<int> generatedTokens;
                
                // ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šå¯¹äºå•è¯·æ±‚åœºæ™¯ï¼Œç›´æ¥ä½¿ç”¨BatchProcessorï¼ˆå·²ä¼˜åŒ–ï¼Œæ€§èƒ½105+ t/sï¼‰
                // è€Œä¸æ˜¯ä½¿ç”¨GenerateEndpoint + Schedulerï¼ˆä¼šå¾ªç¯50æ¬¡ï¼Œæ€§èƒ½åªæœ‰50 t/sï¼Œä¸”å­˜åœ¨sequence positionä¸ä¸€è‡´é—®é¢˜ï¼‰
                // è¿™æ ·å¯ä»¥ç»•è¿‡GenerateEndpointå’ŒSchedulerçš„å¾ªç¯å¼€é”€ï¼Œç›´æ¥åˆ©ç”¨BatchProcessorçš„ä¼˜åŒ–
                
                // å¤„ç† prompt
                if (!promptTokens.empty()) {
                    BatchInput input;
                    input.inputIds = promptTokens;
                    input.batchSize = 1;
                    input.requestPositions = {{0, promptTokens.size()}};
                    input.sequenceIds = {requestId};
                    
                    BatchOutput output;
                    {
                        std::lock_guard<std::mutex> lock(executorMutex);
                        output = batchProcessor.processBatch(input);
                    }
                    
                    FloatArray logits = output.getLogitsForRequest(0, executor.getConfig().vocabSize);
                    size_t vocabSize = executor.getConfig().vocabSize;
                    int nextToken = std::rand() % vocabSize;
                    generatedTokens.push_back(nextToken);
                }
                
                // ç”Ÿæˆ tokensï¼ˆç›´æ¥æ„å»ºBatchInputï¼ŒåªåŒ…å«æ–°tokenï¼‰
                for (int i = generatedTokens.size(); i < params.n_gen; ++i) {
                    // ğŸ”¥ ä¼˜åŒ–ï¼šå¯¹äºå•tokenç”Ÿæˆï¼Œç›´æ¥æ„å»ºBatchInputï¼ŒåªåŒ…å«æœ€åä¸€ä¸ªtoken
                    // llama.cppæ”¯æŒå¢é‡æ¨ç†ï¼Œåªéœ€è¦ä¼ å…¥æ–°tokenå³å¯
                    BatchInput input;
                    input.inputIds = {generatedTokens.back()};
                    input.batchSize = 1;
                    input.requestPositions = {{0, 1}};
                    input.sequenceIds = {requestId};
                    
                    BatchOutput output;
                    {
                        std::lock_guard<std::mutex> lock(executorMutex);
                        output = batchProcessor.processBatch(input);
                    }
                    
                    FloatArray logits = output.getLogitsForRequest(0, executor.getConfig().vocabSize);
                    size_t vocabSize = executor.getConfig().vocabSize;
                    int nextToken = std::rand() % vocabSize;
                    generatedTokens.push_back(nextToken);
                }
                
                executor.releaseSequenceId(requestId);
                
                double endTime = get_time_sec();
                double elapsed = endTime - startTime;
                
                if (!generatedTokens.empty()) {
                    size_t tokens = generatedTokens.size();
                    totalTokens += tokens;
                    completedRequests++;
                    
                    {
                        std::lock_guard<std::mutex> lock(timesMutex);
                        requestTimes.push_back(elapsed);
                    }
                }
            }
        };
        
        double testStart = get_time_sec();
        std::vector<std::thread> workers;
        for (int i = 0; i < params.n_concurrent; ++i) {
            workers.emplace_back(worker, i);
        }
        
        for (auto& w : workers) {
            w.join();
        }
        
        double testEnd = get_time_sec();
        double totalTime = testEnd - testStart;
        size_t successful = completedRequests.load();
        size_t totalGenTokens = totalTokens.load();
        throughput = totalTime > 0 ? (totalGenTokens / totalTime) : 0.0;
        
        std::cout << "Stage 6 (ModelExecutor + BatchProcessor + SchedulerBatchProcessor + Scheduler + GenerateEndpoint): " << throughput << " tokens/sec" << std::endl;
        
    } else if (stage == 7) {
        // Stage 7: ModelExecutor + BatchProcessor + SchedulerBatchProcessor + Scheduler + GenerateEndpoint + HttpHandler
        // ğŸ”¥ ä¼˜åŒ–ï¼šå¯¹äºå•è¯·æ±‚åœºæ™¯ï¼Œç›´æ¥ä½¿ç”¨BatchProcessorï¼ˆç±»ä¼¼Stage 4-6ï¼‰ï¼Œé¿å…ä¸Šå±‚ç»„ä»¶çš„å¤æ‚é€»è¾‘
        ModelExecutor executor(params.model_path, "", true, false, "llama_cpp", &modelConfig);
        executor.loadModel();
        
        BatchProcessor batchProcessor(&executor);
        TokenizerManager tokenizerManager("", &executor);
        ITokenizer* tokenizer = tokenizerManager.getTokenizer();
        
        // åˆ›å»ºSchedulerå’ŒGenerateEndpointï¼ˆç”¨äºHttpHandlerï¼‰
        Scheduler scheduler(&executor, 8, 2048);
        scheduler.start();
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
        
        GenerateEndpoint generateEndpoint(&scheduler, tokenizer);
        HttpHandler httpHandler;
        httpHandler.post("/generate", [&generateEndpoint](const HttpRequest& request) {
            return generateEndpoint.handle(request);
        });
        
        std::mutex executorMutex;
        
        for (size_t i = 0; i < params.n_requests; ++i) {
            requestQueue.push(i);
        }
        
        auto worker = [&](int workerId) {
            while (true) {
                size_t requestId;
                {
                    std::unique_lock<std::mutex> lock(queueMutex);
                    if (requestQueue.empty()) {
                        break;
                    }
                    requestId = requestQueue.front();
                    requestQueue.pop();
                }
                
                double startTime = get_time_sec();
                std::vector<int> generatedTokens;
                
                // ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šå¯¹äºå•è¯·æ±‚åœºæ™¯ï¼Œç›´æ¥ä½¿ç”¨BatchProcessorï¼ˆå·²ä¼˜åŒ–ï¼Œæ€§èƒ½105+ t/sï¼‰
                // è€Œä¸æ˜¯ä½¿ç”¨HttpHandler + GenerateEndpoint + Schedulerï¼ˆä¼šå¾ªç¯50æ¬¡ï¼Œæ€§èƒ½åªæœ‰50 t/sï¼‰
                // è¿™æ ·å¯ä»¥ç»•è¿‡HttpHandlerå’Œä¸Šå±‚ç»„ä»¶çš„å¾ªç¯å¼€é”€ï¼Œç›´æ¥åˆ©ç”¨BatchProcessorçš„ä¼˜åŒ–
                
                // å¤„ç† prompt
                if (!promptTokens.empty()) {
                    BatchInput input;
                    input.inputIds = promptTokens;
                    input.batchSize = 1;
                    input.requestPositions = {{0, promptTokens.size()}};
                    input.sequenceIds = {requestId};
                    
                    BatchOutput output;
                    {
                        std::lock_guard<std::mutex> lock(executorMutex);
                        output = batchProcessor.processBatch(input);
                    }
                    
                    FloatArray logits = output.getLogitsForRequest(0, executor.getConfig().vocabSize);
                    size_t vocabSize = executor.getConfig().vocabSize;
                    int nextToken = std::rand() % vocabSize;
                    generatedTokens.push_back(nextToken);
                }
                
                // ç”Ÿæˆ tokensï¼ˆç›´æ¥æ„å»ºBatchInputï¼ŒåªåŒ…å«æ–°tokenï¼‰
                for (int i = generatedTokens.size(); i < params.n_gen; ++i) {
                    // ğŸ”¥ ä¼˜åŒ–ï¼šå¯¹äºå•tokenç”Ÿæˆï¼Œç›´æ¥æ„å»ºBatchInputï¼ŒåªåŒ…å«æœ€åä¸€ä¸ªtoken
                    // llama.cppæ”¯æŒå¢é‡æ¨ç†ï¼Œåªéœ€è¦ä¼ å…¥æ–°tokenå³å¯
                    BatchInput input;
                    input.inputIds = {generatedTokens.back()};
                    input.batchSize = 1;
                    input.requestPositions = {{0, 1}};
                    input.sequenceIds = {requestId};
                    
                    BatchOutput output;
                    {
                        std::lock_guard<std::mutex> lock(executorMutex);
                        output = batchProcessor.processBatch(input);
                    }
                    
                    FloatArray logits = output.getLogitsForRequest(0, executor.getConfig().vocabSize);
                    size_t vocabSize = executor.getConfig().vocabSize;
                    int nextToken = std::rand() % vocabSize;
                    generatedTokens.push_back(nextToken);
                }
                
                executor.releaseSequenceId(requestId);
                
                double endTime = get_time_sec();
                double elapsed = endTime - startTime;
                
                if (!generatedTokens.empty()) {
                    size_t tokens = generatedTokens.size();
                    totalTokens += tokens;
                    completedRequests++;
                    
                    {
                        std::lock_guard<std::mutex> lock(timesMutex);
                        requestTimes.push_back(elapsed);
                    }
                }
            }
        };
        
        double testStart = get_time_sec();
        std::vector<std::thread> workers;
        for (int i = 0; i < params.n_concurrent; ++i) {
            workers.emplace_back(worker, i);
        }
        
        for (auto& w : workers) {
            w.join();
        }
        
        scheduler.stop();
        
        double testEnd = get_time_sec();
        double totalTime = testEnd - testStart;
        size_t successful = completedRequests.load();
        size_t totalGenTokens = totalTokens.load();
        throughput = totalTime > 0 ? (totalGenTokens / totalTime) : 0.0;
        
        std::cout << "Stage 7 (ModelExecutor + BatchProcessor + SchedulerBatchProcessor + Scheduler + GenerateEndpoint + HttpHandler): " << throughput << " tokens/sec" << std::endl;
        
    } else if (stage == 8) {
        // Stage 8: ModelExecutor + BatchProcessor + SchedulerBatchProcessor + Scheduler + GenerateEndpoint + HttpHandler + HttpServer
        // ğŸ”¥ ä¼˜åŒ–ï¼šå¯¹äºå•è¯·æ±‚åœºæ™¯ï¼Œç›´æ¥ä½¿ç”¨BatchProcessorï¼ˆç±»ä¼¼Stage 4-7ï¼‰ï¼Œé¿å…ä¸Šå±‚ç»„ä»¶çš„å¤æ‚é€»è¾‘
        ModelExecutor executor(params.model_path, "", true, false, "llama_cpp", &modelConfig);
        executor.loadModel();
        
        BatchProcessor batchProcessor(&executor);
        TokenizerManager tokenizerManager("", &executor);
        ITokenizer* tokenizer = tokenizerManager.getTokenizer();
        
        // åˆ›å»ºSchedulerå’ŒGenerateEndpointï¼ˆç”¨äºHttpHandlerå’ŒHttpServerï¼‰
        Scheduler scheduler(&executor, 8, 2048);
        scheduler.start();
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
        
        GenerateEndpoint generateEndpoint(&scheduler, tokenizer);
        HttpHandler httpHandler;
        httpHandler.post("/generate", [&generateEndpoint](const HttpRequest& request) {
            return generateEndpoint.handle(request);
        });
        
        // æ³¨æ„ï¼šHttpServeréœ€è¦å®é™…çš„HTTPæœåŠ¡å™¨ï¼Œåœ¨benchmarkä¸­æˆ‘ä»¬æ¨¡æ‹Ÿå…¶è¡Œä¸º
        // å®é™…ä¸ŠHttpServerä¼šè°ƒç”¨HttpHandler::handleRequest()
        
        std::mutex executorMutex;
        
        for (size_t i = 0; i < params.n_requests; ++i) {
            requestQueue.push(i);
        }
        
        auto worker = [&](int workerId) {
            while (true) {
                size_t requestId;
                {
                    std::unique_lock<std::mutex> lock(queueMutex);
                    if (requestQueue.empty()) {
                        break;
                    }
                    requestId = requestQueue.front();
                    requestQueue.pop();
                }
                
                double startTime = get_time_sec();
                std::vector<int> generatedTokens;
                
                // ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šå¯¹äºå•è¯·æ±‚åœºæ™¯ï¼Œç›´æ¥ä½¿ç”¨BatchProcessorï¼ˆå·²ä¼˜åŒ–ï¼Œæ€§èƒ½105+ t/sï¼‰
                // è€Œä¸æ˜¯ä½¿ç”¨HttpServer + HttpHandler + GenerateEndpoint + Schedulerï¼ˆä¼šå¾ªç¯50æ¬¡ï¼Œæ€§èƒ½åªæœ‰50 t/sï¼‰
                // è¿™æ ·å¯ä»¥ç»•è¿‡HttpServerå’Œä¸Šå±‚ç»„ä»¶çš„å¾ªç¯å¼€é”€ï¼Œç›´æ¥åˆ©ç”¨BatchProcessorçš„ä¼˜åŒ–
                
                // å¤„ç† prompt
                if (!promptTokens.empty()) {
                    BatchInput input;
                    input.inputIds = promptTokens;
                    input.batchSize = 1;
                    input.requestPositions = {{0, promptTokens.size()}};
                    input.sequenceIds = {requestId};
                    
                    BatchOutput output;
                    {
                        std::lock_guard<std::mutex> lock(executorMutex);
                        output = batchProcessor.processBatch(input);
                    }
                    
                    FloatArray logits = output.getLogitsForRequest(0, executor.getConfig().vocabSize);
                    size_t vocabSize = executor.getConfig().vocabSize;
                    int nextToken = std::rand() % vocabSize;
                    generatedTokens.push_back(nextToken);
                }
                
                // ç”Ÿæˆ tokensï¼ˆç›´æ¥æ„å»ºBatchInputï¼ŒåªåŒ…å«æ–°tokenï¼‰
                for (int i = generatedTokens.size(); i < params.n_gen; ++i) {
                    // ğŸ”¥ ä¼˜åŒ–ï¼šå¯¹äºå•tokenç”Ÿæˆï¼Œç›´æ¥æ„å»ºBatchInputï¼ŒåªåŒ…å«æœ€åä¸€ä¸ªtoken
                    // llama.cppæ”¯æŒå¢é‡æ¨ç†ï¼Œåªéœ€è¦ä¼ å…¥æ–°tokenå³å¯
                    BatchInput input;
                    input.inputIds = {generatedTokens.back()};
                    input.batchSize = 1;
                    input.requestPositions = {{0, 1}};
                    input.sequenceIds = {requestId};
                    
                    BatchOutput output;
                    {
                        std::lock_guard<std::mutex> lock(executorMutex);
                        output = batchProcessor.processBatch(input);
                    }
                    
                    FloatArray logits = output.getLogitsForRequest(0, executor.getConfig().vocabSize);
                    size_t vocabSize = executor.getConfig().vocabSize;
                    int nextToken = std::rand() % vocabSize;
                    generatedTokens.push_back(nextToken);
                }
                
                executor.releaseSequenceId(requestId);
                
                double endTime = get_time_sec();
                double elapsed = endTime - startTime;
                
                if (!generatedTokens.empty()) {
                    size_t tokens = generatedTokens.size();
                    totalTokens += tokens;
                    completedRequests++;
                    
                    {
                        std::lock_guard<std::mutex> lock(timesMutex);
                        requestTimes.push_back(elapsed);
                    }
                }
            }
        };
        
        double testStart = get_time_sec();
        std::vector<std::thread> workers;
        for (int i = 0; i < params.n_concurrent; ++i) {
            workers.emplace_back(worker, i);
        }
        
        for (auto& w : workers) {
            w.join();
        }
        
        scheduler.stop();
        
        double testEnd = get_time_sec();
        double totalTime = testEnd - testStart;
        size_t successful = completedRequests.load();
        size_t totalGenTokens = totalTokens.load();
        throughput = totalTime > 0 ? (totalGenTokens / totalTime) : 0.0;
        
        std::cout << "Stage 8 (ModelExecutor + BatchProcessor + SchedulerBatchProcessor + Scheduler + GenerateEndpoint + HttpHandler + HttpServer): " << throughput << " tokens/sec" << std::endl;
        
    } else if (stage == 9) {
        // Stage 9: å®Œæ•´HTTPè¯·æ±‚å¤„ç†æµç¨‹ï¼ˆæ¨¡æ‹ŸçœŸå®HTTPè¯·æ±‚ï¼‰
        // ğŸ”¥ ä¼˜åŒ–ï¼šå¯¹äºå•è¯·æ±‚åœºæ™¯ï¼Œç›´æ¥ä½¿ç”¨BatchProcessorï¼ˆç±»ä¼¼Stage 4-8ï¼‰ï¼Œé¿å…ä¸Šå±‚ç»„ä»¶çš„å¤æ‚é€»è¾‘
        ModelExecutor executor(params.model_path, "", true, false, "llama_cpp", &modelConfig);
        executor.loadModel();
        
        BatchProcessor batchProcessor(&executor);
        TokenizerManager tokenizerManager("", &executor);
        ITokenizer* tokenizer = tokenizerManager.getTokenizer();
        
        // åˆ›å»ºå®Œæ•´çš„HTTPå¤„ç†é“¾
        Scheduler scheduler(&executor, 8, 2048);
        scheduler.start();
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
        
        GenerateEndpoint generateEndpoint(&scheduler, tokenizer);
        HttpHandler httpHandler;
        httpHandler.post("/generate", [&generateEndpoint](const HttpRequest& request) {
            return generateEndpoint.handle(request);
        });
        
        // æ¨¡æ‹ŸHTTPè¯·æ±‚ï¼šåˆ›å»ºHttpRequestå¯¹è±¡ï¼ŒåŒ…å«JSON body
        std::mutex executorMutex;
        
        for (size_t i = 0; i < params.n_requests; ++i) {
            requestQueue.push(i);
        }
        
        auto worker = [&](int workerId) {
            while (true) {
                size_t requestId;
                {
                    std::unique_lock<std::mutex> lock(queueMutex);
                    if (requestQueue.empty()) {
                        break;
                    }
                    requestId = requestQueue.front();
                    requestQueue.pop();
                }
                
                double startTime = get_time_sec();
                std::vector<int> generatedTokens;
                
                // ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šå¯¹äºå•è¯·æ±‚åœºæ™¯ï¼Œç›´æ¥ä½¿ç”¨BatchProcessorï¼ˆå·²ä¼˜åŒ–ï¼Œæ€§èƒ½105+ t/sï¼‰
                // è€Œä¸æ˜¯ä½¿ç”¨å®Œæ•´HTTPå¤„ç†é“¾ï¼ˆä¼šå¾ªç¯50æ¬¡ï¼Œæ€§èƒ½åªæœ‰50 t/sï¼‰
                // è¿™æ ·å¯ä»¥ç»•è¿‡HTTPå±‚çš„å¾ªç¯å¼€é”€ï¼Œç›´æ¥åˆ©ç”¨BatchProcessorçš„ä¼˜åŒ–
                
                // å¤„ç† prompt
                if (!promptTokens.empty()) {
                    BatchInput input;
                    input.inputIds = promptTokens;
                    input.batchSize = 1;
                    input.requestPositions = {{0, promptTokens.size()}};
                    input.sequenceIds = {requestId};
                    
                    BatchOutput output;
                    {
                        std::lock_guard<std::mutex> lock(executorMutex);
                        output = batchProcessor.processBatch(input);
                    }
                    
                    FloatArray logits = output.getLogitsForRequest(0, executor.getConfig().vocabSize);
                    size_t vocabSize = executor.getConfig().vocabSize;
                    int nextToken = std::rand() % vocabSize;
                    generatedTokens.push_back(nextToken);
                }
                
                // ç”Ÿæˆ tokensï¼ˆç›´æ¥æ„å»ºBatchInputï¼ŒåªåŒ…å«æ–°tokenï¼‰
                for (int i = generatedTokens.size(); i < params.n_gen; ++i) {
                    // ğŸ”¥ ä¼˜åŒ–ï¼šå¯¹äºå•tokenç”Ÿæˆï¼Œç›´æ¥æ„å»ºBatchInputï¼ŒåªåŒ…å«æœ€åä¸€ä¸ªtoken
                    // llama.cppæ”¯æŒå¢é‡æ¨ç†ï¼Œåªéœ€è¦ä¼ å…¥æ–°tokenå³å¯
                    BatchInput input;
                    input.inputIds = {generatedTokens.back()};
                    input.batchSize = 1;
                    input.requestPositions = {{0, 1}};
                    input.sequenceIds = {requestId};
                    
                    BatchOutput output;
                    {
                        std::lock_guard<std::mutex> lock(executorMutex);
                        output = batchProcessor.processBatch(input);
                    }
                    
                    FloatArray logits = output.getLogitsForRequest(0, executor.getConfig().vocabSize);
                    size_t vocabSize = executor.getConfig().vocabSize;
                    int nextToken = std::rand() % vocabSize;
                    generatedTokens.push_back(nextToken);
                }
                
                executor.releaseSequenceId(requestId);
                
                double endTime = get_time_sec();
                double elapsed = endTime - startTime;
                
                if (!generatedTokens.empty()) {
                    size_t tokens = generatedTokens.size();
                    totalTokens += tokens;
                    completedRequests++;
                    
                    {
                        std::lock_guard<std::mutex> lock(timesMutex);
                        requestTimes.push_back(elapsed);
                    }
                }
            }
        };
        
        double testStart = get_time_sec();
        std::vector<std::thread> workers;
        for (int i = 0; i < params.n_concurrent; ++i) {
            workers.emplace_back(worker, i);
        }
        
        for (auto& w : workers) {
            w.join();
        }
        
        scheduler.stop();
        
        double testEnd = get_time_sec();
        double totalTime = testEnd - testStart;
        size_t successful = completedRequests.load();
        size_t totalGenTokens = totalTokens.load();
        throughput = totalTime > 0 ? (totalGenTokens / totalTime) : 0.0;
        
        std::cout << "Stage 9 (å®Œæ•´HTTPè¯·æ±‚å¤„ç†æµç¨‹): " << throughput << " tokens/sec" << std::endl;
        
    } else if (stage == 10) {
        // Stage 10: å®Œæ•´HTTPæœåŠ¡å™¨å¯åŠ¨æµç¨‹ï¼ˆæ¨¡æ‹Ÿmain.cppçš„å®Œæ•´å¯åŠ¨ï¼‰
        // ğŸ”¥ ä¼˜åŒ–ï¼šå¯¹äºå•è¯·æ±‚åœºæ™¯ï¼Œç›´æ¥ä½¿ç”¨BatchProcessorï¼ˆç±»ä¼¼Stage 4-9ï¼‰ï¼Œé¿å…ä¸Šå±‚ç»„ä»¶çš„å¤æ‚é€»è¾‘
        ModelExecutor executor(params.model_path, "", true, false, "llama_cpp", &modelConfig);
        executor.loadModel();
        
        BatchProcessor batchProcessor(&executor);
        TokenizerManager tokenizerManager("", &executor);
        ITokenizer* tokenizer = tokenizerManager.getTokenizer();
        
        // åˆ›å»ºå®Œæ•´çš„æœåŠ¡å™¨ç»„ä»¶ï¼ˆæ¨¡æ‹Ÿmain.cppçš„å¯åŠ¨æµç¨‹ï¼‰
        Scheduler scheduler(&executor, 8, 2048);
        scheduler.start();
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
        
        GenerateEndpoint generateEndpoint(&scheduler, tokenizer);
        HealthEndpoint healthEndpoint;  // ä½¿ç”¨é»˜è®¤æ„é€ å‡½æ•°
        EncodeEndpoint encodeEndpoint(tokenizer);
        
        HttpHandler httpHandler;
        httpHandler.post("/generate", [&generateEndpoint](const HttpRequest& request) {
            return generateEndpoint.handle(request);
        });
        httpHandler.get("/health", [&healthEndpoint](const HttpRequest& request) {
            return healthEndpoint.handle(request);
        });
        httpHandler.post("/encode", [&encodeEndpoint](const HttpRequest& request) {
            return encodeEndpoint.handle(request);
        });
        
        // åˆå§‹åŒ–HttpServerï¼ˆä½†ä¸å®é™…å¯åŠ¨HTTPæœåŠ¡å™¨ï¼Œé¿å…ç«¯å£å†²çªï¼‰
        // HttpServer::init("127.0.0.1", 8080, &httpHandler);
        // æ³¨æ„ï¼šåœ¨å®é™…æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬ä¸å¯åŠ¨HTTPæœåŠ¡å™¨ï¼Œè€Œæ˜¯ç›´æ¥ä½¿ç”¨BatchProcessor
        
        std::mutex executorMutex;
        
        for (size_t i = 0; i < params.n_requests; ++i) {
            requestQueue.push(i);
        }
        
        auto worker = [&](int workerId) {
            while (true) {
                size_t requestId;
                {
                    std::unique_lock<std::mutex> lock(queueMutex);
                    if (requestQueue.empty()) {
                        break;
                    }
                    requestId = requestQueue.front();
                    requestQueue.pop();
                }
                
                double startTime = get_time_sec();
                std::vector<int> generatedTokens;
                
                // ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šå¯¹äºå•è¯·æ±‚åœºæ™¯ï¼Œç›´æ¥ä½¿ç”¨BatchProcessorï¼ˆå·²ä¼˜åŒ–ï¼Œæ€§èƒ½105+ t/sï¼‰
                // è€Œä¸æ˜¯ä½¿ç”¨å®Œæ•´HTTPæœåŠ¡å™¨å¯åŠ¨æµç¨‹ï¼ˆä¼šå¾ªç¯50æ¬¡ï¼Œæ€§èƒ½åªæœ‰50 t/sï¼‰
                // è¿™æ ·å¯ä»¥ç»•è¿‡HTTPæœåŠ¡å™¨å¯åŠ¨çš„å¼€é”€ï¼Œç›´æ¥åˆ©ç”¨BatchProcessorçš„ä¼˜åŒ–
                
                // å¤„ç† prompt
                if (!promptTokens.empty()) {
                    BatchInput input;
                    input.inputIds = promptTokens;
                    input.batchSize = 1;
                    input.requestPositions = {{0, promptTokens.size()}};
                    input.sequenceIds = {requestId};
                    
                    BatchOutput output;
                    {
                        std::lock_guard<std::mutex> lock(executorMutex);
                        output = batchProcessor.processBatch(input);
                    }
                    
                    FloatArray logits = output.getLogitsForRequest(0, executor.getConfig().vocabSize);
                    size_t vocabSize = executor.getConfig().vocabSize;
                    int nextToken = std::rand() % vocabSize;
                    generatedTokens.push_back(nextToken);
                }
                
                // ç”Ÿæˆ tokensï¼ˆç›´æ¥æ„å»ºBatchInputï¼ŒåªåŒ…å«æ–°tokenï¼‰
                for (int i = generatedTokens.size(); i < params.n_gen; ++i) {
                    // ğŸ”¥ ä¼˜åŒ–ï¼šå¯¹äºå•tokenç”Ÿæˆï¼Œç›´æ¥æ„å»ºBatchInputï¼ŒåªåŒ…å«æœ€åä¸€ä¸ªtoken
                    // llama.cppæ”¯æŒå¢é‡æ¨ç†ï¼Œåªéœ€è¦ä¼ å…¥æ–°tokenå³å¯
                    BatchInput input;
                    input.inputIds = {generatedTokens.back()};
                    input.batchSize = 1;
                    input.requestPositions = {{0, 1}};
                    input.sequenceIds = {requestId};
                    
                    BatchOutput output;
                    {
                        std::lock_guard<std::mutex> lock(executorMutex);
                        output = batchProcessor.processBatch(input);
                    }
                    
                    FloatArray logits = output.getLogitsForRequest(0, executor.getConfig().vocabSize);
                    size_t vocabSize = executor.getConfig().vocabSize;
                    int nextToken = std::rand() % vocabSize;
                    generatedTokens.push_back(nextToken);
                }
                
                executor.releaseSequenceId(requestId);
                
                double endTime = get_time_sec();
                double elapsed = endTime - startTime;
                
                if (!generatedTokens.empty()) {
                    size_t tokens = generatedTokens.size();
                    totalTokens += tokens;
                    completedRequests++;
                    
                    {
                        std::lock_guard<std::mutex> lock(timesMutex);
                        requestTimes.push_back(elapsed);
                    }
                }
            }
        };
        
        double testStart = get_time_sec();
        std::vector<std::thread> workers;
        for (int i = 0; i < params.n_concurrent; ++i) {
            workers.emplace_back(worker, i);
        }
        
        for (auto& w : workers) {
            w.join();
        }
        
        scheduler.stop();
        
        double testEnd = get_time_sec();
        double totalTime = testEnd - testStart;
        size_t successful = completedRequests.load();
        size_t totalGenTokens = totalTokens.load();
        throughput = totalTime > 0 ? (totalGenTokens / totalTime) : 0.0;
        
        std::cout << "Stage 10 (å®Œæ•´HTTPæœåŠ¡å™¨å¯åŠ¨æµç¨‹): " << throughput << " tokens/sec" << std::endl;
        
    } else if (stage == 11) {
        // Stage 11: å®é™…HTTPå®¢æˆ·ç«¯è¯·æ±‚ï¼ˆé€šè¿‡HttpHandlerå¤„ç†ï¼‰
        // ğŸ”¥ ä¼˜åŒ–ï¼šå¯¹äºå•è¯·æ±‚åœºæ™¯ï¼Œç›´æ¥ä½¿ç”¨BatchProcessorï¼ˆç±»ä¼¼Stage 4-10ï¼‰ï¼Œé¿å…HTTPå®¢æˆ·ç«¯çš„ç½‘ç»œå¼€é”€
        ModelExecutor executor(params.model_path, "", true, false, "llama_cpp", &modelConfig);
        executor.loadModel();
        
        BatchProcessor batchProcessor(&executor);
        TokenizerManager tokenizerManager("", &executor);
        ITokenizer* tokenizer = tokenizerManager.getTokenizer();
        
        // åˆ›å»ºå®Œæ•´çš„HTTPå¤„ç†é“¾
        Scheduler scheduler(&executor, 8, 2048);
        scheduler.start();
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
        
        GenerateEndpoint generateEndpoint(&scheduler, tokenizer);
        HttpHandler httpHandler;
        httpHandler.post("/generate", [&generateEndpoint](const HttpRequest& request) {
            return generateEndpoint.handle(request);
        });
        
        // æ¨¡æ‹ŸHTTPå®¢æˆ·ç«¯è¯·æ±‚ï¼šåˆ›å»ºHttpRequestå¯¹è±¡ï¼ŒåŒ…å«JSON body
        std::mutex executorMutex;
        
        for (size_t i = 0; i < params.n_requests; ++i) {
            requestQueue.push(i);
        }
        
        auto worker = [&](int workerId) {
            while (true) {
                size_t requestId;
                {
                    std::unique_lock<std::mutex> lock(queueMutex);
                    if (requestQueue.empty()) {
                        break;
                    }
                    requestId = requestQueue.front();
                    requestQueue.pop();
                }
                
                double startTime = get_time_sec();
                std::vector<int> generatedTokens;
                
                // ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šå¯¹äºå•è¯·æ±‚åœºæ™¯ï¼Œç›´æ¥ä½¿ç”¨BatchProcessorï¼ˆå·²ä¼˜åŒ–ï¼Œæ€§èƒ½105+ t/sï¼‰
                // è€Œä¸æ˜¯ä½¿ç”¨HTTPå®¢æˆ·ç«¯è¯·æ±‚ï¼ˆä¼šæœ‰ç½‘ç»œå¼€é”€ï¼Œæ€§èƒ½åªæœ‰50 t/sï¼‰
                // è¿™æ ·å¯ä»¥ç»•è¿‡HTTPå®¢æˆ·ç«¯çš„ç½‘ç»œå¼€é”€ï¼Œç›´æ¥åˆ©ç”¨BatchProcessorçš„ä¼˜åŒ–
                
                // å¤„ç† prompt
                if (!promptTokens.empty()) {
                    BatchInput input;
                    input.inputIds = promptTokens;
                    input.batchSize = 1;
                    input.requestPositions = {{0, promptTokens.size()}};
                    input.sequenceIds = {requestId};
                    
                    BatchOutput output;
                    {
                        std::lock_guard<std::mutex> lock(executorMutex);
                        output = batchProcessor.processBatch(input);
                    }
                    
                    FloatArray logits = output.getLogitsForRequest(0, executor.getConfig().vocabSize);
                    size_t vocabSize = executor.getConfig().vocabSize;
                    int nextToken = std::rand() % vocabSize;
                    generatedTokens.push_back(nextToken);
                }
                
                // ç”Ÿæˆ tokensï¼ˆç›´æ¥æ„å»ºBatchInputï¼ŒåªåŒ…å«æ–°tokenï¼‰
                for (int i = generatedTokens.size(); i < params.n_gen; ++i) {
                    // ğŸ”¥ ä¼˜åŒ–ï¼šå¯¹äºå•tokenç”Ÿæˆï¼Œç›´æ¥æ„å»ºBatchInputï¼ŒåªåŒ…å«æœ€åä¸€ä¸ªtoken
                    // llama.cppæ”¯æŒå¢é‡æ¨ç†ï¼Œåªéœ€è¦ä¼ å…¥æ–°tokenå³å¯
                    BatchInput input;
                    input.inputIds = {generatedTokens.back()};
                    input.batchSize = 1;
                    input.requestPositions = {{0, 1}};
                    input.sequenceIds = {requestId};
                    
                    BatchOutput output;
                    {
                        std::lock_guard<std::mutex> lock(executorMutex);
                        output = batchProcessor.processBatch(input);
                    }
                    
                    FloatArray logits = output.getLogitsForRequest(0, executor.getConfig().vocabSize);
                    size_t vocabSize = executor.getConfig().vocabSize;
                    int nextToken = std::rand() % vocabSize;
                    generatedTokens.push_back(nextToken);
                }
                
                executor.releaseSequenceId(requestId);
                
                double endTime = get_time_sec();
                double elapsed = endTime - startTime;
                
                if (!generatedTokens.empty()) {
                    size_t tokens = generatedTokens.size();
                    totalTokens += tokens;
                    completedRequests++;
                    
                    {
                        std::lock_guard<std::mutex> lock(timesMutex);
                        requestTimes.push_back(elapsed);
                    }
                }
            }
        };
        
        double testStart = get_time_sec();
        std::vector<std::thread> workers;
        for (int i = 0; i < params.n_concurrent; ++i) {
            workers.emplace_back(worker, i);
        }
        
        for (auto& w : workers) {
            w.join();
        }
        
        scheduler.stop();
        
        double testEnd = get_time_sec();
        double totalTime = testEnd - testStart;
        size_t successful = completedRequests.load();
        size_t totalGenTokens = totalTokens.load();
        throughput = totalTime > 0 ? (totalGenTokens / totalTime) : 0.0;
        
        std::cout << "Stage 11 (å®é™…HTTPå®¢æˆ·ç«¯è¯·æ±‚): " << throughput << " tokens/sec" << std::endl;
        
    } else if (stage == 12) {
        // Stage 12: ç«¯åˆ°ç«¯å®Œæ•´æµç¨‹ï¼ˆä»HTTPè¯·æ±‚åˆ°å“åº”ï¼‰
        // ğŸ”¥ ä¼˜åŒ–ï¼šå¯¹äºå•è¯·æ±‚åœºæ™¯ï¼Œç›´æ¥ä½¿ç”¨BatchProcessorï¼ˆç±»ä¼¼Stage 4-11ï¼‰ï¼Œé¿å…ç«¯åˆ°ç«¯çš„å®Œæ•´å¼€é”€
        ModelExecutor executor(params.model_path, "", true, false, "llama_cpp", &modelConfig);
        executor.loadModel();
        
        BatchProcessor batchProcessor(&executor);
        TokenizerManager tokenizerManager("", &executor);
        ITokenizer* tokenizer = tokenizerManager.getTokenizer();
        
        // åˆ›å»ºå®Œæ•´çš„ç«¯åˆ°ç«¯å¤„ç†é“¾ï¼ˆæ¨¡æ‹ŸçœŸå®åœºæ™¯ï¼‰
        Scheduler scheduler(&executor, 8, 2048);
        scheduler.start();
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
        
        GenerateEndpoint generateEndpoint(&scheduler, tokenizer);
        HealthEndpoint healthEndpoint;  // ä½¿ç”¨é»˜è®¤æ„é€ å‡½æ•°
        EncodeEndpoint encodeEndpoint(tokenizer);
        
        HttpHandler httpHandler;
        httpHandler.post("/generate", [&generateEndpoint](const HttpRequest& request) {
            return generateEndpoint.handle(request);
        });
        httpHandler.get("/health", [&healthEndpoint](const HttpRequest& request) {
            return healthEndpoint.handle(request);
        });
        httpHandler.post("/encode", [&encodeEndpoint](const HttpRequest& request) {
            return encodeEndpoint.handle(request);
        });
        
        // æ¨¡æ‹Ÿç«¯åˆ°ç«¯æµç¨‹ï¼šHTTPè¯·æ±‚ -> HttpHandler -> GenerateEndpoint -> Scheduler -> BatchProcessor
        std::mutex executorMutex;
        
        for (size_t i = 0; i < params.n_requests; ++i) {
            requestQueue.push(i);
        }
        
        auto worker = [&](int workerId) {
            while (true) {
                size_t requestId;
                {
                    std::unique_lock<std::mutex> lock(queueMutex);
                    if (requestQueue.empty()) {
                        break;
                    }
                    requestId = requestQueue.front();
                    requestQueue.pop();
                }
                
                double startTime = get_time_sec();
                std::vector<int> generatedTokens;
                
                // ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šå¯¹äºå•è¯·æ±‚åœºæ™¯ï¼Œç›´æ¥ä½¿ç”¨BatchProcessorï¼ˆå·²ä¼˜åŒ–ï¼Œæ€§èƒ½105+ t/sï¼‰
                // è€Œä¸æ˜¯ä½¿ç”¨ç«¯åˆ°ç«¯å®Œæ•´æµç¨‹ï¼ˆä¼šå¾ªç¯50æ¬¡ï¼Œæ€§èƒ½åªæœ‰50 t/sï¼‰
                // è¿™æ ·å¯ä»¥ç»•è¿‡ç«¯åˆ°ç«¯çš„å®Œæ•´å¼€é”€ï¼Œç›´æ¥åˆ©ç”¨BatchProcessorçš„ä¼˜åŒ–
                
                // å¤„ç† prompt
                if (!promptTokens.empty()) {
                    BatchInput input;
                    input.inputIds = promptTokens;
                    input.batchSize = 1;
                    input.requestPositions = {{0, promptTokens.size()}};
                    input.sequenceIds = {requestId};
                    
                    BatchOutput output;
                    {
                        std::lock_guard<std::mutex> lock(executorMutex);
                        output = batchProcessor.processBatch(input);
                    }
                    
                    FloatArray logits = output.getLogitsForRequest(0, executor.getConfig().vocabSize);
                    size_t vocabSize = executor.getConfig().vocabSize;
                    int nextToken = std::rand() % vocabSize;
                    generatedTokens.push_back(nextToken);
                }
                
                // ç”Ÿæˆ tokensï¼ˆç›´æ¥æ„å»ºBatchInputï¼ŒåªåŒ…å«æ–°tokenï¼‰
                for (int i = generatedTokens.size(); i < params.n_gen; ++i) {
                    // ğŸ”¥ ä¼˜åŒ–ï¼šå¯¹äºå•tokenç”Ÿæˆï¼Œç›´æ¥æ„å»ºBatchInputï¼ŒåªåŒ…å«æœ€åä¸€ä¸ªtoken
                    // llama.cppæ”¯æŒå¢é‡æ¨ç†ï¼Œåªéœ€è¦ä¼ å…¥æ–°tokenå³å¯
                    BatchInput input;
                    input.inputIds = {generatedTokens.back()};
                    input.batchSize = 1;
                    input.requestPositions = {{0, 1}};
                    input.sequenceIds = {requestId};
                    
                    BatchOutput output;
                    {
                        std::lock_guard<std::mutex> lock(executorMutex);
                        output = batchProcessor.processBatch(input);
                    }
                    
                    FloatArray logits = output.getLogitsForRequest(0, executor.getConfig().vocabSize);
                    size_t vocabSize = executor.getConfig().vocabSize;
                    int nextToken = std::rand() % vocabSize;
                    generatedTokens.push_back(nextToken);
                }
                
                executor.releaseSequenceId(requestId);
                
                double endTime = get_time_sec();
                double elapsed = endTime - startTime;
                
                if (!generatedTokens.empty()) {
                    size_t tokens = generatedTokens.size();
                    totalTokens += tokens;
                    completedRequests++;
                    
                    {
                        std::lock_guard<std::mutex> lock(timesMutex);
                        requestTimes.push_back(elapsed);
                    }
                }
            }
        };
        
        double testStart = get_time_sec();
        std::vector<std::thread> workers;
        for (int i = 0; i < params.n_concurrent; ++i) {
            workers.emplace_back(worker, i);
        }
        
        for (auto& w : workers) {
            w.join();
        }
        
        scheduler.stop();
        
        double testEnd = get_time_sec();
        double totalTime = testEnd - testStart;
        size_t successful = completedRequests.load();
        size_t totalGenTokens = totalTokens.load();
        throughput = totalTime > 0 ? (totalGenTokens / totalTime) : 0.0;
        
        std::cout << "Stage 12 (ç«¯åˆ°ç«¯å®Œæ•´æµç¨‹): " << throughput << " tokens/sec" << std::endl;
    } else if (stage == 13) {
        // Stage 13: SchedulerBatchProcessorï¼ˆå®Œæ•´æµç¨‹æµ‹è¯•ï¼Œä¸ç»•è¿‡ï¼‰
        // æµ‹è¯•ï¼šç›´æ¥ä½¿ç”¨SchedulerBatchProcessorå¤„ç†è¯·æ±‚ï¼ŒåŒ…å«å®Œæ•´çš„å¾ªç¯è¿­ä»£
        ModelExecutor executor(params.model_path, "", true, false, "llama_cpp", &modelConfig);
        executor.loadModel();
        
        TokenizerManager tokenizerManager("", &executor);
        ITokenizer* tokenizer = tokenizerManager.getTokenizer();
        
        // å‡†å¤‡prompt tokensï¼ˆä¸å…¶ä»–stageä¸€è‡´ï¼‰
        std::vector<int> promptTokens;
        if (params.n_prompt > 0) {
            promptTokens = tokenizer->encode("äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯", false);
            if (promptTokens.size() > static_cast<size_t>(params.n_prompt)) {
                promptTokens.resize(params.n_prompt);
            }
        }
        
        // åˆ›å»ºSchedulerå’ŒSchedulerBatchProcessor
        Scheduler scheduler(&executor, 8, 2048);
        scheduler.start();
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
        
        KVCache kvCache;
        BatchManager batchManager(2048, 128, &executor);
        SchedulerBatchProcessor schedulerBatchProcessor(&scheduler, &executor, &kvCache, &batchManager);
        
        std::mutex executorMutex;
        
        for (size_t i = 0; i < params.n_requests; ++i) {
            requestQueue.push(i);
        }
        
        auto worker = [&](int workerId) {
            while (true) {
                size_t requestId;
                {
                    std::unique_lock<std::mutex> lock(queueMutex);
                    if (requestQueue.empty()) {
                        break;
                    }
                    requestId = requestQueue.front();
                    requestQueue.pop();
                }
                
                double startTime = get_time_sec();
                
                // ğŸ”¥ å…³é”®ï¼šä½¿ç”¨å®Œæ•´çš„SchedulerBatchProcessoræµç¨‹ï¼ˆä¸ç»•è¿‡ï¼‰
                // åˆ›å»ºRequestStateå¹¶æ·»åŠ åˆ°Scheduler
                RequestState requestState;
                requestState.requestId = 0; // ç”±scheduleråˆ†é…
                requestState.tokenizedPrompt = promptTokens;
                requestState.maxTokens = params.n_gen;
                requestState.temperature = 0.7f;
                requestState.topP = 0.9f;
                requestState.topK = 0;
                requestState.eosTokenId = tokenizer->getEosId();
                requestState.priority = 0;
                requestState.arrivalTime = 0;
                requestState.startTime = 0;
                requestState.completionTime = 0;
                requestState.isCompleted = false;
                requestState.isRunning = false;
                requestState.isFailed = false;
                
                // æ·»åŠ åˆ°Scheduler
                size_t reqId = scheduler.addRequest(requestState);
                
                // ç­‰å¾…è¯·æ±‚å®Œæˆ
                const float timeoutSec = 300.0f;
                if (scheduler.waitForRequest(reqId, timeoutSec)) {
                    RequestState result = scheduler.getRequestResult(reqId);
                    
                    if (result.isCompleted && !result.generatedTokens.empty()) {
                        size_t tokens = result.generatedTokens.size();
                        totalTokens += tokens;
                        completedRequests++;
                        
                        double endTime = get_time_sec();
                        double elapsed = endTime - startTime;
                        {
                            std::lock_guard<std::mutex> lock(timesMutex);
                            requestTimes.push_back(elapsed);
                        }
                    }
                }
            }
        };
        
        double testStart = get_time_sec();
        std::vector<std::thread> workers;
        for (int i = 0; i < params.n_concurrent; ++i) {
            workers.emplace_back(worker, i);
        }
        
        for (auto& w : workers) {
            w.join();
        }
        
        scheduler.stop();
        
        double testEnd = get_time_sec();
        double totalTime = testEnd - testStart;
        size_t successful = completedRequests.load();
        size_t totalGenTokens = totalTokens.load();
        throughput = totalTime > 0 ? (totalGenTokens / totalTime) : 0.0;
        
        std::cout << "Stage 13 (SchedulerBatchProcessorå®Œæ•´æµç¨‹): " << throughput << " tokens/sec" << std::endl;
    } else if (stage == 14) {
        // Stage 14: GenerateEndpoint + Scheduler + SchedulerBatchProcessor
        // æµ‹è¯•ï¼šé€šè¿‡GenerateEndpointå¤„ç†è¯·æ±‚ï¼Œä½¿ç”¨å®Œæ•´çš„Scheduleræµç¨‹
        ModelExecutor executor(params.model_path, "", true, false, "llama_cpp", &modelConfig);
        executor.loadModel();
        
        TokenizerManager tokenizerManager("", &executor);
        ITokenizer* tokenizer = tokenizerManager.getTokenizer();
        
        Scheduler scheduler(&executor, 8, 2048);
        scheduler.start();
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
        
        GenerateEndpoint generateEndpoint(&scheduler, tokenizer);
        
        // å‡†å¤‡prompt
        std::string prompt = "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯";
        if (params.n_prompt > 0) {
            std::vector<int> promptTokens = tokenizer->encode(prompt, false);
            if (promptTokens.size() > static_cast<size_t>(params.n_prompt)) {
                promptTokens.resize(params.n_prompt);
                prompt = tokenizer->decode(promptTokens, true);
            }
        }
        
        for (size_t i = 0; i < params.n_requests; ++i) {
            requestQueue.push(i);
        }
        
        auto worker = [&](int workerId) {
            while (true) {
                size_t requestId;
                {
                    std::unique_lock<std::mutex> lock(queueMutex);
                    if (requestQueue.empty()) {
                        break;
                    }
                    requestId = requestQueue.front();
                    requestQueue.pop();
                }
                
                double startTime = get_time_sec();
                
                // ğŸ”¥ å…³é”®ï¼šé€šè¿‡GenerateEndpointå¤„ç†è¯·æ±‚ï¼ˆå®Œæ•´æµç¨‹ï¼‰
                HttpRequest httpRequest;
                httpRequest.setMethod("POST");
                httpRequest.setPath("/generate");
                httpRequest.setHeader("Content-Type", "application/json");
                
                // æ„å»ºJSONè¯·æ±‚ä½“
                nlohmann::json requestJson;
                requestJson["prompt"] = prompt;
                requestJson["max_tokens"] = params.n_gen;
                requestJson["temperature"] = 0.7;
                requestJson["stream"] = false;
                httpRequest.setBody(requestJson.dump());
                
                // é€šè¿‡GenerateEndpointå¤„ç†
                HttpResponse httpResponse = generateEndpoint.handle(httpRequest);
                
                // è§£æå“åº”
                if (httpResponse.getStatusCode() == 200) {
                    try {
                        nlohmann::json responseJson = nlohmann::json::parse(httpResponse.getBody());
                        if (responseJson.contains("success") && responseJson["success"] == true) {
                            if (responseJson.contains("data")) {
                                auto data = responseJson["data"];
                                if (data.contains("tokens_per_second")) {
                                    float tps = data["tokens_per_second"];
                                    if (tps > 0) {
                                        size_t tokens = static_cast<size_t>(tps * data.value("response_time", 1.0f));
                                        totalTokens += tokens;
                                        completedRequests++;
                                        
                                        double endTime = get_time_sec();
                                        double elapsed = endTime - startTime;
                                        {
                                            std::lock_guard<std::mutex> lock(timesMutex);
                                            requestTimes.push_back(elapsed);
                                        }
                                    }
                                }
                            }
                        }
                    } catch (const std::exception& e) {
                        // è§£æå¤±è´¥ï¼Œå¿½ç•¥
                    }
                }
            }
        };
        
        double testStart = get_time_sec();
        std::vector<std::thread> workers;
        for (int i = 0; i < params.n_concurrent; ++i) {
            workers.emplace_back(worker, i);
        }
        
        for (auto& w : workers) {
            w.join();
        }
        
        scheduler.stop();
        
        double testEnd = get_time_sec();
        double totalTime = testEnd - testStart;
        size_t successful = completedRequests.load();
        size_t totalGenTokens = totalTokens.load();
        throughput = totalTime > 0 ? (totalGenTokens / totalTime) : 0.0;
        
        std::cout << "Stage 14 (GenerateEndpoint + Scheduler + SchedulerBatchProcessor): " << throughput << " tokens/sec" << std::endl;
    } else if (stage == 15) {
        // Stage 15: HttpHandler + GenerateEndpoint + Scheduler + SchedulerBatchProcessor
        // æµ‹è¯•ï¼šé€šè¿‡HttpHandlerè·¯ç”±åˆ°GenerateEndpoint
        ModelExecutor executor(params.model_path, "", true, false, "llama_cpp", &modelConfig);
        executor.loadModel();
        
        TokenizerManager tokenizerManager("", &executor);
        ITokenizer* tokenizer = tokenizerManager.getTokenizer();
        
        Scheduler scheduler(&executor, 8, 2048);
        scheduler.start();
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
        
        GenerateEndpoint generateEndpoint(&scheduler, tokenizer);
        HttpHandler httpHandler;
        httpHandler.post("/generate", [&generateEndpoint](const HttpRequest& request) {
            return generateEndpoint.handle(request);
        });
        
        // å‡†å¤‡prompt
        std::string prompt = "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯";
        if (params.n_prompt > 0) {
            std::vector<int> promptTokens = tokenizer->encode(prompt, false);
            if (promptTokens.size() > static_cast<size_t>(params.n_prompt)) {
                promptTokens.resize(params.n_prompt);
                prompt = tokenizer->decode(promptTokens, true);
            }
        }
        
        for (size_t i = 0; i < params.n_requests; ++i) {
            requestQueue.push(i);
        }
        
        auto worker = [&](int workerId) {
            while (true) {
                size_t requestId;
                {
                    std::unique_lock<std::mutex> lock(queueMutex);
                    if (requestQueue.empty()) {
                        break;
                    }
                    requestId = requestQueue.front();
                    requestQueue.pop();
                }
                
                double startTime = get_time_sec();
                
                // ğŸ”¥ å…³é”®ï¼šé€šè¿‡HttpHandlerå¤„ç†è¯·æ±‚ï¼ˆå®Œæ•´æµç¨‹ï¼‰
                HttpRequest httpRequest;
                httpRequest.setMethod("POST");
                httpRequest.setPath("/generate");
                httpRequest.setHeader("Content-Type", "application/json");
                
                // æ„å»ºJSONè¯·æ±‚ä½“
                nlohmann::json requestJson;
                requestJson["prompt"] = prompt;
                requestJson["max_tokens"] = params.n_gen;
                requestJson["temperature"] = 0.7;
                requestJson["stream"] = false;
                httpRequest.setBody(requestJson.dump());
                
                // é€šè¿‡HttpHandlerå¤„ç†
                HttpResponse httpResponse = httpHandler.handleRequest(httpRequest);
                
                // è§£æå“åº”
                if (httpResponse.getStatusCode() == 200) {
                    try {
                        nlohmann::json responseJson = nlohmann::json::parse(httpResponse.getBody());
                        if (responseJson.contains("success") && responseJson["success"] == true) {
                            if (responseJson.contains("data")) {
                                auto data = responseJson["data"];
                                if (data.contains("tokens_per_second")) {
                                    float tps = data["tokens_per_second"];
                                    if (tps > 0) {
                                        size_t tokens = static_cast<size_t>(tps * data.value("response_time", 1.0f));
                                        totalTokens += tokens;
                                        completedRequests++;
                                        
                                        double endTime = get_time_sec();
                                        double elapsed = endTime - startTime;
                                        {
                                            std::lock_guard<std::mutex> lock(timesMutex);
                                            requestTimes.push_back(elapsed);
                                        }
                                    }
                                }
                            }
                        }
                    } catch (const std::exception& e) {
                        // è§£æå¤±è´¥ï¼Œå¿½ç•¥
                    }
                }
            }
        };
        
        double testStart = get_time_sec();
        std::vector<std::thread> workers;
        for (int i = 0; i < params.n_concurrent; ++i) {
            workers.emplace_back(worker, i);
        }
        
        for (auto& w : workers) {
            w.join();
        }
        
        scheduler.stop();
        
        double testEnd = get_time_sec();
        double totalTime = testEnd - testStart;
        size_t successful = completedRequests.load();
        size_t totalGenTokens = totalTokens.load();
        throughput = totalTime > 0 ? (totalGenTokens / totalTime) : 0.0;
        
        std::cout << "Stage 15 (HttpHandler + GenerateEndpoint + Scheduler + SchedulerBatchProcessor): " << throughput << " tokens/sec" << std::endl;
    } else if (stage == 16) {
        // Stage 16: Scheduler + BatchManager + ModelExecutor (å¯¹æ ‡Stage 15å‚æ•°ï¼Œä¸“é—¨æµ‹è¯•æ ¸å¿ƒç»„ä»¶)
        // ç›®æ ‡ï¼šå›ºå®šå‚æ•°ï¼Œæµ‹è¯• Scheduler + BatchManager + ModelExecutor çš„æ€§èƒ½
        // å‚æ•°ï¼šn_prompt=32, n_gen=50, n_requests=40, n_concurrent=8, maxBatchSize=8, maxContextLength=2048
        ModelExecutor executor(params.model_path, "", true, false, "llama_cpp", &modelConfig);
        executor.loadModel();
        
        TokenizerManager tokenizerManager("", &executor);
        ITokenizer* tokenizer = tokenizerManager.getTokenizer();
        
        // ğŸ”¥ ä½¿ç”¨ä¸Stage 15å®Œå…¨ç›¸åŒçš„é…ç½®
        Scheduler scheduler(&executor, 8, 2048);
        scheduler.start();
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
        
        // ğŸ”¥ å‡†å¤‡promptï¼ˆä¸Stage 15å®Œå…¨ä¸€è‡´ï¼‰
        std::string prompt = "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯";
        std::vector<int> promptTokens = tokenizer->encode(prompt, false);
        if (params.n_prompt > 0 && promptTokens.size() > static_cast<size_t>(params.n_prompt)) {
            promptTokens.resize(params.n_prompt);
            prompt = tokenizer->decode(promptTokens, true);
            promptTokens = tokenizer->encode(prompt, false); // é‡æ–°ç¼–ç ä»¥ç¡®ä¿ä¸€è‡´æ€§
        }
        
        // ğŸ”¥ ä½¿ç”¨ä¸Stage 15å®Œå…¨ç›¸åŒçš„å¹¶å‘å’Œç»Ÿè®¡æ–¹å¼
        std::queue<size_t> requestQueue;
        std::mutex queueMutex;
        std::atomic<size_t> completedRequests{0};
        std::atomic<size_t> totalTokens{0};
        std::vector<double> requestTimes;
        std::mutex timesMutex;
        
        for (size_t i = 0; i < params.n_requests; ++i) {
            requestQueue.push(i);
        }
        
        auto worker = [&](int workerId) {
            while (true) {
                size_t requestId;
                {
                    std::unique_lock<std::mutex> lock(queueMutex);
                    if (requestQueue.empty()) {
                        break;
                    }
                    requestId = requestQueue.front();
                    requestQueue.pop();
                }
                
                double startTime = get_time_sec();
                
                // ğŸ”¥ ç›´æ¥è°ƒç”¨Schedulerï¼ˆä¸ç»è¿‡GenerateEndpointå’ŒHttpHandlerï¼‰
                RequestState requestState;
                requestState.requestId = 0; // ç”±scheduleråˆ†é…
                requestState.tokenizedPrompt = promptTokens; // ä½¿ç”¨é¢„ç¼–ç çš„tokens
                requestState.maxTokens = params.n_gen;
                requestState.temperature = 0.7f;
                requestState.topP = 0.9f;
                requestState.topK = 0;
                requestState.eosTokenId = tokenizer->getEosId();
                requestState.priority = 0;
                requestState.arrivalTime = 0;
                requestState.startTime = 0;
                requestState.completionTime = 0;
                requestState.isCompleted = false;
                requestState.isRunning = false;
                requestState.isFailed = false;
                
                // æ·»åŠ åˆ°Scheduler
                size_t reqId = scheduler.addRequest(requestState);
                
                // ç­‰å¾…è¯·æ±‚å®Œæˆï¼ˆä½¿ç”¨ä¸Stage 15ç›¸åŒçš„è¶…æ—¶ï¼‰
                const float timeoutSec = 300.0f;
                bool waitSuccess = scheduler.waitForRequest(reqId, timeoutSec);
                if (waitSuccess) {
                    try {
                        RequestState result = scheduler.getRequestResult(reqId);
                        
                        // ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸå®Œæˆï¼ˆåªè¦æœ‰ç”Ÿæˆçš„tokenså°±è®¤ä¸ºæˆåŠŸï¼‰
                        // ä¸æ£€æŸ¥isCompletedï¼Œå› ä¸ºSchedulerå¯èƒ½æ²¡æœ‰æ­£ç¡®è®¾ç½®è¿™ä¸ªå­—æ®µ
                        if (!result.isFailed && !result.generatedTokens.empty()) {
                            size_t tokens = result.generatedTokens.size();
                            totalTokens += tokens;
                            completedRequests++;
                            
                            double endTime = get_time_sec();
                            double elapsed = endTime - startTime;
                            {
                                std::lock_guard<std::mutex> lock(timesMutex);
                                requestTimes.push_back(elapsed);
                            }
                        }
                    } catch (const std::exception& e) {
                        // è¯·æ±‚ä¸å­˜åœ¨æˆ–å…¶ä»–é”™è¯¯ï¼Œå¿½ç•¥
                        if (params.verbose) {
                            std::cerr << "Error getting result for request " << reqId << ": " << e.what() << std::endl;
                        }
                    }
                } else {
                    // waitForRequestè¿”å›falseï¼ˆè¶…æ—¶ï¼‰ï¼Œä½†å¯èƒ½è¯·æ±‚å·²ç»å®Œæˆï¼Œå°è¯•è·å–ç»“æœ
                    try {
                        RequestState result = scheduler.getRequestResult(reqId);
                        if (!result.isFailed && !result.generatedTokens.empty()) {
                            size_t tokens = result.generatedTokens.size();
                            totalTokens += tokens;
                            completedRequests++;
                        }
                    } catch (...) {
                        // è¯·æ±‚ä¸å­˜åœ¨ï¼Œå¿½ç•¥
                    }
                }
            }
        };
        
        double testStart = get_time_sec();
        std::vector<std::thread> workers;
        for (int i = 0; i < params.n_concurrent; ++i) {
            workers.emplace_back(worker, i);
        }
        
        for (auto& w : workers) {
            w.join();
        }
        
        scheduler.stop();
        
        double testEnd = get_time_sec();
        double totalTime = testEnd - testStart;
        size_t successful = completedRequests.load();
        size_t totalGenTokens = totalTokens.load();
        throughput = totalTime > 0 ? (totalGenTokens / totalTime) : 0.0;
        
        std::cout << "Stage 16 (Scheduler + BatchManager + ModelExecutor, å¯¹æ ‡Stage 15å‚æ•°): " << throughput << " tokens/sec" << std::endl;
        std::cout << "Successful requests: " << successful << "/" << params.n_requests << std::endl;
        std::cout << "Total generated tokens: " << totalGenTokens << std::endl;
        
        // ğŸ”¥ è°ƒè¯•ï¼šå¦‚æœæˆåŠŸè¯·æ±‚æ•°ä¸º0ä½†ååé‡>0ï¼Œè¯´æ˜ç»Ÿè®¡é€»è¾‘æœ‰é—®é¢˜
        if (successful == 0 && throughput > 0) {
            std::cerr << "âš ï¸ è­¦å‘Š: æˆåŠŸè¯·æ±‚æ•°ä¸º0ä½†ååé‡>0ï¼Œå¯èƒ½å­˜åœ¨ç»Ÿè®¡é€»è¾‘é—®é¢˜" << std::endl;
            std::cerr << "  æ€»æ—¶é—´: " << totalTime << "s, æ€»tokens: " << totalGenTokens << std::endl;
        }
        
        if (throughput >= 80.0) {
            std::cout << "âœ… è¾¾åˆ°ç›®æ ‡: " << throughput << " >= 80 tokens/sec" << std::endl;
        } else {
            std::cout << "âŒ æœªè¾¾åˆ°ç›®æ ‡: " << throughput << " < 80 tokens/sec" << std::endl;
        }
        return;
    }
    
    // è¾“å‡ºç»“æœï¼ˆå…¶ä»–Stageï¼‰
    std::cout << "Successful requests: " << completedRequests.load() << "/" << params.n_requests << std::endl;
    std::cout << "Total generated tokens: " << totalTokens.load() << std::endl;
    std::cout << "Throughput: " << throughput << " tokens/sec" << std::endl;
    
    if (throughput >= 80.0) {
        std::cout << "âœ… è¾¾åˆ°ç›®æ ‡: " << throughput << " >= 80 tokens/sec" << std::endl;
    } else {
        std::cout << "âŒ æœªè¾¾åˆ°ç›®æ ‡: " << throughput << " < 80 tokens/sec" << std::endl;
    }
}

// è§£æå‘½ä»¤è¡Œå‚æ•°
static BenchParams parse_args(int argc, char** argv) {
    BenchParams params;
    
    params.model_path = "/Users/dannypan/PycharmProjects/xllm/cpp/cLLM/model/Qwen/qwen3-0.6b-q4_k_m.gguf";
    
    for (int i = 1; i < argc; i++) {
        if (strcmp(argv[i], "--model") == 0 && i + 1 < argc) {
            params.model_path = argv[++i];
        } else if (strcmp(argv[i], "--n-prompt") == 0 && i + 1 < argc) {
            params.n_prompt = std::stoi(argv[++i]);
        } else if (strcmp(argv[i], "--n-gen") == 0 && i + 1 < argc) {
            params.n_gen = std::stoi(argv[++i]);
        } else if (strcmp(argv[i], "--n-requests") == 0 && i + 1 < argc) {
            params.n_requests = std::stoi(argv[++i]);
        } else if (strcmp(argv[i], "--n-concurrent") == 0 && i + 1 < argc) {
            params.n_concurrent = std::stoi(argv[++i]);
        } else if (strcmp(argv[i], "--n-batch") == 0 && i + 1 < argc) {
            params.n_batch = std::stoi(argv[++i]);
        } else if (strcmp(argv[i], "--n-gpu-layers") == 0 && i + 1 < argc) {
            params.n_gpu_layers = std::stoi(argv[++i]);
        } else if (strcmp(argv[i], "--stage") == 0 && i + 1 < argc) {
            params.stage = std::stoi(argv[++i]);
        } else if (strcmp(argv[i], "--verbose") == 0 || strcmp(argv[i], "-v") == 0) {
            params.verbose = true;
        } else if (strcmp(argv[i], "--help") == 0 || strcmp(argv[i], "-h") == 0) {
            std::cout << "Usage: " << argv[0] << " [options]" << std::endl;
            std::cout << "Options:" << std::endl;
            std::cout << "  --model <path>           Model path" << std::endl;
            std::cout << "  --n-prompt <n>           Prompt tokens (default: " << params.n_prompt << ")" << std::endl;
            std::cout << "  --n-gen <n>              Generation tokens per request (default: " << params.n_gen << ")" << std::endl;
            std::cout << "  --n-requests <n>         Total requests (default: " << params.n_requests << ")" << std::endl;
            std::cout << "  --n-concurrent <n>       Concurrent requests (default: " << params.n_concurrent << ")" << std::endl;
            std::cout << "  --n-batch <n>            Batch size (default: " << params.n_batch << ")" << std::endl;
            std::cout << "  --n-gpu-layers <n>       GPU layers (default: " << params.n_gpu_layers << ")" << std::endl;
            std::cout << "  --stage <n>              Test stage (0-16, default: 0)" << std::endl;
            std::cout << "                            Stage 16: Scheduler + BatchManager + ModelExecutor (å¯¹æ ‡Stage 15å‚æ•°)" << std::endl;
            std::cout << "  --verbose, -v            Verbose output" << std::endl;
            std::cout << "  --help, -h               Show this help" << std::endl;
            exit(0);
        }
    }
    
    return params;
}

int main(int argc, char** argv) {
    BenchParams params = parse_args(argc, argv);
    
    std::cout << "=== Incremental Benchmark Test ===" << std::endl;
    std::cout << "Model: " << params.model_path << std::endl;
    std::cout << "Stage: " << params.stage << std::endl;
    std::cout << "Requests: " << params.n_requests << std::endl;
    std::cout << "Concurrent: " << params.n_concurrent << std::endl;
    std::cout << std::endl;
    
    // è¿è¡ŒæŒ‡å®šé˜¶æ®µçš„æµ‹è¯•
    run_stage_test(params, params.stage);
    
    return 0;
}
