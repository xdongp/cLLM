# cLLM 配置文件 - llama.cpp 后端专用

# 服务器配置
server:
  host: "0.0.0.0"
  port: 8080
  num_threads: 8
  min_threads: 4
  use_libtorch: false

# 模型配置 - 使用 GGUF 模型
model:
  path: "/Users/dannypan/PycharmProjects/xllm/cpp/cLLM/model/Qwen/qwen3-0.6b-q4_k_m.gguf"
  vocab_size: 151936
  max_context_length: 2048
  default_max_tokens: 100
  quantization: "q4_k_m"

# HTTP 配置
http:
  max_input_tokens: 120
  timeout_ms: 30000

# API 端点配置
api:
  endpoints:
    health:
      name: "health"
      path: "/health"
      method: "GET"
    generate:
      name: "generate"
      path: "/generate"
      method: "POST"
    generate_stream:
      name: "generate_stream"
      path: "/generate_stream"
      method: "POST"
    encode:
      name: "encode"
      path: "/encode"
      method: "POST"

  response:
    content_type:
      json: "application/json"
      stream: "text/event-stream"
    headers:
      cache_control: "no-cache"
      connection: "keep-alive"

  defaults:
    prompt: ""
    max_tokens: 100
    temperature: 0.7
    top_p: 0.9

  timeouts:
    min: 60.0
    max: 600.0
    token_factor: 10.0

# 调度器配置
scheduler:
  max_iterations: 2000
  batch_timeout_ms: 100
  max_batch_size: 128
  context_usage_threshold: 0.75
  default_temperature: 0.7
  default_top_p: 0.9
  default_top_k: 50
  default_max_tokens: 100
  request_timeout: 120.0
  loop_interval: 1
  idle_loop_interval: 10
  wait_poll_interval_ms: 1

# 资源配置
resources:
  max_batch_size: 128
  max_context_length: 2048
  kv_cache_max_size: 64
  kv_cache_max_memory_mb: 2048
  memory_limit_mb: 8192

# 缓存配置
cache:
  default_max_size: 64
  default_max_memory_mb: 2048
  enable_lru: true
  enable_memory_limit: true
  enable_stats: true
  eviction_threshold: 0.8
  cleanup_interval: 500

# 后端配置 - 使用 llama.cpp
backend:
  type: "llama_cpp"
  
  # llama.cpp 后端配置
  llama_cpp:
    n_batch: 512
    n_threads: 8
    n_gpu_layers: 99      # 启用 Metal GPU 加速 (99=全部层到GPU)
    n_seq_max: 64         # 增加序列数量支持更高并发
    use_mmap: true
    use_mlock: false

# 动态批处理调谐器配置
dynamic_batch_tuner:
  enabled: true
  strategy: "hybrid"
  search_algorithm: "adaptive_step"
  fixed_batch_size: 24
  min_batch_size: 16
  max_batch_size: 48
  initial_batch_size: 24
  time_increase_threshold: 0.30
  time_decrease_threshold: 0.10
  validation_interval: 50
  max_consecutive_time_increases: 5
  auto_adjust_enabled: true
  probe_batch_count: 10
  validation_batch_count: 10
  adjustment_factor: 0.50
  exploration_interval: 200

# 日志配置
logging:
  level: "warn"   # 减少日志输出以提高性能
  file: "logs/cllm_server.log"
  max_size_mb: 100
  max_files: 5
