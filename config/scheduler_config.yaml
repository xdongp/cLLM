# 采样器配置
sampler:
  temperature: 0.7
  top_k: 50
  top_p: 0.9
  greedy_threshold: 0.0

# 调度器配置
scheduler:
  max_batch_size: 8
  max_context_length: 2048
  context_usage_threshold: 0.75
  default_temperature: 0.7
  default_top_p: 0.9
  default_top_k: 50
  default_max_tokens: 100
  request_timeout: 300.0
  loop_interval: 100
  idle_loop_interval: 10000
  max_iterations: 1000
  batch_timeout_ms: 1000

# HTTP配置
http:
  max_input_tokens: 7000
  port: 8080
  timeout_ms: 30000

# 模型配置
model:
  max_context_length: 2048
  default_max_tokens: 100
  temperature: 0.7
  top_k: 50
  top_p: 0.9

# 队列配置
queue:
  max_size: 1000
  priority_weight: 1.0

# 内存配置
memory:
  cache_limit_mb: 2048
  monitor_interval_ms: 1000

# 服务器资源配置
resources:
  max_batch_size: 8
  max_context_length: 2048
  kv_cache_max_size: 100
  kv_cache_max_memory_mb: 4096
  memory_limit_mb: 16384
  num_threads: 8

# 缓存配置
cache:
  default_max_size: 10
  default_max_memory_mb: 0
  enable_lru: true
  enable_memory_limit: false
  enable_stats: true
  eviction_threshold: 0.9
  cleanup_interval: 1000

# API配置（测试使用默认值）
api:
  endpoints:
    health: {}
    generate: {}
    generate_stream: {}
    encode: {}
  defaults: {}
  response:
    content_type:
      json: "application/json"
      stream: "text/event-stream"
    headers:
      cache_control: "no-cache"
      connection: "keep-alive"
  timeouts:
    min: 60.0
    max: 600.0
    token_factor: 10.0

# 后端配置
backend:
  llama_cpp:
    n_seq_max: 16