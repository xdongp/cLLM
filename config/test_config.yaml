# 测试配置文件 - 用于集成测试
# 最小配置,仅包含必需字段

model:
  vocab_size: 151936  # Qwen3 model vocab size
  max_sequence_length: 2048

sampler:
  temperature: 0.7
  top_k: 50
  top_p: 0.9
  greedy_threshold: 0.0

scheduler:
  max_iterations: 1000
  batch_timeout_ms: 500
  max_batch_size: 32
  default_temperature: 0.7
  default_top_p: 0.9
  default_top_k: 50
  default_max_tokens: 100
  request_timeout: 1.0
  loop_interval: 10
  idle_loop_interval: 100
  wait_poll_interval_ms: 10
  context_usage_threshold: 0.9

server:
  port: 8080
  host: "0.0.0.0"

http:
  max_input_tokens: 7000
  timeout_ms: 30000

resources:
  max_batch_size: 8
  max_context_length: 2048
  kv_cache_max_size: 100
  kv_cache_max_memory_mb: 4096
  memory_limit_mb: 16384
  num_threads: 8
  min_threads: 4

queue:
  max_size: 1000
  priority_weight: 1.0

memory:
  cache_limit_mb: 2048
  monitor_interval_ms: 1000

cache:
  default_max_size: 1000
  default_max_memory_mb: 100
  enable_lru: true
  enable_memory_limit: true
  enable_stats: true
  eviction_threshold: 0.9
  cleanup_interval: 60

profiling:
  enable: false
  output_dir: "./profiling_results"
  detailed_metrics: false

# API配置（测试使用默认值）
api:
  endpoints:
    health: {}
    generate: {}
    generate_stream: {}
    encode: {}
  defaults:
    prompt: "test"
  response:
    content_type:
      json: "application/json"
      stream: "text/event-stream"
    headers:
      cache_control: "no-cache"
      connection: "keep-alive"
  timeouts:
    min: 60.0
    max: 600.0
    token_factor: 10.0

# 后端配置
backend:
  llama_cpp:
    n_seq_max: 16
