# cLLM 配置文件（模板）
# 复制为 config.yaml 后按需修改

# 服务器配置
server:
  host: "0.0.0.0"          # 服务器监听地址
  port: 8080               # 服务器监听端口
  num_threads: 8           # Drogon 工作线程数（0 表示使用硬件并发数）- 优化：增加线程数提升并发处理能力
  min_threads: 4           # 最小线程数（避免 /generate 阻塞导致 /health 卡住）- 优化：增加最小线程数
  use_libtorch: false      # 是否使用 LibTorch 后端（否则使用 Kylin）

# 模型配置
model:
  path: "/Users/dannypan/PycharmProjects/xllm/cpp/cLLM/model/Qwen/qwen3-0.6b-q8_0.gguf"   # 模型目录或模型文件路径 (int8 量化)
  vocab_size: 151936       # 词表大小（可被后端自动探测覆盖）
  max_context_length: 2048 # 最大上下文长度
  default_max_tokens: 500  # 默认最大生成 token 数 - 优化：对标Ollama，增加max_tokens以提升吞吐量
  quantization: "int8"     # 量化类型: fp16, int8, int4

# HTTP 配置
http:
  max_input_tokens: 120    # 最大输入 token（过长 prompt 会被裁剪）
  timeout_ms: 30000        # HTTP 请求超时时间(毫秒)

# API 端点配置
api:
  endpoints:
    health:
      name: "health"
      path: "/health"
      method: "GET"
    generate:
      name: "generate"
      path: "/generate"
      method: "POST"
    generate_stream:
      name: "generate_stream"
      path: "/generate_stream"
      method: "POST"
    encode:
      name: "encode"
      path: "/encode"
      method: "POST"

  response:
    content_type:
      json: "application/json"
      stream: "text/event-stream"
    headers:
      cache_control: "no-cache"
      connection: "keep-alive"

  defaults:
    prompt: ""
    max_tokens: 100
    temperature: 0.7
    top_p: 0.9

  timeouts:
    min: 60.0
    max: 600.0
    token_factor: 10.0

# 调度器配置
scheduler:
  max_iterations: 2000      # 优化：增加最大迭代次数以处理更多请求
  batch_timeout_ms: 100     # 优化：减少批处理超时以更快响应
  max_batch_size: 64       # 优化：增加最大批处理大小以提升吞吐量
  context_usage_threshold: 0.75
  default_temperature: 0.7
  default_top_p: 0.9
  default_top_k: 50
  default_max_tokens: 100
  request_timeout: 120.0    # 优化：增加请求超时时间以适应并发场景
  loop_interval: 5          # 优化：减少循环间隔以更快处理请求
  idle_loop_interval: 50     # 优化：减少空闲循环间隔
  wait_poll_interval_ms: 5   # 优化：减少等待轮询间隔

# 资源配置
resources:
  max_batch_size: 8        # 优化：减少最大批处理大小以避免内存问题
  max_context_length: 2048
  kv_cache_max_size: 100    # 优化：减少KV cache大小以避免内存问题
  kv_cache_max_memory_mb: 4096  # 优化：减少KV cache内存限制
  memory_limit_mb: 8192

# 缓存配置
cache:
  default_max_size: 200    # 优化：增加缓存大小以支持更多并发请求
  default_max_memory_mb: 8192  # 优化：增加缓存内存限制
  enable_lru: true
  enable_memory_limit: true
  enable_stats: true
  eviction_threshold: 0.9    # 优化：提高驱逐阈值以减少缓存抖动
  cleanup_interval: 500    # 优化：减少清理间隔以更快释放内存

# LibTorch 后端配置
backend:
  libtorch:
    seq_len_candidates: [8, 16, 32, 64, 128, 256]
    fallback_seq_len: 8
  
  # llama.cpp 后端配置
  llama_cpp:
    n_batch: 512          # 优化：减少批处理大小以避免内存问题
    n_threads: 8          # 优化：增加线程数以充分利用CPU
    n_gpu_layers: 99       # GPU层数（99表示使用全部GPU层，启用Metal加速）
    n_seq_max: 32         # 优化：增加最大序列数以支持更多并发请求（从8增加到32，提升并发性能）
    use_mmap: true        # 使用内存映射
    use_mlock: false      # 使用内存锁定

# 日志配置
logging:
  level: "info"
  file: ""
