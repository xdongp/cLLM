# cLLM é…ç½®æ–‡ä»¶ï¼ˆæ¨¡æ¿ï¼‰
# å¤åˆ¶ä¸º config.yaml åæŒ‰éœ€ä¿®æ”¹

# æœåŠ¡å™¨é…ç½®
server:
  host: "0.0.0.0"          # æœåŠ¡å™¨ç›‘å¬åœ°å€
  port: 8080               # æœåŠ¡å™¨ç›‘å¬ç«¯å£
  num_threads: 8          # Drogon å·¥ä½œçº¿ç¨‹æ•°ï¼ˆ0 è¡¨ç¤ºä½¿ç”¨ç¡¬ä»¶å¹¶å‘æ•°ï¼‰- åŸºçº¿é…ç½®
  min_threads: 4           # æœ€å°çº¿ç¨‹æ•°ï¼ˆé¿å… /generate é˜»å¡å¯¼è‡´ /health å¡ä½ï¼‰- åŸºçº¿é…ç½®
  use_libtorch: false      # æ˜¯å¦ä½¿ç”¨ LibTorch åç«¯ï¼ˆå¦åˆ™ä½¿ç”¨ Kylinï¼‰

# æ¨¡å‹é…ç½®
model:
  path: "/Users/dannypan/PycharmProjects/xllm/cpp/cLLM/model/Qwen/qwen3-0.6b-q4_k_m.gguf"   # æ¨¡å‹ç›®å½•æˆ–æ¨¡å‹æ–‡ä»¶è·¯å¾„ (q4_k_m é‡åŒ–)
  vocab_size: 151936       # è¯è¡¨å¤§å°ï¼ˆå¯è¢«åç«¯è‡ªåŠ¨æ¢æµ‹è¦†ç›–ï¼‰
  max_context_length: 2048 # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
  default_max_tokens: 100  # é»˜è®¤æœ€å¤§ç”Ÿæˆ token æ•° - ä¿®å¤ï¼šä¸APIé»˜è®¤å€¼ä¸€è‡´ï¼Œé¿å…è¦†ç›–ç”¨æˆ·è¯·æ±‚çš„max_tokens
  quantization: "q4_k_m"   # é‡åŒ–ç±»å‹: fp16, int8, int4, q4_k_m

# HTTP é…ç½®
http:
  max_input_tokens: 120    # æœ€å¤§è¾“å…¥ tokenï¼ˆè¿‡é•¿ prompt ä¼šè¢«è£å‰ªï¼‰
  timeout_ms: 30000        # HTTP è¯·æ±‚è¶…æ—¶æ—¶é—´(æ¯«ç§’)

# API ç«¯ç‚¹é…ç½®
api:
  endpoints:
    health:
      name: "health"
      path: "/health"
      method: "GET"
    generate:
      name: "generate"
      path: "/generate"
      method: "POST"
    generate_stream:
      name: "generate_stream"
      path: "/generate_stream"
      method: "POST"
    encode:
      name: "encode"
      path: "/encode"
      method: "POST"

  response:
    content_type:
      json: "application/json"
      stream: "text/event-stream"
    headers:
      cache_control: "no-cache"
      connection: "keep-alive"

  defaults:
    prompt: ""
    max_tokens: 100
    temperature: 0.7
    top_p: 0.9

  timeouts:
    min: 60.0
    max: 600.0
    token_factor: 10.0

# è°ƒåº¦å™¨é…ç½®
scheduler:
  max_iterations: 2000      # ä¼˜åŒ–ï¼šå¢åŠ æœ€å¤§è¿­ä»£æ¬¡æ•°ä»¥å¤„ç†æ›´å¤šè¯·æ±‚
  batch_timeout_ms: 100     # ä¼˜åŒ–ï¼šå‡å°‘æ‰¹å¤„ç†è¶…æ—¶ä»¥æ›´å¿«å“åº”
  max_batch_size: 128      # ğŸ”¥ ä¼˜åŒ–ï¼šå¤§å¹…å¢åŠ æœ€å¤§æ‰¹å¤„ç†å¤§å°ä»¥æœ€å¤§åŒ–ååé‡
  context_usage_threshold: 0.75
  default_temperature: 0.7
  default_top_p: 0.9
  default_top_k: 50
  default_max_tokens: 100
  request_timeout: 120.0    # ä¼˜åŒ–ï¼šå¢åŠ è¯·æ±‚è¶…æ—¶æ—¶é—´ä»¥é€‚åº”å¹¶å‘åœºæ™¯
  loop_interval: 1          # ä¼˜åŒ–ï¼šæçŸ­å¾ªç¯é—´éš”ä»¥æœ€å¤§åŒ–ååé‡
  idle_loop_interval: 10     # ä¼˜åŒ–ï¼šæçŸ­ç©ºé—²å¾ªç¯é—´éš”
  wait_poll_interval_ms: 1   # ä¼˜åŒ–ï¼šæçŸ­ç­‰å¾…è½®è¯¢é—´éš”ï¼ˆå·²ä½¿ç”¨æ¡ä»¶å˜é‡ï¼Œæ­¤å‚æ•°ä¸å†ä½¿ç”¨ï¼‰

# èµ„æºé…ç½®
resources:
  max_batch_size: 128      # ğŸ”¥ ä¼˜åŒ–ï¼šå¤§å¹…å¢åŠ æœ€å¤§æ‰¹å¤„ç†å¤§å°ä»¥æœ€å¤§åŒ–GPUå¹¶è¡Œèƒ½åŠ›
  max_context_length: 2048
  kv_cache_max_size: 100    # ä¼˜åŒ–ï¼šå‡å°‘KV cacheå¤§å°ä»¥é¿å…å†…å­˜é—®é¢˜
  kv_cache_max_memory_mb: 4096  # ä¼˜åŒ–ï¼šå‡å°‘KV cacheå†…å­˜é™åˆ¶
  memory_limit_mb: 8192

# ç¼“å­˜é…ç½®
cache:
  default_max_size: 200    # ä¼˜åŒ–ï¼šå¢åŠ ç¼“å­˜å¤§å°ä»¥æ”¯æŒæ›´å¤šå¹¶å‘è¯·æ±‚
  default_max_memory_mb: 8192  # ä¼˜åŒ–ï¼šå¢åŠ ç¼“å­˜å†…å­˜é™åˆ¶
  enable_lru: true
  enable_memory_limit: true
  enable_stats: true
  eviction_threshold: 0.9    # ä¼˜åŒ–ï¼šæé«˜é©±é€é˜ˆå€¼ä»¥å‡å°‘ç¼“å­˜æŠ–åŠ¨
  cleanup_interval: 500    # ä¼˜åŒ–ï¼šå‡å°‘æ¸…ç†é—´éš”ä»¥æ›´å¿«é‡Šæ”¾å†…å­˜

# LibTorch åç«¯é…ç½®
backend:
  libtorch:
    seq_len_candidates: [8, 16, 32, 64, 128, 256]
    fallback_seq_len: 8
  
  # llama.cpp åç«¯é…ç½®
  llama_cpp:
    n_batch: 512          # æ‰¹å¤„ç†å¤§å°ï¼ˆ512æ˜¯å½“å‰æœ€ä¼˜é…ç½®ï¼‰
    n_threads: 8          # åŸºçº¿é…ç½®ï¼šä¸CPUæ ¸å¿ƒæ•°ä¸€è‡´ï¼ˆ8æ ¸ï¼‰
    n_gpu_layers: 99       # GPUå±‚æ•°ï¼ˆ99è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨GPUå±‚ï¼Œå¯ç”¨MetalåŠ é€Ÿï¼‰
    n_seq_max: 64         # åŸºçº¿é…ç½®ï¼šæ”¯æŒåˆç†å¹¶å‘
    use_mmap: true        # ä½¿ç”¨å†…å­˜æ˜ å°„
    use_mlock: false      # ä½¿ç”¨å†…å­˜é”å®š

# æ—¥å¿—é…ç½®
logging:
  level: "debug"         # æ—¥å¿—çº§åˆ«: trace, debug, info, warn, error, critical
  file: "logs/cllm_server.log"  # æ—¥å¿—æ–‡ä»¶è·¯å¾„
  max_size_mb: 100         # æ—¥å¿—æ–‡ä»¶æœ€å¤§å¤§å°(MB)
  max_files: 5            # ä¿ç•™çš„æ—¥å¿—æ–‡ä»¶æ•°é‡
