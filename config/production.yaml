# ============================================
# cLLM Production Configuration
# 统一配置文件 - 合并自原有的6个配置文件
# 生成时间: 2026-01-11
# ============================================

# ============================================
# 模型配置
# ============================================
model:
  type: "qwen3"
  vocab_size: 151936  # Qwen3 actual vocab size
  hidden_size: 4096
  num_layers: 32
  num_attention_heads: 32
  num_key_value_heads: 32
  max_sequence_length: 2048
  intermediate_size: 11008
  quantization: "none"  # Options: none, fp16, int8, int4
  use_kv_cache: true
  use_memory_compression: false
  
  # 模型路径(支持环境变量)
  path: "${CLLM_MODEL_PATH:-/path/to/model}"

# ============================================
# 服务器配置
# ============================================
server:
  host: "127.0.0.1"  # Use localhost for security, change to 0.0.0.0 if needed
  port: 8080
  worker_threads: 8  # HTTP worker threads
  
  # API配置
  api:
    max_input_tokens: 2048
    max_output_tokens: 2048
    timeout_ms: 30000
    
    # 限流配置(可选)
    rate_limit:
      enabled: false
      requests_per_minute: 60
      max_concurrent_per_ip: 10

# ============================================
# 推理配置
# ============================================
inference:
  # 批处理配置
  batch:
    max_size: 32  # Increased from 8 to 32 for better throughput
    timeout_ms: 500
    context_length: 2048
    context_usage_threshold: 0.75
    
  # 采样器配置
  sampler:
    # 默认采样参数
    temperature: 0.7  # [0.0, 2.0] recommended range
    top_k: 50  # [-1 = disabled, or positive integer]
    top_p: 0.9  # [0.0, 1.0]
    greedy_threshold: 0.0  # Use greedy when temperature < threshold
    
    # 预设配置(可选,供客户端选择)
    presets:
      greedy:
        temperature: 0.0
        top_k: 1
        top_p: 1.0
      balanced:
        temperature: 0.7
        top_k: 50
        top_p: 0.9
      creative:
        temperature: 1.2
        top_k: 100
        top_p: 0.95
    
  # KV缓存配置
  kv_cache:
    max_entries: 1000  # Maximum number of cached entries
    max_memory_mb: 8192  # 8GB memory limit
    
    # 淘汰策略
    eviction:
      strategy: "lru"  # Least Recently Used
      threshold: 0.85  # Trigger eviction at 85% capacity
      cleanup_interval_ms: 5000  # Cleanup every 5 seconds
    
    # 监控与统计
    monitoring:
      enable_stats: true
      stats_interval_ms: 1000

# ============================================
# 资源管理
# ============================================
resources:
  memory_limit_mb: 16384  # 16GB total memory limit
  inference_threads: 8  # Number of inference threads
  
  # 限制配置
  limits:
    max_concurrent_requests: 1000
    max_request_size_mb: 10
    max_sequence_length: 2048

# ============================================
# 调度器配置
# ============================================
scheduler:
  # 请求管理
  request:
    default_max_tokens: 100
    timeout_sec: 60  # Request timeout in seconds
    max_iterations: 1000  # Max inference iterations per request
  
  # 循环控制
  loop:
    active_interval_us: 100  # 100 microseconds when active
    idle_interval_us: 10000  # 10 milliseconds when idle

# ============================================
# 队列配置
# ============================================
queue:
  max_size: 1000  # Maximum pending requests
  priority_weight: 1.0  # Weight for priority scheduling

# ============================================
# 内存管理
# ============================================
memory:
  cache_limit_mb: 2048  # General cache limit
  monitor_interval_ms: 1000  # Memory monitoring interval

# ============================================
# Profiling配置(生产环境建议关闭)
# ============================================
profiling:
  enable: false
  output_dir: "./profiling_results"
  detailed_metrics: false

# ============================================
# 日志配置
# ============================================
logging:
  level: "INFO"  # DEBUG, INFO, WARN, ERROR
  output: "stdout"  # stdout, stderr, or file path
  rotation:
    enabled: false
    max_size_mb: 100
    max_files: 10
