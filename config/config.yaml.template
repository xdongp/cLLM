# ============================================================
# cLLM 配置文件模板
# ============================================================
#
# 使用方法:
#   1. 复制此文件为 config.yaml
#   2. 根据需要修改配置
#   3. 启动服务: ./build/bin/cllm_server --config config/config.yaml
#
# 预设配置文件:
#   - config_cpu.yaml:      Kylin 后端 + CPU 模式
#   - config_gpu.yaml:      Kylin 后端 + Metal GPU 模式
#   - config_llama_cpp.yaml: llama.cpp 后端 + GPU 模式
#
# ============================================================

# ============================================================
# 服务器配置
# ============================================================
server:
  host: "0.0.0.0"          # 服务器监听地址
  port: 8080               # 服务器监听端口
  num_threads: 8           # HTTP 工作线程数（0 = 使用硬件并发数）
  min_threads: 4           # 最小线程数（保证轻量端点不被阻塞）
  
  # [已废弃] use_libtorch: 请使用 backend.type 替代
  # use_libtorch: false

# ============================================================
# 模型配置
# ============================================================
model:
  # 模型路径:
  # - HuggingFace 格式: 包含 config.json, model.safetensors, tokenizer.json 的目录
  # - GGUF 格式: .gguf 文件路径
  path: "/path/to/model"
  
  vocab_size: 151936       # 词表大小（可被后端自动探测覆盖）
  max_context_length: 4096 # 最大上下文长度（输入 + 输出的总 token 数）
  default_max_tokens: 1024 # 默认最大生成 token 数
  
  # 量化类型:
  # - Kylin 后端: fp32 | fp16 | int8
  # - llama.cpp 后端: 由 GGUF 文件决定 (q4_k_m, q8_0, f16 等)
  quantization: "int8"

# ============================================================
# HTTP 配置
# ============================================================
http:
  max_input_tokens: 2048   # 最大输入 token（超长 prompt 会被裁剪）
  timeout_ms: 120000       # HTTP 请求超时（毫秒）

# ============================================================
# API 端点配置
# [已废弃] API 端点已硬编码，无需在配置文件中配置
# 硬编码端点:
#   - GET  /              显示所有可用端点
#   - GET  /health       健康检查
#   - POST /generate      文本生成（非流式）
#   - POST /generate_stream  文本生成（流式）
#   - POST /encode       Token 编码
#   - POST /benchmark    性能基准测试
#   - GET  /model/info  模型信息
# ============================================================
# 调度器配置
# ============================================================
scheduler:
  # 最大迭代次数
  # 必须 >= max_context_length 以支持长序列生成
  max_iterations: 5000
  
  batch_timeout_ms: 100    # 批处理超时（毫秒）
  max_batch_size: 16       # 最大批处理大小
  context_usage_threshold: 0.75  # 上下文使用阈值
  
  # 采样参数默认值
  default_temperature: 0.7
  default_top_p: 0.9
  default_top_k: 50
  default_max_tokens: 1024
  
  # 请求处理参数
  request_timeout: 300.0   # 请求超时（秒）
  loop_interval: 5         # 调度循环间隔（毫秒）
  idle_loop_interval: 50   # 空闲时循环间隔（毫秒）
  wait_poll_interval_ms: 5 # 等待轮询间隔（毫秒）

# ============================================================
# 资源配置
# ============================================================
resources:
  max_batch_size: 16       # 最大批处理大小
  max_context_length: 4096 # 最大上下文长度
  kv_cache_max_size: 32    # KV 缓存最大条目数
  kv_cache_max_memory_mb: 4096  # KV 缓存最大内存（MB）
  memory_limit_mb: 8192    # 总内存限制（MB）

# ============================================================
# 缓存配置
# ============================================================
cache:
  default_max_size: 100    # 默认最大缓存条目数
  default_max_memory_mb: 4096  # 默认最大缓存内存（MB）
  enable_lru: true         # 启用 LRU 淘汰策略
  enable_memory_limit: true
  enable_stats: true       # 启用统计信息
  eviction_threshold: 0.9  # 淘汰阈值（内存使用率）
  cleanup_interval: 500    # 清理间隔（毫秒）

# ============================================================
# 后端配置
# ============================================================
backend:
  # 后端类型（必填）:
  #   kylin:     自研推理后端，支持 HuggingFace safetensors 模型
  #   llama_cpp: llama.cpp 后端，支持 GGUF 量化模型
  #   libtorch:  LibTorch 后端，支持 TorchScript 模型
  type: "kylin"
  
  # ------------------------------------------------------------
  # Kylin 后端配置
  # 自研高性能推理引擎，支持 HuggingFace 模型格式
  # ------------------------------------------------------------
  kylin:
    # 设备后端:
    #   cpu:   使用 CPU 推理
    #   cuda:  使用 NVIDIA GPU (需要 CUDA)
    #   metal: 使用 Apple Metal (macOS)
    device_backend: "cpu"
    
    # 算子后端:
    #   auto:   自动选择最优算子
    #   native: 使用原生 C++ 实现
    #   ggml:   使用 GGML 算子（推荐，性能更好）
    operator_backend: "ggml"
    
    # 计算线程数（0 = 自动）
    n_threads: 8

  # ------------------------------------------------------------
  # llama.cpp 后端配置
  # 高性能量化推理引擎，支持 GGUF 模型格式
  # ------------------------------------------------------------
  llama_cpp:
    # 批处理大小（prompt 处理）
    n_batch: 512
    
    # CPU 计算线程数
    n_threads: 8
    
    # GPU 层数:
    #   0:  仅使用 CPU
    #   N:  前 N 层在 GPU
    #   99: 全部层在 GPU（Metal/CUDA）
    n_gpu_layers: 0
    
    # 最大并发序列数
    # 影响 KV cache 内存使用，建议根据显存调整
    n_seq_max: 8
    
    # 内存映射选项
    use_mmap: true         # 使用内存映射加载模型
    use_mlock: false       # 锁定内存（防止交换到磁盘）

  # ------------------------------------------------------------
  # LibTorch 后端配置
  # PyTorch C++ 推理引擎，支持 TorchScript 模型格式
  # ------------------------------------------------------------
  libtorch:
    # 预编译的序列长度候选（用于优化）
    seq_len_candidates: [8, 16, 32, 64, 128, 256]
    
    # 回退序列长度
    fallback_seq_len: 8

# ============================================================
# 动态批处理调谐器配置
# ============================================================
dynamic_batch_tuner:
  enabled: false           # 是否启用动态批处理调谐
  
  # 策略:
  #   static:  使用固定批处理大小
  #   dynamic: 根据负载动态调整
  #   hybrid:  混合模式
  strategy: "static"
  
  fixed_batch_size: 8      # 固定批处理大小（strategy=static 时生效）
  min_batch_size: 1        # 最小批处理大小
  max_batch_size: 64       # 最大批处理大小
  initial_batch_size: 8    # 初始批处理大小
  
  # 调谐参数
  probing_growth_factor: 2.0
  max_probing_attempts: 10
  time_increase_threshold: 0.30
  adjustment_factor: 0.30
  validation_interval: 50
  exploration_interval: 200
  probe_batch_count: 10
  validation_batch_count: 10
  max_consecutive_time_increases: 5

# ============================================================
# 日志配置
# ============================================================
logging:
  # 日志级别: trace | debug | info | warn | error
  level: "info"
  
  # 日志文件路径（空 = 仅输出到控制台）
  file: ""
  
  # 日志轮转配置
  max_size_mb: 100         # 单个日志文件最大大小（MB）
  max_files: 5             # 保留的日志文件数量
