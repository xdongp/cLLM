# cLLM 配置文件 - GPU 模式

server:
  host: "0.0.0.0"
  port: 8081
  num_threads: 8
  min_threads: 4

model:
  path: "/Users/dannypan/PycharmProjects/cLLM/model/Qwen/Qwen3-0.6B"
  vocab_size: 151936
  max_context_length: 8192
  default_max_tokens: 2048
  quantization: "fp16"

http:
  max_input_tokens: 4096
  timeout_ms: 300000

api:
  response:
    content_type:
      json: "application/json"
      stream: "text/event-stream"
    headers:
      cache_control: "no-cache"
      connection: "keep-alive"
  defaults:
    prompt: ""
    max_tokens: 100
    temperature: 0.7
    top_p: 0.9
  timeouts:
    min: 60.0
    max: 600.0
    token_factor: 10.0

scheduler:
  max_iterations: 10000
  batch_timeout_ms: 100
  max_batch_size: 8
  context_usage_threshold: 0.75
  default_temperature: 0.7
  default_top_p: 0.9
  default_top_k: 50
  default_max_tokens: 2048
  request_timeout: 600.0
  loop_interval: 5
  idle_loop_interval: 50
  wait_poll_interval_ms: 5

resources:
  max_batch_size: 8
  max_context_length: 8192

backend:
  type: "kylin"
  kylin:
    device_backend: "metal"
    operator_backend: "ggml"
    n_threads: 8
  llama_cpp:
    n_batch: 512
    n_threads: 8
    n_gpu_layers: 0
    n_seq_max: 8
    use_mmap: true
    use_mlock: false
  libtorch:
    seq_len_candidates: [8, 16, 32, 64, 128, 256]
    fallback_seq_len: 8

dynamic_batch_tuner:
  enabled: false
  strategy: "static"
  fixed_batch_size: 8
  min_batch_size: 1
  max_batch_size: 64
  initial_batch_size: 8
  probing_growth_factor: 2.0
  max_probing_attempts: 10
  time_increase_threshold: 0.30
  adjustment_factor: 0.30
  validation_interval: 50
  exploration_interval: 200
  probe_batch_count: 10
  validation_batch_count: 10
  max_consecutive_time_increases: 5

logging:
  level: "info"
  file: ""
  max_size_mb: 100
  max_files: 5
