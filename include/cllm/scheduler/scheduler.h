/**
 * @file scheduler.h
 * @brief è°ƒåº¦å™¨æ ¸å¿ƒç±»ï¼Œè´Ÿè´£è¯·æ±‚è°ƒåº¦å’Œæ‰¹å¤„ç†
 * @author cLLM Team
 * @date 2024-01-09
 */

#ifndef CLLM_SCHEDULER_H
#define CLLM_SCHEDULER_H

#include <string>
#include <vector>
#include <map>
#include <thread>
#include <mutex>
#include <shared_mutex>
#include <condition_variable>
#include <atomic>
#include <functional>

#include "cllm/scheduler/config.h"
#include "cllm/scheduler/stats.h"
#include "cllm/scheduler/tracker.h"
#include "cllm/scheduler/batch_processor.h"
#include "cllm/common/queue.h"
#include "cllm/common/request_state.h"
#include "cllm/batch/manager.h"
#include "cllm/model/executor.h"
#include "cllm/kv_cache/cache.h"
#include "cllm/common/config.h"

namespace cllm {

class DynamicBatchTuner;
class HybridBatchStrategy;

/**
 * @brief è°ƒåº¦å™¨é”™è¯¯ç±»å‹æšä¸¾
 */
enum class SchedulerError {
    SCHEDULER_NOT_RUNNING,       ///< è°ƒåº¦å™¨æœªè¿è¡Œ
    REQUEST_NOT_FOUND,           ///< è¯·æ±‚æœªæ‰¾åˆ°
    REQUEST_TIMEOUT,             ///< è¯·æ±‚è¶…æ—¶
    REQUEST_QUEUE_FULL,          ///< è¯·æ±‚é˜Ÿåˆ—å·²æ»¡
    BATCH_PROCESSING_FAILED,     ///< æ‰¹å¤„ç†å¤±è´¥
    INVALID_REQUEST              ///< æ— æ•ˆè¯·æ±‚
};

/**
 * @brief è°ƒåº¦å™¨å¼‚å¸¸ç±»
 */
class SchedulerException : public std::runtime_error {
public:
    /**
     * @brief æ„é€ å‡½æ•°
     * @param error é”™è¯¯ç±»å‹
     * @param message é”™è¯¯æ¶ˆæ¯
     */
    SchedulerException(SchedulerError error, const std::string& message)
        : std::runtime_error(message), error_(error) {}
    
    /**
     * @brief è·å–é”™è¯¯ç±»å‹
     * @return é”™è¯¯ç±»å‹
     */
    SchedulerError getError() const { return error_; }
    
private:
    SchedulerError error_;  ///< é”™è¯¯ç±»å‹
};

/**
 * @brief Phase 7: å“åº”å›è°ƒå‡½æ•°ç±»å‹
 * @param requestId è¯·æ±‚ID
 * @param state è¯·æ±‚çŠ¶æ€
 */
using ResponseCallback = std::function<void(size_t requestId, const RequestState& state)>;

/**
 * @brief æ‰¹å¤„ç†æ± ï¼ˆä¼˜åŒ–ï¼šå‡å°‘å†…å­˜åˆ†é…ï¼‰
 * 
 * é¢„åˆ†é…æ‰¹å¤„ç†å¯¹è±¡ï¼Œé¿å…é¢‘ç¹çš„å†…å­˜åˆ†é…å’Œé‡Šæ”¾ã€‚
 * æé«˜CPUç¼“å­˜å‹å¥½æ€§ï¼Œå‡å°‘å†…å­˜ç¢ç‰‡ã€‚
 */
class BatchPool {
private:
    static constexpr size_t POOL_SIZE = 16;
    static constexpr size_t BATCH_CAPACITY = 32;
    
    std::array<std::vector<RequestState>, POOL_SIZE> pool_;
    std::atomic<size_t> nextIndex_{0};
    
public:
    BatchPool() {
        for (auto& batch : pool_) {
            batch.reserve(BATCH_CAPACITY);
        }
    }
    
    /**
     * @brief ä»æ± ä¸­è·å–ä¸€ä¸ªæ‰¹å¤„ç†å¯¹è±¡
     * @return æ‰¹å¤„ç†å¯¹è±¡çš„å¼•ç”¨
     */
    std::vector<RequestState>& acquire() {
        size_t index = nextIndex_.fetch_add(1, std::memory_order_relaxed) % POOL_SIZE;
        auto& batch = pool_[index];
        batch.clear();
        return batch;
    }
    
    /**
     * @brief é‡Šæ”¾æ‰¹å¤„ç†å¯¹è±¡ï¼ˆå®é™…ä¸Šä¸éœ€è¦åšä»»ä½•äº‹ï¼‰
     * @param batch æ‰¹å¤„ç†å¯¹è±¡
     */
    void release(std::vector<RequestState>& batch) {
        // ä¸éœ€è¦åšä»»ä½•äº‹ï¼Œå¯¹è±¡åœ¨æ± ä¸­å¤ç”¨
        (void)batch;
    }
};

/**
 * @brief è°ƒåº¦å™¨ç±»
 * 
 * è´Ÿè´£è¯·æ±‚çš„è°ƒåº¦ã€æ‰¹å¤„ç†å’Œæ‰§è¡Œç®¡ç†ã€‚
 * ç»´æŠ¤è¯·æ±‚é˜Ÿåˆ—ï¼Œåè°ƒæ¨¡å‹æ‰§è¡Œå™¨å’ŒKVç¼“å­˜ï¼Œå¤„ç†å¤šä¸ªå¹¶å‘è¯·æ±‚ã€‚
 */
class Scheduler {
public:
    /**
     * @brief æ„é€ å‡½æ•°
     * @param modelExecutor æ¨¡å‹æ‰§è¡Œå™¨å®ä¾‹
     * @param maxBatchSize æœ€å¤§æ‰¹å¤„ç†å¤§å°
     * @param maxContextLength æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
     */
    Scheduler(
        ModelExecutor* modelExecutor,
        size_t maxBatchSize = 8,
        size_t maxContextLength = 2048
    );
    
    /**
     * @brief æ„é€ å‡½æ•°ï¼ˆå…¼å®¹æ—§æ¥å£ï¼Œä»…ç”¨äºæµ‹è¯•ï¼‰
     * @param modelPath æ¨¡å‹è·¯å¾„
     * @param quantization é‡åŒ–ç±»å‹
     * @param maxBatchSize æœ€å¤§æ‰¹å¤„ç†å¤§å°
     * @param maxContextLength æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
     */
    Scheduler(
        const std::string& modelPath,
        const std::string& quantization = "",
        size_t maxBatchSize = 8,
        size_t maxContextLength = 2048
    );

    
    /**
     * @brief ææ„å‡½æ•°
     */
    ~Scheduler();
    
    /**
     * @brief å¯åŠ¨è°ƒåº¦å™¨
     */
    void start();
    
    /**
     * @brief åœæ­¢è°ƒåº¦å™¨
     */
    void stop();
    
    /**
     * @brief æ·»åŠ è¯·æ±‚åˆ°é˜Ÿåˆ—
     * @param request è¯·æ±‚çŠ¶æ€å¯¹è±¡
     * @return è¯·æ±‚ID
     */
    size_t addRequest(const RequestState& request);
    
    /**
     * @brief ç§»é™¤è¯·æ±‚
     * @param requestId è¯·æ±‚ID
     * @return true å¦‚æœæˆåŠŸç§»é™¤ï¼Œfalse å¦åˆ™
     */
    bool removeRequest(size_t requestId);
    
    /**
     * @brief è·å–è¯·æ±‚ç»“æœ
     * @param requestId è¯·æ±‚ID
     * @return è¯·æ±‚çŠ¶æ€å¯¹è±¡
     */
    RequestState getRequestResult(size_t requestId);
    
    /**
     * @brief ç­‰å¾…è¯·æ±‚å®Œæˆ
     * @param requestId è¯·æ±‚ID
     * @param timeout è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
     * @return true å¦‚æœè¯·æ±‚å®Œæˆï¼Œfalse å¦‚æœè¶…æ—¶
     */
    bool waitForRequest(size_t requestId, float timeout = 300.0f);
    
    /**
     * @brief è·å–æ‰€æœ‰è¿è¡Œä¸­çš„è¯·æ±‚
     * @return è¿è¡Œä¸­è¯·æ±‚çš„å‘é‡
     */
    std::vector<RequestState> getRunningRequests() const;
    
    /**
     * @brief è·å–æ‰€æœ‰å·²å®Œæˆçš„è¯·æ±‚
     * @return å·²å®Œæˆè¯·æ±‚çš„å‘é‡
     */
    std::vector<RequestState> getCompletedRequests() const;
    
    /**
     * @brief è·å–é˜Ÿåˆ—å¤§å°
     * @return é˜Ÿåˆ—ä¸­ç­‰å¾…çš„è¯·æ±‚æ•°é‡
     */
    size_t getQueueSize() const;
    
    /**
     * @brief è·å–ç»Ÿè®¡ä¿¡æ¯
     * @return è°ƒåº¦å™¨ç»Ÿè®¡ä¿¡æ¯
     */
    SchedulerStats getStats() const;
    
    /**
     * @brief é‡ç½®ç»Ÿè®¡ä¿¡æ¯
     */
    void resetStats();
    
    /**
     * @brief Phase 6: è·å–è¿è¡Œä¸­è¯·æ±‚æ•°é‡
     * @return è¿è¡Œä¸­è¯·æ±‚æ•°é‡
     */
    size_t getRunningCount() const;
    
    /**
     * @brief Phase 6: è·å–æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
     * @return æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
     */
    size_t getMaxConcurrentRequests() const;
    
    /**
     * @brief Phase 7: è®¾ç½®å“åº”å›è°ƒå‡½æ•°
     * @param callback å›è°ƒå‡½æ•°
     */
    void setResponseCallback(ResponseCallback callback);
    
    // Phase 7: è§¦å‘å“åº”å›è°ƒï¼ˆä¾›å†…éƒ¨ä½¿ç”¨ï¼‰
    void triggerResponseCallback(size_t requestId, const RequestState& state);
    
private:
    void schedulerLoop();  ///< è°ƒåº¦å™¨ä¸»å¾ªç¯
    void processRequests();  ///< å¤„ç†è¯·æ±‚
    void processBatch(std::vector<RequestState>& batch);  ///< å¤„ç†æ‰¹æ¬¡
    void checkRequestTimeout();  ///< Phase 3: æ£€æŸ¥è¯·æ±‚è¶…æ—¶
    void checkKVCachEviction();  ///< Phase 5: æ£€æŸ¥KVç¼“å­˜æ·˜æ±°
    size_t getCurrentTime();  ///< è·å–å½“å‰æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    
    /**
     * @brief åˆ¤æ–­åç«¯æ˜¯å¦éœ€è¦å¤–éƒ¨ KVCache
     * @param backendType åç«¯ç±»å‹
     * @return true å¦‚æœéœ€è¦å¤–éƒ¨ KVCacheï¼Œfalse å¦åˆ™ï¼ˆå¦‚ llama.cpp åç«¯å†…éƒ¨ç®¡ç†ï¼‰
     */
    bool needsExternalKVCache(const std::string& backendType) const;
    
    RequestQueue requestQueue_;        ///< è¯·æ±‚é˜Ÿåˆ—
    BatchManager batchManager_;        ///< æ‰¹å¤„ç†ç®¡ç†å™¨
    ModelExecutor* modelExecutor_;  ///< æ¨¡å‹æ‰§è¡Œå™¨
    KVCache* kvCache_ = nullptr;    ///< KVç¼“å­˜ï¼ˆllama.cpp åç«¯ä¸º nullptrï¼‰
    bool ownsModelExecutor_;        ///< æ˜¯å¦æ‹¥æœ‰æ¨¡å‹æ‰§è¡Œå™¨æ‰€æœ‰æƒ
    RequestTracker requestTracker_;    ///< è¯·æ±‚è·Ÿè¸ªå™¨
    
    BatchPool batchPool_;  ///< æ‰¹å¤„ç†æ± ï¼ˆä¼˜åŒ–ï¼šå‡å°‘å†…å­˜åˆ†é…ï¼‰
    
    std::map<size_t, RequestState> runningRequests_;    ///< è¿è¡Œä¸­çš„è¯·æ±‚
    std::map<size_t, RequestState> completedRequests_;  ///< å·²å®Œæˆçš„è¯·æ±‚
    
    std::thread schedulerThread_;      ///< è°ƒåº¦å™¨çº¿ç¨‹
    std::thread cleanupThread_;        ///< å¼‚æ­¥æ¸…ç†çº¿ç¨‹
    std::atomic<bool> running_{false}; ///< è¿è¡ŒçŠ¶æ€
    
    size_t maxBatchSize_;              ///< æœ€å¤§æ‰¹å¤„ç†å¤§å°
    size_t maxContextLength_;          ///< æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
    SchedulerConfig config_;           ///< è°ƒåº¦å™¨é…ç½®
    
    mutable std::mutex queueMutex_;     ///< é˜Ÿåˆ—äº’æ–¥é”
    mutable std::shared_mutex requestsMutex_;  ///< è¯·æ±‚è¯»å†™é”ï¼ˆä¼˜åŒ–ï¼šè¯»å¤šå†™å°‘åœºæ™¯ï¼‰
    mutable std::mutex statsMutex_;     ///< ç»Ÿè®¡äº’æ–¥é”
    std::condition_variable_any resultCondition_;  ///< ç»“æœæ¡ä»¶å˜é‡ï¼ˆä¼˜åŒ–ï¼šæ”¯æŒshared_mutexï¼‰
    
    // ğŸ”¥ ä¼˜åŒ–æ­¥éª¤1: åŸå­æ“ä½œåªè¯»ç¼“å­˜ï¼ˆå‡å°‘é”ç«äº‰ï¼‰
    std::atomic<size_t> cachedQueueSize_{0};      ///< é˜Ÿåˆ—å¤§å°ç¼“å­˜ï¼ˆåŸå­æ“ä½œï¼Œå¿«é€Ÿè¯»å–ï¼‰
    std::atomic<size_t> cachedRunningCount_{0};   ///< è¿è¡Œä¸­è¯·æ±‚æ•°ç¼“å­˜ï¼ˆåŸå­æ“ä½œï¼Œå¿«é€Ÿè¯»å–ï¼‰
    std::condition_variable queueCondition_;   ///< é˜Ÿåˆ—æ¡ä»¶å˜é‡
    
    SchedulerStats stats_;             ///< ç»Ÿè®¡ä¿¡æ¯
    
    // Phase 7: å“åº”å›è°ƒ
    ResponseCallback responseCallback_;  ///< å“åº”å›è°ƒå‡½æ•°
    mutable std::mutex callbackMutex_;   ///< å›è°ƒäº’æ–¥é”
    
    // å¼‚æ­¥èµ„æºæ¸…ç†
    std::queue<size_t> cleanupQueue_;    ///< æ¸…ç†ä»»åŠ¡é˜Ÿåˆ—
    mutable std::mutex cleanupMutex_;    ///< æ¸…ç†é˜Ÿåˆ—äº’æ–¥é”
    std::condition_variable cleanupCondition_;  ///< æ¸…ç†æ¡ä»¶å˜é‡
    void cleanupLoop();                 ///< æ¸…ç†çº¿ç¨‹å¾ªç¯
    void cleanupRequestAsync(size_t requestId);  ///< å¼‚æ­¥æ¸…ç†è¯·æ±‚èµ„æº
    
    std::unique_ptr<HybridBatchStrategy> hybridStrategy_;  ///< æ··åˆæ‰¹å¤„ç†ç­–ç•¥
    size_t staticBatchSize_;  ///< é™æ€æ‰¹å¤„ç†ç­–ç•¥çš„å›ºå®š batch size
};

}

#endif
