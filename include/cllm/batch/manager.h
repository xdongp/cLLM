/**
 * @file manager.h
 * @brief æ‰¹å¤„ç†ç®¡ç†å™¨ï¼Œè´Ÿè´£æ‰¹æ¬¡ç»„è£…å’Œå¤„ç†
 * @author cLLM Team
 * @date 2024-01-09
 */

#ifndef CLLM_BATCH_MANAGER_H
#define CLLM_BATCH_MANAGER_H

#include "cllm/common/request_state.h"
#include "cllm/batch/input.h"
#include "cllm/batch/output.h"
#include "cllm/batch/stats.h"
#include "cllm/sampler.h"
#include "cllm/common/config.h"
#include <vector>
#include <mutex>
#include <cstddef>

namespace cllm {

struct RequestState;
class ModelExecutor;

/**
 * @brief æ‰¹å¤„ç†ç®¡ç†å™¨ç±»
 * 
 * è´Ÿè´£å°†è¯·æ±‚ç»„è£…æˆæ‰¹æ¬¡ï¼Œå‡†å¤‡æ‰¹å¤„ç†è¾“å…¥ï¼Œå¤„ç†æ‰¹å¤„ç†è¾“å‡ºã€‚
 * æ”¯æŒåŠ¨æ€æ‰¹å¤„ç†å¤§å°å’Œä¸Šä¸‹æ–‡é•¿åº¦ç®¡ç†ã€‚
 */
class BatchManager {
public:
    /**
     * @brief æ„é€ å‡½æ•°
     * @param maxContextLength æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
     * @param maxBatchSize æœ€å¤§æ‰¹å¤„ç†å¤§å°
     */
    explicit BatchManager(size_t maxContextLength, size_t maxBatchSize = 32);
    
    /**
     * @brief æ„é€ å‡½æ•°ï¼ˆå¸¦æ¨¡å‹æ‰§è¡Œå™¨ï¼‰
     * @param maxContextLength æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
     * @param maxBatchSize æœ€å¤§æ‰¹å¤„ç†å¤§å°
     * @param executor æ¨¡å‹æ‰§è¡Œå™¨æŒ‡é’ˆ
     */
    explicit BatchManager(size_t maxContextLength, size_t maxBatchSize, ModelExecutor* executor);
    
    /**
     * @brief ææ„å‡½æ•°
     */
    ~BatchManager();
    
    /**
     * @brief ç¦æ­¢æ‹·è´æ„é€ 
     */
    BatchManager(const BatchManager&) = delete;
    
    /**
     * @brief ç¦æ­¢æ‹·è´èµ‹å€¼
     */
    BatchManager& operator=(const BatchManager&) = delete;
    
    /**
     * @brief ç»„è£…ä¸€ä¸ªæ‰¹æ¬¡
     * @param pendingRequests å¾…å¤„ç†è¯·æ±‚
     * @param runningRequests è¿è¡Œä¸­è¯·æ±‚
     * @param availableSeqIds å¯ç”¨çš„åºåˆ—IDæ•°é‡ï¼ˆç”¨äºé™åˆ¶æ‰¹å¤„ç†å¤§å°ï¼Œ0è¡¨ç¤ºä¸é™åˆ¶ï¼‰
     * @return ç»„è£…å¥½çš„æ‰¹æ¬¡è¯·æ±‚
     */
    std::vector<RequestState> formBatch(
        const std::vector<RequestState>& pendingRequests,
        const std::vector<RequestState>& runningRequests,
        size_t availableSeqIds = 0
    );
    
    /**
     * @brief ç»„è£…å¤šä¸ªæ‰¹æ¬¡
     * @param pendingRequests å¾…å¤„ç†è¯·æ±‚
     * @param runningRequests è¿è¡Œä¸­è¯·æ±‚
     * @return ç»„è£…å¥½çš„å¤šä¸ªæ‰¹æ¬¡
     */
    std::vector<RequestState> formMultipleBatches(
        const std::vector<RequestState>& pendingRequests,
        const std::vector<RequestState>& runningRequests
    );
    
    /**
     * @brief å‡†å¤‡æ‰¹å¤„ç†è¾“å…¥
     * @param batch è¯·æ±‚æ‰¹æ¬¡
     * @return æ‰¹å¤„ç†è¾“å…¥æ•°æ®
     */
    BatchInput prepareBatchInput(const std::vector<RequestState>& batch);
    
    /**
     * @brief ğŸ”¥ ä¼˜åŒ–: å¢é‡å‡†å¤‡æ‰¹å¤„ç†è¾“å…¥ï¼ˆåªæ›´æ–°æ–°å¢çš„tokensï¼Œå‡å°‘æ•°æ®å¤åˆ¶ï¼‰
     * @param batch è¯·æ±‚æ‰¹æ¬¡ï¼ˆåŒ…å«æ›´æ–°åçš„generatedTokensï¼‰
     * @param previousInput ä¸Šæ¬¡çš„æ‰¹å¤„ç†è¾“å…¥ï¼ˆç”¨äºå¢é‡æ›´æ–°ï¼‰
     * @param previousTokenCounts ä¸Šæ¬¡æ¯ä¸ªè¯·æ±‚çš„tokenæ•°é‡ï¼ˆç”¨äºæ£€æµ‹å˜åŒ–ï¼‰
     * @return æ›´æ–°åçš„æ‰¹å¤„ç†è¾“å…¥æ•°æ®
     */
    BatchInput prepareBatchInputIncremental(
        const std::vector<RequestState>& batch,
        const BatchInput& previousInput,
        const std::vector<size_t>& previousTokenCounts
    );
    
    /**
     * @brief å¤„ç†æ‰¹å¤„ç†è¾“å‡º
     * @param batch è¯·æ±‚æ‰¹æ¬¡
     * @param output æ‰¹å¤„ç†è¾“å‡ºæ•°æ®
     */
    void processBatchOutput(
        std::vector<RequestState>& batch,
        const BatchOutput& output
    );
    
    /**
     * @brief è®¡ç®—æœ€ä¼˜æ‰¹å¤„ç†å¤§å°
     * @param requests è¯·æ±‚åˆ—è¡¨
     * @param avgRequestLength å¹³å‡è¯·æ±‚é•¿åº¦
     * @return æœ€ä¼˜æ‰¹å¤„ç†å¤§å°
     */
    size_t calculateOptimalBatchSize(
        const std::vector<RequestState>& requests,
        size_t avgRequestLength
    );

    /**
     * @brief æ›´æ–°æœ€å¤§æ‰¹å¤„ç†å¤§å°
     * @param maxBatchSize æœ€å¤§æ‰¹å¤„ç†å¤§å°
     */
    void setMaxBatchSize(size_t maxBatchSize);

    /**
     * @brief è·å–å½“å‰æœ€å¤§æ‰¹å¤„ç†å¤§å°
     * @return æœ€å¤§æ‰¹å¤„ç†å¤§å°
     */
    size_t getMaxBatchSize() const;
    
    /**
     * @brief æ ¹æ®ç³»ç»Ÿè´Ÿè½½åŠ¨æ€è°ƒæ•´æ‰¹å¤„ç†å¤§å°
     * @param queueSize é˜Ÿåˆ—å¤§å°
     * @param runningCount è¿è¡Œä¸­è¯·æ±‚æ•°
     * @return åŠ¨æ€æ‰¹å¤„ç†å¤§å°
     */
    size_t adaptiveBatchSize(size_t queueSize, size_t runningCount);
    
    /**
     * @brief æ›´æ–°æ‰¹å¤„ç†æ—¶é—´
     * @param processingTimeMs æ‰¹å¤„ç†æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
     */
    void updateBatchProcessingTime(size_t processingTimeMs);
    
    /**
     * @brief æ£€æŸ¥æ˜¯å¦å¯ä»¥å°†è¯·æ±‚æ·»åŠ åˆ°æ‰¹æ¬¡
     * @param request è¯·æ±‚å¯¹è±¡
     * @param currentBatch å½“å‰æ‰¹æ¬¡
     * @param currentBatchLength å½“å‰æ‰¹æ¬¡é•¿åº¦
     * @param dynamicBatchSize åŠ¨æ€æ‰¹å¤„ç†å¤§å°
     * @return true å¦‚æœå¯ä»¥æ·»åŠ ï¼Œfalse å¦åˆ™
     */
    bool canAddToBatch(
        const RequestState& request,
        const std::vector<RequestState>& currentBatch,
        size_t currentBatchLength,
        size_t dynamicBatchSize
    );
    
    /**
     * @brief è·å–ç»Ÿè®¡ä¿¡æ¯
     * @return æ‰¹å¤„ç†ç»Ÿè®¡ä¿¡æ¯
     */
    BatchStats getStats() const;
    
    /**
     * @brief é‡ç½®ç»Ÿè®¡ä¿¡æ¯
     */
    void resetStats();
    
private:
    size_t calculateRunningRequestsLength(  ///< è®¡ç®—è¿è¡Œä¸­è¯·æ±‚çš„æ€»é•¿åº¦
        const std::vector<RequestState>& runningRequests
    );
    
    size_t calculateAverageRequestLength(  ///< è®¡ç®—å¹³å‡è¯·æ±‚é•¿åº¦
        const std::vector<RequestState>& requests
    );
    
    void updateStats(const std::vector<RequestState>& batch);  ///< æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
    
    void checkStoppingConditions(RequestState& request, int nextToken);  ///< æ£€æŸ¥åœæ­¢æ¡ä»¶
    
    size_t maxContextLength_;       ///< æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
    size_t maxBatchSize_;           ///< æœ€å¤§æ‰¹å¤„ç†å¤§å°
    float contextUsageThreshold_;   ///< ä¸Šä¸‹æ–‡ä½¿ç”¨é˜ˆå€¼
    
    Sampler sampler_;               ///< é‡‡æ ·å™¨
    ModelExecutor* executor_;       ///< æ¨¡å‹æ‰§è¡Œå™¨æŒ‡é’ˆ
    
    mutable std::mutex statsMutex_; ///< ç»Ÿè®¡ä¿¡æ¯äº’æ–¥é”
    BatchStats stats_;              ///< ç»Ÿè®¡ä¿¡æ¯
    
    size_t lastBatchProcessingTimeMs_;  ///< ä¸Šæ¬¡æ‰¹å¤„ç†æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    size_t adaptiveBatchSize_;          ///< è‡ªé€‚åº”æ‰¹å¤„ç†å¤§å°
    size_t minAdaptiveBatchSize_;      ///< æœ€å°è‡ªé€‚åº”æ‰¹å¤„ç†å¤§å°
    size_t maxAdaptiveBatchSize_;      ///< æœ€å¤§è‡ªé€‚åº”æ‰¹å¤„ç†å¤§å°
};

}

#endif
