/**
 * @file llama_cpp_backend.h
 * @brief llama.cpp æ¨ç†åç«¯å®ç°ï¼ˆä½¿ç”¨ llama.cpp C APIï¼‰
 * 
 * å‚è€ƒæ–‡æ¡£ï¼šllama.cppåç«¯é›†æˆè®¾è®¡.md
 */
#pragma once

#include "cllm/inference/backend_interface.h"
#include "cllm/inference/kv_cache_manager.h"
#include "cllm/kylin/core/tensor.h"
#include "cllm/model/config.h"
#include "cllm/tokenizer/gguf_tokenizer.h"

#include <memory>
#include <string>
#include <vector>

// Forward declarations for llama.cpp types
// æ³¨æ„ï¼šå®é™…ç±»å‹å®šä¹‰åœ¨ llama.h ä¸­
// åœ¨å®ç°æ–‡ä»¶ä¸­åŒ…å« llama.h
struct llama_model;
struct llama_context;
struct llama_model_params;
struct llama_context_params;

namespace cllm {
namespace inference {

/**
 * @brief llama.cpp æ¨ç†åç«¯
 *
 * ä½¿ç”¨ llama.cpp C API è¿›è¡Œæ¨ç†ï¼š
 * - åŠ è½½ GGUF æ ¼å¼æ¨¡å‹
 * - æ”¯æŒå¤šç§é‡åŒ–æ ¼å¼ï¼ˆQ4_K_M, Q8_0, F16, F32 ç­‰ï¼‰
 * - åˆ©ç”¨ llama.cpp çš„ä¼˜åŒ–å®ç°
 * - æ”¯æŒ GPU åŠ é€Ÿï¼ˆMetal/CUDAï¼Œå¦‚æœç¼–è¯‘æ—¶å¯ç”¨ï¼‰
 * 
 * å®ç° IBackend æ¥å£ï¼Œå¯ä¸ Kylinã€LibTorch åç«¯æ— ç¼åˆ‡æ¢
 */
class LlamaCppBackend : public IBackend {
public:
    /**
     * @brief æ„é€ å‡½æ•°
     * @param config æ¨¡å‹é…ç½®
     * @param modelPath GGUF æ¨¡å‹è·¯å¾„ï¼ˆ.gguf æ–‡ä»¶ï¼‰
     */
    explicit LlamaCppBackend(const ModelConfig &config, const std::string &modelPath);

    /**
     * @brief ææ„å‡½æ•°
     */
    ~LlamaCppBackend() override;

    // ========== IBackend æ¥å£å®ç° ==========

    /**
     * @brief åˆå§‹åŒ–åŠ è½½æ¨¡å‹
     * @return true æˆåŠŸï¼Œfalse å¤±è´¥
     */
    bool initialize() override;

    /**
     * @brief å•åºåˆ—å‰å‘æ¨ç†
     * @param inputIds è¾“å…¥ token id åºåˆ—
     * @return [seq_len, vocab_size] logits å¼ é‡
     */
    Tensor forward(const std::vector<int> &inputIds) override;

    /**
     * @brief æ‰¹å¤„ç†å‰å‘æ¨ç†
     * @param flatInputIds å±•å¹³åçš„æ‰€æœ‰ token id
     * @param requestPositions æ¯ä¸ªè¯·æ±‚çš„èµ·æ­¢ä½ç½®
     * @param batchSize æ‰¹å¤§å°
     * @param sequenceIds æ¯ä¸ªè¯·æ±‚çš„åºåˆ—IDï¼ˆrequestIdï¼‰ï¼Œç”¨äºåºåˆ—IDç®¡ç†
     * @return [total_tokens, vocab_size] logits å¼ é‡
     */
    Tensor forwardBatch(
        const std::vector<int> &flatInputIds,
        const std::vector<std::pair<size_t, size_t>> &requestPositions,
        size_t batchSize,
        const std::vector<size_t> &sequenceIds = {}
    ) override;

    /**
     * @brief è·å–åç«¯åç§°
     */
    std::string getName() const override { return "llama.cpp"; }

    /**
     * @brief è·å–æ¨¡å‹æ˜¯å¦å·²åŠ è½½
     */
    bool isInitialized() const override { return initialized_; }

    /**
     * @brief è·å–æ¨¡å‹é…ç½®
     */
    const ModelConfig &getConfig() const override { return config_; }

    /**
     * @brief è®¾ç½®çº¿ç¨‹æ•°ï¼ˆCPU æ¨ç†ï¼‰
     * 
     * @param numThreads çº¿ç¨‹æ•°ï¼Œ0 è¡¨ç¤ºä½¿ç”¨é»˜è®¤å€¼
     */
    void setNumThreads(int numThreads);

    /**
     * @brief è®¾ç½® GPU å±‚æ•°ï¼ˆGPU åŠ é€Ÿï¼‰
     * 
     * @param nGpuLayers GPU å±‚æ•°ï¼Œ0 è¡¨ç¤ºä»…ä½¿ç”¨ CPU
     */
    void setNGpuLayers(int nGpuLayers);

    /**
     * @brief ä¸ºè¯·æ±‚åˆ†é…åºåˆ—ID
     * @param requestId è¯·æ±‚ID
     * @return åˆ†é…çš„åºåˆ—IDï¼Œå¦‚æœå¤±è´¥è¿”å› -1
     */
    int32_t allocateSequenceId(size_t requestId);

    /**
     * @brief é‡Šæ”¾è¯·æ±‚çš„åºåˆ—ID
     * @param requestId è¯·æ±‚ID
     * @return true å¦‚æœæˆåŠŸé‡Šæ”¾ï¼Œfalse å¦åˆ™
     */
    bool releaseSequenceId(size_t requestId);

    /**
     * @brief è·å–è¯·æ±‚å¯¹åº”çš„åºåˆ—ID
     * @param requestId è¯·æ±‚ID
     * @return åºåˆ—IDï¼Œå¦‚æœæœªåˆ†é…è¿”å› -1
     */
    int32_t getSequenceId(size_t requestId) const;

    /**
     * @brief æ¸…ç†è¯·æ±‚çš„KVç¼“å­˜ï¼ˆPhase 4ï¼‰
     * @param requestId è¯·æ±‚ID
     * @return true å¦‚æœæˆåŠŸæ¸…ç†ï¼Œfalse å¦åˆ™
     * 
     * åè°ƒ KV ç¼“å­˜ç®¡ç†å™¨æ¸…ç† llama.cpp ä¸­çš„ KV ç¼“å­˜ã€‚
     */
    bool cleanupKVCache(size_t requestId);

    /**
     * @brief è·å–KVç¼“å­˜ç®¡ç†å™¨ï¼ˆPhase 4ï¼‰
     * @return KVç¼“å­˜ç®¡ç†å™¨çš„æŒ‡é’ˆ
     */
    KVCacheManager* getKVCacheManager() { return kvCacheManager_.get(); }

    /**
     * @brief è·å– llama.cpp ä¸Šä¸‹æ–‡å¥æŸ„ï¼ˆPhase 5ï¼‰
     * @return llama.cpp ä¸Šä¸‹æ–‡å¥æŸ„
     */
    ::llama_context* getContext() { return ctx_; }

    /**
     * @brief ç›‘æ§åºåˆ—IDæ± ä½¿ç”¨æƒ…å†µ
     * @return åºåˆ—IDæ± ä½¿ç”¨ç‡ï¼ˆ0.0-1.0ï¼‰
     * 
     * ç”¨äºç›‘æ§åºåˆ—IDæ± çš„ä½¿ç”¨æƒ…å†µï¼Œå½“ä½¿ç”¨ç‡è¶…è¿‡é˜ˆå€¼æ—¶è®°å½•è­¦å‘Šã€‚
     */
    double getSequenceIdPoolUsage() const;

    /**
     * @brief è·å–å¯ç”¨åºåˆ—IDæ•°é‡
     * @return å¯ç”¨åºåˆ—IDæ•°é‡
     */
    size_t getAvailableSequenceIdCount() const;

private:
    /**
     * @brief ä» ModelConfig åˆ›å»º llama_model_params
     */
    void createModelParams();

    /**
     * @brief ä» ModelConfig åˆ›å»º llama_context_params
     */
    void createContextParams();

    /**
     * @brief æ ¡éªŒ vocab_size ä¸€è‡´æ€§
     * @return true ä¸€è‡´ï¼Œfalse ä¸ä¸€è‡´
     */
    bool validateVocabSize();

    /**
     * @brief å°† std::vector<int> è½¬æ¢ä¸º llama_token æ•°ç»„
     */
    std::vector<int32_t> convertToLlamaTokens(const std::vector<int> &inputIds);

    /**
     * @brief åˆå§‹åŒ–åºåˆ—IDæ± 
     */
    void initializeSequenceIdPool();

    std::string modelPath_;              ///< GGUF æ¨¡å‹è·¯å¾„
    ModelConfig config_;                 ///< æ¨¡å‹é…ç½®
    ::llama_model* model_;         ///< llama.cpp æ¨¡å‹å¥æŸ„
    ::llama_context* ctx_;          ///< llama.cpp ä¸Šä¸‹æ–‡å¥æŸ„
    std::unique_ptr<GGUFTokenizer> tokenizer_; ///< GGUF tokenizerï¼ˆç”¨äºç¼–ç /è§£ç ï¼‰
    bool initialized_;                   ///< æ˜¯å¦å·²åˆå§‹åŒ–
    
    // llama.cpp å‚æ•°ç»“æ„ï¼ˆåœ¨ initialize æ—¶åˆ›å»ºï¼‰
    // ä½¿ç”¨æŒ‡é’ˆé¿å…åœ¨å¤´æ–‡ä»¶ä¸­éœ€è¦å®Œæ•´ç±»å‹å®šä¹‰
    ::llama_model_params* modelParams_;
    ::llama_context_params* contextParams_;
    
    int numThreads_;                     ///< CPU çº¿ç¨‹æ•°
    int nGpuLayers_;                     ///< GPU å±‚æ•°
    size_t currentPosition_;              ///< å½“å‰æ¨ç†ä½ç½®ï¼ˆç”¨äºå¢é‡æ¨ç†ï¼‰

    // Phase 2: åºåˆ—IDç®¡ç†
    // ğŸ”¥ ä¼˜åŒ–: ä½¿ç”¨æ— é”æ•°æ®ç»“æ„å‡å°‘é”ç«äº‰
    // ä½¿ç”¨åŸå­æ“ä½œå’Œthread_localç¼“å­˜æ¥ä¼˜åŒ–åºåˆ—IDåˆ†é…
    std::map<size_t, int32_t> requestIdToSeqId_;  ///< requestId åˆ° seqId çš„æ˜ å°„ï¼ˆéœ€è¦é”ä¿æŠ¤ï¼‰
    std::vector<int32_t> availableSeqIds_;         ///< å¯ç”¨åºåˆ—IDæ± ï¼ˆéœ€è¦é”ä¿æŠ¤ï¼‰
    mutable std::mutex sequenceIdMutex_;             ///< åºåˆ—IDç®¡ç†äº’æ–¥é”ï¼ˆmutableï¼Œå…è®¸ const æ–¹æ³•ä½¿ç”¨ï¼‰
    
    // ğŸ”¥ ä¼˜åŒ–: æ‰¹é‡åˆ†é…åºåˆ—IDï¼Œå‡å°‘é”ç«äº‰
    static constexpr size_t BATCH_ALLOCATION_SIZE = 8;  ///< æ‰¹é‡åˆ†é…å¤§å°
    std::atomic<size_t> nextSeqId_{0};                  ///< ä¸‹ä¸€ä¸ªåºåˆ—IDï¼ˆåŸå­æ“ä½œï¼Œç”¨äºå¿«é€Ÿåˆ†é…ï¼‰
    int32_t nSeqMax_;                              ///< æœ€å¤§åºåˆ—æ•°ï¼ˆä»é…ç½®è¯»å–ï¼‰
    
    // ğŸ”¥ åºåˆ—ä½ç½®è·Ÿè¸ªï¼šè·Ÿè¸ªæ¯ä¸ªåºåˆ—IDçš„å½“å‰æ€»é•¿åº¦ï¼ˆç´¯è®¡ä½ç½®ï¼‰
    // ç”¨äºç¡®ä¿ llama.cpp çš„ä½ç½®è¿ç»­æ€§è¦æ±‚
    std::map<int32_t, size_t> seqIdToPosition_;    ///< seqId åˆ°å½“å‰æ€»é•¿åº¦çš„æ˜ å°„ï¼ˆéœ€è¦é”ä¿æŠ¤ï¼‰

    // Phase 4: KVç¼“å­˜ç»Ÿè®¡ç®¡ç†
    std::unique_ptr<KVCacheManager> kvCacheManager_;  ///< KVç¼“å­˜ç»Ÿè®¡ç®¡ç†å™¨
};

} // namespace inference
} // namespace cllm
