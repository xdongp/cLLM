/**
 * @file libtorch_api_test.cpp
 * @brief LibTorch æ¨ç†å¼•æ“ API é›†æˆæµ‹è¯•
 * 
 * æ¨¡æ‹Ÿå®Œæ•´çš„ cLLM ç”Ÿæˆæµç¨‹ï¼š
 * Tokenizer â†’ InferenceEngine (LibTorch) â†’ Sampler â†’ TokenizerDecode
 */

#include "cllm/inference/inference_engine.h"
#include "cllm/model/config.h"
#include "cllm/tokenizer/manager.h"
#include "cllm/sampler.h"
#include "cllm/memory/float_array.h"
#include <iostream>
#include <vector>
#include <string>
#include <chrono>
#include <iomanip>
#include <thread>

using namespace cllm;
using namespace cllm::inference;

// åŠ è½½ Qwen3-0.6B æ¨¡å‹é…ç½®
ModelConfig loadQwen3Config() {
    ModelConfig config;
    config.vocabSize = 151936;
    config.hiddenSize = 1024;
    config.numLayers = 28;
    config.numAttentionHeads = 16;
    config.numKeyValueHeads = 8;
    config.intermediateSize = 3072;
    config.maxSequenceLength = 40960;
    config.modelType = "qwen";
    return config;
}

/**
 * @class TextGenerator
 * @brief æ–‡æœ¬ç”Ÿæˆå™¨ï¼Œé›†æˆæ‰€æœ‰ç»„ä»¶
 */
class TextGenerator {
public:
    TextGenerator(const std::string& modelPath, 
                 const std::string& tokenizerPath,
                 const ModelConfig& config)
        : modelPath_(modelPath)
        , tokenizerPath_(tokenizerPath)
        , config_(config)
        , initialized_(false) {}
    
    bool initialize() {
        try {
            std::cout << "\n========================================" << std::endl;
            std::cout << "  LibTorch API é›†æˆæµ‹è¯•åˆå§‹åŒ–" << std::endl;
            std::cout << "========================================\n" << std::endl;
            
            // 1. åˆå§‹åŒ– Tokenizer
            std::cout << "[1/3] åˆå§‹åŒ– Tokenizer..." << std::endl;
            tokenizer_ = std::make_unique<TokenizerManager>(tokenizerPath_, nullptr);
            if (tokenizer_->getTokenizer() == nullptr) {
                std::cerr << "  âœ— Tokenizer åˆå§‹åŒ–å¤±è´¥ï¼" << std::endl;
                return false;
            }
            std::cout << "  âœ“ Tokenizer åˆå§‹åŒ–æˆåŠŸ" << std::endl;
            
            // 2. åˆå§‹åŒ– InferenceEngine (LibTorch åç«¯)
            std::cout << "[2/3] åˆå§‹åŒ– InferenceEngine (LibTorch)..." << std::endl;
            engine_ = std::make_unique<InferenceEngine>(config_, modelPath_, true);
            if (!engine_->initialize()) {
                std::cerr << "  âœ— InferenceEngine åˆå§‹åŒ–å¤±è´¥ï¼" << std::endl;
                return false;
            }
            std::cout << "  âœ“ InferenceEngine åˆå§‹åŒ–æˆåŠŸ" << std::endl;
            
            // 3. åˆå§‹åŒ– Sampler
            std::cout << "[3/3] åˆå§‹åŒ– Sampler..." << std::endl;
            SamplerConfig samplerConfig;
            samplerConfig.setTemperature(0.8f);
            samplerConfig.setTopK(50);
            samplerConfig.setTopP(0.9f);
            sampler_ = std::make_unique<Sampler>(samplerConfig);
            std::cout << "  âœ“ Sampler åˆå§‹åŒ–æˆåŠŸ (temperature=0.8, top_k=50, top_p=0.9)" << std::endl;
            
            initialized_ = true;
            std::cout << "\nâœ“ æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–å®Œæˆ" << std::endl;
            std::cout << "========================================\n" << std::endl;
            return true;
            
        } catch (const std::exception& e) {
            std::cerr << "âœ— åˆå§‹åŒ–å¤±è´¥: " << e.what() << std::endl;
            return false;
        }
    }
    
    /**
     * @brief ç”Ÿæˆæ–‡æœ¬ï¼ˆè‡ªå›å½’ç”Ÿæˆï¼‰
     * @param prompt è¾“å…¥æç¤ºè¯
     * @param maxNewTokens æœ€å¤§ç”Ÿæˆ token æ•°
     * @return ç”Ÿæˆçš„å®Œæ•´æ–‡æœ¬
     */
    std::string generate(const std::string& prompt, int maxNewTokens = 20) {
        if (!initialized_) {
            throw std::runtime_error("TextGenerator not initialized");
        }
        
        std::cout << "\n========================================" << std::endl;
        std::cout << "  æ–‡æœ¬ç”Ÿæˆæµ‹è¯•" << std::endl;
        std::cout << "========================================" << std::endl;
        std::cout << "\nğŸ“ è¾“å…¥æç¤ºè¯: \"" << prompt << "\"" << std::endl;
        std::cout << "ğŸ¯ æœ€å¤§ç”Ÿæˆ tokens: " << maxNewTokens << "\n" << std::endl;
        
        // 1. Tokenize è¾“å…¥
        auto startEncode = std::chrono::high_resolution_clock::now();
        std::vector<int> tokenIds = tokenizer_->encode(prompt);
        auto endEncode = std::chrono::high_resolution_clock::now();
        auto encodeMs = std::chrono::duration_cast<std::chrono::microseconds>(endEncode - startEncode).count() / 1000.0;
        
        std::cout << "[Step 1] Tokenize è¾“å…¥" << std::endl;
        std::cout << "  - Token IDs: [";
        for (size_t i = 0; i < tokenIds.size() && i < 10; ++i) {
            std::cout << tokenIds[i];
            if (i < tokenIds.size() - 1 && i < 9) std::cout << ", ";
        }
        if (tokenIds.size() > 10) std::cout << ", ...";
        std::cout << "]" << std::endl;
        std::cout << "  - Token æ•°é‡: " << tokenIds.size() << std::endl;
        std::cout << "  - è€—æ—¶: " << encodeMs << " ms\n" << std::endl;
        
        // 2. è‡ªå›å½’ç”Ÿæˆ
        std::cout << "[Step 2] è‡ªå›å½’ç”Ÿæˆ" << std::endl;
        std::vector<int> generatedIds = tokenIds;
        std::vector<double> inferenceTime;
        std::vector<double> samplingTime;
        
        for (int step = 0; step < maxNewTokens; ++step) {
            // å‡†å¤‡è¾“å…¥ï¼ˆå–æœ€å 8 tokens æˆ–å¡«å……åˆ° 8ï¼‰
            std::vector<int> currentInput = prepareInput(generatedIds);
            
            // æ¨ç†
            auto startInfer = std::chrono::high_resolution_clock::now();
            Tensor logits = engine_->forward(currentInput);
            auto endInfer = std::chrono::high_resolution_clock::now();
            auto inferMs = std::chrono::duration_cast<std::chrono::milliseconds>(endInfer - startInfer).count();
            inferenceTime.push_back(inferMs);
            
            // è·å–æœ€åä¸€ä¸ªä½ç½®çš„ logits
            size_t lastPos = currentInput.size() - 1;
            FloatArray lastLogits(config_.vocabSize);
            for (size_t i = 0; i < config_.vocabSize; ++i) {
                lastLogits.data()[i] = logits.data()[lastPos * config_.vocabSize + i];
            }
            
            // é‡‡æ ·
            auto startSample = std::chrono::high_resolution_clock::now();
            int nextToken = sampler_->sample(lastLogits, 0.8f, 50, 0.9f);
            auto endSample = std::chrono::high_resolution_clock::now();
            auto sampleUs = std::chrono::duration_cast<std::chrono::microseconds>(endSample - startSample).count() / 1000.0;
            samplingTime.push_back(sampleUs);
            
            generatedIds.push_back(nextToken);
            
            // è§£ç å½“å‰ token
            std::string tokenText = tokenizer_->decode({nextToken});
            
            // å®æ—¶è¾“å‡º
            std::cout << "  Step " << std::setw(2) << (step + 1) << ": "
                      << "token_" << std::setw(6) << nextToken 
                      << " â†’ \"" << tokenText << "\" "
                      << "(æ¨ç†: " << inferMs << " ms, é‡‡æ ·: " << sampleUs << " ms)"
                      << std::endl;
            
            // æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†ç»“æŸ token
            if (nextToken == 151643 || nextToken == 2) {  // <|endoftext|> or EOS
                std::cout << "\n  âš ï¸  æ£€æµ‹åˆ°ç»“æŸ tokenï¼Œæå‰åœæ­¢ç”Ÿæˆ" << std::endl;
                break;
            }
        }
        
        // 3. è§£ç å®Œæ•´è¾“å‡º
        auto startDecode = std::chrono::high_resolution_clock::now();
        std::string generatedText = tokenizer_->decode(generatedIds);
        auto endDecode = std::chrono::high_resolution_clock::now();
        auto decodeMs = std::chrono::duration_cast<std::chrono::microseconds>(endDecode - startDecode).count() / 1000.0;
        
        std::cout << "\n[Step 3] è§£ç å®Œæ•´è¾“å‡º" << std::endl;
        std::cout << "  - ç”Ÿæˆ token æ•°: " << (generatedIds.size() - tokenIds.size()) << std::endl;
        std::cout << "  - æ€» token æ•°: " << generatedIds.size() << std::endl;
        std::cout << "  - è§£ç è€—æ—¶: " << decodeMs << " ms" << std::endl;
        
        // 4. æ€§èƒ½ç»Ÿè®¡
        std::cout << "\n[Step 4] æ€§èƒ½ç»Ÿè®¡" << std::endl;
        double totalInferMs = 0;
        double totalSampleMs = 0;
        for (double t : inferenceTime) totalInferMs += t;
        for (double t : samplingTime) totalSampleMs += t;
        
        double avgInferMs = inferenceTime.empty() ? 0 : totalInferMs / inferenceTime.size();
        double avgSampleMs = samplingTime.empty() ? 0 : totalSampleMs / samplingTime.size();
        
        std::cout << "  - æ€»æ¨ç†æ—¶é—´: " << totalInferMs << " ms" << std::endl;
        std::cout << "  - å¹³å‡æ¨ç†æ—¶é—´: " << avgInferMs << " ms/token" << std::endl;
        std::cout << "  - æ€»é‡‡æ ·æ—¶é—´: " << totalSampleMs << " ms" << std::endl;
        std::cout << "  - å¹³å‡é‡‡æ ·æ—¶é—´: " << avgSampleMs << " ms/token" << std::endl;
        std::cout << "  - ç«¯åˆ°ç«¯å»¶è¿Ÿ: " << (encodeMs + totalInferMs + totalSampleMs + decodeMs) << " ms" << std::endl;
        
        if (!inferenceTime.empty()) {
            double tokensPerSec = 1000.0 / avgInferMs;
            std::cout << "  - ååé‡: " << tokensPerSec << " tokens/sec" << std::endl;
        }
        
        std::cout << "\n========================================" << std::endl;
        std::cout << "ğŸ“„ ç”Ÿæˆç»“æœ:\n" << std::endl;
        std::cout << generatedText << std::endl;
        std::cout << "\n========================================\n" << std::endl;
        
        return generatedText;
    }
    
private:
    std::vector<int> prepareInput(const std::vector<int>& tokenIds) {
        std::vector<int> input;
        
        if (tokenIds.size() >= 8) {
            // å–æœ€å 8 ä¸ª tokens
            input = std::vector<int>(tokenIds.end() - 8, tokenIds.end());
        } else {
            // å¡«å……åˆ° 8 ä¸ª tokensï¼ˆå‰é¢å¡«å…… pad tokenï¼‰
            size_t padSize = 8 - tokenIds.size();
            input.resize(8);
            for (size_t i = 0; i < padSize; ++i) {
                input[i] = 151643;  // <|endoftext|>
            }
            for (size_t i = 0; i < tokenIds.size(); ++i) {
                input[padSize + i] = tokenIds[i];
            }
        }
        
        return input;
    }
    
    std::string modelPath_;
    std::string tokenizerPath_;
    ModelConfig config_;
    bool initialized_;
    
    std::unique_ptr<TokenizerManager> tokenizer_;
    std::unique_ptr<InferenceEngine> engine_;
    std::unique_ptr<Sampler> sampler_;
};

int main(int argc, char* argv[]) {
    std::cout << "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—" << std::endl;
    std::cout << "â•‘   LibTorch æ¨ç†å¼•æ“ API é›†æˆæµ‹è¯•      â•‘" << std::endl;
    std::cout << "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n" << std::endl;
    
    // é…ç½®è·¯å¾„
    const std::string modelPath = "/Users/dannypan/PycharmProjects/xllm/model/Qwen/qwen3_0.6b_torchscript_fp32.pt";
    const std::string tokenizerPath = "/Users/dannypan/PycharmProjects/xllm/model/Qwen/Qwen3-0.6B";
    
    // æµ‹è¯•ç”¨ä¾‹
    std::vector<std::string> testPrompts = {
        "Hello",
        "What is AI?",
        "The weather today is"
    };
    
    // ä»å‘½ä»¤è¡Œå‚æ•°è·å–æç¤ºè¯
    if (argc > 1) {
        testPrompts = {argv[1]};
    }
    
    try {
        // åˆå§‹åŒ–ç”Ÿæˆå™¨
        ModelConfig config = loadQwen3Config();
        TextGenerator generator(modelPath, tokenizerPath, config);
        
        if (!generator.initialize()) {
            std::cerr << "âŒ ç”Ÿæˆå™¨åˆå§‹åŒ–å¤±è´¥ï¼" << std::endl;
            return 1;
        }
        
        // è¿è¡Œæµ‹è¯•
        for (const auto& prompt : testPrompts) {
            try {
                std::string result = generator.generate(prompt, 10);
                
                // ç­‰å¾…ä¸€ä¸‹å†è¿›è¡Œä¸‹ä¸€ä¸ªæµ‹è¯•
                if (&prompt != &testPrompts.back()) {
                    std::cout << "\nç­‰å¾… 2 ç§’...\n" << std::endl;
                    std::this_thread::sleep_for(std::chrono::seconds(2));
                }
            } catch (const std::exception& e) {
                std::cerr << "âŒ ç”Ÿæˆå¤±è´¥: " << e.what() << std::endl;
            }
        }
        
        std::cout << "\nâœ“ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼" << std::endl;
        return 0;
        
    } catch (const std::exception& e) {
        std::cerr << "âŒ é”™è¯¯: " << e.what() << std::endl;
        return 1;
    }
}
