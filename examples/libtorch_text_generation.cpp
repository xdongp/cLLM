/**
 * @file libtorch_text_generation.cpp
 * @brief LibTorch æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹
 * 
 * å®Œæ•´æ¼”ç¤ºï¼šè¾“å…¥æ–‡æœ¬ -> Tokenize -> æ¨ç† -> é‡‡æ · -> è§£ç  -> è¾“å‡ºæ–‡æœ¬
 */

#include "cllm/inference/inference_engine.h"
#include "cllm/model/config.h"
#include "cllm/tokenizer/manager.h"
#include "cllm/sampler.h"
#include "cllm/memory/float_array.h"
#include <iostream>
#include <vector>
#include <string>
#include <chrono>

using namespace cllm;
using namespace cllm::inference;

// åŠ è½½ Qwen3-0.6B æ¨¡å‹é…ç½®
ModelConfig loadQwen3Config() {
    ModelConfig config;
    config.vocabSize = 151936;
    config.hiddenSize = 1024;
    config.numLayers = 28;
    config.numAttentionHeads = 16;
    config.numKeyValueHeads = 8;
    config.intermediateSize = 3072;
    config.maxSequenceLength = 40960;
    config.modelType = "qwen";
    return config;
}

int main(int argc, char* argv[]) {
    std::cout << "=== LibTorch æ–‡æœ¬ç”Ÿæˆæµ‹è¯• ===" << std::endl;
    
    // é…ç½®è·¯å¾„
    const std::string modelPath = "/Users/dannypan/PycharmProjects/xllm/model/Qwen/qwen3_0.6b_torchscript_fp32.pt";
    const std::string tokenizerPath = "/Users/dannypan/PycharmProjects/xllm/model/Qwen/Qwen3-0.6B";
    
    // è¾“å…¥æ–‡æœ¬
    std::string inputText = "Hello";
    if (argc > 1) {
        inputText = argv[1];
    }
    
    std::cout << "\nğŸ“ è¾“å…¥æ–‡æœ¬: \"" << inputText << "\"" << std::endl;
    
    // 1. åˆå§‹åŒ– Tokenizer
    std::cout << "\n[1] åˆå§‹åŒ– Tokenizer..." << std::endl;
    cllm::TokenizerManager tokenizer(tokenizerPath, nullptr);
    if (tokenizer.getTokenizer() == nullptr) {
        std::cerr << "âŒ Tokenizer åˆå§‹åŒ–å¤±è´¥ï¼" << std::endl;
        return 1;
    }
    std::cout << "  âœ“ Tokenizer åˆå§‹åŒ–æˆåŠŸ" << std::endl;
    
    // 2. ç¼–ç è¾“å…¥æ–‡æœ¬
    std::cout << "\n[2] ç¼–ç è¾“å…¥æ–‡æœ¬..." << std::endl;
    std::vector<int> inputIds = tokenizer.encode(inputText);
    std::cout << "  - åŸå§‹æ–‡æœ¬: \"" << inputText << "\"" << std::endl;
    std::cout << "  - Token IDs: [";
    for (size_t i = 0; i < inputIds.size(); ++i) {
        std::cout << inputIds[i];
        if (i < inputIds.size() - 1) std::cout << ", ";
    }
    std::cout << "]" << std::endl;
    std::cout << "  - Token æ•°é‡: " << inputIds.size() << std::endl;
    
    // æ˜¾ç¤ºæ¯ä¸ª token çš„è§£ç 
    std::cout << "  - Token è§£ç :" << std::endl;
    for (size_t i = 0; i < inputIds.size(); ++i) {
        std::string decoded = tokenizer.decode({inputIds[i]});
        std::cout << "    [" << i << "] " << inputIds[i] << " -> \"" << decoded << "\"" << std::endl;
    }
    
    // å¦‚æœè¾“å…¥å¤ªçŸ­ï¼Œå¡«å……åˆ° 8 ä¸ª tokensï¼ˆLibTorch trace å›ºå®šé•¿åº¦ï¼‰
    std::vector<int> paddedIds = inputIds;
    if (paddedIds.size() < 8) {
        std::cout << "\n  âš ï¸  è¾“å…¥é•¿åº¦ < 8ï¼Œå¡«å……åˆ° 8 tokensï¼ˆLibTorch trace é™åˆ¶ï¼‰" << std::endl;
        while (paddedIds.size() < 8) {
            paddedIds.push_back(151643);  // <|endoftext|> token
        }
        std::cout << "  - å¡«å……å: [";
        for (size_t i = 0; i < paddedIds.size(); ++i) {
            std::cout << paddedIds[i];
            if (i < paddedIds.size() - 1) std::cout << ", ";
        }
        std::cout << "]" << std::endl;
    } else if (paddedIds.size() > 8) {
        std::cout << "\n  âš ï¸  è¾“å…¥é•¿åº¦ > 8ï¼Œæˆªæ–­åˆ° 8 tokensï¼ˆLibTorch trace é™åˆ¶ï¼‰" << std::endl;
        paddedIds.resize(8);
    }
    
    // 3. åˆå§‹åŒ–æ¨ç†å¼•æ“
    std::cout << "\n[3] åˆå§‹åŒ–æ¨ç†å¼•æ“ï¼ˆLibTorch åç«¯ï¼‰..." << std::endl;
    ModelConfig config = loadQwen3Config();
    InferenceEngine engine(config, modelPath, true);  // useLibTorch = true
    
    if (!engine.initialize()) {
        std::cerr << "âŒ æ¨ç†å¼•æ“åˆå§‹åŒ–å¤±è´¥ï¼" << std::endl;
        return 1;
    }
    std::cout << "  âœ“ æ¨ç†å¼•æ“åˆå§‹åŒ–æˆåŠŸ" << std::endl;
    
    // 4. æ‰§è¡Œæ¨ç†
    std::cout << "\n[4] æ‰§è¡Œæ¨¡å‹æ¨ç†..." << std::endl;
    auto startInfer = std::chrono::high_resolution_clock::now();
    Tensor logits = engine.forward(paddedIds);
    auto endInfer = std::chrono::high_resolution_clock::now();
    auto inferMs = std::chrono::duration_cast<std::chrono::milliseconds>(endInfer - startInfer).count();
    
    std::cout << "  - è¾“å…¥å½¢çŠ¶: [" << paddedIds.size() << "]" << std::endl;
    std::cout << "  - è¾“å‡ºå½¢çŠ¶: [" << logits.shape()[0] << ", " << logits.shape()[1] << "]" << std::endl;
    std::cout << "  - æ¨ç†è€—æ—¶: " << inferMs << " ms" << std::endl;
    
    // 5. é‡‡æ ·ä¸‹ä¸€ä¸ª token
    std::cout << "\n[5] ä»æ¯ä¸ªä½ç½®é‡‡æ ·é¢„æµ‹ token..." << std::endl;
    
    // åˆ›å»ºé‡‡æ ·å™¨
    SamplerConfig samplerConfig;
    samplerConfig.setTemperature(0.8f);
    samplerConfig.setTopK(50);
    samplerConfig.setTopP(0.9f);
    Sampler sampler(samplerConfig);
    
    std::cout << "  é‡‡æ ·é…ç½®:" << std::endl;
    std::cout << "    - temperature: " << samplerConfig.getTemperature() << std::endl;
    std::cout << "    - top_k: " << samplerConfig.getTopK() << std::endl;
    std::cout << "    - top_p: " << samplerConfig.getTopP() << std::endl;
    
    // å¯¹åŸå§‹è¾“å…¥çš„æ¯ä¸ªä½ç½®è¿›è¡Œé‡‡æ ·
    std::cout << "\n  é¢„æµ‹ç»“æœï¼ˆåŸå§‹è¾“å…¥çš„æ¯ä¸ªä½ç½®ï¼‰:" << std::endl;
    std::vector<int> predictedIds;
    for (size_t pos = 0; pos < inputIds.size(); ++pos) {
        // è·å–è¯¥ä½ç½®çš„ logits
        FloatArray posLogits(config.vocabSize);
        for (size_t i = 0; i < config.vocabSize; ++i) {
            posLogits.data()[i] = logits.data()[pos * config.vocabSize + i];
        }
        
        // é‡‡æ ·
        int nextToken = sampler.sample(posLogits, samplerConfig.getTemperature(), 
                                      samplerConfig.getTopK(), samplerConfig.getTopP());
        predictedIds.push_back(nextToken);
        
        // è§£ç 
        std::string predicted = tokenizer.decode({nextToken});
        std::string current = tokenizer.decode({inputIds[pos]});
        
        std::cout << "    ä½ç½® " << pos << ": \"" << current << "\" (token_" << inputIds[pos] 
                  << ") -> é¢„æµ‹ä¸‹ä¸€ä¸ª: \"" << predicted << "\" (token_" << nextToken << ")" << std::endl;
        
        // æ˜¾ç¤º top-5 å€™é€‰
        std::vector<std::pair<float, int>> logit_pairs;
        for (size_t i = 0; i < config.vocabSize; ++i) {
            logit_pairs.push_back({posLogits.data()[i], static_cast<int>(i)});
        }
        std::partial_sort(logit_pairs.begin(), logit_pairs.begin() + 5, logit_pairs.end(),
                          [](const auto& a, const auto& b) { return a.first > b.first; });
        
        std::cout << "      Top-5: ";
        for (int i = 0; i < 5; ++i) {
            std::string candidateText = tokenizer.decode({logit_pairs[i].second});
            std::cout << "\"" << candidateText << "\"(" << logit_pairs[i].first << ") ";
        }
        std::cout << std::endl;
    }
    
    // 6. è‡ªå›å½’ç”Ÿæˆï¼ˆç®€å•ç‰ˆæœ¬ï¼Œåªç”Ÿæˆå‡ ä¸ª tokenï¼‰
    std::cout << "\n[6] è‡ªå›å½’ç”Ÿæˆï¼ˆç»­å†™ 5 ä¸ª tokenï¼‰..." << std::endl;
    std::vector<int> generatedIds = inputIds;  // ä»åŸå§‹è¾“å…¥å¼€å§‹
    const int maxNewTokens = 5;
    
    std::cout << "  åˆå§‹åºåˆ—: \"" << inputText << "\"" << std::endl;
    
    for (int step = 0; step < maxNewTokens; ++step) {
        // å‡†å¤‡è¾“å…¥ï¼ˆä¿æŒ 8 tokensï¼‰
        std::vector<int> currentInput = generatedIds;
        if (currentInput.size() > 8) {
            // å–æœ€å 8 ä¸ª tokens
            currentInput = std::vector<int>(generatedIds.end() - 8, generatedIds.end());
        } else if (currentInput.size() < 8) {
            // å¡«å……åˆ° 8
            while (currentInput.size() < 8) {
                currentInput.insert(currentInput.begin(), 151643);  // å‰é¢å¡«å……
            }
        }
        
        // æ¨ç†
        Tensor stepLogits = engine.forward(currentInput);
        
        // è·å–æœ€åä¸€ä¸ªä½ç½®çš„ logits
        size_t lastPos = currentInput.size() - 1;
        FloatArray lastLogits(config.vocabSize);
        for (size_t i = 0; i < config.vocabSize; ++i) {
            lastLogits.data()[i] = stepLogits.data()[lastPos * config.vocabSize + i];
        }
        
        // é‡‡æ ·
        int nextToken = sampler.sample(lastLogits, samplerConfig.getTemperature(),
                                      samplerConfig.getTopK(), samplerConfig.getTopP());
        generatedIds.push_back(nextToken);
        
        // è§£ç å¹¶æ˜¾ç¤º
        std::string nextText = tokenizer.decode({nextToken});
        std::cout << "  Step " << (step + 1) << ": ç”Ÿæˆ token_" << nextToken 
                  << " -> \"" << nextText << "\"" << std::endl;
    }
    
    // 7. è§£ç å®Œæ•´è¾“å‡º
    std::cout << "\n[7] è§£ç å®Œæ•´ç”Ÿæˆç»“æœ..." << std::endl;
    std::string generatedText = tokenizer.decode(generatedIds);
    std::cout << "  - ç”Ÿæˆçš„ token æ•°: " << generatedIds.size() << std::endl;
    std::cout << "  - å®Œæ•´è¾“å‡º: \"" << generatedText << "\"" << std::endl;
    
    // 8. æ€§èƒ½ç»Ÿè®¡
    std::cout << "\n[8] æ€§èƒ½ç»Ÿè®¡..." << std::endl;
    std::cout << "  - é¦–æ¬¡æ¨ç†: " << inferMs << " ms" << std::endl;
    std::cout << "  - ç”Ÿæˆ token æ•°: " << maxNewTokens << std::endl;
    std::cout << "  - å¹³å‡æ¯ token: ~" << (inferMs / 8) << " ms" << std::endl;
    
    std::cout << "\n=== æµ‹è¯•å®Œæˆ ===" << std::endl;
    
    return 0;
}
